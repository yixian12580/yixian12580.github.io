<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[k8s部署Metallb]]></title>
    <url>%2F2025%2F124e38afa1.html</url>
    <content type="text"><![CDATA[MetalLB 是一个 Kubernetes 的 LoadBalancer 解决方案，专门用于裸机（Bare Metal）集群。由于 Kubernetes 本身在裸机环境中没有内置的负载均衡（不像云环境有 ELB、ALB），MetalLB 通过 BGP 或 ARP 提供 LoadBalancer 类型的服务支持，使得服务可以获得外部可访问的 IP 地址。MetalLB 模式对比模式说明适用场景L2 模式（ARP/NDP 广播）通过 ARP 或 NDP 让局域网设备知道 LoadBalancer IP适用于 简单网络，推荐BGP 模式（边界网关协议）让 Kubernetes 节点与物理路由器对等连接，实现外网负载均衡适用于 复杂网络（如 ISP、数据中心）MetalLB 安装部署 MetalLBmetallb-native.yaml：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690691692693694695696697698699700701702703704705706707708709710711712713714715716717718719720721722723724725726727728729730731732733734735736737738739740741742743744745746747748749750751752753754755756757758759760761762763764765766767768769770771772773774775776777778779780781782783784785786787788789790791792793794795796797798799800801802803804805806807808809810811812813814815816817818819820821822823824825826827828829830831832833834835836837838839840841842843844845846847848849850851852853854855856857858859860861862863864865866867868869870871872873874875876877878879880881882883884885886887888889890891892893894895896897898899900901902903904905906907908909910911912913914915916917918919920921922923924925926927928929930931932933934935936937938939940941942943944945946947948949950951952953954955956957958959960961962963964965966967968969970971972973974975976977978979980981982983984985986987988989990991992993994995996997998999100010011002100310041005100610071008100910101011101210131014101510161017101810191020102110221023102410251026102710281029103010311032103310341035103610371038103910401041104210431044104510461047104810491050105110521053105410551056105710581059106010611062106310641065106610671068106910701071107210731074107510761077107810791080108110821083108410851086108710881089109010911092109310941095109610971098109911001101110211031104110511061107110811091110111111121113111411151116111711181119112011211122112311241125112611271128112911301131113211331134113511361137113811391140114111421143114411451146114711481149115011511152115311541155115611571158115911601161116211631164116511661167116811691170117111721173117411751176117711781179118011811182118311841185118611871188118911901191119211931194119511961197119811991200120112021203120412051206120712081209121012111212121312141215121612171218121912201221122212231224122512261227122812291230123112321233123412351236123712381239124012411242124312441245124612471248124912501251125212531254125512561257125812591260126112621263126412651266126712681269127012711272127312741275127612771278127912801281128212831284128512861287128812891290129112921293129412951296129712981299130013011302130313041305130613071308130913101311131213131314131513161317131813191320132113221323132413251326132713281329133013311332133313341335133613371338133913401341134213431344134513461347134813491350135113521353135413551356135713581359136013611362136313641365136613671368136913701371137213731374137513761377137813791380138113821383138413851386138713881389139013911392139313941395139613971398139914001401140214031404140514061407140814091410141114121413141414151416141714181419142014211422142314241425142614271428142914301431143214331434143514361437143814391440144114421443144414451446144714481449145014511452145314541455145614571458145914601461146214631464146514661467146814691470147114721473147414751476147714781479148014811482148314841485148614871488148914901491149214931494149514961497149814991500150115021503150415051506150715081509151015111512151315141515151615171518151915201521152215231524152515261527152815291530153115321533153415351536153715381539154015411542154315441545154615471548154915501551155215531554155515561557155815591560156115621563156415651566156715681569157015711572157315741575157615771578157915801581158215831584158515861587158815891590159115921593159415951596159715981599160016011602160316041605160616071608160916101611161216131614161516161617161816191620162116221623162416251626162716281629163016311632163316341635163616371638163916401641164216431644164516461647164816491650165116521653165416551656165716581659166016611662166316641665166616671668166916701671167216731674167516761677167816791680168116821683168416851686168716881689169016911692169316941695169616971698169917001701170217031704170517061707170817091710171117121713171417151716171717181719172017211722172317241725172617271728172917301731173217331734173517361737173817391740174117421743174417451746174717481749175017511752175317541755175617571758175917601761176217631764176517661767176817691770177117721773177417751776177717781779178017811782178317841785178617871788178917901791179217931794179517961797179817991800180118021803180418051806180718081809181018111812181318141815181618171818181918201821182218231824182518261827182818291830183118321833183418351836183718381839184018411842184318441845184618471848184918501851185218531854185518561857185818591860186118621863186418651866186718681869187018711872187318741875187618771878187918801881apiVersion: v1kind: Namespacemetadata: labels: pod-security.kubernetes.io/audit: privileged pod-security.kubernetes.io/enforce: privileged pod-security.kubernetes.io/warn: privileged name: metallb-system---apiVersion: apiextensions.k8s.io/v1kind: CustomResourceDefinitionmetadata: annotations: controller-gen.kubebuilder.io/version: v0.7.0 name: addresspools.metallb.iospec: conversion: strategy: Webhook webhook: clientConfig: caBundle: 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 service: name: webhook-service namespace: metallb-system path: /convert conversionReviewVersions: - v1alpha1 - v1beta1 group: metallb.io names: kind: AddressPool listKind: AddressPoolList plural: addresspools singular: addresspool scope: Namespaced versions: - deprecated: true deprecationWarning: metallb.io v1alpha1 AddressPool is deprecated name: v1alpha1 schema: openAPIV3Schema: description: AddressPool is the Schema for the addresspools API. properties: apiVersion: description: 'APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources' type: string kind: description: 'Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds' type: string metadata: type: object spec: description: AddressPoolSpec defines the desired state of AddressPool. properties: addresses: description: A list of IP address ranges over which MetalLB has authority. You can list multiple ranges in a single pool, they will all share the same settings. Each range can be either a CIDR prefix, or an explicit start-end range of IPs. items: type: string type: array autoAssign: default: true description: AutoAssign flag used to prevent MetallB from automatic allocation for a pool. type: boolean bgpAdvertisements: description: When an IP is allocated from this pool, how should it be translated into BGP announcements? items: properties: aggregationLength: default: 32 description: The aggregation-length advertisement option lets you “roll up” the /32s into a larger prefix. format: int32 minimum: 1 type: integer aggregationLengthV6: default: 128 description: Optional, defaults to 128 (i.e. no aggregation) if not specified. format: int32 type: integer communities: description: BGP communities items: type: string type: array localPref: description: BGP LOCAL_PREF attribute which is used by BGP best path algorithm, Path with higher localpref is preferred over one with lower localpref. format: int32 type: integer type: object type: array protocol: description: Protocol can be used to select how the announcement is done. enum: - layer2 - bgp type: string required: - addresses - protocol type: object status: description: AddressPoolStatus defines the observed state of AddressPool. type: object required: - spec type: object served: true storage: false subresources: status: &#123;&#125; - deprecated: true deprecationWarning: metallb.io v1beta1 AddressPool is deprecated, consider using IPAddressPool name: v1beta1 schema: openAPIV3Schema: description: AddressPool represents a pool of IP addresses that can be allocated to LoadBalancer services. AddressPool is deprecated and being replaced by IPAddressPool. properties: apiVersion: description: 'APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources' type: string kind: description: 'Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds' type: string metadata: type: object spec: description: AddressPoolSpec defines the desired state of AddressPool. properties: addresses: description: A list of IP address ranges over which MetalLB has authority. You can list multiple ranges in a single pool, they will all share the same settings. Each range can be either a CIDR prefix, or an explicit start-end range of IPs. items: type: string type: array autoAssign: default: true description: AutoAssign flag used to prevent MetallB from automatic allocation for a pool. type: boolean bgpAdvertisements: description: Drives how an IP allocated from this pool should translated into BGP announcements. items: properties: aggregationLength: default: 32 description: The aggregation-length advertisement option lets you “roll up” the /32s into a larger prefix. format: int32 minimum: 1 type: integer aggregationLengthV6: default: 128 description: Optional, defaults to 128 (i.e. no aggregation) if not specified. format: int32 type: integer communities: description: BGP communities to be associated with the given advertisement. items: type: string type: array localPref: description: BGP LOCAL_PREF attribute which is used by BGP best path algorithm, Path with higher localpref is preferred over one with lower localpref. format: int32 type: integer type: object type: array protocol: description: Protocol can be used to select how the announcement is done. enum: - layer2 - bgp type: string required: - addresses - protocol type: object status: description: AddressPoolStatus defines the observed state of AddressPool. type: object required: - spec type: object served: true storage: true subresources: status: &#123;&#125;status: acceptedNames: kind: "" plural: "" conditions: [] storedVersions: []---apiVersion: apiextensions.k8s.io/v1kind: CustomResourceDefinitionmetadata: annotations: controller-gen.kubebuilder.io/version: v0.7.0 creationTimestamp: null name: bfdprofiles.metallb.iospec: group: metallb.io names: kind: BFDProfile listKind: BFDProfileList plural: bfdprofiles singular: bfdprofile scope: Namespaced versions: - additionalPrinterColumns: - jsonPath: .spec.passiveMode name: Passive Mode type: boolean - jsonPath: .spec.transmitInterval name: Transmit Interval type: integer - jsonPath: .spec.receiveInterval name: Receive Interval type: integer - jsonPath: .spec.detectMultiplier name: Multiplier type: integer name: v1beta1 schema: openAPIV3Schema: description: BFDProfile represents the settings of the bfd session that can be optionally associated with a BGP session. properties: apiVersion: description: 'APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources' type: string kind: description: 'Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds' type: string metadata: type: object spec: description: BFDProfileSpec defines the desired state of BFDProfile. properties: detectMultiplier: description: Configures the detection multiplier to determine packet loss. The remote transmission interval will be multiplied by this value to determine the connection loss detection timer. format: int32 maximum: 255 minimum: 2 type: integer echoInterval: description: Configures the minimal echo receive transmission interval that this system is capable of handling in milliseconds. Defaults to 50ms format: int32 maximum: 60000 minimum: 10 type: integer echoMode: description: Enables or disables the echo transmission mode. This mode is disabled by default, and not supported on multi hops setups. type: boolean minimumTtl: description: 'For multi hop sessions only: configure the minimum expected TTL for an incoming BFD control packet.' format: int32 maximum: 254 minimum: 1 type: integer passiveMode: description: 'Mark session as passive: a passive session will not attempt to start the connection and will wait for control packets from peer before it begins replying.' type: boolean receiveInterval: description: The minimum interval that this system is capable of receiving control packets in milliseconds. Defaults to 300ms. format: int32 maximum: 60000 minimum: 10 type: integer transmitInterval: description: The minimum transmission interval (less jitter) that this system wants to use to send BFD control packets in milliseconds. Defaults to 300ms format: int32 maximum: 60000 minimum: 10 type: integer type: object status: description: BFDProfileStatus defines the observed state of BFDProfile. type: object type: object served: true storage: true subresources: status: &#123;&#125;status: acceptedNames: kind: "" plural: "" conditions: [] storedVersions: []---apiVersion: apiextensions.k8s.io/v1kind: CustomResourceDefinitionmetadata: annotations: controller-gen.kubebuilder.io/version: v0.7.0 creationTimestamp: null name: bgpadvertisements.metallb.iospec: group: metallb.io names: kind: BGPAdvertisement listKind: BGPAdvertisementList plural: bgpadvertisements singular: bgpadvertisement scope: Namespaced versions: - additionalPrinterColumns: - jsonPath: .spec.ipAddressPools name: IPAddressPools type: string - jsonPath: .spec.ipAddressPoolSelectors name: IPAddressPool Selectors type: string - jsonPath: .spec.peers name: Peers type: string - jsonPath: .spec.nodeSelectors name: Node Selectors priority: 10 type: string name: v1beta1 schema: openAPIV3Schema: description: BGPAdvertisement allows to advertise the IPs coming from the selected IPAddressPools via BGP, setting the parameters of the BGP Advertisement. properties: apiVersion: description: 'APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources' type: string kind: description: 'Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds' type: string metadata: type: object spec: description: BGPAdvertisementSpec defines the desired state of BGPAdvertisement. properties: aggregationLength: default: 32 description: The aggregation-length advertisement option lets you “roll up” the /32s into a larger prefix. Defaults to 32. Works for IPv4 addresses. format: int32 minimum: 1 type: integer aggregationLengthV6: default: 128 description: The aggregation-length advertisement option lets you “roll up” the /128s into a larger prefix. Defaults to 128. Works for IPv6 addresses. format: int32 type: integer communities: description: The BGP communities to be associated with the announcement. Each item can be a community of the form 1234:1234 or the name of an alias defined in the Community CRD. items: type: string type: array ipAddressPoolSelectors: description: A selector for the IPAddressPools which would get advertised via this advertisement. If no IPAddressPool is selected by this or by the list, the advertisement is applied to all the IPAddressPools. items: description: A label selector is a label query over a set of resources. The result of matchLabels and matchExpressions are ANDed. An empty label selector matches all objects. A null label selector matches no objects. properties: matchExpressions: description: matchExpressions is a list of label selector requirements. The requirements are ANDed. items: description: A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values. properties: key: description: key is the label key that the selector applies to. type: string operator: description: operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist. type: string values: description: values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch. items: type: string type: array required: - key - operator type: object type: array matchLabels: additionalProperties: type: string description: matchLabels is a map of &#123;key,value&#125; pairs. A single &#123;key,value&#125; in the matchLabels map is equivalent to an element of matchExpressions, whose key field is "key", the operator is "In", and the values array contains only "value". The requirements are ANDed. type: object type: object type: array ipAddressPools: description: The list of IPAddressPools to advertise via this advertisement, selected by name. items: type: string type: array localPref: description: The BGP LOCAL_PREF attribute which is used by BGP best path algorithm, Path with higher localpref is preferred over one with lower localpref. format: int32 type: integer nodeSelectors: description: NodeSelectors allows to limit the nodes to announce as next hops for the LoadBalancer IP. When empty, all the nodes having are announced as next hops. items: description: A label selector is a label query over a set of resources. The result of matchLabels and matchExpressions are ANDed. An empty label selector matches all objects. A null label selector matches no objects. properties: matchExpressions: description: matchExpressions is a list of label selector requirements. The requirements are ANDed. items: description: A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values. properties: key: description: key is the label key that the selector applies to. type: string operator: description: operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist. type: string values: description: values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch. items: type: string type: array required: - key - operator type: object type: array matchLabels: additionalProperties: type: string description: matchLabels is a map of &#123;key,value&#125; pairs. A single &#123;key,value&#125; in the matchLabels map is equivalent to an element of matchExpressions, whose key field is "key", the operator is "In", and the values array contains only "value". The requirements are ANDed. type: object type: object type: array peers: description: Peers limits the bgppeer to advertise the ips of the selected pools to. When empty, the loadbalancer IP is announced to all the BGPPeers configured. items: type: string type: array type: object status: description: BGPAdvertisementStatus defines the observed state of BGPAdvertisement. type: object type: object served: true storage: true subresources: status: &#123;&#125;status: acceptedNames: kind: "" plural: "" conditions: [] storedVersions: []---apiVersion: apiextensions.k8s.io/v1kind: CustomResourceDefinitionmetadata: annotations: controller-gen.kubebuilder.io/version: v0.7.0 name: bgppeers.metallb.iospec: conversion: strategy: Webhook webhook: clientConfig: caBundle: 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 service: name: webhook-service namespace: metallb-system path: /convert conversionReviewVersions: - v1beta1 - v1beta2 group: metallb.io names: kind: BGPPeer listKind: BGPPeerList plural: bgppeers singular: bgppeer scope: Namespaced versions: - additionalPrinterColumns: - jsonPath: .spec.peerAddress name: Address type: string - jsonPath: .spec.peerASN name: ASN type: string - jsonPath: .spec.bfdProfile name: BFD Profile type: string - jsonPath: .spec.ebgpMultiHop name: Multi Hops type: string name: v1beta1 schema: openAPIV3Schema: description: BGPPeer is the Schema for the peers API. properties: apiVersion: description: 'APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources' type: string kind: description: 'Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds' type: string metadata: type: object spec: description: BGPPeerSpec defines the desired state of Peer. properties: bfdProfile: type: string ebgpMultiHop: description: EBGP peer is multi-hops away type: boolean holdTime: description: Requested BGP hold time, per RFC4271. type: string keepaliveTime: description: Requested BGP keepalive time, per RFC4271. type: string myASN: description: AS number to use for the local end of the session. format: int32 maximum: 4294967295 minimum: 0 type: integer nodeSelectors: description: Only connect to this peer on nodes that match one of these selectors. items: properties: matchExpressions: items: properties: key: type: string operator: type: string values: items: type: string minItems: 1 type: array required: - key - operator - values type: object type: array matchLabels: additionalProperties: type: string type: object type: object type: array password: description: Authentication password for routers enforcing TCP MD5 authenticated sessions type: string peerASN: description: AS number to expect from the remote end of the session. format: int32 maximum: 4294967295 minimum: 0 type: integer peerAddress: description: Address to dial when establishing the session. type: string peerPort: description: Port to dial when establishing the session. maximum: 16384 minimum: 0 type: integer routerID: description: BGP router ID to advertise to the peer type: string sourceAddress: description: Source address to use when establishing the session. type: string required: - myASN - peerASN - peerAddress type: object status: description: BGPPeerStatus defines the observed state of Peer. type: object type: object served: true storage: false subresources: status: &#123;&#125; - additionalPrinterColumns: - jsonPath: .spec.peerAddress name: Address type: string - jsonPath: .spec.peerASN name: ASN type: string - jsonPath: .spec.bfdProfile name: BFD Profile type: string - jsonPath: .spec.ebgpMultiHop name: Multi Hops type: string name: v1beta2 schema: openAPIV3Schema: description: BGPPeer is the Schema for the peers API. properties: apiVersion: description: 'APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources' type: string kind: description: 'Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds' type: string metadata: type: object spec: description: BGPPeerSpec defines the desired state of Peer. properties: bfdProfile: description: The name of the BFD Profile to be used for the BFD session associated to the BGP session. If not set, the BFD session won't be set up. type: string ebgpMultiHop: description: To set if the BGPPeer is multi-hops away. Needed for FRR mode only. type: boolean holdTime: description: Requested BGP hold time, per RFC4271. type: string keepaliveTime: description: Requested BGP keepalive time, per RFC4271. type: string myASN: description: AS number to use for the local end of the session. format: int32 maximum: 4294967295 minimum: 0 type: integer nodeSelectors: description: Only connect to this peer on nodes that match one of these selectors. items: description: A label selector is a label query over a set of resources. The result of matchLabels and matchExpressions are ANDed. An empty label selector matches all objects. A null label selector matches no objects. properties: matchExpressions: description: matchExpressions is a list of label selector requirements. The requirements are ANDed. items: description: A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values. properties: key: description: key is the label key that the selector applies to. type: string operator: description: operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist. type: string values: description: values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch. items: type: string type: array required: - key - operator type: object type: array matchLabels: additionalProperties: type: string description: matchLabels is a map of &#123;key,value&#125; pairs. A single &#123;key,value&#125; in the matchLabels map is equivalent to an element of matchExpressions, whose key field is "key", the operator is "In", and the values array contains only "value". The requirements are ANDed. type: object type: object type: array password: description: Authentication password for routers enforcing TCP MD5 authenticated sessions type: string passwordSecret: description: passwordSecret is name of the authentication secret for BGP Peer. the secret must be of type "kubernetes.io/basic-auth", and created in the same namespace as the MetalLB deployment. The password is stored in the secret as the key "password". properties: name: description: name is unique within a namespace to reference a secret resource. type: string namespace: description: namespace defines the space within which the secret name must be unique. type: string type: object peerASN: description: AS number to expect from the remote end of the session. format: int32 maximum: 4294967295 minimum: 0 type: integer peerAddress: description: Address to dial when establishing the session. type: string peerPort: default: 179 description: Port to dial when establishing the session. maximum: 16384 minimum: 0 type: integer routerID: description: BGP router ID to advertise to the peer type: string sourceAddress: description: Source address to use when establishing the session. type: string required: - myASN - peerASN - peerAddress type: object status: description: BGPPeerStatus defines the observed state of Peer. type: object type: object served: true storage: true subresources: status: &#123;&#125;status: acceptedNames: kind: "" plural: "" conditions: [] storedVersions: []---apiVersion: apiextensions.k8s.io/v1kind: CustomResourceDefinitionmetadata: annotations: controller-gen.kubebuilder.io/version: v0.7.0 creationTimestamp: null name: communities.metallb.iospec: group: metallb.io names: kind: Community listKind: CommunityList plural: communities singular: community scope: Namespaced versions: - name: v1beta1 schema: openAPIV3Schema: description: Community is a collection of aliases for communities. Users can define named aliases to be used in the BGPPeer CRD. properties: apiVersion: description: 'APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources' type: string kind: description: 'Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds' type: string metadata: type: object spec: description: CommunitySpec defines the desired state of Community. properties: communities: items: properties: name: description: The name of the alias for the community. type: string value: description: The BGP community value corresponding to the given name. type: string type: object type: array type: object status: description: CommunityStatus defines the observed state of Community. type: object type: object served: true storage: true subresources: status: &#123;&#125;status: acceptedNames: kind: "" plural: "" conditions: [] storedVersions: []---apiVersion: apiextensions.k8s.io/v1kind: CustomResourceDefinitionmetadata: annotations: controller-gen.kubebuilder.io/version: v0.7.0 creationTimestamp: null name: ipaddresspools.metallb.iospec: group: metallb.io names: kind: IPAddressPool listKind: IPAddressPoolList plural: ipaddresspools singular: ipaddresspool scope: Namespaced versions: - additionalPrinterColumns: - jsonPath: .spec.autoAssign name: Auto Assign type: boolean - jsonPath: .spec.avoidBuggyIPs name: Avoid Buggy IPs type: boolean - jsonPath: .spec.addresses name: Addresses type: string name: v1beta1 schema: openAPIV3Schema: description: IPAddressPool represents a pool of IP addresses that can be allocated to LoadBalancer services. properties: apiVersion: description: 'APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources' type: string kind: description: 'Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds' type: string metadata: type: object spec: description: IPAddressPoolSpec defines the desired state of IPAddressPool. properties: addresses: description: A list of IP address ranges over which MetalLB has authority. You can list multiple ranges in a single pool, they will all share the same settings. Each range can be either a CIDR prefix, or an explicit start-end range of IPs. items: type: string type: array autoAssign: default: true description: AutoAssign flag used to prevent MetallB from automatic allocation for a pool. type: boolean avoidBuggyIPs: default: false description: AvoidBuggyIPs prevents addresses ending with .0 and .255 to be used by a pool. type: boolean required: - addresses type: object status: description: IPAddressPoolStatus defines the observed state of IPAddressPool. type: object required: - spec type: object served: true storage: true subresources: status: &#123;&#125;status: acceptedNames: kind: "" plural: "" conditions: [] storedVersions: []---apiVersion: apiextensions.k8s.io/v1kind: CustomResourceDefinitionmetadata: annotations: controller-gen.kubebuilder.io/version: v0.7.0 creationTimestamp: null name: l2advertisements.metallb.iospec: group: metallb.io names: kind: L2Advertisement listKind: L2AdvertisementList plural: l2advertisements singular: l2advertisement scope: Namespaced versions: - additionalPrinterColumns: - jsonPath: .spec.ipAddressPools name: IPAddressPools type: string - jsonPath: .spec.ipAddressPoolSelectors name: IPAddressPool Selectors type: string - jsonPath: .spec.interfaces name: Interfaces type: string - jsonPath: .spec.nodeSelectors name: Node Selectors priority: 10 type: string name: v1beta1 schema: openAPIV3Schema: description: L2Advertisement allows to advertise the LoadBalancer IPs provided by the selected pools via L2. properties: apiVersion: description: 'APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources' type: string kind: description: 'Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds' type: string metadata: type: object spec: description: L2AdvertisementSpec defines the desired state of L2Advertisement. properties: interfaces: description: A list of interfaces to announce from. The LB IP will be announced only from these interfaces. If the field is not set, we advertise from all the interfaces on the host. items: type: string type: array ipAddressPoolSelectors: description: A selector for the IPAddressPools which would get advertised via this advertisement. If no IPAddressPool is selected by this or by the list, the advertisement is applied to all the IPAddressPools. items: description: A label selector is a label query over a set of resources. The result of matchLabels and matchExpressions are ANDed. An empty label selector matches all objects. A null label selector matches no objects. properties: matchExpressions: description: matchExpressions is a list of label selector requirements. The requirements are ANDed. items: description: A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values. properties: key: description: key is the label key that the selector applies to. type: string operator: description: operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist. type: string values: description: values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch. items: type: string type: array required: - key - operator type: object type: array matchLabels: additionalProperties: type: string description: matchLabels is a map of &#123;key,value&#125; pairs. A single &#123;key,value&#125; in the matchLabels map is equivalent to an element of matchExpressions, whose key field is "key", the operator is "In", and the values array contains only "value". The requirements are ANDed. type: object type: object type: array ipAddressPools: description: The list of IPAddressPools to advertise via this advertisement, selected by name. items: type: string type: array nodeSelectors: description: NodeSelectors allows to limit the nodes to announce as next hops for the LoadBalancer IP. When empty, all the nodes having are announced as next hops. items: description: A label selector is a label query over a set of resources. The result of matchLabels and matchExpressions are ANDed. An empty label selector matches all objects. A null label selector matches no objects. properties: matchExpressions: description: matchExpressions is a list of label selector requirements. The requirements are ANDed. items: description: A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values. properties: key: description: key is the label key that the selector applies to. type: string operator: description: operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist. type: string values: description: values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch. items: type: string type: array required: - key - operator type: object type: array matchLabels: additionalProperties: type: string description: matchLabels is a map of &#123;key,value&#125; pairs. A single &#123;key,value&#125; in the matchLabels map is equivalent to an element of matchExpressions, whose key field is "key", the operator is "In", and the values array contains only "value". The requirements are ANDed. type: object type: object type: array type: object status: description: L2AdvertisementStatus defines the observed state of L2Advertisement. type: object type: object served: true storage: true subresources: status: &#123;&#125;status: acceptedNames: kind: "" plural: "" conditions: [] storedVersions: []---apiVersion: v1kind: ServiceAccountmetadata: labels: app: metallb name: controller namespace: metallb-system---apiVersion: v1kind: ServiceAccountmetadata: labels: app: metallb name: speaker namespace: metallb-system---apiVersion: rbac.authorization.k8s.io/v1kind: Rolemetadata: labels: app: metallb name: controller namespace: metallb-systemrules:- apiGroups: - "" resources: - secrets verbs: - create - delete - get - list - patch - update - watch- apiGroups: - "" resourceNames: - memberlist resources: - secrets verbs: - list- apiGroups: - apps resourceNames: - controller resources: - deployments verbs: - get- apiGroups: - metallb.io resources: - bgppeers verbs: - get - list- apiGroups: - metallb.io resources: - addresspools verbs: - get - list - watch- apiGroups: - metallb.io resources: - bfdprofiles verbs: - get - list - watch- apiGroups: - metallb.io resources: - ipaddresspools verbs: - get - list - watch- apiGroups: - metallb.io resources: - bgpadvertisements verbs: - get - list - watch- apiGroups: - metallb.io resources: - l2advertisements verbs: - get - list - watch- apiGroups: - metallb.io resources: - communities verbs: - get - list - watch---apiVersion: rbac.authorization.k8s.io/v1kind: Rolemetadata: labels: app: metallb name: pod-lister namespace: metallb-systemrules:- apiGroups: - "" resources: - pods verbs: - list- apiGroups: - "" resources: - secrets verbs: - get - list - watch- apiGroups: - metallb.io resources: - addresspools verbs: - get - list - watch- apiGroups: - metallb.io resources: - bfdprofiles verbs: - get - list - watch- apiGroups: - metallb.io resources: - bgppeers verbs: - get - list - watch- apiGroups: - metallb.io resources: - l2advertisements verbs: - get - list - watch- apiGroups: - metallb.io resources: - bgpadvertisements verbs: - get - list - watch- apiGroups: - metallb.io resources: - ipaddresspools verbs: - get - list - watch- apiGroups: - metallb.io resources: - communities verbs: - get - list - watch---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: labels: app: metallb name: metallb-system:controllerrules:- apiGroups: - "" resources: - services verbs: - get - list - watch- apiGroups: - "" resources: - services/status verbs: - update- apiGroups: - "" resources: - events verbs: - create - patch- apiGroups: - policy resourceNames: - controller resources: - podsecuritypolicies verbs: - use- apiGroups: - admissionregistration.k8s.io resources: - validatingwebhookconfigurations - mutatingwebhookconfigurations verbs: - create - delete - get - list - patch - update - watch- apiGroups: - apiextensions.k8s.io resources: - customresourcedefinitions verbs: - create - delete - get - list - patch - update - watch---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: labels: app: metallb name: metallb-system:speakerrules:- apiGroups: - "" resources: - services - endpoints - nodes verbs: - get - list - watch- apiGroups: - discovery.k8s.io resources: - endpointslices verbs: - get - list - watch- apiGroups: - "" resources: - events verbs: - create - patch- apiGroups: - policy resourceNames: - speaker resources: - podsecuritypolicies verbs: - use---apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata: labels: app: metallb name: controller namespace: metallb-systemroleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: controllersubjects:- kind: ServiceAccount name: controller namespace: metallb-system---apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata: labels: app: metallb name: pod-lister namespace: metallb-systemroleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: pod-listersubjects:- kind: ServiceAccount name: speaker namespace: metallb-system---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: labels: app: metallb name: metallb-system:controllerroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: metallb-system:controllersubjects:- kind: ServiceAccount name: controller namespace: metallb-system---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: labels: app: metallb name: metallb-system:speakerroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: metallb-system:speakersubjects:- kind: ServiceAccount name: speaker namespace: metallb-system---apiVersion: v1kind: Secretmetadata: name: webhook-server-cert namespace: metallb-system---apiVersion: v1kind: Servicemetadata: name: webhook-service namespace: metallb-systemspec: ports: - port: 443 targetPort: 9443 selector: component: controller---apiVersion: apps/v1kind: Deploymentmetadata: labels: app: metallb component: controller name: controller namespace: metallb-systemspec: revisionHistoryLimit: 3 selector: matchLabels: app: metallb component: controller template: metadata: annotations: prometheus.io/port: "7472" prometheus.io/scrape: "true" labels: app: metallb component: controller spec: containers: - args: - --port=7472 - --log-level=info env: - name: METALLB_ML_SECRET_NAME value: memberlist - name: METALLB_DEPLOYMENT value: controller image: quay.io/metallb/controller:v0.13.7 livenessProbe: failureThreshold: 3 httpGet: path: /metrics port: monitoring initialDelaySeconds: 10 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 name: controller ports: - containerPort: 7472 name: monitoring - containerPort: 9443 name: webhook-server protocol: TCP readinessProbe: failureThreshold: 3 httpGet: path: /metrics port: monitoring initialDelaySeconds: 10 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 securityContext: allowPrivilegeEscalation: false capabilities: drop: - all readOnlyRootFilesystem: true volumeMounts: - mountPath: /tmp/k8s-webhook-server/serving-certs name: cert readOnly: true nodeSelector: kubernetes.io/os: linux securityContext: fsGroup: 65534 runAsNonRoot: true runAsUser: 65534 serviceAccountName: controller terminationGracePeriodSeconds: 0 volumes: - name: cert secret: defaultMode: 420 secretName: webhook-server-cert---apiVersion: apps/v1kind: DaemonSetmetadata: labels: app: metallb component: speaker name: speaker namespace: metallb-systemspec: selector: matchLabels: app: metallb component: speaker template: metadata: annotations: prometheus.io/port: "7472" prometheus.io/scrape: "true" labels: app: metallb component: speaker spec: containers: - args: - --port=7472 - --log-level=info env: - name: METALLB_NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName - name: METALLB_HOST valueFrom: fieldRef: fieldPath: status.hostIP - name: METALLB_ML_BIND_ADDR valueFrom: fieldRef: fieldPath: status.podIP - name: METALLB_ML_LABELS value: app=metallb,component=speaker - name: METALLB_ML_SECRET_KEY valueFrom: secretKeyRef: key: secretkey name: memberlist image: quay.io/metallb/speaker:v0.13.7 livenessProbe: failureThreshold: 3 httpGet: path: /metrics port: monitoring initialDelaySeconds: 10 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 name: speaker ports: - containerPort: 7472 name: monitoring - containerPort: 7946 name: memberlist-tcp - containerPort: 7946 name: memberlist-udp protocol: UDP readinessProbe: failureThreshold: 3 httpGet: path: /metrics port: monitoring initialDelaySeconds: 10 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 securityContext: allowPrivilegeEscalation: false capabilities: add: - NET_RAW drop: - ALL readOnlyRootFilesystem: true hostNetwork: true nodeSelector: kubernetes.io/os: linux serviceAccountName: speaker terminationGracePeriodSeconds: 2 tolerations: - effect: NoSchedule key: node-role.kubernetes.io/master operator: Exists - effect: NoSchedule key: node-role.kubernetes.io/control-plane operator: Exists---apiVersion: admissionregistration.k8s.io/v1kind: ValidatingWebhookConfigurationmetadata: creationTimestamp: null name: metallb-webhook-configurationwebhooks:- admissionReviewVersions: - v1 clientConfig: service: name: webhook-service namespace: metallb-system path: /validate-metallb-io-v1beta2-bgppeer failurePolicy: Fail name: bgppeersvalidationwebhook.metallb.io rules: - apiGroups: - metallb.io apiVersions: - v1beta2 operations: - CREATE - UPDATE resources: - bgppeers sideEffects: None- admissionReviewVersions: - v1 clientConfig: service: name: webhook-service namespace: metallb-system path: /validate-metallb-io-v1beta1-addresspool failurePolicy: Fail name: addresspoolvalidationwebhook.metallb.io rules: - apiGroups: - metallb.io apiVersions: - v1beta1 operations: - CREATE - UPDATE resources: - addresspools sideEffects: None- admissionReviewVersions: - v1 clientConfig: service: name: webhook-service namespace: metallb-system path: /validate-metallb-io-v1beta1-bfdprofile failurePolicy: Fail name: bfdprofilevalidationwebhook.metallb.io rules: - apiGroups: - metallb.io apiVersions: - v1beta1 operations: - CREATE - DELETE resources: - bfdprofiles sideEffects: None- admissionReviewVersions: - v1 clientConfig: service: name: webhook-service namespace: metallb-system path: /validate-metallb-io-v1beta1-bgpadvertisement failurePolicy: Fail name: bgpadvertisementvalidationwebhook.metallb.io rules: - apiGroups: - metallb.io apiVersions: - v1beta1 operations: - CREATE - UPDATE resources: - bgpadvertisements sideEffects: None- admissionReviewVersions: - v1 clientConfig: service: name: webhook-service namespace: metallb-system path: /validate-metallb-io-v1beta1-community failurePolicy: Fail name: communityvalidationwebhook.metallb.io rules: - apiGroups: - metallb.io apiVersions: - v1beta1 operations: - CREATE - UPDATE resources: - communities sideEffects: None- admissionReviewVersions: - v1 clientConfig: service: name: webhook-service namespace: metallb-system path: /validate-metallb-io-v1beta1-ipaddresspool failurePolicy: Fail name: ipaddresspoolvalidationwebhook.metallb.io rules: - apiGroups: - metallb.io apiVersions: - v1beta1 operations: - CREATE - UPDATE resources: - ipaddresspools sideEffects: None- admissionReviewVersions: - v1 clientConfig: service: name: webhook-service namespace: metallb-system path: /validate-metallb-io-v1beta1-l2advertisement failurePolicy: Fail name: l2advertisementvalidationwebhook.metallb.io rules: - apiGroups: - metallb.io apiVersions: - v1beta1 operations: - CREATE - UPDATE resources: - l2advertisements sideEffects: None应用yaml文件：1kubectl apply -f metallb-native.yaml查看MetalLB Pod 运行情况1kubectl get pods -n metallb-system123NAME READY STATUS RESTARTS AGEcontroller-5c5f5b9f77-2kz5n 1/1 Running 0 2mspeaker-9zl7j 1/1 Running 0 2mcontroller：管理 MetalLB 的控制器。speaker：在每个节点上运行，负责广播负载均衡 IP。配置 MetalLB IP 池创建 IPAddressPoolMetalLB 需要一个 IP 池（地址范围），用于分配 LoadBalancer IP。ip-pool.yaml文件：12345678apiVersion: metallb.io/v1beta1kind: IPAddressPoolmetadata: name: my-ip-pool namespace: metallb-systemspec: addresses: - 10.168.2.249-10.168.2.250 #- 10.168.2.249/32 # 仅使用 10.168.2.249 这个 IP应用yaml文件：1kubectl apply -f ip-pool.yaml注意：addresses 定义了一段可以分配的 IP 地址范围，需确保它在你的网络中可用（跟你的k8s节点同一个网段，且IP没被分配）你可以调整 IP 范围以适应你的网络。创建 L2Advertisement（Layer 2 模式）使用L2（ARP 广播）模式，还需要创建 L2Advertisement 资源。l2-advertisement.yaml文件：12345apiVersion: metallb.io/v1beta1kind: L2Advertisementmetadata: name: my-l2-advertisement namespace: metallb-system应用yaml文件：1kubectl apply -f l2-advertisement.yaml创建完之后我们可以查看一下k8s的ingress-nginx的svc信息，看external-ip，没装前是pending部署完Metallb之后再查看：然后我们创建Ingress，然后把域名解析到这个external-ip就可以访问集群内的网站。删除Metallb如果需要删除Metallb：123kubectl delete -f l2-advertisement.yamlkubectl delete -f ip-pool.yamlkubectl delete -f metallb-native.yaml]]></content>
      <categories>
        <category>技术</category>
        <category>K8S</category>
      </categories>
      <tags>
        <tag>Metallb</tag>
        <tag>负载均衡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubesphere安装Istio]]></title>
    <url>%2F2025%2F12d6f5ac4d.html</url>
    <content type="text"><![CDATA[KubeSphere 服务网格基于Istio，将微服务治理和流量管理可视化。它拥有强大的工具包，包括熔断机制、蓝绿部署、金丝雀发布、流量镜像、链路追踪、可观测性和流量控制等。KubeSphere 服务网格支持代码无侵入的微服务治理，帮助开发者快速上手，Istio 的学习曲线也极大降低。KubeSphere 服务网格的所有功能都旨在满足用户的业务需求。kubesphere安装Istio登录控制台kubesphere已经更新到4.2.0版本，Istio已经解耦出来，作为扩展组件安装，可以直接从控制台扩展中心安装，可参考官方文档：安装扩展组件 - KubeSphere，这里不再赘述。由于我的kubesphere是3.3.2版本，此版本集成了Istio，在控制台修改一下参数就可以自动安装：登录控制台，左上角点击平台管理-集群管理：在定制资源定义，搜索clusterconf：点进这个ClusterConfiguration之后：末尾servicemesh修改参数：123456789servicemesh:enabled: true # 将“false”更改为“true”。istio: # Customizing the istio installation configuration, refer to https://istio.io/latest/docs/setup/additional-setup/customize-installation/ components: ingressGateways: - name: istio-ingressgateway # 将服务暴露至服务网格之外。默认不开启。 enabled: false cni: enabled: false # 启用后，会在 Kubernetes pod 生命周期的网络设置阶段完成 Istio 网格的 pod 流量转发设置工作。确定保存后，即可检查Istio组件的安装过程：1kubectl logs -n kubesphere-system $(kubectl get pod -n kubesphere-system -l 'app in (ks-install, ks-installer)' -o jsonpath='&#123;.items[0].metadata.name&#125;') -f安装完成后，日志输出如下：1234567891011121314151617181920212223Waiting for all tasks to be completed ...task network status is successful (1/5)task openpitrix status is successful (2/5)task multicluster status is successful (3/5)task monitoring status is successful (4/5)task servicemesh status is successful (5/5)**************************************************Collecting installation results ...######################################################## Welcome to KubeSphere! ########################################################Console: http://192.168.88.20:30880Account: adminPassword: P@88w0rdNOTES： 1. After you log into the console, please check the monitoring status of service components in "Cluster Management". If any service is not ready, please wait patiently until all components are up and running. 2. Please change the default password after login.这就代表完成了Istio的安装及初始化。不需要理会这个原始密码，重新登录还是要用已经更改的密码。验证Istio组件安装情况在WebUI可以看到：系统组件中已经出现了Istio组件。但是点进去发现：此时不但Istio异常，连带之前正常的Prometheus也一并异常了：等待一段时间，等他拉取镜像，启动容器即可。我的是能直接启动成功，有人反馈说容易一直卡在容器创建中，一直启动不了看pod的events如下：1234567891011121314Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 43m default-scheduler Successfully assigned istio-system/istiod-1-11-2-54dd699c87-99krn to zhiyong-ksp1 Warning FailedCreatePodSandBox 43m kubelet Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox "5d0a3bdb6dea937aa5b118bbd00305a1542111c97af84a3cbdd8f188b1681687": plugin type="calico" failed (add): error getting ClusterInformation: connection is unauthorized: Unauthorized Warning FailedCreatePodSandBox 43m kubelet Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox "ff84de82acfd944be7f3804c96f39ab976ae4d6810b7e0364c90560a4b4070e7": plugin type="calico" failed (add): error getting ClusterInformation: connection is unauthorized: Unauthorized Warning FailedCreatePodSandBox 42m kubelet Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox "6337bea6f7c16cd9adcff0d2b75238beb4365dc4b880d4c8e4f4535885d59d30": plugin type="calico" failed (add): error getting ClusterInformation: connection is unauthorized: Unauthorized Warning FailedCreatePodSandBox 42m kubelet Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox "42e08603d4d7e7d1713eecbb21af258022e3fb50c6f5611808b3e2755d50d980": plugin type="calico" failed (add): error getting ClusterInformation: connection is unauthorized: Unauthorized Warning FailedCreatePodSandBox 42m kubelet Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox "51a6b5b8ea5a63f4be828a0c855802e42640324c440fcc3487c535123d7b3372": plugin type="calico" failed (add): error getting ClusterInformation: connection is unauthorized: Unauthorized Warning FailedCreatePodSandBox 42m kubelet Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox "dada948b2a416a0ec925b7f67a101b8fd48fdad9fb20d6c41eaf1bbad0a18e57": plugin type="calico" failed (add): error getting ClusterInformation: connection is unauthorized: Unauthorized Warning FailedCreatePodSandBox 41m kubelet Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox "df3487e020c1e7eb527cc0fce1fe990873bd20f46cbf04de99005e0da5896abe": plugin type="calico" failed (add): error getting ClusterInformation: connection is unauthorized: Unauthorized Warning FailedCreatePodSandBox 41m kubelet Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox "92e739549a96aa03ea864188abc1b91c9a45394dae28ad97234fa1caf4d52240": plugin type="calico" failed (add): error getting ClusterInformation: connection is unauthorized: Unauthorized Warning FailedCreatePodSandBox 41m kubelet Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox "bc5d1999a2d5ad4d7cf5c1e1c3c7c1a80dee02b806d0be2e15c326e2d82f4af5": plugin type="calico" failed (add): error getting ClusterInformation: connection is unauthorized: Unauthorized Warning FailedCreatePodSandBox 3m2s (x176 over 41m) kubelet (combined from similar events): Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox "be41a317c2e14b4096f2f8f0d4bfaa8a80572f7365ab3d92c20be75fe97304f4": plugin type="calico" failed (add): error getting ClusterInformation: connection is unauthorized: Unauthorized出现了网络没有认证通过的问题，根据报错日志，基本上确定是Calico的问题。解决：把/etc/cni/net.d/目录下的calico-kubeconfig和10-calico.conflist文件移到另一个目录下备份，然后重启机器刷新Calico的配置。等待容器启动完成即可。自制应用kubesphere的服务网格功能是在企业空间的自制应用里使用的，所以我们先要创建企业空间，然后把项目加到企业空间里，再创建一个自制应用，这个自制应用就是我们部署的整个系统，然后再把各个微服务通过标签加入到这个自制应用里。创建自制应用基本信息要启用应用治理才可以使用Istio功能服务设置无状态服务为deployment，有状态服务为StatefulSet填名称，镜像，存储等等信息路由设置注：这里只创建了Ingress，这样是访问不了集群内部的，需要启用网关，用nodeport的方式会有一个nodeport把这个域名解析到任意一个k8s节点，然后就可以域名+nodeport的方式访问：或者如果部署了ingress-nginx-controller的话：在ingress里加入ingressClassName: nginx123456789101112131415161718192021222324kind: IngressapiVersion: networking.k8s.io/v1metadata: name: bookinfo-ingress namespace: bookinfo labels: app.kubernetes.io/name: bookinfo app.kubernetes.io/version: v1 annotations: kubesphere.io/creator: admin nginx.ingress.kubernetes.io/upstream-vhost: productpage.bookinfo.svc.cluster.localspec: ingressClassName: nginx #加入这个参数 rules: - host: bookinfo.keyfel.com http: paths: - path: / pathType: ImplementationSpecific backend: service: name: productpage port: number: 9080就可以直接用域名访问创建完成这样创建的deploy，svc，StatefulSet，Ingress等等都会自动加入匹配此自制应用的标签以及注解：Ingress：svc：deploy：已运行资源加入自制应用但是由于我们之前已经在运行的微服务，并没有这些标签，所以我们需要在已经存在的资源加入这些标签及注解，以加入此自制应用来使用Istio功能。Deployment 有 app version 这两个 label；Service 有 app Label；且 Deploy 与 service 的 App Label 一致，等于 Service Name（Istio 需要）在一个应用内，所有资源需要有这两个标签 app.kubernetes.io/name=, app.kubernetes.io/version=（Application 需要）Deployment Name 为 Service Name 后面加 v1；如 Serevice 为 nginx, deployment 为 nginx-v1Deployment Template 中有相应 Annotation （Istio Sidecar 自动注入需要）1234template: metadata: annotations: sidecar.istio.io/inject: "true"​ 5.Service/Deployment 有相应 Annotation （KubeSphere CRD Controller 会自动将 Service 同步为 Virtual Service/DestinationRules，CRD controller 需要）1234567891011# Servicekind: Servicemetadata: annotations: servicemesh.kubesphere.io/enabled: "true"# Deploymentkind: Deploymentmetadata: annotations: servicemesh.kubesphere.io/enabled: "true"（注：已创建的deployment的selector修改不了，只能把yaml文件复制出来，把原deployment删了再根据yaml创建新的）即：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849deployment需要加：metadata: labels: app: $SERVICE app.kubernetes.io/name: $自制应用名称 app.kubernetes.io/version: v1 version: v1 annotations: servicemesh.kubesphere.io/enabled: 'true'spec: selector: matchLabels: app: $SERVICE app.kubernetes.io/name: $自制应用名称 app.kubernetes.io/version: v1 version: v1 template: metadata: labels: app: $SERVICE app.kubernetes.io/name: $自制应用名称 app.kubernetes.io/version: v1 version: v1 annotations: sidecar.istio.io/inject: 'true' svc需要加：metadata: labels: app: $SERVICE app.kubernetes.io/name: $自制应用名称 app.kubernetes.io/version: v1 annotations: servicemesh.kubesphere.io/enabled: "true"spec: ports: - name: http-web selector: app: $SERVICE app.kubernetes.io/name: $自制应用名称 app.kubernetes.io/version: v1 Ingress需要加：metadata: labels: app.kubernetes.io/name: $自制应用名称 app.kubernetes.io/version: v1 annotations: nginx.ingress.kubernetes.io/upstream-vhost: $SERVICE.$NAMESPACE.svc.cluster.localIngres-nginx注入sidecar容器Istio有自己的网关ingress-gateway，但是配置Gateway在Kubesphere没法可视化，所以还是使用可以可视化的ingress-nginx，需要在ingress-nginx的deployment注入sidecar容器以及在Ingress的注解加上nginx.ingress.kubernetes.io/upstream-vhost: $SERVICE.$NAMESPACE.svc.cluster.local，然后通过Ingress来访问就可以正常使用Istio的功能。注入sidecar：1234567891011121314151617181920spec: replicas: 2 selector: matchLabels: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx template: metadata: creationTimestamp: null labels: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.8.1 pod-template-hash: 6646d898d8 annotations: kubesphere.io/restartedAt: '2025-07-08T02:17:42.455Z' sidecar.istio.io/inject: 'true' #加入这个注解需要注意一下，可能本地集群部署，没有负载均衡，所以ingress-nginx开启了hostNetwork: true和hostPort: 80，直接在node节点上监听了80端口，但是如果开启了hostNetwork: true，sidecar容器是注入不了的。所以这里可以把hostNetwork: true去掉，只保留hostPort: 80，这样iptables会创建规则，把流量转到k8s集群内的pod，还是可以使用node节点的80端口，又可以注入sidecar容器。或者使用metallb负载均衡器，这里不再赘述，metallb部署可以看我另一篇文章。使用Istio部署实例应用bookinfo演示Istio功能部署完成：可以点击流量监控查看拓扑图：以及链路追踪：熔断功能：点击具体的微服务，右边有个流量管理，我们点击启用熔断器，确认就可以使用熔断功能，会自动更新相关的VirtualService和DestinationRule：查看DestinationRule：金丝雀发布点击灰度发布，选择金丝雀发布： 选择哪个微服务需要灰度发布：配置要灰度的版本信息：配置灰度策略：按比例或者指定参数：创建完发布任务之后就灰度版本就发布了，VirtualService和DestinationRule会自动调整，可以查看验证一下：VirtualService：DestinationRule：验证灰度结果：刷新可以看到两个版本在切换：v1版本：gray版本：回到控制台，点击发布任务：可以点进去查看当前版本和灰度版本的流量，请求成功率等等信息，以及调整灰度比例：灰度版本有问题的话可以回滚到v1版本，灰度版本验证没问题需要全量发布的话可以选择gray版本接管流量。因为直接用灰度版本去接管全部流量的话，每次灰度发布都会创建不同的deploy，因为我的CI/CD用jenkins发布，写死了deploy的名称，所以我的做法是验证完灰度版本没问题，先把gray版本接管所有流量，然后把v1版本的镜像改为灰度版本的镜像，等容器启动完之后，现在两个版本都是gray版本的镜像，再把v1版本接管所有流量，然后再把发布任务删除（必须要有一个版本接管全部流量之后才删的了灰度发布任务），这样也就相当于是gray版本全量发布了。删除灰度发布任务后，非接管流量的那个版本对应的deploy等等资源也会自动删除。按需注入sidecar如果微服务很多，每个微服务都注入sidecar容器的话，会有性能损耗，有延迟，需要的服务器资源也多，所以我们可以按需注入sidecar容器，需要灰度发布的微服务就注入sidecar，不需要的就不注入，不注入的话请求就是k8s默认的负载均衡。但是要注意不能使用mTLS，使用mTLS的话，没注入sidecar容器的都会通信失败，且如果被调用方需要灰度，那么被调用方和调用方都需要注入sidecar，规则才能生效。即： 所以如果你的微服务相互间都有调用的话，那还是需要都注入sidecar，规则才能生效。使用Istio遇到的问题nacos注册微服务注入Sidecar后服务注册到Nacos不稳定，时有时无。解决办法：nacos的headless svc的9848,9849端口加上appProtocol: tcp1234567891011121314151617181920212223242526272829303132333435363738kind: ServiceapiVersion: v1metadata: name: nacos-headless namespace: qifu labels: app: nacosspec: ports: - name: server protocol: TCP port: 8848 targetPort: 8848 - name: client-rpc protocol: TCP appProtocol: tcp #添加这个 port: 9848 targetPort: 9848 - name: raft-rpc protocol: TCP appProtocol: tcp #添加这个 port: 9849 targetPort: 9849 - name: old-raft-rpc protocol: TCP port: 7848 targetPort: 7848 selector: app: nacos clusterIP: None clusterIPs: - None type: ClusterIP sessionAffinity: None ipFamilies: - IPv4 ipFamilyPolicy: SingleStack internalTrafficPolicy: Cluster参考文档资料：1.https://github.com/alibaba/nacos/issues/101412.https://github.com/nacos-group/nacos-k8s/issues/2213.https://istio.io/latest/docs/ops/configuration/traffic-management/protocol-selection/tcp探针健康检查失败Istio官网提出：TCP 探针检查需要特殊处理，因为lstio 将所有传入的流量重定向到 Sidecar，所以所有TCP 端口都显示为开放。kubelet仅检查某个进程是否正在监听指定的端口，因此只要 Sidecar 正在运行，该探针就总会成功。所以即使pod主容器还没启动完成，pod也会显示就绪。解决办法是使用httpget探针检测，我用的Istio版本是1.11.2，此问题在1.12.0已修复，或者升级Istio。nacos访问失败nacos服务也是k8s集群里跑的，通过Ingress访问，Ingress-nginx注入sidecar后，发现访问nacos报404.解决办法：nacos的Ingress添加注解nginx.ingress.kubernetes.io/upstream-vhost: $svc.$ns.svc.cluster.local但是又出现新的问题：Ingress添加注解nginx.ingress.kubernetes.io/upstream-vhost后，浏览器访问不是根路径时，重写路径会导致浏览器访问会跳转到k8s集群内部的值： $svc.$ns.svc.cluster.local，从而访问失败。解决办法：方法一：下游服务是nginx时，可修改nginx配置：123location /system &#123; return 301 $http_x_forwarded_proto://$http_x_forwarded_host/system/; &#125;方法二：修改Ingress，加上以下注解，重定向：1234567891011121314151617181920212223242526272829kind: IngressapiVersion: networking.k8s.io/v1metadata: name: nacos namespace: qifu annotations: nginx.ingress.kubernetes.io/proxy-redirect-from: 'http://nacos-headless.qifu.svc.cluster.local/' nginx.ingress.kubernetes.io/proxy-redirect-to: '$scheme://$host/' nginx.ingress.kubernetes.io/server-snippet: | proxy_set_header Host $host; nginx.ingress.kubernetes.io/upstream-vhost: nacos-headless.qifu.svc.cluster.local nginx.ingress.kubernetes.io/use-regex: 'true'spec: ingressClassName: nginx tls: - hosts: - nacos.keyfel.com secretName: keyfel-secrect rules: - host: nacos.keyfel.com http: paths: - path: / pathType: Prefix backend: service: name: nacos-headless port: number: 8848一个Ingress多个路径怎么配置注解可能有一个Ingress不同路径路由到不同服务的情况，但是ingress-nginx注入sidecar之后，需要添加注解nginx.ingress.kubernetes.io/upstream-vhost，只有一个值匹配不了多个服务，所以我们需要把多个服务拆解为多个ingress管理。Ingress-nginx获取不到真实IPIngress-nginx注入sidecar后日志获取到的来源IP全部为127.0.0.6，无法获取真实IP(前端传入的X-Forwarded-For中的IP被覆盖)。解决办法：1.ingress controller的cm中加上如下data:1use-forwarded-headers: "true"然后重启这个pod，发送测试请求：1curl -H "X-Forwarded-For: 1.2.3.4" http://your-domain.com查看ingress-nginx日志：1kubectl logs -f deployment/ingress-nginx-controller -n ingress-nginx发现已经可以透传Header中的外部IP。然后添加以下envoyfilter：123456789101112131415161718192021222324kubectl apply -f - &lt;&lt;EOFapiVersion: networking.istio.io/v1alpha3kind: EnvoyFiltermetadata: name: preserve-client-ip namespace: istio-systemspec: configPatches: - applyTo: NETWORK_FILTER match: context: ANY listener: filterChain: filter: name: "envoy.filters.network.http_connection_manager" patch: operation: MERGE value: typed_config: "@type": "type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager" skip_xff_append: false xff_num_trusted_hops: 1 use_remote_address: trueEOF发现可以正常获取真实IP了：]]></content>
      <categories>
        <category>技术</category>
        <category>K8S</category>
      </categories>
      <tags>
        <tag>服务网格</tag>
        <tag>Istio</tag>
        <tag>灰度发布</tag>
        <tag>kubesphere</tag>
        <tag>熔断</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker安装Mailcow自建域名邮箱]]></title>
    <url>%2F2025%2F1179e51416.html</url>
    <content type="text"><![CDATA[几乎所有市面上的免费邮箱服务，都是以牺牲你的隐私和数据为代价，利用你的大数据来进行广告行为分析来盈利，而且免费邮箱都有邮箱数量，强制绑定手机号等等限制，所以为了保护我们的数据隐私以及创建无限数量邮箱，我们可以自己自建邮箱服务。服务器准备按照官网的说法，推荐的最小配置要求如下：Resourcemailcow: dockerizedCPU1 GHz内存最低 6 GiB + 1 GiB swap (实际测试 4 GiB 内存也足够)硬盘20 GiB (不包含邮件的占用)系统x86_64并且从防火墙放行这几个 TCP 端口：服务协议端口容器名Postfix SMTPTCP25postfix-mailcowPostfix SMTPSTCP465postfix-mailcowPostfix SubmissionTCP587postfix-mailcowDovecot IMAPTCP143dovecot-mailcowDovecot IMAPSTCP993dovecot-mailcowDovecot POP3TCP110dovecot-mailcowDovecot POP3STCP995dovecot-mailcowDovecot ManageSieveTCP4190dovecot-mailcowHTTP(S)TCP80/443nginx-mailcow请注意，因为垃圾邮件滥用的原因，很多国外的 VPS 商家并不允许架设邮件发送服务器，并且默认 25 端口的出口方向是屏蔽的，请自行咨询厂商。设置 DNS 解析记录假设邮箱服务器需要使用域名 mail.example.com，想搭建 `username@example.com` 的邮箱；服务器 IPv4 为 192.0.2.25，IPv6 为 2001:db8::25，那么请预先做好如下解析：域名解析类型解析值mail.example.comA192.0.2.25mail.example.comAAAA2001:db8::25example.comMX10 mail.example.com.example.comTXT“v=spf1 mx ~all”_dmarc.example.comTXT“v=DMARC1; p=reject; sp=reject; adkim=s; aspf=s;”autodiscover.example.comCNAMEmail.example.com.autoconfig.example.comCNAMEmail.example.com.注意：某些 DNS 厂商的控制面板添加 MX 和 CNAME 记录时不需要输入最后的点号，添加 TXT 记录时不需要最前面和最后面的引号。另外需要联系你的 VPS 厂商，设置 PTR 记录，即 IP 反向解析，请设置 192.0.2.25 和 2001:db8::25 的 PTR 记录为 mail.example.com. 提高邮件到达率。安装 Mailcow安装依赖工具1yum install -y git openssl curl gawk coreutils grep jq拉取项目代码12git clone https://github.com/mailcow/mailcow-dockerizedcd mailcow-dockerized生成配置文件，请注意使用 FQDN (比如 mail.example.com) 作为 hostname：1bash generate_config.sh按照提示输入自己的需求后即可生成好配置文件 mailcow.conf，如有需要可以自己修改这个文件。官方提供的docker-compose.yaml有些镜像由于网络原因拉不下来，我在网上找了国内网络可以拉取的对应镜像，如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172[root@localhost mailcow]# cat docker-compose.yml |grep -B2 image unbound-mailcow: image: swr.cn-north-4.myhuaweicloud.com/ddn-k8s/ghcr.io/mailcow/unbound:1.24-- mysql-mailcow: image: swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/library/mariadb:10.11-- redis-mailcow: image: docker.1ms.run/library/redis:7.4.6-alpine-- clamd-mailcow: image: swr.cn-north-4.myhuaweicloud.com/ddn-k8s/ghcr.io/mailcow/clamd:1.71-- rspamd-mailcow: image: ghcr.io/mailcow/rspamd:2.4-- php-fpm-mailcow: image: ghcr.io/mailcow/phpfpm:1.94-- sogo-mailcow: image: ghcr.io/mailcow/sogo:1.136-- dovecot-mailcow: image: swr.cn-north-4.myhuaweicloud.com/ddn-k8s/ghcr.io/mailcow/dovecot:2.35-- postfix-mailcow: image: swr.cn-north-4.myhuaweicloud.com/ddn-k8s/ghcr.io/mailcow/postfix:1.81-- postfix-tlspol-mailcow: image: swr.cn-north-4.myhuaweicloud.com/ddn-k8s/ghcr.io/mailcow/postfix-tlspol:1.0-- memcached-mailcow: image: swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/memcached:alpine-- - sogo-mailcow - rspamd-mailcow image: ghcr.io/mailcow/nginx:1.05-- unbound-mailcow: condition: service_healthy image: swr.cn-north-4.myhuaweicloud.com/ddn-k8s/ghcr.io/mailcow/acme:1.94-- netfilter-mailcow: image: ghcr.io/mailcow/netfilter:1.63-- watchdog-mailcow: image: swr.cn-north-4.myhuaweicloud.com/ddn-k8s/ghcr.io/mailcow/watchdog:2.09-- dockerapi-mailcow: image: swr.cn-north-4.myhuaweicloud.com/ddn-k8s/ghcr.io/mailcow/dockerapi:2.11-- olefy-mailcow: image: swr.cn-north-4.myhuaweicloud.com/ddn-k8s/ghcr.io/mailcow/olefy:1.15-- ofelia-mailcow: image: docker.1ms.run/mcuadros/ofelia:latest然后拉取 Docker 镜像并启动12docker compose pulldocker compose up -d耐心等待几分钟后即可访问 https://mail.example.com/admin 默认用户名 admin 默认密码 moohoo，建议立马修改并开启 2FA 两步验证确保安全。添加域名和邮箱进入 Mailcow 后台后，我们可以在顶部的 E-mail &gt; 配置 里添加域名按照自己的要求填入各种设置：如果需要立马生效 Web 客户端，可以选择 Add domain and restart SOGo：开启 DKIM 并添加 DNS 记录开启 DKIM 后邮件发信到达率更高，你可以登录 Mailcow 后台后在 系统 &gt; 配置 &gt; 选项 &gt; ARC/DKIM keys 查看你域名的 dkim 记录值：右边那一串 v=DKIM1;k=rsa;t=s;s=email;p= 的 2048 位字符即你的 DKIM 值，如果未开启，可以在下方输入域名，选择 2048 位，然后点 + Add 按钮添加默认添加完域名后即开启了 DKIM，且 Selector 设置为 dkim，然后我们需要添加如下 DNS 记录：域名解析类型解析值dkim._domainkey.example.comTXT“v=DKIM1;k=rsa;t=s;s=email;p=blablablablablabla”某些 DNS 厂商的后台可能无法直接添加 2048 位 DKIM 的 TXT 记录，因为 TXT 类型的 DNS 记录最大长度为 255 个字符，那么请手工截断成两个 TXT 记录，第一个需要 255 个字符，第二个记录为剩下的字符串添加邮箱用户我们可以在 Mailboxes 这个 tab 里选择 +Add mailbox 按钮添加用户：按要求提示填写即可：测试邮件我们使用刚开的用户登录 Mailcow 自带的 SOGo，默认情况下地址为 https://mail.example.com登录之后可以测试是否能正常收发邮件：发邮件：收邮件：想看邮件分数可以在 mail-tester.com 发送一封 Plain Text 格式的测试邮件，稍等片刻后即可查看你的邮件分数，可以按照他的提示来修改以提高分数，避免发送邮件被放到垃圾邮箱：]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>自建邮箱</tag>
        <tag>Mailcow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s动态扩缩容部署]]></title>
    <url>%2F2025%2F113cc2bcea.html</url>
    <content type="text"><![CDATA[Horizontal Pod Autoscaler（HPA，Pod水平自动伸缩），根据平均 CPU 利用率、平均内存利用率或你指定的任何其他自定义指标自动调整 Deployment 、ReplicaSet 或 StatefulSet 或其他类似资源，实现部署的自动扩展和缩减，让部署的规模接近于实际服务的负载。HPA不适用于无法缩放的对象，例如DaemonSet。我的k8s集群是用kubesphere来管理的，配置hap发现没有生效，查看文档发现需要先部署metrics-server来获取资源指标：可以看到所有的TARGETS都是，但是后边有%80、%100的阀值。1234567[root@master knativetest]# kubectl get hpa -ANAMESPACE NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGEistio-system cluster-local-gateway Deployment/cluster-local-gateway &lt;unknown&gt;/80% 1 5 1 33distio-system istio-ingressgateway Deployment/istio-ingressgateway &lt;unknown&gt;/80% 4 5 4 33distio-system istiod Deployment/istiod &lt;unknown&gt;/80% 1 5 1 33dknative-serving activator Deployment/activator &lt;unknown&gt;/100% 1 20 1 33dtekton-pipelines tekton-pipelines-webhook Deployment/tekton-pipelines-webhook &lt;unknown&gt;/100% 1 5 1 7d21h查看一下hpa的详情：123456789101112131415161718192021222324252627282930[root@master knativetest]# kubectl describe hpa/istio-ingressgateway -n istio-system Name: istio-ingressgatewayNamespace: istio-systemLabels: app=istio-ingressgateway install.operator.istio.io/owning-resource=unknown install.operator.istio.io/owning-resource-namespace=istio-system istio=ingressgateway istio.io/rev=default operator.istio.io/component=IngressGateways operator.istio.io/managed=Reconcile operator.istio.io/version=1.8.0 release=istioAnnotations: &lt;none&gt;CreationTimestamp: Fri, 20 Nov 2020 17:13:57 +0800Reference: Deployment/istio-ingressgatewayMetrics: ( current / target ) resource cpu on pods (as a percentage of request): &lt;unknown&gt; / 1%Min replicas: 1Max replicas: 5Deployment pods: 1 current / 0 desiredConditions: Type Status Reason Message ---- ------ ------ ------- AbleToScale True SucceededGetScale the HPA controller was able to get the target's current scale ScalingActive False FailedGetResourceMetric the HPA was unable to compute the replica count: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedGetResourceMetric 19m (x1734 over 5d18h) horizontal-pod-autoscaler unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io) Warning FailedGetResourceMetric 4m37s (x13 over 16m) horizontal-pod-autoscaler unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)报错信息是不能够从metrcis api中拿到服务的指标。所以我们得先安装metrics-server：metrics-server的yaml文件：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187apiVersion: v1kind: ServiceAccountmetadata: labels: k8s-app: metrics-server name: metrics-server namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: labels: k8s-app: metrics-server rbac.authorization.k8s.io/aggregate-to-admin: "true" rbac.authorization.k8s.io/aggregate-to-edit: "true" rbac.authorization.k8s.io/aggregate-to-view: "true" name: system:aggregated-metrics-readerrules:- apiGroups: - metrics.k8s.io resources: - pods - nodes verbs: - get - list - watch---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: labels: k8s-app: metrics-server name: system:metrics-serverrules:- apiGroups: - "" resources: - pods - nodes - nodes/stats - namespaces - configmaps verbs: - get - list - watch---apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata: labels: k8s-app: metrics-server name: metrics-server-auth-reader namespace: kube-systemroleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: extension-apiserver-authentication-readersubjects:- kind: ServiceAccount name: metrics-server namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: labels: k8s-app: metrics-server name: metrics-server:system:auth-delegatorroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:auth-delegatorsubjects:- kind: ServiceAccount name: metrics-server namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: labels: k8s-app: metrics-server name: system:metrics-serverroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:metrics-serversubjects:- kind: ServiceAccount name: metrics-server namespace: kube-system---apiVersion: v1kind: Servicemetadata: labels: k8s-app: metrics-server name: metrics-server namespace: kube-systemspec: ports: - name: https port: 443 protocol: TCP targetPort: https selector: k8s-app: metrics-server---apiVersion: apps/v1kind: Deploymentmetadata: labels: k8s-app: metrics-server name: metrics-server namespace: kube-systemspec: selector: matchLabels: k8s-app: metrics-server strategy: rollingUpdate: maxUnavailable: 0 template: metadata: labels: k8s-app: metrics-server spec: containers: - args: - --cert-dir=/tmp - --secure-port=4443 - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname - --kubelet-use-node-status-port - --kubelet-insecure-tls image: registry.aliyuncs.com/google_containers/metrics-server:v0.4.1 imagePullPolicy: IfNotPresent livenessProbe: failureThreshold: 3 httpGet: path: /livez port: https scheme: HTTPS periodSeconds: 10 name: metrics-server ports: - containerPort: 4443 name: https protocol: TCP readinessProbe: failureThreshold: 3 httpGet: path: /readyz port: https scheme: HTTPS periodSeconds: 10 securityContext: readOnlyRootFilesystem: true runAsNonRoot: true runAsUser: 1000 volumeMounts: - mountPath: /tmp name: tmp-dir nodeSelector: kubernetes.io/os: linux priorityClassName: system-cluster-critical serviceAccountName: metrics-server volumes: - emptyDir: &#123;&#125; name: tmp-dir---apiVersion: apiregistration.k8s.io/v1kind: APIServicemetadata: labels: k8s-app: metrics-server name: v1beta1.metrics.k8s.iospec: group: metrics.k8s.io groupPriorityMinimum: 100 insecureSkipTLSVerify: true service: name: metrics-server namespace: kube-system version: v1beta1 versionPriority: 100部署：1kubectl apply -f metrics-server.yaml部署完之后再看下hpa状态：1234567naison@P_CAIWFENG-MB0 knativetest % kubectl get hpa -ANAMESPACE NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGEistio-system cluster-local-gateway Deployment/cluster-local-gateway 12%/80% 1 5 1 33distio-system istio-ingressgateway Deployment/istio-ingressgateway 11%/80% 4 5 4 33distio-system istiod Deployment/istiod 0%/80% 1 5 1 33dknative-serving activator Deployment/activator &lt;unknown&gt;/100% 1 20 1 33dtekton-pipelines tekton-pipelines-webhook Deployment/tekton-pipelines-webhook 3%/100% 1 5 1 7d21h还有一个hpa没生效，查hpa官方文档发现，如果不设置container的request request值，那么hpa是无法生效的。创建一个hpa：1234567891011121314151617181920apiVersion: autoscaling/v2kind: HorizontalPodAutoscalermetadata: name: hap-nginxspec: maxReplicas: 10 # 最大扩容到10个节点（pod） minReplicas: 1 # 最小扩容1个节点（pod） metrics: - resource: name: cpu target: averageUtilization: 40 # CPU 平局资源使用率达到40%就开始扩容，低于40%就是缩容 # 设置内存 # AverageValue：40 type: Utilization type: Resource scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: hap-nginx测试：压测的时候发现，不一会就扩容了很多pod，之前测试出来的最优性能时，pod大概是4个，但是这个不一会儿就已经8个了，肯定有问题。hpa的计算公式：1desiredReplicas = ceil[currentReplicas * ( currentMetricValue / desiredMetricValue )]意思是：1需要的pod数 = 向下取整[当前pod数 * ( pod当前使用值 / pod request值)]注意这里是pod，request值也就是resource的request值。查看deployment的container中的resources标签值，发现是100m，原来是分母太小了，导致pod只用了一点儿资源，就扩容了。所以把request值设置为合理的大小就行。]]></content>
      <categories>
        <category>技术</category>
        <category>K8S</category>
      </categories>
      <tags>
        <tag>hpa</tag>
        <tag>自动扩缩容</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql高可用MHA部署]]></title>
    <url>%2F2025%2F10131ef651.html</url>
    <content type="text"><![CDATA[MySQL的MHA（Master High Availability）是一种用于MySQL高可用性的解决方案，旨在提高MySQL数据库集群的可用性和故障恢复能力。MHA主要通过自动故障切换来确保数据库的高可用性，它能够在主数据库出现故障时，自动将从数据库提升为主数据库，以避免长时间的服务中断。MHA组件介绍MHA Manager（MHA 管理器）功能：MHA Manager 是 MHA 的控制中心，负责监控 MySQL 主从数据库的健康状态，并在主服务器故障时触发自动故障切换。角色：监控数据库主从节点的状态。在主服务器发生故障时，选择健康的从服务器并提升为新的主服务器。管理数据库节点的配置（如：主库和从库的切换）。在主库恢复后，自动将恢复的主库重新配置为从库。部署：MHA Manager 通常运行在一个独立的服务器上，不与数据库实例共享。MHA Node（MHA 节点）功能：MHA Node 是 MHA 管理器和 MySQL 数据库之间的中介，负责与 MHA Manager 进行通信，并执行主从切换操作。角色：向 MHA Manager 上报数据库节点（主库和从库）的健康状态。参与故障切换流程，包括主库的提升和从库的降级。部署：每个 MySQL 实例（主服务器和从服务器）都需要安装和配置 MHA Node，以便与 MHA Manager 通信。集群规划服务器初始化服务器免密MHA Manager使用SSH登录到主从数据库节点，执行故障检测、切换主库等操作，包括在故障切换过程中通过SSH访问数据库服务器执行切换操作（例如停止服务、修改配置文件等）。123456789101112131415161718192021222324252627#MHA Manager免密[root@test-server-01 ~]# ssh-keygen -t rsa[root@test-server-01 ~]# ssh-copy-id 192.168.40.180[root@test-server-01 ~]# ssh-copy-id 192.168.40.181[root@test-server-01 ~]# ssh-copy-id 192.168.40.182[root@test-server-01 ~]# ssh-copy-id 192.168.40.183#MySQL Master[root@test-server-02 ~]# ssh-keygen -t rsa[root@test-server-02 ~]# ssh-copy-id 192.168.40.180[root@test-server-02 ~]# ssh-copy-id 192.168.40.181[root@test-server-02 ~]# ssh-copy-id 192.168.40.182[root@test-server-02 ~]# ssh-copy-id 192.168.40.183#MySQL Slave 1[root@test-server-03 ~]# ssh-keygen -t rsa[root@test-server-03 ~]# ssh-copy-id 192.168.40.180[root@test-server-03 ~]# ssh-copy-id 192.168.40.181[root@test-server-03 ~]# ssh-copy-id 192.168.40.182[root@test-server-03 ~]# ssh-copy-id 192.168.40.183#MySQL Slave 2[root@test-server-04 ~]# ssh-keygen -t rsa[root@test-server-04 ~]# ssh-copy-id 192.168.40.180[root@test-server-04 ~]# ssh-copy-id 192.168.40.181[root@test-server-04 ~]# ssh-copy-id 192.168.40.182[root@test-server-04 ~]# ssh-copy-id 192.168.40.183关闭防火墙12#所有机器 [root@test-server-01 ~]# systemctl stop firewalld ; systemctl disable firewalld配置时间同步12345678910#所有机器#安装ntpdate命令[root@test-server-01 ~]# yum install ntpdate -y#跟网络时间做同步[root@test-server-01 ~]# ntpdate cn.pool.ntp.org#把时间同步做成计划任务[root@test-server-01 ~]# crontab -e* * * * * /usr/sbin/ntpdate cn.pool.ntp.orgMySQL集群部署MySQL组件下载地址选择对应版本下载：安装MySQL集群三台MySQL节点操作1.上传安装包到/usr/local/src目录12[root@test-server02 src]# ll mysql-5.7.20-linux-glibc2.12-x86_64.tar.gz-rw-r--r-- 1 root root 641127384 Feb 19 13:39 mysql-5.7.20-linux-glibc2.12-x86_64.tar.gz2.解压安装包1[root@test-server02 src]# tar -xvf mysql-5.7.20-linux-glibc2.12-x86_64.tar.gz3.移动安装包，重新命名1[root@test-server02 src]# mv mysql-5.7.20-linux-glibc2.12-x86_64 /usr/local/mysql5.74.设置路径环境变量12[root@test-server02 src]# echo 'export PATH=/usr/local/mysql5.7/bin:$PATH' &gt;&gt; /etc/profile[root@test-server02 src]# source /etc/profile5.创建MySQL用户1[root@test-server02 src]# useradd -M -s /sbin/nologin mysql6.创建MySQL数据目录1[root@test-server02 src]# mkdir -pv /opt/mysqldb/&#123;temp,log,data&#125;7.赋予MySQL用户权限1[root@test-server02 src]# chown -R mysql:mysql /opt/mysqldb8.初始化MySQL数据目录123456789101112[root@test-server02 src]# mysqld --initialize --datadir=/opt/mysqldb/data --user=mysql2025-02-19T06:40:36.493733Z 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details).2025-02-19T06:40:36.494435Z 0 [ERROR] Can't find error-message file '/usr/local/mysql/share/errmsg.sys'. Check error-message file location and 'lc-messages-dir' configuration directive.2025-02-19T06:40:37.775262Z 0 [Warning] InnoDB: New log files created, LSN=457902025-02-19T06:40:37.983917Z 0 [Warning] InnoDB: Creating foreign key constraint system tables.2025-02-19T06:40:38.309684Z 0 [Warning] No existing UUID has been found, so we assume that this is the first time that this server has been started. Generating a new UUID: 6e2db754-ee8c-11ef-b1a1-000c292b8fdc.2025-02-19T06:40:38.375782Z 0 [Warning] Gtid table is not ready to be used. Table 'mysql.gtid_executed' cannot be opened.2025-02-19T06:40:38.378205Z 1 [Note] A temporary password is generated for root@localhost: n-HaNrreh2_t#ERROR不需要管，出现同上日志代表初始化成功。n-HaNrreh2_t：MySQL初始化之后的密码，需要保存下来等会使用9.配置MySQL123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172#MySQL Master配置[root@test-server02 src]# cat /etc/my.cnf[mysqld]basedir=/usr/local/mysql5.7user=mysqlport=3306datadir=/opt/mysqldb/datalog-error=/opt/mysqldb/log/err.logpid-file=/opt/mysqldb/temp/mysqld.pidsocket=/opt/mysqldb/temp/mysqld.socksymbolic-links=0server_id=1gtid-mode=onenforce-gtid-consistency=truerelay_log_purge=0log_slave_updates=ONlog_bin=/opt/mysqldb/log/binlogbinlog_format=ROW[client]socket=/opt/mysqldb/temp/mysqld.sockdefault-character-set=utf8#MySQL Slave1配置[root@test-server03 src]# cat /etc/my.cnf[mysqld]basedir=/usr/local/mysql5.7user=mysqlport=3306datadir=/opt/mysqldb/datalog-error=/opt/mysqldb/log/err.logpid-file=/opt/mysqldb/temp/mysqld.pidsocket=/opt/mysqldb/temp/mysqld.socksymbolic-links=0server_id=2gtid-mode=onenforce-gtid-consistency=truerelay_log_purge=0log_slave_updates=ONlog_bin=/opt/mysqldb/log/binlogbinlog_format=ROW[client]socket=/opt/mysqldb/temp/mysqld.sockdefault-character-set=utf8#MySQL Slave2配置[root@test-server04 src]# cat /etc/my.cnf[mysqld]basedir=/usr/local/mysql5.7user=mysqlport=3306datadir=/opt/mysqldb/datalog-error=/opt/mysqldb/log/err.logpid-file=/opt/mysqldb/temp/mysqld.pidsocket=/opt/mysqldb/temp/mysqld.socksymbolic-links=0server_id=3gtid-mode=onenforce-gtid-consistency=truerelay_log_purge=0log_slave_updates=ONlog_bin=/opt/mysqldb/log/binlogbinlog_format=ROW[client]socket=/opt/mysqldb/temp/mysqld.sockdefault-character-set=utf8#server_id三台不能相同，数据目录改成自己的10.创建MySQL启动文件123456789101112131415161718192021222324252627282930313233343536#三台MySQL节点操作[root@test-server02 src]# cat /usr/lib/systemd/system/mysqld.service[Unit]Description=MySQL ServerDocumentation=man:mysqld(8)Documentation=http://dev.mysql.com/doc/refman/en/using-systemd.htmlAfter=network.targetAfter=syslog.target[Install]WantedBy=multi-user.target[Service]User=mysqlGroup=mysqlType=forkingPIDFile=/opt/mysqldb/temp/mysqld.pid# Disable service start and stop timeout logic of systemd for mysqld service.TimeoutSec=0# Execute pre and post scripts as rootPermissionsStartOnly=true# Needed to create system tables# ExecStartPre=/usr/bin/mysqld_pre_systemd# Start main serviceExecStart=/usr/local/mysql5.7/bin/mysqld --daemonize --pid-file=/opt/mysqldb/temp/mysqld.pid $MYSQLD_OPTS# Use this to switch malloc implementation# EnvironmentFile=-/etc/sysconfig/mysql# Sets open_files_limitLimitNOFILE = 5000Restart=on-failureRestartPreventExitStatus=1PrivateTmp=false#目录根据自己情况修改#赋予执行权限[root@test-server02 src]# chmod 644 /usr/lib/systemd/system/mysqld.service11.启动MySQL12345678910111213#三台MySQL节点操作#systemd重新加载配置[root@test-server02 src]# systemctl daemon-reload#设置开机自启[root@test-server02 src]# systemctl enable mysqldCreated symlink from /etc/systemd/system/multi-user.target.wants/mysqld.service to /usr/lib/systemd/system/mysqld.service.#重启MySQL[root@test-server02 src]# systemctl restart mysqld#查看MySQL状态[root@test-server02 src]# systemctl status mysqld12.修改MySQL密码12345#登录MySQL[root@test-server02 src]# mysql -uroot -pn-HaNrreh2_t #每台MySQL的密码不同#三台MySQL节点操作mysql&gt; ALTER USER 'root'@'localhost' IDENTIFIED BY '123456';配置一主两从1.授予权限三台MySQL节点操作1234567#授予该用户进行主从复制所需的权限mysql&gt; grant replication slave on *.* to 'repl_user'@'192.168.40.%' identified by '123456';Query OK, 0 rows affected, 1 warning (0.00 sec)#刷新mysql&gt; flush privileges;Query OK, 0 rows affected (0.01 sec)2.查看二进制文件名和位置信息master节点操作123456mysql&gt; show master status;+---------------+----------+--------------+------------------+------------------------------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |+---------------+----------+--------------+------------------+------------------------------------------+| binlog.000001 | 1467 | | | 6e2db754-ee8c-11ef-b1a1-000c292b8fdc:1-6 |+---------------+----------+--------------+------------------+------------------------------------------+3.配置从节点Slave 1和Slave 2执行1234567891011121314151617181920212223#根据自己的信息进行修改mysql&gt; change master to master_host='192.168.40.181',master_user='repl_user',master_password='123456',master_log_file='binlog.000001',master_log_pos=1467;Query OK, 0 rows affected, 2 warnings (0.03 sec)mysql&gt; start slave;Query OK, 0 rows affected (0.01 sec)mysql&gt; show slave status \G;*************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 192.168.40.181 Master_User: repl_user Master_Port: 3306 Connect_Retry: 60 Master_Log_File: binlog.000001 Read_Master_Log_Pos: 1467 Relay_Log_File: test-server03-relay-bin.000002 Relay_Log_Pos: 317 Relay_Master_Log_File: binlog.000001 Slave_IO_Running: Yes Slave_SQL_Running: Yes#两个YES代表MySQL已经成功设置成了主从4.读写分离Slave 1和Slave 2执行123#两个从库必须设置为只读模式：mysql&gt; set global read_only=1;Query OK, 0 rows affected (0.00 sec)测试MySQL主从MySQL Master操作1234567891011CREATE DATABASE test_db;USE test_db;CREATE TABLE test_table ( id INT PRIMARY KEY, name VARCHAR(50));INSERT INTO test_table (id, name) VALUES (1, 'John');INSERT INTO test_table (id, name) VALUES (2, 'Alice');MySQL Slave 1和Slave 2操作123456789mysql&gt; select * from test_db.test_table;+----+-------+| id | name |+----+-------+| 1 | John || 2 | Alice |+----+-------+2 rows in set (0.00 sec)#出来数据代表主从成功赋予MHA用户连接权限MySQL Master操作即可1234mysql&gt; grant all privileges on *.* to 'mha_user'@'192.168.40.%' identified by '123456';Query OK, 0 rows affected, 1 warning (0.00 sec)#主从复制已完成，所以从不用单独操作安装MHA环境安装MHA依赖环境12345678910#四台机器全部操作[root@test-server-01 ~]# yum install epel-release --nogpgcheck -y[root@test-server-01 ~]# yum install -y perl-DBD-MySQL \perl-Config-Tiny \perl-Log-Dispatch \perl-Parallel-ForkManager \perl-ExtUtils-CBuilder \perl-ExtUtils-MakeMaker \perl-CPAN安装MHA NodeMHA Node下载地址四台机器全部安装MHA Node1.上传部署包到/opt目录12[root@test-server-01 opt]# ll mha4mysql-node-0.58.tar.gz-rw-r--r-- 1 root root 56220 Feb 20 09:09 mha4mysql-node-0.58.tar.gz2.生成Makefile123456789101112[root@test-server-01 opt]# tar -xf mha4mysql-node-0.58.tar.gz[root@test-server-01 opt]# cd mha4mysql-node-0.58[root@test-server-01 mha4mysql-node-0.58]# perl Makefile.PL*** Module::AutoInstall version 1.06*** Checking for Perl dependencies...[Core Features]- DBI ...loaded. (1.627)- DBD::mysql ...loaded. (4.023)*** Module::AutoInstall configuration finished.Checking if your kit is complete...Looks goodWriting Makefile for mha4mysql::node3.编译安装123456789101112[root@test-server-01 mha4mysql-node-0.58]# make &amp;&amp; make install#安装完成之后 /usr/local/bin会生成四个文件apply_diff_relay_logsfilter_mysqlbinlogpurge_relay_logssave_binary_logs1.apply_diff_relay_logs #识别差异的中继日志事件并将其差异的事件应用于其他的 slave2.filter_mysqlbinlog #去除不必要的 ROLLBACK 事件（MHA 已不再使用这个工具）3.purge_relay_logs #清除中继日志（不会阻塞 SQL 线程）4.save_binary_logs #保存和复制 master 的二进制日志安装MHA ManagerMHA Manager下载地址MHA Manager节点操作1.上传部署包到opt目录12[root@test-server-01 opt]# ll mha4mysql-manager-0.58.tar.gz-rw-r--r-- 1 root root 119801 Feb 20 09:37 mha4mysql-manager-0.58.tar.gz2.生成Makefile1234567891011121314151617[root@test-server-01 opt]# tar -xf mha4mysql-manager-0.58.tar.gz[root@test-server-01 opt]# cd mha4mysql-manager-0.58[root@test-server-01 mha4mysql-manager-0.58]# perl Makefile.PL*** Module::AutoInstall version 1.06*** Checking for Perl dependencies...[Core Features]- DBI ...loaded. (1.627)- DBD::mysql ...loaded. (4.023)- Time::HiRes ...loaded. (1.9725)- Config::Tiny ...loaded. (2.14)- Log::Dispatch ...loaded. (2.41)- Parallel::ForkManager ...loaded. (1.18)- MHA::NodeConst ...loaded. (0.58)*** Module::AutoInstall configuration finished.Checking if your kit is complete...Looks goodWriting Makefile for mha4mysql::manager3.编译安装12345678910111213141516171819202122[root@test-server-01 mha4mysql-manager-0.58]# make &amp;&amp; make install#安装完成之后 /usr/local/bin会生成几个文件masterha_check_replmasterha_check_sshmasterha_check_statusmasterha_conf_hostmasterha_managermasterha_master_monitormasterha_master_switchmasterha_secondary_checkmasterha_stop1. masterha_check_repl：用于检查 MySQL 主从复制的状态，确保主从复制正常运行并进行必要的修复操作。2. masterha_check_ssh：检查 MySQL 主从节点之间的 SSH 连接状态，确保 SSH 连接正常，这对 MHA 工具的正常操作至关重要。3. masterha_check_status：用于检查 MySQL 主从复制环境的状态，包括检查主从复制延迟等信息。4. masterha_conf_host：用于配置 MHA 工具中的主机信息，包括主从节点的连接信息等。5. masterha_manager：主要的 MHA 工具，用于监控和管理 MySQL 主从复制环境的自动故障转移和故障恢复。6. masterha_master_monitor：用于监控 MySQL 主服务器的状态，以及检测主服务器是否发生故障。7. masterha_master_switch：用于手动切换 MySQL 主从复制环境中的主服务器，执行主服务器的切换操作。8. masterha_secondary_check：用于检查 MySQL 主从复制中的辅助节点（Secondary）状态，确保辅助节点正常。9. masterha_stop：用于停止 MHA 工具或相关服务的操作。配置MHA环境1.复制相关脚本到/usr/local/bin目录123456789101112[root@test-server-01 mha4mysql-manager-0.58]# cp -r samples/scripts /usr/local/bin/[root@test-server-01 mha4mysql-manager-0.58]# ll /usr/local/bin/scripts/total 32-rwxr-xr-x 1 root root 3648 Feb 20 09:52 master_ip_failover-rwxr-xr-x 1 root root 9870 Feb 20 09:52 master_ip_online_change-rwxr-xr-x 1 root root 11867 Feb 20 09:52 power_manager-rwxr-xr-x 1 root root 1360 Feb 20 09:52 send_report1.master_ip_failover #自动切换时 VIP 管理的脚本2.master_ip_online_change #在线切换时 vip 的管理3.power_manager #故障发生后关闭主机的脚本4.send_report #因故障切换后发送报警的脚本2.复制自动切换VIP管理脚本到/usr/local/bin目录。使用该脚本管理VIP123[root@test-server-01 mha4mysql-manager-0.58]# cp -r /usr/local/bin/scripts/master_ip_failover /usr/local/bin/[root@test-server-01 mha4mysql-manager-0.58]# cp -r /usr/local/bin/scripts/master_ip_online_change /usr/local/bin/#master_ip_online_change脚本没有修改不知道会不会用到，但是下面配置中指定了该路径3.修改脚本内容12my $ssh_start_vip = "/sbin/ifconfig $ifdev:$key $vip";my $ssh_stop_vip = "/sbin/ifconfig $ifdev:$key down";上面代码中的$ifdev一开始是直接写的网卡名，发现之后直接改成了变量名但是没做实验，vip如果飘逸不成功可以从这地方排查注：只需要修改$vip $brdc $ifdev即可1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374[root@test-server-01 mha4mysql-manager-0.58]# cd /usr/local/bin/#删除原有的 复制下面的直接粘贴即可。VIP根据自己的网段修改#粘贴的时候记得 vim 编辑文件之后：Esc :set paste之后再进行粘贴[root@test-server-01 bin]# vim master_ip_failover#!/usr/bin/env perluse strict;use warnings FATAL =&gt; 'all';use Getopt::Long;my ($command, $ssh_user, $orig_master_host, $orig_master_ip, $orig_master_port, $new_master_host, $new_master_ip, $new_master_port);my $vip = '192.168.40.200';my $brdc = '192.168.40.255';my $ifdev = 'ens33';my $key = '1';my $ssh_start_vip = "/sbin/ifconfig $ifdev:$key $vip";my $ssh_stop_vip = "/sbin/ifconfig $ifdev:$key down";my $exit_code = 0;GetOptions( 'command=s' =&gt; \$command, 'ssh_user=s' =&gt; \$ssh_user, 'orig_master_host=s' =&gt; \$orig_master_host, 'orig_master_ip=s' =&gt; \$orig_master_ip, 'orig_master_port=i' =&gt; \$orig_master_port, 'new_master_host=s' =&gt; \$new_master_host, 'new_master_ip=s' =&gt; \$new_master_ip, 'new_master_port=i' =&gt; \$new_master_port,);exit &amp;main();sub main &#123; print "\n\nIN SCRIPT TEST====$ssh_stop_vip==$ssh_start_vip===\n\n"; if ($command eq "stop" || $command eq "stopssh") &#123; my $exit_code = 1; eval &#123; print "Disabling the VIP on old master: $orig_master_host\n"; &amp;stop_vip(); $exit_code = 0; &#125;; if ($@) &#123; warn "Got Error: $@\n"; exit $exit_code; &#125; exit $exit_code; &#125; elsif ($command eq "start") &#123; my $exit_code = 10; eval &#123; print "Enabling the VIP - $vip on the new master - $new_master_host\n"; &amp;start_vip(); $exit_code = 0; &#125;; if ($@) &#123; warn $@; exit $exit_code; &#125; exit $exit_code; &#125; elsif ($command eq "status") &#123; print "Checking the Status of the script.. OK\n"; exit 0; &#125; else &#123; &amp;usage(); exit 1; &#125;&#125;sub start_vip() &#123; `ssh $ssh_user\@$new_master_host "$ssh_start_vip"`;&#125;sub stop_vip() &#123; `ssh $ssh_user\@$orig_master_host "$ssh_stop_vip"`;&#125;sub usage &#123; print "Usage: master_ip_failover --command=start|stop|stopssh|status --orig_master_host=host --orig_master_ip=ip --orig_master_port=port --new_master_host=host --new_master_ip=ip --new_master_port=port\n";&#125;4.创建MHA目录并拷贝文件master_binlog_dir=/opt/mysqldb/data 是MySQL二进制日志文件目录，该目录写错会导致MySQL MHA集群VIP飘逸失败1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465[root@test-server-01 bin]# mkdir /etc/masterha[root@test-server-01 bin]# cp -r /opt/mha4mysql-manager-0.58/samples/conf/app1.cnf /etc/masterha/[root@test-server-01 bin]# cat /etc/masterha/app1.cnf[server default]manager_log=/etc/masterha/manager.logmanager_workdir=/etc/masterha/mhamaster_binlog_dir=/opt/mysqldb/data #指定 MySQL 主服务器的二进制日志文件目录。master_ip_failover_script=/usr/local/bin/master_ip_failovermaster_ip_online_change_script=/usr/local/bin/master_ip_online_changeuser=mha_userpassword=123456port=3306ping_interval=1remote_workdir=/etc/masterha/mha-noderepl_user=repl_userrepl_password=123456secondary_check_script=/usr/local/bin/masterha_secondary_check -s 192.168.40.182 -s 192.168.40.183shutdown_script=""ssh_user=root[server1]hostname=192.168.40.181port=3306[server2]candidate_master=1check_repl_delay=0hostname=192.168.40.182port=3306[server3]hostname=192.168.40.183port=3306#配置解释1.[server default]：这是默认服务器配置部分，包含了 MHA 管理器的相关配置。manager_log：指定 MHA 管理器的日志文件路径。manager_workdir：指定 MHA 管理器的工作目录。master_binlog_dir：指定 MySQL 主服务器的二进制日志文件目录。master_ip_failover_script：指定 IP 切换脚本，用于在主服务器发生故障时执行 IP 切换。master_ip_online_change_script：指定在线 IP 变更脚本。user：MHA 用户名。password：MHA 用户密码。port：MySQL 端口号。ping_interval：指定 MHA 管理器检测 MySQL 高可用的间隔时间。remote_workdir：指定节点服务器的工作目录。repl_user：MySQL 复制用户。repl_password：MySQL 复制用户密码。secondary_check_script：指定用于检查备用服务器的脚本。shutdown_script：指定关闭脚本。ssh_user：指定用于 SSH 连接的用户名。2.[server1]：定义了一个 MySQL 服务器节点，其中包含：hostname：主 MySQL 服务器的 IP 地址。port：MySQL 端口号。3.[server2]：定义了另一个 MySQL 服务器节点，标记为候选主服务器，包含：candidate_master：标记此服务器为候选主服务器。check_repl_delay：检查复制延迟设置为 0。hostname：第二个 MySQL 服务器的 IP 地址。port：MySQL 端口号。4.[server3]：定义了另一个 MySQL 服务器节点，其中包含：hostname：第三个 MySQL 服务器的 IP 地址。port：MySQL 端口号。5.MySQL Master机器手动开启VIP12345678910111213141516171819202122232425262728#没有ifconfig提前安装,全部机器安装[root@test-server02 ~]# yum install net-tools -y#Master执行[root@test-server02 ~]# /sbin/ifconfig ens33:1 192.168.40.200/24You have new mail in /var/spool/mail/root[root@test-server02 ~]# ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:0c:29:2b:8f:dc brd ff:ff:ff:ff:ff:ff inet 192.168.40.181/24 brd 192.168.40.255 scope global noprefixroute ens33 valid_lft forever preferred_lft forever inet 192.168.40.200/24 brd 192.168.40.255 scope global secondary ens33:1 valid_lft forever preferred_lft forever inet6 fe80::eb22:4e3:6568:88de/64 scope link noprefixroute valid_lft forever preferred_lft forever inet6 fe80::38d1:dfef:9158:a02b/64 scope link tentative noprefixroute dadfailed valid_lft forever preferred_lft forever3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default link/ether 02:42:d0:ec:ae:7f brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever #192.168.40.200VIP 已经出现6.MHA Manager测试12345678910111.Manager节点测试ssh免密[root@test-server-01 ~]# masterha_check_ssh -conf=/etc/masterha/app1.cnfThu Feb 20 10:53:10 2025 - [info] All SSH connection tests passed successfully.#最后以上输出以上内容代表正常2.Manager节点测试MySQL主从状态[root@test-server-01 ~]# masterha_check_repl -conf=/etc/masterha/app1.cnfMySQL Replication Health is OK.#最后以上输出以上内容代表正常7.启动MHA Manager12345678#启动MHA[root@test-server-01 ~]# nohup masterha_manager --conf=/etc/masterha/app1.cnf --remove_dead_master_conf --ignore_last_failover &lt; /dev/null &gt; /etc/masterha/manager.log 2&gt;&amp;1 &amp;[1] 10828#查看MHA状态[root@test-server-01 ~]# masterha_check_status --conf=/etc/masterha/app1.cnfapp1 (pid:10828) is running(0:PING_OK), master:192.168.40.181#启动成功MySQL MHA高可用集群测试通过VIP连接MySQL123456789101112131415161718192021222324252627#通过VIP连接数据库[root@test-server-01 ~]# mysql -h192.168.40.200 -umha_user -p123456Welcome to the MariaDB monitor. Commands end with ; or \g.Your MySQL connection id is 18Server version: 5.7.20-log MySQL Community Server (GPL)Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.#Master创建数据库MySQL [(none)]&gt; create database test;Query OK, 1 row affected (0.01 sec)#Slaver 1和Slaver 2查看mysql&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || mysql || performance_schema || sys || test || test_db |+--------------------+6 rows in set (0.01 sec)#成功模拟故障测试1.查看manager日志，观察manager状态1[root@test-server-01 ~]# tailf -n100 /etc/masterha/manager.log2.停止Master MySQL1[root@test-server02 ~]# systemctl stop mysqld3.Slave 1节点查看vip是否飘逸12345678910111213141516171819202122232425[root@test-server-03 ~]# ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:0c:29:b9:50:43 brd ff:ff:ff:ff:ff:ff inet 192.168.40.182/24 brd 192.168.40.255 scope global noprefixroute ens33 valid_lft forever preferred_lft forever inet 192.168.40.200/24 brd 192.168.40.255 scope global secondary ens33:1 valid_lft forever preferred_lft forever inet6 fe80::eb22:4e3:6568:88de/64 scope link tentative noprefixroute dadfailed valid_lft forever preferred_lft forever inet6 fe80::38d1:dfef:9158:a02b/64 scope link tentative noprefixroute dadfailed valid_lft forever preferred_lft forever inet6 fe80::1c4c:acc5:6453:90f3/64 scope link tentative noprefixroute dadfailed valid_lft forever preferred_lft forever3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default link/ether 02:42:56:61:72:fd brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever #此时发现vip 200已经飘逸到了182节点 这个是slave 1的节点4.manager日志12345678910111213#manager日志中最下面出现下方日志就证明已经切换成功了----- Failover Report -----app1: MySQL Master failover 192.168.40.181(192.168.40.181:3306) to 192.168.40.182(192.168.40.182:3306) succeededMaster 192.168.40.181(192.168.40.181:3306) is down!Check MHA Manager logs at test-server-01:/etc/masterha/manager.log for details.Started automated(non-interactive) failover.Invalidated master IP address on 192.168.40.181(192.168.40.181:3306)Selected 192.168.40.182(192.168.40.182:3306) as a new master.192.168.40.182(192.168.40.182:3306): OK: Applying all logs succeeded.192.168.40.182(192.168.40.182:3306): OK: Activated master IP address.192.168.40.183(192.168.40.183:3306): OK: Slave started, replicating from 192.168.40.182(192.168.40.182:3306)192.168.40.182(192.168.40.182:3306): Resetting slave info succeeded.Master failover to 192.168.40.182(192.168.40.182:3306) completed successfully.5.查看MySQL状态登录MySQL Slave2查看从状态1234567891011121314151617[root@test-server-04 ~]# mysql -uroot -p123456mysql&gt; show slave status \G;*************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 192.168.40.182 Master_User: repl_user Master_Port: 3306 Connect_Retry: 60 Master_Log_File: binlog.000002 Read_Master_Log_Pos: 1700 Relay_Log_File: test-server-04-relay-bin.000003 Relay_Log_Pos: 744 Relay_Master_Log_File: binlog.000002 Slave_IO_Running: Yes Slave_SQL_Running: Yes#Master_Host已经成功由192.168.40.181切换成了192.168.40.181。到此MHA集群已经成功搭建了6.故障修复恢复MySQL1[root@test-server-02 ~]# systemctl start mysqld主数据库查看master状态12345678910111213141516171819202122232425262728293031323334353637mysql&gt; show master status;+---------------+----------+--------------+------------------+------------------------------------------------------------------------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |+---------------+----------+--------------+------------------+------------------------------------------------------------------------------------+| binlog.000002 | 1700 | | | 7a69b6e0-eff7-11ef-b307-000c292b8fdc:3-7,7a7fcdf5-eff7-11ef-ab74-000c29b95043:1-2 |+---------------+----------+--------------+------------------+------------------------------------------------------------------------------------+1 row in set (0.00 sec)#正常写入写入数据的情况下Position会一直变，可以和研发沟通空闲时间进行锁库进行配置1. 锁库命令：FLUSH TABLE WITH READ LOCK;2. 解锁命令： unlock tables;#配置完解开即可#原master做成Slave数据库mysql&gt; change master to master_host='192.168.40.182',master_user='repl_user',master_password='123456',master_log_file='binlog.000002',master_log_pos=1700;Query OK, 0 rows affected, 2 warnings (0.03 sec)mysql&gt; start slave;Query OK, 0 rows affected (0.01 sec)mysql&gt; show slave status \G;*************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 192.168.40.182 Master_User: repl_user Master_Port: 3306 Connect_Retry: 60 Master_Log_File: binlog.000002 Read_Master_Log_Pos: 1700 Relay_Log_File: test-server-02-relay-bin.000002 Relay_Log_Pos: 317 Relay_Master_Log_File: binlog.000002 Slave_IO_Running: Yes Slave_SQL_Running: Yes#已经成功变成了Slave,并且成功连接到了新的Master.7.修改MHA配置由于停掉了MySQL， /etc/masterha/app1.cnf配置里面默认删除一开始定义的[server1]，现在新增一下12345678910111213141516171819202122232425262728293031[root@test-server-01 masterha]# cat /etc/masterha/app1.cnf[server default]manager_log=/etc/masterha/manager.logmanager_workdir=/etc/masterha/mhamaster_binlog_dir=/opt/mysqldb/datamaster_ip_failover_script=/usr/local/bin/master_ip_failovermaster_ip_online_change_script=/usr/local/bin/master_ip_online_changeuser=mha_userpassword=123456port=3306ping_interval=1remote_workdir=/etc/masterha/mha-noderepl_user=repl_userrepl_password=123456secondary_check_script=/usr/local/bin/masterha_secondary_check -s 192.168.40.182 -s 192.168.40.183shutdown_script=""ssh_user=root[server1]hostname=192.168.40.181port=3306[server2]candidate_master=1check_repl_delay=0hostname=192.168.40.182port=3306[server3]hostname=192.168.40.183port=33068.MHA测试12345678910111.Manager节点测试ssh免密[root@test-server-01 ~]# masterha_check_ssh -conf=/etc/masterha/app1.cnfThu Feb 20 10:53:10 2025 - [info] All SSH connection tests passed successfully.#最后以上输出以上内容代表正常2.Manager节点测试MySQL主从状态[root@test-server-01 ~]# masterha_check_repl -conf=/etc/masterha/app1.cnfMySQL Replication Health is OK.#最后以上输出以上内容代表正常9.启动manager12#启动MHA[root@test-server-01 ~]# nohup masterha_manager --conf=/etc/masterha/app1.cnf --remove_dead_master_conf --ignore_last_failover &lt; /dev/null &gt; /etc/masterha/manager.log 2&gt;&amp;1 &amp;到这部署就成功了。]]></content>
      <categories>
        <category>技术</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>MHA</tag>
        <tag>MySQL高可用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[crane-scheduler基于真实负载进行k8s调度]]></title>
    <url>%2F2025%2F10df35fc4.html</url>
    <content type="text"><![CDATA[原生 kubernetes 调度器只能基于资源的 resource request 进行调度，然而 Pod 的真实资源使用率，往往与其所申请资源的 request/limit 差异很大，导致集群负载不均的问题。背景将服务部署在Kubernetes集群上是当今许多企业的首选方案，其能帮助企业自动化部署、弹性伸缩以及容错处理等工作，减少了人工操作和维护工作量，提高了服务的可靠性和稳定性，有效实现了降本增效。但kubernetes 的原生调度器只能通过资源请求来调度 pod，这很容易造成一系列负载不均的问题：集群中的部分节点，资源的真实使用率远低于 resource request，却没有被调度更多的 Pod，这造成了比较大的资源浪费。而集群中的另外一些节点，其资源的真实使用率事实上已经过载，却无法为调度器所感知到，这极大可能影响到业务的稳定性。这些无疑都与企业上云的最初目的相悖，为业务投入了足够的资源，却没有达到理想的效果。crane-scheduler打破了资源 resource request 与真实使用率之间的鸿沟，着力于调度层面，让调度器直接基于真实使用率进行调度。crane-scheduler基于集群的真实负载数据构造了一个简单却有效的模型，作用于调度过程中的 Filter 与 Score 阶段，并提供了一种灵活的调度策略配置方式，从而有效缓解集群中资源负载不均问题，真正实现将本增效。调度框架Kubernetes 调度框架Kubernetes官方提供了可插拔架构的调度框架，能够进一步扩展Kubernetes调度器，下图展示了调度框架中的调度上下文及其中的扩展点，一个扩展可以注册多个扩展点，以便可以执行更复杂的有状态的任务。如上图，Pod 调度流程：123456789101112Sort - 用于对 Pod 的待调度队列进行排序，以决定先调度哪个 PodPre-filter - 用于对 Pod 的信息进行预处理Filter - 用于排除那些不能运行该 Pod 的节点Post-filter - 一个通知类型的扩展点,更新内部状态，或者产生日志Scoring - 用于为所有可选节点进行打分Normalize scoring - 在调度器对节点进行最终排序之前修改每个节点的评分结果Reserve - 使用该扩展点获得节点上为 Pod 预留的资源，该事件发生在调度器将 Pod 绑定到节点前Permit - 用于阻止或者延迟 Pod 与节点的绑定Pre-bind - 用于在 Pod 绑定之前执行某些逻辑Bind - 用于将 Pod 绑定到节点上Post-bind - 是一个通知性质的扩展Unreserve - 如果为 Pod 预留资源，又在被绑定过程中被拒绝绑定，则将被调用crane-scheduler调度框架Crane-scheduler 总体架构：动态调度器总体架构如上图所示，主要有两个组件组成：Node-annotator定期从 Prometheus 拉取数据，并以注释的形式在节点上用时间戳标记它们。Dynamic plugin直接从节点的注释中读取负载数据，过滤并基于简单的算法对候选节点进行评分。同时动态调度器提供了一个默认值调度策略并支持用户自定义策略。默认策略依赖于以下指标：123456cpu_usage_avg_5mcpu_usage_max_avg_1hcpu_usage_max_avg_1dmem_usage_avg_5mmem_usage_max_avg_1hmem_usage_max_avg_1d在调度的Filter阶段，如果该节点的实际使用率大于上述任一指标的阈值，则该节点将被过滤。而在Score阶段，最终得分是这些指标值的加权和。在生产集群中，可能会频繁出现调度热点，因为创建 Pod 后节点的负载不能立即增加。因此定义了一个额外的指标，名为Hot Value，表示节点最近几次的调度频率。并且节点的最终优先级是最终得分减去Hot Value。github项目地址：GitHub - gocrane/crane-scheduler: Crane scheduler is a Kubernetes scheduler which can schedule pod based on actual node load.方案一：部署crane-scheduler配置Prometheus Rules生成新的查询表达式：12345678910111213141516171819202122232425262728293031323334353637383940apiVersion: monitoring.coreos.com/v1kind: PrometheusRulemetadata: name: example-record labels: prometheus: k8s role: alert-rulesspec: groups: - name: cpu_mem_usage_active interval: 30s rules: - record: cpu_usage_active expr: 100 - (avg by (instance) (irate(node_cpu_seconds_total&#123;mode="idle"&#125;[30s])) * 100) - record: mem_usage_active expr: 100*(1-node_memory_MemAvailable_bytes/node_memory_MemTotal_bytes) - name: cpu-usage-5m interval: 5m rules: - record: cpu_usage_max_avg_1h expr: max_over_time(cpu_usage_avg_5m[1h]) - record: cpu_usage_max_avg_1d expr: max_over_time(cpu_usage_avg_5m[1d]) - name: cpu-usage-1m interval: 1m rules: - record: cpu_usage_avg_5m expr: avg_over_time(cpu_usage_active[5m]) - name: mem-usage-5m interval: 5m rules: - record: mem_usage_max_avg_1h expr: max_over_time(mem_usage_avg_5m[1h]) - record: mem_usage_max_avg_1d expr: max_over_time(mem_usage_avg_5m[1d]) - name: mem-usage-1m interval: 1m rules: - record: mem_usage_avg_5m expr: avg_over_time(mem_usage_active[5m])注：Prometheus 的采样间隔必须小于30秒，不然可能会导致规则无法正常生效。如：cpu_usage_active。或者把采集时间加大：12345- name: cpu_mem_usage_active interval: 1m rules: - record: cpu_usage_active expr: 100 - (avg by (instance) (irate(node_cpu_seconds_total&#123;mode="idle"&#125;[2m])) * 100)安装 Crane-scheduler12helm repo add crane https://gocrane.github.io/helm-chartshelm install scheduler -n crane-system --create-namespace --set global.prometheusAddr="http://prometheus-k8s.kubesphere-monitoring-system.svc:9090" crane/scheduler安装完成后会创建两个deploy：调度规则：1234syncPolicy: 用户可以自定义负载数据的类型与拉取周期；predicate: Filter 策略，若候选节点的当前负载数据超过了任一所配置的指标阈值，则这个节点将会被过滤；priority：在 Score 策略中配置相关指标的权重，候选节点的最终得分为不同指标得分的加权和；hotValue：定义调度热点规则，最终节点的 Priority 为上一小节中的 Score 减去 Hot Value使用Crane-scheduler这里有两种方式可供选择：作为k8s原生调度器之外的第二个调度器替代k8s原生调度器成为默认的调度器作为k8s原生调度器之外的第二个调度器在 pod spec.schedulerName 指定 crane-scheduler：12345678910111213141516171819202122232425262728293031apiVersion: apps/v1kind: Deploymentmetadata: name: cpu-stressspec: selector: matchLabels: app: cpu-stress replicas: 1 template: metadata: labels: app: cpu-stress spec: schedulerName: crane-scheduler hostNetwork: true tolerations: - key: node.kubernetes.io/network-unavailable operator: Exists effect: NoSchedule containers: - name: stress image: docker.io/gocrane/stress:latest command: ["stress", "-c", "1"] resources: requests: memory: "1Gi" cpu: "1" limits: memory: "1Gi" cpu: "1"替代k8s原生调度器成为默认的调度器1.修改kube调度器的配置文件（scheduler config.yaml）以启用动态调度器插件并配置插件参数：123456789101112131415161718apiVersion: kubescheduler.config.k8s.io/v1beta2kind: KubeSchedulerConfiguration...profiles:- schedulerName: default-scheduler plugins: filter: enabled: - name: Dynamic score: enabled: - name: Dynamic weight: 3 pluginConfig: - name: Dynamic args: policyConfigPath: /etc/kubernetes/policy.yaml...2.修改kube-scheduler.yaml，并将kube调度器映像替换为Crane schedule:123... image: docker.io/gocrane/crane-scheduler:0.0.23...3.安装crane-scheduler-controller：1kubectl apply ./deploy/controller/rbac.yaml &amp;&amp; kubectl apply -f ./deploy/controller/deployment.yaml这里使用k8s原生调度器之外的第二个调度器，在yaml文件指定schedulerName: crane-scheduler。crane-sheduler 会将监控指标数据写在 node annotation 上，可以通过kubectl describe nodes 查看：创建一个pod，看是否调度成功：看实际效果：之前没使用crane-sheduler之前，内存使用不均衡，有的node内存使用率五十，有的node已经到了快九十（忘记截图了），使用之后会趋向均衡：问题以上的部署是在内网环境测试的，k8s版本是1.22，在生产环境部署的时候就报错了，生产环境的k8s版本是1.27crane-sheduler报错日志：123W1017 08:34:15.712878 1 reflector.go:324] pkg/mod/k8s.io/client-go@v0.23.3/tools/cache/reflector.go:167: failed to list *v1beta1.CSIStorageCapacity: the server could not find the requested resource E1017 08:34:15.712901 1 reflector.go:138] pkg/mod/k8s.io/client-go@v0.23.3/tools/cache/reflector.go:167: Failed to watch *v1beta1.CSIStorageCapacity: failed to list *v1beta1.CSIStorageCapacity: the server could not find the requested resource新创建的pod会一直卡在pending状态。这里把crane的镜像地址改一下就行：12345crane-scheduler:ghcr.io/gocrane/crane/crane-scheduler:pr-43-b871212crane-scheduler-controller:ghcr.io/gocrane/crane/crane-scheduler-controller:pr-43-b871212方案二：通过脚本定时监控node内存使用率为了实现这个解决方案，可以使用 Kubernetes 中的污点（Taint）和容忍度（Toleration）机制。首先添加一个污点到这个节点，以标识它当前无法容纳高内存需求的 Pods。然后，在这些 Pods 的 YAML 文件中添加容忍度字段，以允许它们在具有更充足内存资源的其他节点上运行。最后，设置 SchedulingDisabled 标志，以确保后续的 Pods 不会被调度到这个节点上。123456789101112131415161718192021222324252627282930#!/bin/bash# 功能：控制节点内存# 如果节点内存占用率超过 90%，则禁止在该节点上调度 Pod# 如果节点内存占用率低于平均值且平均值小于 90%，则允许在该节点上调度 Pod# 使用 kubectl top 命令获取节点的内存占用率，并忽略掉一些特定的节点data=$(kubectl top node | grep -v "MEMORY" | grep -v "cn-shenzhen" | sed "s/%//g")# 计算所有节点的内存占用率的平均值avg=$(echo "$data" | awk '&#123; sum += $NF &#125; END &#123; print sum / NR &#125;' | awk -F. '&#123; print $1 &#125;')# 逐行读取每个节点的信息（节点名称和内存占用率）echo "$data" | awk '&#123; print $1, $NF &#125;' | while read linedo # 获取节点名称和内存占用率 n=$(echo $line | awk '&#123; print $1 &#125;') m=$(echo $line | awk '&#123; print $2 &#125;') # 如果内存占用率超过 90%，并且该节点没有被设置为不可调度状态，则禁止在该节点上调度 Pod if [ "$m" -ge "90" ];then if kubectl get node | grep $n | grep -q "SchedulingDisabled";then continue else kubectl cordon $n fi # 如果内存占用率低于平均值且平均值小于 90%，并且该节点已被设置为不可调度状态，则允许在该节点上调度 Pod elif [ "$m" -lt "$avg" ] &amp;&amp; [ "$avg" -lt "90" ];then if kubectl get node | grep $n | grep -q "SchedulingDisabled";then kubectl uncordon $n fi fidone优化：实现了以下功能：123456使用 kubectl top 命令获取节点的内存占用率，并忽略掉特定节点（例如 “MEMORY” 和 “cn-shenzhen”）的数据。计算所有节点的内存占用率的平均值。遍历每个节点的内存占用率，并根据阈值动态调整节点的调度状态。如果某个节点的内存占用率超过 90%，并且该节点没有被设置为不可调度状态，则禁止在该节点上调度 Pod（使用 kubectl cordon 命令）。如果某个节点的内存占用率低于下限阈值，并且该节点已被设置为不可调度状态，则允许在该节点上调度 Pod（使用 kubectl uncordon 命令）。这样，脚本会根据节点的内存占用率动态调整节点的调度状态，以确保每台机器资源使用率基本一样，并在内存占用率达到 90% 时禁止调度 Pod。脚本：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465#/**********************************************************# * Description : 根据节点的内存占用率动态调整节点的调度状态，以确保每台机器资源使用率基本一样，并在内存占用率达到 90% 时禁止调度 Pod。# * *******************************************************/#/bin/bash# 日志函数，接收时间、日志级别和日志内容作为参数log() &#123; local timestamp=$(date +"%Y-%m-%d %H:%M:%S") local level=$1 local message=$2 echo "[$timestamp] [$level] $message"&#125;log "info" "开始执行脚本"# 使用 kubectl top 命令获取节点的内存占用率，并忽略掉一些特定的节点data=$(kubectl top node | grep -v "MEMORY" | grep -v "cn-shenzhen" | sed "s/%//g")# 计算所有节点的内存占用率的平均值avg=$(echo "$data" | awk '&#123; sum += $NF &#125; END &#123; print sum / NR &#125;' | awk -F. '&#123; print $1 &#125;')# 逐行读取每个节点的信息（节点名称和内存占用率）echo "$data" | awk '&#123; print $1, $NF &#125;' | while read linedo # 获取节点名称和内存占用率 n=$(echo $line | awk '&#123; print $1 &#125;') m=$(echo $line | awk '&#123; print $2 &#125;') # 输出节点名称和内存占用率日志 log "info" "节点 $n 的内存占用率为 $m%" # 如果内存占用率超过90%，并且该节点没有被设置为不可调度状态，则禁止在该节点上调度 Pod if [ "$m" -ge 90 ];then if kubectl get node $n | grep -q "SchedulingDisabled";then log "info" "节点 $n 内存占用率超过90%，已经禁止调度 Pod" else log "info" "节点 $n 内存占用率超过90%，禁止调度 Pod" kubectl cordon $n fi else # 根据平均阈值动态调整阈值范围（比如将平均阈值的上限和下限设置为平均值的上下10%） threshold=$(echo "$avg * 0.1" | bc) upper_threshold=$(printf "%.0f" $(echo "$avg + $threshold" | bc)) lower_threshold=$(printf "%.0f" $(echo "$avg - $threshold" | bc)) # 如果内存占用率超过上限阈值，并且该节点没有被设置为不可调度状态，则禁止在该节点上调度 Pod if [ "$m" -ge "$upper_threshold" ];then if kubectl get node $n | grep -q "SchedulingDisabled";then log "info" "节点 $n 内存占用率过高，已经禁止调度 Pod" else log "info" "节点 $n 内存占用率过高，禁止调度 Pod" kubectl cordon $n fi # 如果内存占用率低于下限阈值，并且该节点已被设置为不可调度状态，则允许在该节点上调度 Pod elif [ "$m" -lt "$lower_threshold" ];then if kubectl get node $n | grep -q "SchedulingDisabled";then log "info" "节点 $n 内存占用率较低，允许调度 Pod" kubectl uncordon $n fi fi fidonelog "info" "脚本执行完成"原理该脚本的原理是通过获取节点的内存占用率，并根据预设的阈值进行判断和操作。具体流程如下：123451. 使用 kubectl top 命令获取节点的内存占用率，并忽略掉特定的节点。2.计算所有节点的内存占用率的平均值。3.逐行读取每个节点的信息，包括节点名称和内存占用率。4.如果节点内存占用率超过设定的阈值（例如 90%），并且该节点没有被设置为不可调度状态，则禁止在该节点上调度 Pod。5.如果节点内存占用率低于平均值且平均值小于阈值，并且该节点已被设置为不可调度状态，则允许在该节点上调度 Pod。通过这样的原理，我们可以在集群中实现对节点内存的动态管理，确保节点资源的合理利用和容器应用的稳定运行。优势123自动化管理： 该脚本实现了自动化的节点内存管理，无需手动干预，减轻了运维人员的负担。实时监测：通过定期执行脚本，可以实时监测节点的内存占用情况，及时做出调整，提高了容器应用的性能和可用性。智能决策：根据设定的阈值和平均值，脚本能够智能地决策是否禁止或允许在节点上调度 Pod，确保资源的合理分配。缺点12依赖性： 该脚本依赖于 Kubernetes 命令行工具 kubectl 和集群的配置，因此需要保证环境的正确配置和可用性。单一维度： 该脚本仅基于节点的内存占用率进行管理，没有考虑其他资源（如 CPU、存储）的情况，因此在综合资源管理方面还有待完善。由于配置了systemReserved，kubeReserved以及硬驱逐等，kubectl top nodes监控到的数据和node实际的使用率对不上，所以没使用这个方案。]]></content>
      <categories>
        <category>技术</category>
        <category>K8S</category>
      </categories>
      <tags>
        <tag>crane-scheduler</tag>
        <tag>scheduler</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s灰度发布配置]]></title>
    <url>%2F2025%2F09c37847c2.html</url>
    <content type="text"><![CDATA[在现代微服务架构中，应用的更新和发布是一个高频且关键的操作。如何在不影响用户体验的前提下，安全、平稳地将新版本应用推送到生产环境，是每个开发者和运维团队必须面对的挑战。灰度发布（Gray Release）作为一种渐进式发布策略，能够有效降低发布风险，而 Kubernetes 的 Ingress 注解功能为我们提供了一种简单而强大的实现方式。灰度发布，也称为金丝雀发布（Canary Release），是一种渐进式的应用发布策略。它的核心思想是：将新版本应用逐步推送给一小部分用户，观察其运行状态，确认无误后再逐步扩大范围，最终完成全量发布。相比于全量发布，灰度发布具有以下优势：降低风险：通过小范围验证，避免因新版本问题导致全局故障。快速回滚：如果新版本出现问题，可以快速切换回旧版本。用户体验优化：逐步发布可以减少对用户的影响。Kubernetes 中的灰度发布实现方式在 Kubernetes 中，灰度发布可以通过多种方式实现，例如：Istio：通过服务网格实现高级流量管理。Ingress 注解：通过 Nginx Ingress Controller 的注解功能实现流量分割。Deployment + Service：手动控制流量切换。通过Istio实现灰度发布之前在Istio服务网格篇已经讲解过：服务网格Istio安装及使用 | 逸贤 | Blog通过 Ingress 注解实现灰度发布Nginx Ingress Controller 提供了丰富的注解（Annotations），可以轻松实现灰度发布。以下是具体步骤：部署新旧版本应用首先，我们需要部署两个版本的应用程序：旧版本（v1）：当前正在运行的生产版本。新版本（v2）：待发布的新版本。创建 v1 版本 Deployment 和 Service12345678910111213141516171819202122232425262728293031# v1 版本 DeploymentapiVersion: apps/v1kind: Deploymentmetadata: name: my-app-v1spec: replicas: 3 template: metadata: labels: app: my-app version: v1 spec: containers: - name: my-app image: my-app:v1 ports: - containerPort: 80# v1 版本 ServiceapiVersion: v1kind: Servicemetadata: name: my-app-v1spec: selector: app: my-app version: v1 ports: - port: 80 targetPort: 80创建 v2 版本 Deployment 和 Service12345678910111213141516171819202122232425262728293031# v2 版本 DeploymentapiVersion: apps/v1kind: Deploymentmetadata: name: my-app-v2spec: replicas: 3 template: metadata: labels: app: my-app version: v2 spec: containers: - name: my-app image: my-app:v2 ports: - containerPort: 80# v2 版本 ServiceapiVersion: v1kind: Servicemetadata: name: my-app-v2spec: selector: app: my-app version: v2 ports: - port: 80 targetPort: 80配置 Ingress 实现灰度发布通过 Nginx Ingress Controller 的 canary 注解，我们可以轻松实现流量分割。创建 Ingress 资源1234567891011121314151617181920212223242526272829apiVersion: networking.k8s.io/v1kind: Ingressmetadata: name: my-app-ingress annotations: nginx.ingress.kubernetes.io/canary: "true" # 启用灰度发布 nginx.ingress.kubernetes.io/canary-weight: "10" # 10% 流量到新版本spec: rules: - host: my-app.example.com http: paths: - path: / pathType: Prefix backend: service: name: my-app-v2 # 新版本服务 port: number: 80 - host: my-app.example.com http: paths: - path: / pathType: Prefix backend: service: name: my-app-v1 # 旧版本服务 port: number: 80关键注解说明：nginx.ingress.kubernetes.io/canary: &quot;true&quot;：启用灰度发布功能。nginx.ingress.kubernetes.io/canary-weight: &quot;10&quot;：将 10% 的流量分配到新版本（v2），剩余 90% 的流量继续使用旧版本（v1）。逐步调整流量权重在灰度发布过程中，可以逐步增加新版本的流量比例。例如：初始阶段：10% 流量到 v2。验证通过后：将权重调整为 50%。最终阶段：将权重调整为 100%，完成全量发布。只需修改 canary-weight 注解的值即可：1nginx.ingress.kubernetes.io/canary-weight: "50" # 50% 流量到新版本监控与回滚在灰度发布过程中，务必监控新版本的运行状态，包括：应用日志：检查是否有错误或异常。性能指标：如响应时间、错误率等。用户反馈：收集用户的使用体验。如果发现问题，可以通过调整 canary-weight 注解将流量切回旧版本：1nginx.ingress.kubernetes.io/canary-weight: "0" # 所有流量切回旧版本灰度发布的进阶用法除了基于权重的流量分割，Nginx Ingress Controller 还支持以下灰度发布策略：基于请求头的流量分割通过 nginx.ingress.kubernetes.io/canary-by-header 注解，将特定请求头的流量路由到新版本。123456789101112131415161718192021222324252627282930apiVersion: networking.k8s.io/v1kind: Ingressmetadata: name: my-app-ingress annotations: nginx.ingress.kubernetes.io/canary: "true" nginx.ingress.kubernetes.io/canary-by-header: "X-Canary" nginx.ingress.kubernetes.io/canary-by-header-value: "true"spec: rules: - host: my-app.example.com http: paths: - path: / pathType: Prefix backend: service: name: my-app-v2 port: number: 80 - host: my-app.example.com http: paths: - path: / pathType: Prefix backend: service: name: my-app-v1 port: number: 80说明当请求头中包含 X-Canary: true 时，流量会被路由到新版本（v2）。其他请求继续使用旧版本（v1）。基于 Cookie 的流量分割过 nginx.ingress.kubernetes.io/canary-by-cookie 注解，将特定 Cookie 的流量路由到新版本。1234567891011121314151617181920212223242526272829apiVersion: networking.k8s.io/v1kind: Ingressmetadata: name: my-app-ingress annotations: nginx.ingress.kubernetes.io/canary: "true" nginx.ingress.kubernetes.io/canary-by-cookie: "canary"spec: rules: - host: my-app.example.com http: paths: - path: / pathType: Prefix backend: service: name: my-app-v2 port: number: 80 - host: my-app.example.com http: paths: - path: / pathType: Prefix backend: service: name: my-app-v1 port: number: 80说明当请求中包含 canary=true 的 Cookie 时，流量会被路由到新版本（v2）。其他请求继续使用旧版本（v1）。组合使用可以同时使用权重、请求头和 Cookie 实现更复杂的灰度发布策略。1234567891011121314151617181920212223242526272829303132apiVersion: networking.k8s.io/v1kind: Ingressmetadata: name: my-app-ingress annotations: nginx.ingress.kubernetes.io/canary: "true" nginx.ingress.kubernetes.io/canary-weight: "10" nginx.ingress.kubernetes.io/canary-by-header: "X-Canary" nginx.ingress.kubernetes.io/canary-by-header-value: "true" nginx.ingress.kubernetes.io/canary-by-cookie: "canary"spec: rules: - host: my-app.example.com http: paths: - path: / pathType: Prefix backend: service: name: my-app-v2 port: number: 80 - host: my-app.example.com http: paths: - path: / pathType: Prefix backend: service: name: my-app-v1 port: number: 80说明10% 的流量会被分配到新版本（v2）。如果请求头中包含 X-Canary: true 或 Cookie 中包含 canary=true，流量也会被路由到新版本。通过Deployment+Service实现deploy-blue.yaml：12345678910111213141516171819202122232425262728293031323334353637383940apiVersion: apps/v1kind: Deploymentmetadata: name: deploy-bluespec: replicas: 3 selector: matchLabels: app: hellok8s version: "1.0" template: metadata: labels: app: hellok8s version: "1.0" spec: containers: - name: hellok8s image: hellok8s:1.0 imagePullPolicy: IfNotPresent ports: - containerPort: 8080 startupProbe: httpGet: path: / port: 8080 initialDelaySeconds: 5 periodSeconds: 10 readinessProbe: httpGet: path: / port: 8080 initialDelaySeconds: 5 periodSeconds: 10 livenessProbe: httpGet: path: / port: 8080 initialDelaySeconds: 5 periodSeconds: 10运行并查看：12kubectl apply -f deploy-blue.yamlkubectl get pod -owide在另一个终端开启监控：1kubectl get pod -owide -w执行更新并暂停：1kubectl set image deploy deploy-blue hellok8s=hellok8s:2.0 &amp;&amp; kubectl rollout pause deploy deploy-blue命令说明：1234567set image 命令用来给工作负载更新镜像。命令格式为：kubectl set image deploy deploy-name container-name=image-name:image-tag&amp;&amp; 表示上一条命令执行成功之后才执行下一条命令rollout pause 命令用来暂停滚动更新过程。命令格式为：kubectl rollout pause deploy deploy-name查看监控：一个新的pod会被创建，与此同时所有旧的pod还在运行。一旦新的pod成功运行，服务的一部分请求将被切换到新的pod。这样相当于运行了一个金丝雀版本。金丝雀发布是一种可以将应用程序的出错版本和其影响到的用户的风险化为最小的技术。与其直接向每个用户发布新版本，不如用新版本替换一个或一小部分的pod。通过这种方式，在升级的初期只有少数用户会访问新版本。验证新版本是否正常工作之后，可以将剩余的pod继续升级或者回滚到上一个的版本。查看部署详情：会发现 paused 被系统设置为 true1kubectl get deploy deploy-blue -ojson执行回滚：1kubectl rollout undo deploy deploy-blue它会提示你，要先恢复（resume）rollout 过程，然后才能执行回滚命令。恢复 Rollout：1kubectl rollout resume deployment/deploy-blue查看监控： 可以看到，所有的 pod，都已被更新成新版。执行回滚（现在可以正常回滚了）：1kubectl rollout undo deploy deploy-blueDeployment pause 的金丝雀发布是一种伪金丝雀发布，它有以下几个问题：1）在滚动升级过程中，想要在⼀个确切的位置暂停滚动升级无法做到。2）无法实现流量的按比例分配。3）需要恢复更新才能执行回滚。相当于将有问题的版本全部更新完才能回滚，将影响面扩大化了。可以使用 kubectl set image 旧版本来规避 kubectl rollout resume还有一种就是deployment+service灰度发布的方式就是部署新旧版本的deployment，新旧版本应用使用同样的lable，service会关联新旧版本的deployment，通过控制新旧版本的pod数量来控制发布比例。]]></content>
      <categories>
        <category>技术</category>
        <category>K8S</category>
      </categories>
      <tags>
        <tag>灰度发布</tag>
        <tag>金丝雀发布</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Istio灰度发布的流量策略]]></title>
    <url>%2F2025%2F09872234a1.html</url>
    <content type="text"><![CDATA[灰度发布就是先上线一个新版本，然后根据规则将一小部分用户导向到新应用，观察新版本在生产环境的表现，如果达到预期，则逐步将流量切换到新版本中。将所有流量导向一个版本12345678910111213141516apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: nginx-serverspec: hosts: - "nginx.yscloud.com" gateways: - nginx-server-gateway http: - route: - destination: host: nginx-server subset: v2 port: number: 80将流量进行拆分按照权重进行拆分1234567891011121314151617181920212223apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: nginx-serverspec: hosts: - "nginx.yscloud.com" gateways: - nginx-server-gateway http: - route: - destination: host: nginx-server subset: v1 port: number: 80 weight: 90 - destination: host: nginx-server subset: v2 port: number: 80 weight: 10按照header进行拆分1234567891011121314151617181920apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: nginx-serverspec: hosts: - "nginx.yscloud.com" gateways: - nginx-server-gateway http: - match: - headers: test: exact: header route: - destination: host: nginx-server subset: v1 port: number: 80按照浏览器进行拆分其实也是按照header来进行区分1234567891011121314151617181920apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: nginx-serverspec: hosts: - "nginx.yscloud.com" gateways: - nginx-server-gateway http: - match: - headers: User-Agent: regex: ".*Firefox/.*" route: - destination: host: nginx-server subset: v2 port: number: 80按照请求地址进行拆分123456789101112131415161718192021apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: nginx-serverspec: hosts: - "nginx.yscloud.com" gateways: - nginx-server-gateway http: - match: - uri: prefix: "/v2" rewrite: uri: / route: - destination: host: nginx-server subset: v2 port: number: 80按照用户源IP进行流量拆分1234567891011121314151617181920212223................ - match: - headers: X-Real-IP: regex: ".*192.168.3.148.*" route: - destination: host: nginx-server subset: v2 port: number: 80 - match: - headers: X-Real-IP: regex: ".*192.168.2.117.*" route: - destination: host: nginx-server subset: v1 port: number: 80多个条件同时满足时，进行流量拆分12345678910111213141516171819202122apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: nginx-serverspec: hosts: - "nginx.yscloud.com" gateways: - nginx-server-gateway http: - match: - headers: test: exact: header User-Agent: regex: ".*Chrome/.*" route: - destination: host: nginx-server subset: v1 port: number: 80满足其中一个条件1234567891011121314151617181920212223apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: nginx-serverspec: hosts: - "nginx.yscloud.com" gateways: - nginx-server-gateway http: - match: - headers: test: exact: header - headers: User-Agent: regex: ".*Chrome/.*" route: - destination: host: nginx-server subset: v1 port: number: 80当对同一目标有多个规则时，会按照在 VirtualService 中的顺序进行应用，换句话说，列表中的第一条规则具有最高优先级。PS：如果系统分布式部署，有A,B,C,D,E服务，如果仅需要对D服务进行灰度发布时，仅对D服务配置VirtualService和DestinationRule就行，其他服务不需要配置。]]></content>
      <categories>
        <category>技术</category>
        <category>K8S</category>
      </categories>
      <tags>
        <tag>服务网格</tag>
        <tag>Istio</tag>
        <tag>灰度发布</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[服务网格Istio安装及使用]]></title>
    <url>%2F2025%2F098b941b9d.html</url>
    <content type="text"><![CDATA[在k8s中配置启用了istio之后，在创建pod之后会自动注入一个Envoy这样的容器，可理解为代理容器，也就是说，创建pod是除了定义的一个主容器，还会在启动istio之后自动注入一个容器，一个pod有两个容器，该代理容器主要是用来做流量的控制和管理，能做到流量的拦截，能控制熔断、超时、重试，按流量的百分比代理等作用。流量管理熔断当在 K8s集群中使用 Istio 时，可以通过 Istio 提供的熔断功能来增加微服务架构的稳定性和可靠性。熔断是一种故障保护机制，用于在服务之间的通信中防止故障扩散，并提供更好的容错能力。在微服务架构中，一个应用通常由许多小型的、相互协作的服务组成。当某个服务发生故障或变得不可用时，如果不采取措施，可能会导致连锁反应，影响到整个系统的可用性。熔断机制旨在解决这个问题，其核心思想是在服务之间设置阈值和超时时间，并对请求进行监控。当服务的错误率或响应时间超过预设的阈值时，熔断器会打开，拒绝向该服务发送更多请求，并快速失败返回错误，而不是等待超时。应用场景如下：12345678910111.快速失败返回:当目标服务不可用时，不再尝试等待请求超时，而是快速返回错误，从而避免资源浪费和潜在的长时间等待。2.故障隔离:熔断机制阻止故障扩散，使问题局限在出现故障的服务，而不会影响到整个应用程序。3.恢复机制:熔断器会定期尝试发起一些请求到目标服务，以检查其是否已经恢复。如果服务恢复正常，则熔断器会逐渐关闭，允许流量再次流向目标服务。4.自动重试:一旦目标服务恢复，熔断器会逐渐允许一部分流量通过，如果没有再次出现问题，会逐渐恢复到正常状态，否则继续保持熔断状态。超时在 K8s集群中结合 Istio，可以使用 Istio 的流量管理功能来控制服务之间的请求超时。超时是指在一定时间内，如果某个请求没有得到及时响应，就会被认为是超时。通过设置请求超时时间，可以对服务之间的通信进行控制，确保请求在合理的时间内得到响应，避免请求无限期地等待导致资源浪费或影响整体系统的响应性能。Istio 的超时控制允许你为每个服务之间的请求设置最大的等待时间。当某个请求在指定的超时时间内没有得到响应时，Istio 会终止该请求并返回一个错误响应给客户端。这样可以防止请求在后端服务长时间等待，从而避免请求积压，同时提高系统的稳定性和可用性。超时控制的意义如下：123456789101.防止长时间等待:通过设置合理的请求超时时间，避免了客户端请求在后端服务长时间等待导致的响应延迟，从而提高了用户体验。2.快速失败机制:当后端服务无法及时响应请求时，超时控制会快速返回错误响应，而不是让请求无限期地等待，从而避免资源浪费。3.故障隔离:如果某个服务出现故障或变得不可用，超时控制可以快速终止对该服务的请求，避免故障扩散到其他服务。在 Istio 中，超时控制是通过配置请求超时策略来实现的。你可以为每个服务定义超时时间，也可以为整个服务部署或命名空间设置默认的超时时间。这样，Istio 会根据配置中的超时时间来对请求进行限制，保证请求在规定时间内得到响应。Istio组件pilotPilot 是 Istio 控制平面的组件，在istio系统中，Pilot 完成以下任务：它从服务注册中心（如Kubernetes的etcd或Consul）获取服务和实例的信息，并为Envoy生成配置，Envoy 根据 Pilot 发过来的配置里的内容，完成具体流量的转发。Pilot可以被认为是团队的领袖。它负责监督和指导队伍中的每个Envoy。Pilot指导整个团队中每个服务的位置，以及它们应该如何相互协作。当有新队员加入或者队员的位置变化时，Pilot会负责通知每个队员，确保大家都知道最新的情况，不会出现迷路或者错过重要信息，会把要做的事形成文件下发到每个envoy队员里。envoyEnvoy是Istio中的代理，我们如果开启了istio功能，会在pod里自动注入Envoy代理容器，它负责处理服务之间的所有网络通信，拦截并转发所有的HTTP、TCP和gRPC流量。Envoy提供强大的流量控制和管理功能，如路由、重试、超时和故障注入等。Envoy和主容器同属于一个Pod，共享网络和命名空间，Envoy代理进出Pod 的流量，并将流量按照外部请求的规则作用于主容器中。Envoy可以看作是服务之间的守护者。它像一个中间人一样，坐在每个服务旁边（就像每个队员身边都有一个保镖一样）。它负责处理服务之间的所有信息传递，确保信息传送得又快又准确。如果有请求从一个服务发出，Envoy会帮它找到正确的目标服务并将请求送达过去。它还能处理各种不同类型的请求，就像精通各种语言的翻译一样。galleyGalley是Istio的配置管理组件，负责验证、转换和分发配置给其他Istio组件。它监听Kubernetes的配置更改，例如Service、Deployment等，然后根据规则和策略生成Istio所需的配置，并将其提供给Pilot和其他组件。Galley可以被看作是团队中的文件管理员。它负责管理团队中所有的文件和信息，确保每个队员都能得到正确的信息和文件。当有新的文件产生或者文件发生变化时，Galley会及时通知团队中的每个成员，确保大家都使用的是最新的文件，不会出现信息不同步的问题。Ingressgatewayistio-ingressgateway是Istio服务网格中的一个特殊网关，它作为整个网格的入口，接收外部请求，并将它们引导到内部的服务。可以将它比作一个大门保安，负责接收外部人员的访问请求，然后根据配置的规则将请求分发给网格内部的服务。简单来说，当有外部的请求访问Istio服务网格时，它们会先被送到istio-ingressgateway。这个网关会检查请求，并根据一些规则判断该请求应该交给哪个服务来处理。然后，它将请求转发给网格内部的相应服务，从而实现外部请求与内部服务的连接。egressgatewayistio-egressgateway是Istio服务网格中的另一个特殊网关，它负责处理网格内部服务对外部服务的访问请求。可以将它看作是一个网格内部的出口，负责将内部服务需要访问的外部服务请求发送到外部。以将istio-egressgateway比作一个秘书，它会代表网格内部的服务，帮助它们联系外部的服务。当网格内部的服务需要访问外部服务时，它们会将请求交给istio-egressgateway，然后由这个网关将请求发送给外部服务。安装/卸载istio当安装完成istio之后，就会自动生成三个pod，分别为：istiod、Ingressgateway、egressgateway，然后就可以使用它的熔断、超时、重试等功能。123注意：由于k8s集群在1.24版本之后使用的容器运行时不同，那么所安装istio的方法也不同，下面是k8s集群版本1.24前后的安装istio方式在大于等于k8s集群1.24版本的情况下需要安装的istio版本要大于等于1.18版本。k8s集群小于等于1.23版本安装istiok8s集群版本：1.23.1下载istio的安装包：官网下载地址：GCS browser: istio-release这里使用的版本是istio:1.13.1 (可使用wget 下载)1wget https://gcsweb.istio.io/gcs/istio-release/releases/1.13.1/istio-1.13.1-linux-amd64.tar.gz123456789101112131415161718192021#下载到k8s集群的master节点并解压[root@k8s-master ~]# tar zxf istio-1.13.1.tar.gz[root@k8s-master ~]# cd istio-1.13.1[root@k8s-master istio-1.13.1]# cp bin/istioctl /usr/bin/ //拷贝执行文件到/usr/bin下[root@k8s-master istio-1.13.1]# istioctl install --set profile=demo -y //初始化安装，如下所示表示安装成功。✔ Istio core installed ✔ Istiod installed ✔ Egress gateways installed ✔ Ingress gateways installed ✔ Installation completeMaking this installation the default for injection and validation.Thank you for installing Istio 1.13. Please take a few minutes to tell us about your install/upgrade experience! https://forms.gle/pzWZpAvMVBecaQ9h9#安装成功之后，查看istio的pod是否运行成功，如下[root@k8s-master istio-1.13.1]# kubectl get pods -n istio-systemNAME READY STATUS RESTARTS AGEistio-egressgateway-76c96658fd-78ll2 1/1 Running 0 114sistio-ingressgateway-569d7bfb4-pg5vz 1/1 Running 0 114sistiod-74c64d89cb-xdxn4 1/1 Running 0 2m41sk8s集群大于等于1.24版本安装istiok8s集群版本：1.28.1下载istio的安装包：官网下载地址： GCS browser: istio-release这里使用的版本是istio:1.182 (可使用wget 下载)1wget https://gcsweb.istio.io/gcs/istio-release/releases/1.18.2/istio-1.18.2-linux-amd64.tar.gz1234567891011121314151617181920212223242526[root@k8s-master ~]# tar -zxvf istio-1.18.2-linux-amd64.tar.gz[root@k8s-master ~]# cd istio-1.18.2[root@k8s-master istio-1.18.2]# cp bin/istioctl /bin/[root@k8s-master istio-1.18.2]# istioctl install --set profile=demo -y //如下表示安装成功✔ Istio core installed ✔ Istiod installed ✔ Ingress gateways installed ✔ Egress gateways installed ✔ Installation complete Making this installation the default for injection and validation.#验证pod运行情况[root@k8s-master istio-1.18.2]# kubectl get pods -n istio-systemNAME READY STATUS RESTARTS AGEistio-egressgateway-5499894f8d-nvtrv 1/1 Running 0 22sistio-ingressgateway-77fbf9d476-thtnp 1/1 Running 0 22sistiod-77c989fb-4xf8g 1/1 Running 0 27s#不同于k8s集群1.23以下版本的安装方法就是使用的容器运行时不同，#如使用containerd初始化安装istio时下载镜像失败，那么就是containerd没有配置好或者镜像源不可用等问题#可以使用docker下载相关依赖镜像在打包传到containerd#使用docker下载如下两个镜像并打包传到k8s工作节点上的containerd中，再进行初始化安装istio步骤即可。#/istio/pilot:1.18.2#/istio/proxyv2:1.18.2安装命令选项123456istioctl install --set profile=demo -yprofile=""1.demo：适用于生产环境，会创建三个pod，包含istiod、ingressgateway、egressgateway2.default：这是一个最小配置，仅包含最基本的组件，如istiod和ingressgateway3.minimal：这个配置相对更加精简，只包含了必需的组件，适用于资源受限或轻量级的环境。只会安装istiod。通过如下命令可以看到选择安装的模式下都有哪些配置文件如查看demo模式：1istioctl profile dump demo配置外部ip安装完之后由于我们没有负载均衡的环境，所以EXTERNAL-IP一直是pending的状态1kubectl get svc istio-ingressgateway -n istio-system我们可以直接用节点ip+nodeport的方式访问，但是如果我们想要直接用域名访问，不在后面加nodeport的话，需要配置EXTERNAL-IP。可以安装MetallLB，或者直接在svc里指定EXTERNAL-IP，这里我简单演示，就直接指定EXTERNAL-IP。EXTERNAL-IP要和节点ip在同一个网段，我的局域网IP段是10.168.2.0/2412345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758kind: ServiceapiVersion: v1metadata: name: istio-ingressgateway namespace: istio-system labels: app: istio-ingressgateway install.operator.istio.io/owning-resource: unknown install.operator.istio.io/owning-resource-namespace: istio-system istio: ingressgateway istio.io/rev: default operator.istio.io/component: IngressGateways operator.istio.io/managed: Reconcile operator.istio.io/version: 1.13.1 release: istiospec: ports: - name: status-port protocol: TCP port: 15021 targetPort: 15021 nodePort: 31500 - name: http2 protocol: TCP port: 80 targetPort: 8080 nodePort: 30543 - name: https protocol: TCP port: 443 targetPort: 8443 nodePort: 31268 - name: tcp protocol: TCP port: 31400 targetPort: 31400 nodePort: 32276 - name: tls protocol: TCP port: 15443 targetPort: 15443 nodePort: 31693 selector: app: istio-ingressgateway istio: ingressgateway clusterIP: 10.233.13.10 clusterIPs: - 10.233.13.10 type: LoadBalancer externalIPs: ##添加这两行，指定externalIP - 10.168.2.249 sessionAffinity: None externalTrafficPolicy: Cluster ipFamilies: - IPv4 ipFamilyPolicy: SingleStack allocateLoadBalancerNodePorts: true internalTrafficPolicy: Cluster修改完之后再查看，EXTERNAL-IP已配置：1kubectl get svc istio-ingressgateway -n istio-system这个其实是通过kube-proxy去转发流量，也就是iptables规则（kube-ipvs）：现在把域名解析到这个EXTERNAL-IP就可以通过域名访问，而不需要在后面加nodeport。卸载istio1234567#该卸载方式会保留名称空间等一些组件遗留[root@k8s-master istio-1.18.2]# istioctl manifest generate --set profile=demo | kubectl delete -f -#执行完上述卸载命令之后也可以再次执行如下命令完全卸载。[root@k8s-master istio-1.18.2]# istioctl x uninstall --purge //提示中输入y即可完全卸载#如需重新安装继续执行istioctl install --set profile=demo -y即可。k8s集群部署在线书店，启用istio使用istio自带的工具进行部署，如下步骤中的yaml文件都在解压istio压缩包中自带的。在线书店（Bookinfo）应用分为四个单独的微服务12341）productpage这个微服务会调用details和reviews两个微服务，用来生成前端页面；2）details这个微服务中包含了书籍的信息；3）reviews这个微服务中包含了书籍相关的评论，它还会调用ratings微服务；4）ratings这个微服务中包含了由书籍评价组成的评级信息。reviews微服务有3个版本1）v1版本不会调用ratings服务；2）v2版本会调用ratings服务，并使用1到5个黑色星形图标来显示评分信息；3）v3版本会调用ratings服务，并使用1到5个红色星形图标来显示评分信息。配置istio自动注入代理容器功能部署应用前还需要为默认名称空间打上一个标签，可理解为开启istio自动注入代理容器功能。因为下面步骤创建的pod等资源都在默认名称空间下创建，所以为默认名称空间打赏标签，istio默认自动注入 sidecar，需要为default命名空间打上标签 istio-injection=enabled，这样在默认空间下创建pod istio就可以自动注入一个代理容器。注：在别的名称空间创建pod时，要想istio自动注入代理容器，也要打上相同的标签12345678#打标签[root@k8s-master ~]# kubectl label namespace default istio-injection=enablednamespace/default labeled #验证查看[root@k8s-master ~]# kubectl get ns --show-labels | egrep -i "status|default" NAME STATUS AGE LABELSdefault Active 18d istio-injection=enabled,kubernetes.io/metadata.name=default关闭 istio自动注入代理容器功能如上所示，要想关闭istio自动注入代理容器功能，则需要将istio-injection=enabled该标签从名称空间上取消。为了不影响下方的实验，该步骤功能了解即可，不用操作。12345678#删除标签[root@k8s-master ~]# kubectl label ns default istio-injection-namespace/default unlabeled#验证查看[root@k8s-master ~]# kubectl get ns --show-labels | egrep -i "status|default" NAME STATUS AGE LABELSdefault Active 18d kubernetes.io/metadata.name=default部署应用如下步骤使用的集群版本为1.28.1，容器运行时使用的是containerd准备如下镜像到k8s集群的工作节点中（不准备也可以，直接使用istio自带的yaml文件时也会自动下载）123456/istio/examples-bookinfo-details-v1:1.17.0/istio/examples-bookinfo-productpage-v1:1.17.0/istio/examples-bookinfo-ratings-v1:1.17.0/istio/examples-bookinfo-reviews-v1:1.17.0/istio/examples-bookinfo-reviews-v2:1.17.0/istio/examples-bookinfo-reviews-v3:1.17.012345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697#进入到istio压缩包解压后的路径中，使用istio自带的yaml文件创建资源[root@k8s-master ~]# cd istio-1.18.2/samples/bookinfo/platform/kube/ //进入到目标路径[root@k8s-master kube]# kubectl apply -f bookinfo.yaml //找到目标yaml文件并执行创建service/details createdserviceaccount/bookinfo-details createddeployment.apps/details-v1 createdservice/ratings createdserviceaccount/bookinfo-ratings createddeployment.apps/ratings-v1 createdservice/reviews createdserviceaccount/bookinfo-reviews createddeployment.apps/reviews-v1 createddeployment.apps/reviews-v2 createddeployment.apps/reviews-v3 createdservice/productpage createdserviceaccount/bookinfo-productpage createddeployment.apps/productpage-v1 created#验证在默认名称空间中创建的pod，yaml文件中默认定义的是一个容器服务，这里的2/2说明自动注入的代理容器也成功了。[root@k8s-master kube]# kubectl get pods -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESdetails-v1-65fbc6fbcf-f69qp 2/2 Running 0 5m22s 10.244.194.75 k8s-worker1 &lt;none&gt; &lt;none&gt;productpage-v1-78d88f8f58-v57dx 2/2 Running 0 5m22s 10.244.194.77 k8s-worker1 &lt;none&gt; &lt;none&gt;ratings-v1-7fcc55c79f-vqmhc 2/2 Running 0 5m22s 10.244.194.76 k8s-worker1 &lt;none&gt; &lt;none&gt;reviews-v1-66c7d54957-nw96c 2/2 Running 0 5m22s 10.244.194.72 k8s-worker1 &lt;none&gt; &lt;none&gt;reviews-v2-844777b87c-nch5l 2/2 Running 0 5m22s 10.244.194.73 k8s-worker1 &lt;none&gt; &lt;none&gt;reviews-v3-b48665d5c-7m96m 2/2 Running 0 5m22s 10.244.194.74 k8s-worker1 &lt;none&gt; &lt;none&gt;#验证查看创建svc[root@k8s-master kube]# kubectl get svc -owide | grep -v kubernetesNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORdetails ClusterIP 10.101.228.3 &lt;none&gt; 9080/TCP 5m55s app=detailsproductpage ClusterIP 10.102.85.97 &lt;none&gt; 9080/TCP 5m55s app=productpageratings ClusterIP 10.99.175.240 &lt;none&gt; 9080/TCP 5m55s app=ratingsreviews ClusterIP 10.99.185.174 &lt;none&gt; 9080/TCP 5m55s app=reviews#使用七层代理访问bookinfo的前端pod（productpage-v1-78d88f8f58-v57dx），需要创建网关及虚拟服务[root@k8s-master kube]# cd [root@k8s-master ~]# cd istio-1.18.2/samples/bookinfo/networking/ //进入目标路径[root@k8s-master networking]# cat bookinfo-gateway.yamlapiVersion: /v1alpha3kind: Gateway #在默认名称空间创建网关资源，安装完成istio之后才会有的一个资源metadata: name: bookinfo-gatewayspec: selector: #标签选择器指定了具有istio: ingressgateway这个标签的pod，详见下图一， istio: ingressgateway servers: #如下指定了监听端口80及http的协议 - port: number: 80 name: http protocol: HTTP hosts: #“*”表示访问任何域名，只要是来自http协议及端口80的请求都交给具有上面标签的pod，也就是下图一那个pod - "*"---apiVersion: /v1alpha3kind: VirtualService #创建虚拟服务，将上面创建的网关绑定到虚拟服务上。metadata: name: bookinfospec: hosts: - "*" gateways: #指定上面网关的名称进行绑定 - bookinfo-gateway http: - match: #定义路由规则，如果请求的是如下定义的接口，（如：/productpage），那么将会该请求代理给下面定义的productpage - uri: exact: /productpage - uri: prefix: /static - uri: exact: /login - uri: exact: /logout - uri: prefix: /api/v1/products route: #如上描述，将会代理给这里定义的productpage这个svc，而productpage这个svc的端口是9080，如下图二所示 - destination: host: productpage port: number: 9080 #创建该yaml文件[root@k8s-master networking]# kubectl apply -f bookinfo-gateway.yaml gateway./bookinfo-gateway createdvirtualservice./bookinfo created#查看验证[root@k8s-master networking]# kubectl get gatewayNAME AGEbookinfo-gateway 51s[root@k8s-master networking]# kubectl get virtualserviceNAME GATEWAYS HOSTS AGEbookinfo ["bookinfo-gateway"] ["*"] 63s 访问验证1234567891011121314151617181920212223242526272829303132333435363738394041424344#访问上面bookinfo-gateway.yaml文件中gateway资源中指定的具有istio=ingressgateway标签的pod的前端service#查看具有该标签的pod[root@k8s-master networking]# kubectl get pods -l istio=ingressgateway -A -owide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESistio-system istio-ingressgateway-77fbf9d476-tr2gd 1/1 Running 0 23h 10.244.194.71 k8s-worker1 &lt;none&gt; &lt;none&gt;#查看该pod的前端service，通过名称确认是istio-ingressgateway[root@k8s-master networking]# kubectl get svc -n istio-systemNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEistio-egressgateway ClusterIP 10.110.37.178 &lt;none&gt; 80/TCP,443/TCP 23histio-ingressgateway LoadBalancer 10.96.68.15 &lt;pending&gt; 15021:32520/TCP,80:31718/TCP,443:32483/TCP,31400:31230/TCP,15443:32205/TCP 23histiod ClusterIP 10.102.48.108 &lt;none&gt; 15010/TCP,15012/TCP,443/TCP,15014/TCP 23h#查看该svc的endpoints，通过ip再次确认[root@k8s-master networking]# kubectl describe svc istio-ingressgateway -n istio-system | grep -i endpointsEndpoints: 10.244.194.71:15021Endpoints: 10.244.194.71:8080Endpoints: 10.244.194.71:8443Endpoints: 10.244.194.71:31400Endpoints: 10.244.194.71:15443#访问流量走向过程：#浏览器访问k8s控制/工作节点的ip加具有istio=ingressgateway这个标签pod的前端service对应80端口的端口，也就是31718#控制节点IP:192.168.57.131:31718#访问该地址，就会把请求通过这个31718端口代理给该service的后端pod，也就是具有istio=ingressgateway这个标签pod#然后访问指定路由/productpage，（192.168.57.131:31718/productpage），#这样就能根据上面bookinfo-gateway.yaml文件定义的路由规则访问到productpage这个service的9080端口#如下：[root@k8s-master networking]# kubectl get svc | egrep -i "name|productpage"NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEproductpage ClusterIP 10.102.85.97 &lt;none&gt; 9080/TCP 79m#通过endpoints查看该service的后端pod[root@k8s-master networking]# kubectl describe svc productpage | grep -i endpointsEndpoints: 10.244.194.77:9080#查看具有该ip的pod是哪个[root@k8s-master networking]# kubectl get pods -owide | egrep -i "name|10.244.194.77"NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESproductpage-v1-78d88f8f58-v57dx 2/2 Running 0 83m 10.244.194.77 k8s-worker1 &lt;none&gt; &lt;none&gt;#浏览器访问如下图所示： 192.168.57.131:31718/productpage通过Istio实现灰度发布灰度发布也叫金丝雀部署 ，是指通过控制流量的比例，实现新老版本的逐步更替。比如对于服务A 有两个版本 ， 当前两个版本同时部署， 控制流量的访问版本1比例90% ，访问版本2比例10% ，然后根据运行的效果逐步调整流量访问占比，直至最终版本1下线。工作节点准备两个镜像进行测试实验：这里使用的是nginx镜像，该镜像服务状态均一致，只是版本不同，用于测试灰度发布nginx-v1 返回版本1nginx-v2 返回本本2创建pod12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182#使用nginx-v1镜像创建版本为1的pod[root@k8s-master ~]# vim istio-pod-v1.yamlapiVersion: apps/v1kind: Deploymentmetadata: name: v1 labels: app: v1spec: replicas: 1 selector: matchLabels: app: nginx-v1 test: abc template: metadata: labels: app: nginx-v1 test: abc spec: containers: - name: nginx image: nginx:v1 imagePullPolicy: IfNotPresent ports: - containerPort: 80#创建[root@k8s-master ~]# kubectl apply -f istio-pod-v1.yaml deployment.apps/v1 created[root@k8s-master ~]# kubectl get pods -owide #READY显示2/2，表示istio的代理容器也运行成功 NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESv1-6b67b54dfc-qvjvl 2/2 Running 0 13s 10.244.194.95 k8s-worker1 &lt;none&gt; &lt;none&gt;#测试返回值版本为 v1[root@k8s-master ~]# curl 10.244.194.95nginx-v1#使用nginx-v2镜像创建版本为2的pod[root@k8s-master ~]# cp istio-pod-v1.yaml istio-pod-v2.yaml [root@k8s-master ~]# vim istio-pod-v2.yamlapiVersion: apps/v1kind: Deploymentmetadata: name: v2 labels: app: v2spec: replicas: 1 selector: matchLabels: app: nginx-v2 test: abc template: metadata: labels: app: nginx-v2 test: abc spec: containers: - name: nginx image: nginx:v2 imagePullPolicy: IfNotPresent ports: - containerPort: 80 #创建[root@k8s-master ~]# kubectl apply -f istio-pod-v2.yaml deployment.apps/v2 created[root@k8s-master ~]# kubectl get pods -owide #READY显示2/2，表示istio的代理容器也运行成功NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESv1-6b67b54dfc-qvjvl 2/2 Running 0 2m29s 10.244.194.95 k8s-worker1 &lt;none&gt; &lt;none&gt;v2-68c9d69c4f-jktr5 2/2 Running 0 25s 10.244.194.96 k8s-worker1 &lt;none&gt; &lt;none&gt;#测试返回值版本为 v2[root@k8s-master ~]# curl 10.244.194.96nginx-v2创建service创建service代理两个pod:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657#创建四层代理servic#根据定义的标签确认后端的pod目标，找到具有test=abc的pod[root@k8s-master ~]# kubectl get pods -l test=abcNAME READY STATUS RESTARTS AGEv1-6b67b54dfc-qvjvl 2/2 Running 0 4m56sv2-68c9d69c4f-jktr5 2/2 Running 0 2m52s#创建service[root@k8s-master ~]# cat istio-pod-service.yaml apiVersion: v1kind: Servicemetadata: name: istio-pod-nginx labels: test: abcspec: selector: test: abc ports: - protocol: TCP port: 80 targetPort: 80#创建[root@k8s-master ~]# kubectl apply -f istio-pod-service.yaml service/istio-pod-nginx created[root@k8s-master ~]# kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEistio-pod-nginx ClusterIP 10.96.179.74 &lt;none&gt; 80/TCP 3s[root@k8s-master ~]# kubectl describe svc istio-pod-nginxName: istio-pod-nginxNamespace: defaultLabels: test=abcAnnotations: &lt;none&gt;Selector: test=abcType: ClusterIPIP Family Policy: SingleStackIP Families: IPv4IP: 10.96.179.74IPs: 10.96.179.74Port: &lt;unset&gt; 80/TCPTargetPort: 80/TCPEndpoints: 10.244.194.95:80,10.244.194.96:80 //这里看到绑定的ip地址就是刚刚创建的nginx的pod的IP地址Session Affinity: NoneEvents: &lt;none&gt;#测试请求service，如下所示请求是轮询[root@k8s-master ~]# curl 10.96.179.74nginx-v2[root@k8s-master ~]# curl 10.96.179.74nginx-v1[root@k8s-master ~]# curl 10.96.179.74nginx-v2[root@k8s-master ~]# curl 10.96.179.74nginx-v1创建Gateway使用Gateway，通过istio的ingressgetway这个service，根据它映射的物理机端口31718，通过访问k8s集群节点的IP+31718，将请求交给到刚刚创建的pod前端的四层代理service，然后最终再通过这个四层代理service访问到它代理的后端pod。istio的ingressgetway这个service的物理机映射端口如下图所示：123456789101112131415161718192021222324[root@k8s-master ~]# vim istio-pod-gzteway.yamlapiVersion: /v1beta1kind: Gatewaymetadata: name: istio-pod-gatewayspec: selector: istio: ingressgateway #查找具有该标签的pod servers: - port: number: 80 name: http protocol: HTTP hosts: #表示只要满足http协议的80端口的流量，那么任何域名都可以访问。 - "*" #创建[root@k8s-master ~]# kubectl apply -f istio-pod-gateway.yaml gateway./istio-pod-gateway created[root@k8s-master ~]# kubectl get gatewayNAME AGEbookinfo-gateway 130mistio-pod-gateway 7s如果要配置https证书的话，修改如下（需要先申请证书，再引用）：注：生成的证书需要和ingress-gateway在同一个namespace1234567891011121314151617apiVersion: /v1beta1kind: Gatewaymetadata: name: istio-pod-gatewayspec: selector: istio: ingressgateway servers: - port: number: 443 name: https protocol: HTTPS hosts: - public-gateway.example.com # 这里要与 Certificate 资源中的 dnsNames 匹配 tls: mode: SIMPLE credentialName: public-gateway-example-com-tls # 这里要与生成的 secretName 匹配创建虚拟服务VirtualService定义访问流量的比例:123456789101112131415161718192021222324252627282930[root@k8s-master ~]# vim istio-pod-vlbetal.yamlapiVersion: /v1beta1kind: VirtualServicemetadata: name: istio-pod-vlbetalspec: hosts: - "*" gateways: #绑定网关，指定gateway的名称 - istio-pod-gateway http: - route: #路由 - destination: #目标地址 host: istio-pod-nginx.default.svc.cluster.local #这里写目标地址1的名称，写的是四层代理service的名称+所在名称空间+完整域名的后缀 subset: v1 #定义子级权重。(子级也需要定义，定义自己指定的是哪个pod) weight: 90 #定义v1的权重为90接收90%的流量 - destination: host: istio-pod-nginx.default.svc.cluster.local #这里写目标地址2的名称，同上前面是service名称及所在名称空间，后面的后缀为默认的。 subset: v2 weight: 10 #同理，v2的权重为10，接收10%的流量 #创建[root@k8s-master ~]# kubectl apply -f istio-pod-vlbetal.yaml virtualservice./istio-pod-vlbetal created[root@k8s-master ~]# kubectl get virtualServiceNAME GATEWAYS HOSTS AGEbookinfo ["bookinfo-gateway"] ["*"] 163mistio-pod-vlbetal ["istio-pod-gateway"] ["*"] 10s创建目标规则创建上方virtualService的yaml文件中定义的子级指向规则：123456789101112131415161718192021222324252627282930313233343536373839404142434445[root@k8s-master ~]# vim istio-pod-vlbetal-destination.yamlapiVersion: /v1beta1kind: DestinationRulemetadata: name: istio-pod-vlbetal-destination spec: host: istio-pod-nginx.default.svc.cluster.local #这里同样定义service的完整域名 subsets: #定义子级 - name: v1 #通过pod标签进行绑定指定 labels: #子级名称为v1的绑定具有如下标签的pod app: nginx-v1 - name: v2 #子级名称为v2的绑定具有如下标签的pod labels: app: nginx-v2#确认具有如上定义标签的pod[root@k8s-master ~]# kubectl get pods -l app=nginx-v1NAME READY STATUS RESTARTS AGEv1-6b67b54dfc-qvjvl 2/2 Running 0 73m[root@k8s-master ~]# kubectl get pods -l app=nginx-v2NAME READY STATUS RESTARTS AGEv2-68c9d69c4f-jktr5 2/2 Running 0 71m#创建[root@k8s-master ~]# kubectl apply -f istio-pod-vlbetal-destination.yaml destinationrule./istio-pod-vlbetal-destination created[root@k8s-master ~]# kubectl get destinationRuleNAME HOST AGEistio-pod-vlbetal-destination istio-pod-nginx.default.svc.cluster.local 15s#验证，定义了v1权重90，v2权重10，也就是说，访问100次只有10次可以访问到v2#如下访问k8s集群任意的节点IP+istio的ingressgateway的service映射的物理机端口31718#curl 192.168.57.131:31718#使用for循环请求一百次查看结果如下[root@k8s-master ~]# for i in &#123;1..100&#125;;do curl 192.168.57.131:31718;done &gt; 1.txt#查看请求结果如下：（与设置的权重百分比虽有偏差，但是非常接近）[root@k8s-master ~]# cat 1.txt | grep v1 | wc -l89[root@k8s-master ~]# cat 1.txt | grep v2 | wc -l11逐步增加 v2 版本的流量比例如果 v2 版本运行稳定，我们可以逐步增加其流量比例。修改 istio-pod-vlbetal.yaml 文件中的 weight 值，例如将 v1 的权重设置为 50，v2 的权重设置为 50，然后使用 kubectl apply -fistio-pod-vlbetal.yaml命令更新配置。高级灰度策略除了简单的流量比例切分，Istio 还支持更高级的灰度策略，例如：基于用户 ID 的灰度： 将特定用户 ID 的流量路由到新版本。基于 HTTP Header 的灰度： 根据 HTTP Header 中的特定字段，将流量路由到新版本。基于 Cookie 的灰度： 根据 Cookie 中的特定字段，将流量路由到新版本。这些高级策略可以帮助我们实现更精细的灰度发布，例如只让内部员工或特定用户体验新版本。以下是一个基于 HTTP Header 的灰度示例：123456789101112131415161718192021222324252627# productpage-virtualservice-header.yamlapiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: productpagespec: hosts: - "productpage" gateways: - your-gateway # 替换为你的 Gateway 名称 http: - match: - headers: user-agent: regex: ".*Mobile.*" route: - destination: host: productpage subset: v2 - route: - destination: host: productpage subset: v1 # 解释：# match：使用 headers 字段匹配 HTTP Header。这里将所有 User-Agent 包含 Mobile 字符串的请求路由到 v2 版本。istio的核心功能熔断熔断的目的是在出现故障或异常情况时，对服务进行自动的限流和隔离，以保护整个系统的稳定性和可用性。首先创建一个pod服务进行熔断测试。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970#参考使用istio自带的yaml文件[root@k8s-master ~]# cd istio-1.18.2/samples/httpbin/[root@k8s-master httpbin]# cat httpbin.yaml //该yaml文件创建了sa账号，service及podapiVersion: v1kind: ServiceAccountmetadata: name: httpbin---apiVersion: v1kind: Servicemetadata: name: httpbin labels: app: httpbin service: httpbinspec: ports: - name: http port: 8000 targetPort: 80 selector: app: httpbin---apiVersion: apps/v1kind: Deploymentmetadata: name: httpbinspec: replicas: 1 selector: matchLabels: app: httpbin version: v1 template: metadata: labels: app: httpbin version: v1 spec: serviceAccountName: httpbin containers: - image: /kong/httpbin imagePullPolicy: IfNotPresent name: httpbin ports: - containerPort: 80#创建[root@k8s-master httpbin]# kubectl apply -f httpbin.yaml serviceaccount/httpbin createdservice/httpbin createddeployment.apps/httpbin created[root@k8s-master httpbin]# kubectl get pods -owideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATEShttpbin-65975d4c6f-2nvnw 2/2 Running 0 14s 10.244.194.97 k8s-worker1 &lt;none&gt; &lt;none&gt;[root@k8s-master httpbin]# kubectl get sa NAME SECRETS AGEhttpbin 0 18s[root@k8s-master httpbin]# kubectl get sve[root@k8s-master httpbin]# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEhttpbin ClusterIP 10.98.237.91 &lt;none&gt; 8000/TCP 29s[root@k8s-master httpbin]# kubectl describe svc httpbin | grep -i endpointsEndpoints: 10.244.194.97:80 //确认该service已成功代理到新创建的httpbin的pod配置熔断规则对上方创建的pod服务进行配置熔断规则：12345678910111213141516171819202122232425262728[root@k8s-master ~]# vim destinationrule.yamlapiVersion: /v1beta1kind: DestinationRulemetadata: name: httpbinspec: host: httpbin #host指定service的名称，通过service找到它代理的后端pod trafficPolicy: #配置熔断的策略 connectionPool: #配置熔断的连接池 tcp: #指定限制tcp协议连接的数量 maxConnections: 1 #指定每个目标服务最大连接数为1（目标就是上方定义的hosts指定的service所代理的后端pod） http: #指定http协议的连接数 http1MaxPendingRequests: 1 #每个连接最大的挂起请求数为1，超过即阻塞 maxRequestsPerConnection: 1 #每个连接最大的请求数为1，每个连接最多只能处理一个请求 outlierDetection: #检测目标服务的异常情况（目标就是上方定义的hosts指定的service所代理的后端pod） consecutiveGatewayErrors: 1 #访问目标服务的时候，目标会返回结果，如果返回错误大于等于1，则认为它异常 interval: 1s #探测时间为1秒探测一次 baseEjectionTime: 3m #定义重新连接时间，探测异常之后，三分钟内不进行连接 maxEjectionPercent: 100 #100表示，只要探测一次失败，就将连接100%拒绝#创建[root@k8s-master ~]# kubectl apply -f destinationrule.yaml destinationrule./httpbin created[root@k8s-master ~]# kubectl get DestinationRuleNAME HOST AGEhttpbin httpbin 8s创建客户端模拟请求使用istio工具自带的yaml文件创建一个客户端进行模拟访问httpbin pod服务，从而测试熔断的规则是否生效。该pod中运行的服务是fortio，fortio是专门对网站进行压测的。通过该工具调用httpbin服务从而测试熔断规则。创建fortio1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071#进入到目标路径[root@k8s-master ~]# cd istio-1.18.2/samples/httpbin/sample-client#找到如下yaml文件#该文件定义了一个servic，一个deployment控制器，该控制器管理一个pod副本，使用的镜像就是fortio/fortio:latest_release[root@k8s-master sample-client]# vim fortio-deploy.yamlapiVersion: v1kind: Servicemetadata: name: fortio labels: app: fortio service: fortiospec: ports: - port: 8080 name: http selector: app: fortio---apiVersion: apps/v1kind: Deploymentmetadata: name: fortio-deployspec: replicas: 1 selector: matchLabels: app: fortio template: metadata: annotations: # This annotation causes Envoy to serve cluster.outbound statistics via 15000/stats # in addition to the stats normally served by Istio. The Circuit Breaking example task # gives an example of inspecting Envoy stats via proxy config. /config: |- proxyStatsMatcher: inclusionPrefixes: - "cluster.outbound" - "cluster_manager" - "listener_manager" - "server" - "cluster.xds-grpc" labels: app: fortio spec: containers: - name: fortio image: fortio/fortio:latest_release imagePullPolicy: Always ports: - containerPort: 8080 name: http-fortio - containerPort: 8079 name: grpc-ping #创建[root@k8s-master sample-client]# kubectl apply -f fortio-deploy.yaml service/fortio createddeployment.apps/fortio-deploy created[root@k8s-master sample-client]# kubectl get pods NAME READY STATUS RESTARTS AGEfortio-deploy-77fd47587d-xc2rb 1/2 Running 0 5shttpbin-65975d4c6f-2nvnw 2/2 Running 0 40m[root@k8s-master sample-client]# kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEfortio ClusterIP 10.108.185.62 &lt;none&gt; 8080/TCP 14shttpbin ClusterIP 10.98.237.91 &lt;none&gt; 8000/TCP 40m模拟访问请求12345678910111213141516171819 #测试请求一次，如下显示正常[root@k8s-master sample-client]# kubectl exec fortio-deploy-77fd47587d-xc2rb -c fortio -- /usr/bin/fortio curl http://httpbin:8000/get&#123;"ts":1731640199.687434,"level":"info","r":1,"file":"scli.go","line":122,"msg":"Starting","command":"Φορτίο","version":"1.66.5 h1:WTJzTGOA12YWZSM5g43602lH+GOsmP3eKHXLnuRW4vs= go1.22.7 amd64 linux","go-max-procs":2&#125;&#123;"args":&#123;&#125;,"headers":&#123;"Host":"httpbin:8000","User-Agent":"fortio.org/fortio-1.66.5","X-B3-Parentspanid":"5d47d415a942dccc","X-B3-Sampled":"1","X-B3-Spanid":"66b7ec176aec3566","X-B3-Traceid":"ff306f65ca8d3fe35d47d415a942dccc","X-Envoy-Attempt-Count":"1","X-Forwarded-Client-Cert":"By=spiffe://cluster.local/ns/default/sa/httpbin;Hash=492b9ebc73979709b0651c1233de23aa512178f3199b2d3fab9bf9ede8c38232;Subject=\"\";URI=spiffe://cluster.local/ns/default/sa/default"&#125;,"origin":"127.0.0.6","url":"http://httpbin:8000/get"&#125;HTTP/1.1 200 OKserver: envoydate: Fri, 15 Nov 2024 03:09:59 GMTcontent-type: application/jsoncontent-length: 516access-control-allow-origin: *access-control-allow-credentials: truex-envoy-upstream-service-time: 19参数解释：#exec // 进入到目标pod fortio （自己的fortio名称）#-c //指定pod中的容器 fortio#-- /usr/bin/fortio curl http://httpbin:8000/get //使用该命令请求httpbin12345678910111213#测试多次请求，测试熔断中设置的http协议访问规则最大请求数及并发数等规则是否生效。[root@k8s-master sample-client]# kubectl exec -it fortio-deploy-77fd47587d-xc2rb -c fortio -- /usr/bin/fortio load -c 2 -qps 0 -n 20 -loglevel Warning http://httpbin:8000/get参数解释：#-c 2 //发起两个并发连接#-qps 0 //每秒请求数为0，零表示不限制，测试请求过程中，每秒内都以最大的速率去请求。#-n 20 //总的请求数为20次，配合-c 2 ，总共发起20次请求，两个并发。（也就是一个连接请求20次，并发两个）#-loglevel Warning //定义日志级别为Warning#如下图中的状态码可以看到Code 200 的返回了85.0 %，Code 503 的返回了15.0 %，#可解释为，第一个请求20次的连接来了后就进行了相应，但是在20次请求中还没响应完成，第二个并发的请求就来了，导致了阻塞。#由此可见熔断规则确实生效。（每个服务器不同，响应速度也不同，所以返回状态码的百分比也不相同）超时在多台服务器互相请求时为了避免等待的响应时间过长，从而堆积大量的请求阻塞了自身服务，造成雪崩的影响，这种情况下可以可以使用istio的“超时”功能。如A服务，请求B服务，可以设置超时等待时间，如请求时超过了这个设置的等待时间，那么将不再等待请求，直接返回请求超时错误。创建测试超时使用的pod这里使用了nginx、tomcat镜像创建pod，并创建svc代理这两个pod。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113#使用deployment控制器创建nginx pod副本[root@k8s-master ~]# vim timeout-deploy-nginx.yamlapiVersion: apps/v1kind: Deploymentmetadata: name: nginx labels: server: nginx app: webspec: replicas: 1 selector: matchLabels: server: nginx app: web template: metadata: name: nginx labels: server: nginx app: web spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent #创建[root@k8s-master ~]# kubectl apply -f timeout-deploy-nginx.yaml deployment.apps/nginx created[root@k8s-master ~]# kubectl get pods | egrep -i "name|nginx"NAME READY STATUS RESTARTS AGEnginx-9d6c758bb-98l49 2/2 Running 0 23s[root@k8s-master ~]# #使用deployment控制器创建tomcat pod副本[root@k8s-master ~]# vim timeout-deploy-tomcat.yamlapiVersion: apps/v1kind: Deploymentmetadata: name: tomcat labels: server: tomcat app: webspec: replicas: 1 selector: matchLabels: server: tomcat app: web template: metadata: name: tomcat labels: server: tomcat app: web spec: containers: - name: tomcat image: tomcat imagePullPolicy: IfNotPresent #创建[root@k8s-master ~]# kubectl apply -f timeout-deploy-tomcat.yamldeployment.apps/tomcat created[root@k8s-master ~]# kubectl get pods | egrep -i "name|nginx|tomcat"NAME READY STATUS RESTARTS AGEnginx-9d6c758bb-98l49 2/2 Running 0 4m14stomcat-5fb5bd59cd-86jwr 2/2 Running 0 8s#创建servic 代理nginx及tomcat pod。[root@k8s-master ~]# vim timeout-nginx-tomcat-service.yaml apiVersion: v1kind: Servicemetadata: name: nginx-svcspec: selector: server: nginx ports: - name: http port: 80 targetPort: 80 protocol: TCP---apiVersion: v1kind: Servicemetadata: name: tomcat-svcspec: selector: server: tomcat ports: - name: http port: 8080 targetPort: 8080 protocol: TCP #创建[root@k8s-master ~]# kubectl apply -f timeout-nginx-tomcat-service.yaml service/nginx-svc createdservice/tomcat-svc created[root@k8s-master ~]# kubectl get svc | egrep -i "nginx|tomcat"istio-pod-nginx ClusterIP 10.96.179.74 &lt;none&gt; 80/TCP 6d21hnginx-svc ClusterIP 10.97.48.223 &lt;none&gt; 80/TCP 24stomcat-svc ClusterIP 10.111.194.112 &lt;none&gt; 8080/TCP 24s配置超时12345678910111213141516171819202122232425262728293031323334353637383940#创建pod[root@k8s-master ~]# vim timeout-virtual-nginx-tomcat.yamlapiVersion: /v1beta1kind: VirtualServicemetadata: name: nginx-vsspec: hosts: #指定svc - nginx-svc #指定作用在nginx-svc，相当于作用的是它代理的nginx的pod http: - route: - destination: #指定超时时间 host: nginx-svc #指定目标 timeout: 2s #指定超时时间2秒---apiVersion: /v1beta1kind: VirtualServicemetadata: name: tomcat-vsspec: hosts: - tomcat-svc #同上，这里指定的是tomcat-svc http: - fault: #模拟注入配置故障规则 delay: #配置延迟响应的规则 percentage: #指定延迟流量的百分比 value: 100 #这里指定100，表示100%的流量都会收到延迟响应 fixedDelay: 10s #指定延迟响应时间为10秒 route: - destination: #指定访问的目标主机 host: tomcat-svc #创建[root@k8s-master ~]# kubectl apply -f timeout-virtual-nginx-tomcat.yaml virtualservice./nginx-vs createdvirtualservice./tomcat-vs created[root@k8s-master ~]# kubectl get VirtualService | egrep -i "nginx|tomcat"nginx-vs ["nginx-svc"] 21stomcat-vs ["tomcat-svc"] 21snginx配置反向代理tomcat123456#进入到nginx pod配置反向代理，将nginx请求代理到tomcat[root@k8s-master ~]# kubectl exec -it nginx-9d6c758bb-98l49 -c nginx -- /bin/sh/ # vi /etc/nginx/conf.d/default.conf //修改配置文件增加两行内容，如下图片所示：proxy_pass http://tomcat-svc:8080;proxy_http_version 1.1;/ # nginx -t/ # nginx -s reload //重启nginx/ # exit测试超时规则这里使用了busybox镜像运行pod去访问nginx测试超时规则是否生效。1234567891011121314151617[root@k8s-master ~]# kubectl run busybox --image=busybox --restart=Never --rm -it --image-pull-policy=IfNotPresent -- shIf you don\'t see a command prompt, try pressing enter./ # time wget -q -O - http://nginx-svc:80wget: server returned error: HTTP/1.1 504 Gateway Timeout//这里的504超时，是因为超时规则设置的是最多等待两秒。tomcat配置的是延迟十秒，超过两秒则等待超时Command exited with non-zero status 1real 0m 2.01s //这里显示的是实际执行时间。（2秒）user 0m 0.00ssys 0m 0.00s#测试tomcat响应时间/ # time wget -q -O - http://tomcat-svc:8080 | tail -n 3real 0m 10.01s //这里可看到，执行了十秒才响应。user 0m 0.00ssys 0m 0.00s故障注入和重试“重试”，顾名思义，当第一次连接失败之后，可以根据配置的重试规则次数，在连接失败之后重新连接几次根据上方测试”超时“时使用的nginx代理tomcat为例，测试“故障注入和重试”：1234#因为需要配置“重试”规则，需要将上方测试“超时”时创建的VirtualService资源删除[root@k8s-master ~]# kubectl delete -f timeout-virtual-nginx-tomcat.yaml virtualservice. "nginx-vs" deletedvirtualservice. "tomcat-vs" deleted配置重试规则并注入故障规则123456789101112131415161718192021222324252627282930313233343536373839404142#配置重试规则[root@k8s-master ~]# vim timeout-virtual-nginx-tomcat-02.yamlapiVersion: /v1beta1kind: VirtualServicemetadata: name: nginx-vsspec: hosts: - nginx-svc http: - route: - destination: #指定作用的目标 host: nginx-svc retries: attempts: 3 #指定重试次数 perTryTimeout: 2s #指定每两秒重试一次---apiVersion: /v1beta1kind: VirtualServicemetadata: name: tomcat-vsspec: hosts: - tomcat-svc http: - fault: #模拟注入故障规则 abort: percentage: #指定访问流量百分比 value: 100 httpStatus: 503 #指定请求返回错误结果为503 route: - destination: #指定目标 host: tomcat-svc#创建[root@k8s-master ~]# kubectl apply -f timeout-virtual-nginx-tomcat-02.yaml virtualservice./nginx-vs createdvirtualservice./tomcat-vs created[root@k8s-master ~]# kubectl get VirtualService | egrep -i "nginx|tomcat"nginx-vs ["nginx-svc"] 12stomcat-vs ["tomcat-svc"] 12s测试重试规则继续使用busybox镜像进行访问测试：1234567891011[root@k8s-master ~]# kubectl run busybox --image=busybox --restart=Never --rm -it --image-pull-policy=IfNotPresent -- shIf you don\'t see a command prompt, try pressing enter./ # wget -q -O - http://nginx-svc:80 //访问测试wget: server returned error: HTTP/1.1 503 Service Unavailable/ # exit#观察nginx pod中的istio代理容器日志查看是否发生了重试[root@k8s-master ~]# kubectl logs -f nginx-9d6c758bb-98l49 -c istio-proxy#日志内容如下图所示，重试了四次，且每次都返回了503（首次访问失败并按照配置的重试规则进行重新访问了3次，一共4次）Istio网关额外配置IP黑白名单当服务受到攻击，或者只允许服务在某些 IP 下才可以访问的时候，通常的做法是为服务加上 IP 黑白名单。在配置好Gateway，virtualservice后可以从集群外访问集群内的服务，如果想限制特定的IP访问不了可以加入黑名单：123456789101112131415161718apiVersion: security.istio.io/v1beta1kind: AuthorizationPolicymetadata: name: ingress-policy namespace: istio-systemspec: selector: matchLabels: app: istio-ingressgateway action: DENY rules: - from: - source: ipBlocks: ["38.75.137.213"] to: - operation: hosts: - httpbin.makeoptim.com这里，我的 IP 是 38.75.137.213，而在 AuthorizationPolicy 中，设置了 DENY 并指向 httpbin.makeoptim.com。这时，再次访问 httpbin.makeoptim.com 就会返回 RBAC: access denied，表示已经成功设置了黑名单。12❯ curl httpbin.makeoptim.com/getRBAC: access denied注：1.ipBlocks 可以支持单个 IP (e.g. “1.2.3.4”) 和 CIDR (e.g. “1.2.3.0/24”)2.上面的例子为设置黑名单，白名单就是把 DENY 变成 ALLOW3.to/operation/hosts 可以指定某些 host，这样可以只对某些服务做黑白名单，而不影响到整个服务网格。限制请求体大小对所有 ingressgateway 生效123456789101112131415161718192021222324apiVersion: networking.istio.io/v1alpha3kind: EnvoyFiltermetadata: name: limit-request-size namespace: istio-system # istio-system 表示针对所有命名空间生效spec: workloadSelector: # 选中所有 ingressgateway labels: istio: ingressgateway # 所有 ingressgateway 都带此 label configPatches: - applyTo: HTTP_FILTER match: context: GATEWAY listener: filterChain: filter: name: "envoy.http_connection_manager" patch: operation: INSERT_BEFORE value: name: "envoy.filters.http.buffer" typed_config: "@type": "type.googleapis.com/envoy.extensions.filters.http.buffer.v3.Buffer" max_request_bytes: 1048576 # 1MB, 请求最大限制对指定 ingressgateway 生效123456789101112131415161718192021222324apiVersion: networking.istio.io/v1alpha3kind: EnvoyFiltermetadata: name: limit-request-size namespace: prod # 选择指定 ingressgateway 所在的命名空间spec: workloadSelector: # 选中指定 ingressgateway labels: app: istio-ingressgateway-public # 替换指定 ingressgateway 的 label configPatches: - applyTo: HTTP_FILTER match: context: GATEWAY listener: filterChain: filter: name: "envoy.http_connection_manager" patch: operation: INSERT_BEFORE value: name: "envoy.filters.http.buffer" typed_config: "@type": "type.googleapis.com/envoy.extensions.filters.http.buffer.v3.Buffer" max_request_bytes: 1048576 # 1MB, 请求最大限制]]></content>
      <categories>
        <category>技术</category>
        <category>K8S</category>
      </categories>
      <tags>
        <tag>服务网格</tag>
        <tag>Istio</tag>
        <tag>灰度发布</tag>
        <tag>熔断</tag>
        <tag>超时</tag>
        <tag>故障注入</tag>
        <tag>重试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s更新应用版本优雅终止旧pod]]></title>
    <url>%2F2025%2F09f71c8c0e.html</url>
    <content type="text"><![CDATA[所谓优雅终止，就是保证在销毁 Pod 的时候保证对业务无损，比如在业务发版时，让工作负载能够平滑滚动更新。 Pod 在销毁时，会停止容器内的进程，通常在停止的过程中我们需要执行一些善后逻辑，比如等待存量请求处理完以避免连接中断，或通知相关依赖进行清理等，从而实现优雅终止目的。pod终止流程我们先了解下容器在 Kubernetes 环境中的终止流程:Pod 被删除，状态变为 Terminating。从 API 层面看就是 Pod metadata 中的 deletionTimestamp 字段会被标记上删除时间。kube-proxy watch 到了就开始更新转发规则，将 Pod 从 service 的 endpoint 列表中摘除掉，新的流量不再转发到该 Pod。kubelet watch 到了就开始销毁 Pod。3.1. 如果 Pod 中有 container 配置了 preStop Hook ，将会执行。3.2. 发送 SIGTERM 信号给容器内主进程以通知容器进程开始优雅停止。3.3. 等待 container 中的主进程完全停止，如果在 terminationGracePeriodSeconds 内 (默认 30s) 还未完全停止，就发送 SIGKILL 信号将其强制杀死。3.4. 所有容器进程终止，清理 Pod 资源。3.5. 通知 APIServer Pod 销毁完成，完成 Pod 删除。合理使用preStop在某些极端情况下，Pod 被删除的一小段时间内，仍然可能有新连接被转发过来，因为 kubelet 与 kube-proxy 同时 watch 到 pod 被删除，kubelet 有可能在 kube-proxy 同步完规则前就已经停止容器了，这时可能导致一些新的连接被转发到正在删除的 Pod，而通常情况下，当应用收到 SIGTERM 后都不再接受新连接，只保持存量连接继续处理，所以就可能导致 Pod 删除的瞬间部分请求失败。这种情况下，我们也可以利用 preStop 先 sleep 一小下，等待 kube-proxy 完成规则同步再开始停止容器内进程:123456lifecycle: preStop: exec: command: - sleep - 15s配置保守的更新策略如果对稳定性要求较高，可以设置比较保守的滚动更新策略：保持足够多的可用副本数量。避免在滚动时可以正常处理请求的 Pod 数量减少导致部分请求因后端 Pod 处理不过来而异常。减缓发版速度。一方面可以避免新版应用引入难以发现的问题快速扩散，方便发现后及时回滚恢复；另一方面，如果使用 LB 直通 Pod，更新过程中，云厂商的 service-controller 或 cloud-controller-manager 组件会更新 LB 的后端 rs，这个过程是异步的，在某些极端场景下，可能出现 LB 后端的 rs 还没更新，旧的 Pod 副本已经被销毁了，从而导致流量转发到已销毁的 Pod 而引发异常。给新副本留预热时间。新副本启动时，多给应用一些时间进行准备，避免某些应用虽然探测接口返回就绪，但实际处理能力还没跟上，过早转发请求过来可能导致异常。12345678910111213141516171819202122232425262728293031323334apiVersion: apps/v1kind: Deploymentmetadata: name: nginxspec: replicas: 1 selector: matchLabels: app: nginx strategy: type: RollingUpdate rollingUpdate: # 单个串行升级，等新副本 ready 后才开始销毁旧副本 maxUnavailable: 0 # 更新过程中，允许不可用的旧 Pod 数量 maxSurge: 1 # 更新过程中，允许超出期望副本数的新 Pod 数量 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:latest startupProbe: httpGet: path: / port: 80 successThreshold: 5 # 新副本启动时，连续探测成功多次后才交给 readinessProbe 探测 periodSeconds: 5 readinessProbe: httpGet: path: / port: 80 successThreshold: 1 # 运行过程中探测 1 次成功就认为 ready，可在抖动导致异常后快速恢复服务 periodSeconds: 5长连接场景如果业务是长链接场景，比如游戏、会议、直播等，客户端与服务端会保持着长链接，销毁 Pod 时需要的优雅终止的时间通常比较长 (preStop + 业务进程停止超过 30s)，有的极端情况甚至可能长达数小时，这时候可以根据实际情况自定义 terminationGracePeriodSeconds，避免过早的被 SIGKILL 杀死，示例:具体设置多大可以根据业务场景最坏的情况来预估，比如对战类游戏场景，同一房间玩家的客户端都连接的同一个服务端 Pod，一轮游戏最长半个小时，那么我们就设置 terminationGracePeriodSeconds 为 1800。如果不好预估最坏的情况，最好在业务层面优化下，比如 Pod 销毁时的优雅终止逻辑里面主动通知下客户端，让客户端连到新的后端，然后客户端来保证这两个连接的平滑切换。等旧 Pod 上所有客户端连接都连切换到了新 Pod 上，才最终退出。]]></content>
      <categories>
        <category>技术</category>
        <category>K8S</category>
      </categories>
      <tags>
        <tag>pod无缝更新</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[代码扫描平台SonarQube部署及配置jenkins集成]]></title>
    <url>%2F2025%2F089615066.html</url>
    <content type="text"><![CDATA[Sonar是一个用于代码质量管理的开源平台，用于管理源代码的质量，可以从七个维度检测代码质量，通过插件形式，可以支持包括java,C#,C/C++,PL/SQL,Cobol,JavaScrip,Groovy等等二十几种编程语言的代码质量管理与检测。docker-compose安装SonarQubedocker-compose.yml：12345678910111213141516171819202122232425262728293031323334353637version: '3'services: postgres: image: docker.1ms.run/postgres:12 container_name: postgres restart: always privileged: true volumes: - ./postgres/postgresql:/var/lib/postgresql - ./postgres/data:/var/lib/postgresql/data - /etc/localtime:/etc/localtime:ro ports: - "5432:5432" environment: POSTGRES_USER: sonar POSTGRES_PASSWORD: sonar POSTGRES_DB: sonar TZ: Asia/Shanghai sonar: image: docker.1ms.run/sonarqube:9.9.6-community container_name: sonar restart: always privileged: true depends_on: - postgres volumes: - ./sonarqube/logs:/opt/sonarqube/logs - ./sonarqube/conf:/opt/sonarqube/conf - ./sonarqube/data:/opt/sonarqube/data - ./sonarqube/extensions:/opt/sonarqube/extensions ports: - "19000:9000" environment: SONARQUBE_JDBC_USERNAME: sonar SONARQUBE_JDBC_PASSWORD: sonar SONARQUBE_JDBC_URL: "jdbc:postgresql://postgres:5432/sonar"创建插件目录，上传插件到该目录下：1sonarqube/extensions/plugins插件：多分支：sonarqube-community-branch-plugin-1.14.0.jar下载链接：https://github.com/mc1arke/sonarqube-community-branch-plugin/releases/download/1.14.0/sonarqube-community-branch-plugin-1.14.0.jar汉化：sonar-l10n-zh-plugin-9.9.jar下载链接：https://github.com/xuhuisheng/sonar-l10n-zh/releases注：以上插件适配9.9版本的SonarQube，其他版本的需要找对应的版本插件。配置权限：1chown 1000:1000 -R sonarqube/extensions/plugins加载插件：12345678vim sonarqube/conf/sonar.properties sonar.web.javaAdditionalOpts=-javaagent:/opt/sonarqube/extensions/plugins/sonarqube-community-branch-plugin-1.14.0.jar=websonar.ce.javaAdditionalOpts=-javaagent:/opt/sonarqube/extensions/plugins/sonarqube-community-branch-plugin-1.14.0.jar=ce sonar.jdbc.username=sonarsonar.jdbc.password=sonarsonar.jdbc.url=jdbc:postgresql://postgres:5432/sonar配置权限：1chown 1000:1000 -R sonarqube/conf启动服务：1docker-compose up -d启动完成后，通过10.168.2.236:19000访问，初始账号密码是admin/adminjenkins集成SonarQubejenkins安装SonarQube Scanner插件插件市场的版本跟我们安装的jenkins版本不兼容：可以到以下链接去把插件下载，再上传到jenkins：https://updates.jenkins-ci.org/download/plugins/搜索Sonar，点击进去详情，我们的jenkins版本是2.361.4，所以下载2.15的版本：下载完成后把插件上传到jenkins： sonar生成token： jenkins配置SonarQube服务连接及配置SonarQubeScanner工具先配置凭据，系统管理-manager credentials-全局，创建新凭据：jenkins系统管理处配置sonar连接：配置SonarQubeScanner工具：jenkins系统管理-全局工具配置：在jenkins的job里引入SonarQube扫描自由风格的软件项目类型的job在原本job的基础上，构建步骤里加上Execute SonarQube Scanner： 流水线类型的job我们的后端是java项目，用Maven构建，这里用Maven执行SonarQube扫描：完整pipeline如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101pipeline&#123; agent any options &#123; timestamps() // daysToKeepStr:&lt;Days to keep builds&gt;, numToKeepStr:&lt;Max # of builds to keep&gt; buildDiscarder(logRotator(daysToKeepStr: '7', numToKeepStr: '10')) &#125; environment &#123; // Git Info 需修改 __Git_Addr = 'http://gitlab.xxx.com/saas-back-end/qifu-saas-basic.git' __Git_Name = "$&#123;__Git_Addr.split('/')[-1].split('.git')[0]&#125;" wxbot_id = 'https://qyapi.weixin.qq.com/cgi-bin/webhook/send?key=9245d561-9d29-4d58-be39-5edc3dba293d' &#125; parameters &#123; // TAG //gitParameter(name:'BRANCH', type: 'PT_TAG', defaultValue: 'master', sortMode: 'DESCENDING_SMART', selectedValue: 'TOP', quickFilterEnabled: true) gitParameter branch: '', branchFilter: '.*', defaultValue: 'master', listSize: '', name: 'BRANCH', quickFilterEnabled: true, selectedValue: 'NONE', sortMode: 'NONE', tagFilter: '*', type: 'GitParameterDefinition' &#125; stages&#123; stage("Git Code") &#123; steps&#123; deleteDir() script&#123; checkout([$class: 'GitSCM', branches: [[name: "$&#123;BRANCH&#125;"]], extensions: [], userRemoteConfigs: [[credentialsId: 'gitlab', url: "$&#123;__Git_Addr&#125;"]]]) &#125; &#125; &#125; stage("Build") &#123; steps&#123; dir("$&#123;WORKSPACE&#125;")&#123; script &#123; sh "source /etc/profile;mvn clean package -P development -DskipTests -U" &#125; &#125; &#125; &#125; stage('SonarQube Analysis') &#123; steps &#123; echo "开始执行 SonarQube 检测 ......" withSonarQubeEnv('SonarQube') &#123; sh """ source /etc/profile; mvn sonar:sonar \ -Dsonar.projectKey=qifu-saas-basic \ -Dsonar.java.binaries=target/classes \ """ &#125; &#125; &#125; stage("Docker Build&amp;Push&amp;Publish") &#123; steps&#123; dir("$&#123;WORKSPACE&#125;")&#123; script&#123; jar_name = sh (returnStdout: true, script: 'ls qifu-saas-basic-service/target/ | grep \'.jar\' | grep -v \'sources\' | head -n 1').trim() sh """ rm Dockerfile -rf cp /opt/data/oom-test.sh /opt/data/mc . cp /opt/data/apache-skywalking-java-agent-9.4.0.tgz . cp /opt/data/jacocoagent.jar . JAR_NAME=$&#123;jar_name&#125; echo 'FROM ubuntu:16.04 as jar' &gt;&gt; Dockerfile echo 'WORKDIR /' &gt;&gt; Dockerfile echo 'RUN apt-get update -y' &gt;&gt; Dockerfile echo 'RUN DEBIAN_FRONTEND=noninteractive apt-get install -y wget' &gt;&gt; Dockerfile echo 'RUN wget https://repo1.maven.org/maven2/io/prometheus/jmx/jmx_prometheus_javaagent/0.20.0/jmx_prometheus_javaagent-0.20.0.jar' &gt;&gt; Dockerfile echo 'FROM openjdk:11' &gt;&gt; Dockerfile echo \"ADD qifu-saas-basic-service/target/\$&#123;JAR_NAME&#125; /app/\$&#123;JAR_NAME&#125;\" &gt;&gt; Dockerfile echo 'COPY --from=jar /jmx_prometheus_javaagent-0.20.0.jar /app/jmx_prometheus_javaagent-0.20.0.jar' &gt;&gt; Dockerfile echo 'COPY oom-test.sh /app/' &gt;&gt; Dockerfile echo 'COPY mc /usr/local/sbin/' &gt;&gt; Dockerfile echo 'ADD apache-skywalking-java-agent-9.4.0.tgz /app/' &gt;&gt; Dockerfile echo 'COPY jacocoagent.jar /app/' &gt;&gt; Dockerfile echo 'ENV TZ=Asia/Shanghai' &gt;&gt; Dockerfile echo 'EXPOSE 8088' &gt;&gt; Dockerfile echo 'EXPOSE 8913' &gt;&gt; Dockerfile echo 'EXPOSE 8089' &gt;&gt; Dockerfile echo 'ENTRYPOINT ["java", "-XX:+HeapDumpOnOutOfMemoryError", "-XX:HeapDumpPath=/app/dump.hprof", "-XX:+ExitOnOutOfMemoryError", "-XX:OnOutOfMemoryError=/app/oom-test.sh", "-javaagent:/app/jacocoagent.jar=includes=*,output=tcpserver,port=8089,address=0.0.0.0", "-javaagent:/app/skywalking-agent/skywalking-agent.jar", "-Dskywalking.agent.service_name=qifu-saas-basic-test", "-Dskywalking.collector.backend_service=oap.qifu.svc:11800", "-jar", "-javaagent:/app/jmx_prometheus_javaagent-0.20.0.jar=8088:/app/jmx_exporter.yaml", "/app/$&#123;jar_name&#125;"]' &gt;&gt; Dockerfile """ sh ''' tag=`date +%s`;docker build -t harbor.keyfel.com/qifu-test/qifu-saas-basic:$&#123;VERSION&#125;-$&#123;tag&#125; . docker push harbor.keyfel.com/qifu-test/qifu-saas-basic:$&#123;VERSION&#125;-$&#123;tag&#125; kubectl set image deployment/qifu-saas-basic qifu-saas-basic=harbor.keyfel.com/qifu-test/qifu-saas-basic:$&#123;VERSION&#125;-$tag -n qifu ''' &#125; &#125; &#125; &#125; &#125; post &#123; failure&#123; qyWechatNotification mentionedId: '', mentionedMobile: '16620101812', moreInfo: '', webhookUrl: 'https://qyapi.weixin.qq.com/cgi-bin/webhook/send?key=9245d561-9d29-4d58-be39-5edc3dba293d' &#125; &#125;&#125;如果想要选择勾选是否需要进行代码扫描，以及部署到不同的空间及配置副本数，可参考以下pipeline：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293pipeline &#123; agent any tools&#123; maven 'mvn-3.6.3' &#125; //全局变量 environment &#123; //Harbor仓库地址以及以及镜像所在Harbor项目 Harbor_Registry_URL= 'xxxx' Harbor_Registry_Cert= 'xxxx' Fat_Harbor_Registry_Project= 'xxxx' Pro_Harbor_Registry_Project= 'xxxx' //gitee代码仓库地址以及项目名称 Gitee_Registry_URL= 'xxxx' Gitee_Code_Project= 'gotone-dw-api' //-SonqrQube信息 SonarQube_URL= 'http://internal.sonarqube.com' SonarQube_Secret= '7231ab346d6bf1875e7a6da5353a872aab25afa0' //项目所在kubernetes命名空间（环境） Kubernetes_Project_Namespace_fat= 'xxx' &#125; parameters &#123; gitParameter branch: '', branchFilter: 'origin/(.*)', defaultValue: 'master', description: '选择拉取代码的分支', name: 'Branch', quickFilterEnabled: false, selectedValue: 'NONE', sortMode: 'NONE', tagFilter: '*', type: 'GitParameterDefinition' booleanParam description: '是否进行代码质量检测；', name: 'IS_CODE_DETECTION' booleanParam description: '是否部署到fat环境；', name: 'IS_DEPLOY_FAT' booleanParam description: '是否部署到fat1环境；', name: 'IS_DEPLOY_FAT1' booleanParam description: '是否将本次构建镜像推送到pro线上仓库；', name: 'Push_Image_To_Pro_Registry' choice(name:'Replicase', choices:'1\n3\n5', description:'请选择副本数(如果对此参数不清楚的话，默认即可);' ) &#125; stages &#123; stage('拉取代码') &#123; steps &#123; xxxx &#125; &#125; stage('质量检测') &#123; when &#123; expression &#123; return params.IS_CODE_DETECTION &#125; &#125; steps &#123; withSonarQubeEnv('SonarQube') &#123; sh """ mvn sonar:sonar \ -Dsonar.projectKey=$&#123;Gitee_Code_Project&#125; \ -Dsonar.host.url=$&#123;SonarQube_URL&#125; \ -Dsonar.login=$&#123;SonarQube_Secret&#125; """ &#125; &#125; &#125; stage('代码编译') &#123; steps &#123; sh """ mvn xxxx这里是正常编译代码的步骤 """ &#125; &#125; stage('构建并上传到测试仓库') &#123; steps &#123; xxxx xxxx &#125; &#125; //-部署到fat环境 stage('部署到测试环境') &#123; when &#123; expression &#123; return params.IS_DEPLOY_FAT &#125; &#125; steps &#123; xxx &#125; &#125; stage('将本次构建的镜像传到pro仓库') &#123; when &#123; expression &#123; return params.Push_Image_To_Pro_Registry &#125; &#125; steps &#123; xxx &#125; &#125; stage('清除本地镜像') &#123; steps &#123; xxx &#125; &#125; &#125;&#125;以上是通过Maven去扫描的，如果是前端的项目，也是流水线类型的job的话，在原本的流水线添加此步骤即可：配置好之后我们构建成功后就可以在SonarQube平台看到扫描结果：Jenkins定时扫描代码可以把所有需要扫描的项目放到一个一个pipeline里，定时凌晨执行，如果项目太多，并行执行的话可能会导致机器资源使用率飙升导致宕机，所以需要限制最大并行数，同时如果有某个项目扫描失败不会导致整个流水线退出，而是继续执行扫描下一个项目，最后全部扫描完之后再把扫描失败的项目列出来。前端代码扫描123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201pipeline &#123; agent &#123; label 'k8s-node4' &#125; options &#123; timestamps() buildDiscarder(logRotator(daysToKeepStr: '7', numToKeepStr: '10')) timeout(time: 5, unit: 'HOURS') &#125; triggers &#123; cron('H 2 * * 1') &#125; parameters &#123; choice( name: 'SCAN_PROJECTS', choices: [ 'ALL', 'boss-basic-data', 'boss-user-4pl-3pl', 'keyfil-website', 'oversea-client', 'qifu-saas-f', 'qifu-saas-m1', 'tenant-website' ], description: '选择要扫描的项目' ) choice( name: 'MAX_PARALLEL', choices: ['1', '2', '3', '4', '5'], description: '选择最大并行任务数' ) &#125; environment &#123; SONAR_SERVER = 'SonarQube' WXBOT_URL = 'https://qyapi.weixin.qq.com/cgi-bin/webhook/send?key=9245d561-9d29-4d58-be39-5edc3dba293d' &#125; stages &#123; stage('Initialize') &#123; steps &#123; script &#123; // 初始化失败项目列表 env.FAILED_PROJECTS = "" &#125; &#125; &#125; stage('Check System Resources') &#123; steps &#123; script &#123; sh ''' p=($(grep "cpu " /proc/stat)) sleep 1 c=($(grep "cpu " /proc/stat)) id=$((c[4]-p[4])) tot=$((c[1]+c[2]+c[3]+c[4]+c[5]+c[6]+c[7]+c[8]-p[1]-p[2]-p[3]-p[4]-p[5]-p[6]-p[7]-p[8])) echo "当前CPU使用率: $((100*(tot-id)/tot))%" ''' sh 'echo "当前内存使用率: $(free | grep Mem | awk \'&#123;print \$3/\$2 * 100.0&#125;\')%"' &#125; &#125; &#125; stage('Multi-Project Scan') &#123; steps &#123; script &#123; def projectMap = [ "boss-basic-data": "http://gitlab.xxx.com/saas-front-end/boss-basic-data.git", "boss-user-4pl-3pl": "http://gitlab.xxx.com/saas-front-end/boss-user-4pl-3pl.git", "keyfil-website": "http://gitlab.xxx.com/saas-front-end/keyfil-website.git", "oversea-client": "http://gitlab.xxx.com/saas-front-end/oversea-client.git", "qifu-saas-f": "http://gitlab.xxx.com/saas-front-end/qifu-saas-f.git", "qifu-saas-m1": "http://gitlab.xxx.com/saas-front-end/qifu-saas-m1.git", "tenant-website": "http://gitlab.xxx.com/saas-front-end/tenant-website.git" ] def projects = getTargetProjects(projectMap) int maxParallel = params.MAX_PARALLEL.toInteger() def batches = projects.collate(maxParallel) batches.eachWithIndex &#123; batch, index -&gt; stage("Batch $&#123;index + 1&#125;") &#123; def parallelStages = [:] batch.each &#123; projectName -&gt; parallelStages[projectName] = &#123; stage("Scan $&#123;projectName&#125;") &#123; timeout(time: 180, unit: 'MINUTES') &#123; try &#123; scanSingleProject(projectMap, projectName) echo "✅ 项目 $&#123;projectName&#125; 扫描成功" &#125; catch (Exception e) &#123; echo "❌ 项目 $&#123;projectName&#125; 扫描失败: $&#123;e.toString()&#125;" currentBuild.result = 'FAILURE' // 更新失败项目列表 def currentFailed = env.FAILED_PROJECTS ?: "" def newFailed = currentFailed ? "$&#123;currentFailed&#125;,$&#123;projectName&#125;" : projectName env.FAILED_PROJECTS = newFailed &#125; &#125; &#125; &#125; &#125; parallel parallelStages &#125; &#125; &#125; &#125; &#125; &#125;post &#123; always &#123; script &#123; echo "所有项目扫描完成" sh ''' p=($(grep "cpu " /proc/stat)) sleep 1 c=($(grep "cpu " /proc/stat)) id=$((c[4]-p[4])) tot=$((c[1]+c[2]+c[3]+c[4]+c[5]+c[6]+c[7]+c[8]-p[1]-p[2]-p[3]-p[4]-p[5]-p[6]-p[7]-p[8])) echo "最终CPU使用率: $((100*(tot-id)/tot))%" ''' sh 'echo "最终内存使用率: $(free | grep Mem | awk \'&#123;print \$3/\$2 * 100.0&#125;\')%"' &#125; &#125; failure &#123; script &#123; echo "有项目扫描失败，发送微信通知" // 从环境变量获取失败项目列表 def failedProjects = env.FAILED_PROJECTS ?: "无" // 使用 echo 列出失败项目 if (failedProjects != "无") &#123; echo "===== 扫描失败的项目列表 =====" failedProjects.split(',').each &#123; project -&gt; echo "❌ $&#123;project&#125;" &#125; echo "=============================" &#125; else &#123; echo "没有项目被标记为失败" &#125; // 构建完整的通知消息 def fullMessage = """ &lt;font color="warning"&gt;SonarQube扫描失败通知&lt;/font&gt; &gt; 最大并行数: $&#123;params.MAX_PARALLEL&#125; &gt; 失败项目: $&#123;failedProjects&#125; &gt; 构建信息: $&#123;currentBuild.fullDisplayName&#125; &gt; [查看控制台]($&#123;env.BUILD_URL&#125;console) """ // 发送通知 qyWechatNotification( mentionedId: '', mentionedMobile: '15622091064', moreInfo: fullMessage, webhookUrl: env.WXBOT_URL ) &#125; &#125;&#125;&#125;def getTargetProjects(projectMap) &#123; params.SCAN_PROJECTS == 'ALL' ? projectMap.keySet() as List : [params.SCAN_PROJECTS]&#125;def scanSingleProject(projectMap, projectName) &#123; def gitUrl = projectMap[projectName] dir(projectName) &#123; deleteDir() checkout([$class: 'GitSCM', branches: [[name: 'test']], extensions: [], userRemoteConfigs: [[credentialsId: 'gitlab', url: gitUrl]]]) script&#123; scannerHome = tool 'Sonar-scanner' &#125; withSonarQubeEnv(env.SONAR_SERVER) &#123; sh """ source /etc/profile;$&#123;scannerHome&#125;/bin/sonar-scanner \ -Dsonar.projectKey=$&#123;projectName&#125; \ -Dsonar.sources=. \ -Dsonar.sourceEncoding=UTF-8 \ -Dsonar.inclusions=**/*.js,**/*.jsx,**/*.ts,**/*.tsx,**/*.vue \ -Dsonar.nodejs.executable=/usr/local/node-v18.20.5-linux-x64-glibc-217/bin/node \ -Dsonar.exclusions=./node_modules/,./dist/,./.git/ \ -Dsonar.analysis.analysisMode=incremental \ -Dsonar.exclusions=**/*.md,**/*.png \ -Dsonar.javascript.node.maxspace=4096 """ &#125; &#125;&#125;后端代码扫描123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267pipeline &#123; agent &#123; label 'k8s-node4' &#125; options &#123; timestamps() buildDiscarder(logRotator(daysToKeepStr: '7', numToKeepStr: '10')) timeout(time: 3, unit: 'HOURS') &#125; triggers &#123; cron('H 0 * * 1') &#125; parameters &#123; choice( name: 'SCAN_PROJECTS', choices: [ 'ALL', 'infrastructure', 'qifu-bmp-pms', 'qifu-erp-application', 'qifu-saas-aggregation', 'qifu-saas-basic', 'qifu-saas-bc', 'qifu-saas-bpmn', 'qifu-saas-cbl-application', 'qifu-saas-cms', 'qifu-saas-cms-gateway', 'qifu-saas-crm-module', 'qifu-saas-customer-application', 'qifu-saas-data-processing', 'qifu-saas-eg', 'qifu-saas-etm', 'qifu-saas-fms', 'qifu-saas-foundation', 'qifu-saas-gateway', 'qifu-saas-inventory', 'qifu-saas-log', 'qifu-saas-m', 'qifu-saas-market', 'qifu-saas-mdm-module', 'qifu-saas-message', 'qifu-saas-oms', 'qifu-saas-org', 'qifu-saas-owms', 'qifu-saas-owms-application', 'qifu-saas-pcs', 'qifu-saas-portal', 'qifu-saas-quote', 'qifu-saas-s', 'qifu-saas-scm', 'qifu-saas-server-application', 'qifu-saas-stp', 'qifu-saas-tms', 'qifu-saas-uaa', 'qifu-saas-wechat', 'qifu-saas-wms' ], description: '选择要扫描的项目' ) choice( name: 'MAX_PARALLEL', choices: ['1', '2', '3', '4', '5'], description: '选择最大并行任务数' ) &#125; environment &#123; SONAR_SERVER = 'SonarQube' WXBOT_URL = 'https://qyapi.weixin.qq.com/cgi-bin/webhook/send?key=9245d561-9d29-4d58-be39-5edc3dba293d' &#125; stages &#123; stage('Initialize') &#123; steps &#123; script &#123; // 初始化失败项目列表 env.FAILED_PROJECTS = "" &#125; &#125; &#125; stage('Check System Resources') &#123; steps &#123; script &#123; sh ''' p=($(grep "cpu " /proc/stat)) sleep 1 c=($(grep "cpu " /proc/stat)) id=$((c[4]-p[4])) tot=$((c[1]+c[2]+c[3]+c[4]+c[5]+c[6]+c[7]+c[8]-p[1]-p[2]-p[3]-p[4]-p[5]-p[6]-p[7]-p[8])) echo "当前CPU使用率: $((100*(tot-id)/tot))%" ''' sh 'echo "当前内存使用率: $(free | grep Mem | awk \'&#123;print \$3/\$2 * 100.0&#125;\')%"' &#125; &#125; &#125; stage('Multi-Project Scan') &#123; steps &#123; script &#123; def projectMap = [ "infrastructure": "http://gitlab.xxx.com/infrastructure/infrastructure.git", "qifu-bmp-pms": "http://gitlab.xxx.com/saas-back-end/qifu-bmp-pms.git", "qifu-erp-application": "http://gitlab.xxx.com/saas-back-end/qifu-erp-application.git", "qifu-saas-aggregation": "http://gitlab.xxx.com/saas-back-end/qifu-saas-aggregation.git", "qifu-saas-basic": "http://gitlab.xxx.com/saas-back-end/qifu-saas-basic.git", "qifu-saas-bc": "http://gitlab.xxx.com/saas-back-end/qifu-saas-bc.git", "qifu-saas-bpmn": "http://gitlab.xxx.com/saas-back-end/qifu-saas-bpmn.git", "qifu-saas-cbl-application": "http://gitlab.xxx.com/saas-back-end/qifu-saas-cbl-application.git", "qifu-saas-cms": "http://gitlab.xxx.com/saas-back-end/qifu-saas-cms.git", "qifu-saas-cms-gateway": "http://gitlab.xxx.com/saas-back-end/qifu-saas-cms-gateway.git", "qifu-saas-crm-module": "http://gitlab.xxx.com/saas-back-end/qifu-saas-crm-module.git", "qifu-saas-customer-application": "http://gitlab.xxx.com/saas-back-end/qifu-saas-customer-application.git", "qifu-saas-data-processing": "http://gitlab.xxx.com/saas-back-end/qifu-saas-data-processing.git", "qifu-saas-eg": "http://gitlab.xxx.com/saas-back-end/qifu-saas-eg.git", "qifu-saas-etm": "http://gitlab.xxx.com/saas-back-end/qifu-saas-etm.git", "qifu-saas-fms": "http://gitlab.xxx.com/saas-back-end/qifu-saas-fms.git", "qifu-saas-foundation": "http://gitlab.xxx.com/saas-back-end/qifu-saas-foundation.git", "qifu-saas-gateway": "http://gitlab.xxx.com/saas-back-end/qifu-saas-gateway.git", "qifu-saas-inventory": "http://gitlab.xxx.com/saas-back-end/qifu-saas-inventory.git", "qifu-saas-log": "http://gitlab.xxx.com/saas-back-end/qifu-saas-log.git", "qifu-saas-m": "http://gitlab.xxx.com/saas-back-end/qifu-saas-m.git", "qifu-saas-market": "http://gitlab.xxx.com/saas-back-end/qifu-saas-market.git", "qifu-saas-mdm-module": "http://gitlab.xxx.com/saas-back-end/qifu-saas-mdm-module.git", "qifu-saas-message": "http://gitlab.xxx.com/saas-back-end/qifu-saas-message.git", "qifu-saas-oms": "http://gitlab.xxx.com/saas-back-end/qifu-saas-oms.git", "qifu-saas-org": "http://gitlab.xxx.com/saas-back-end/qifu-saas-org.git", "qifu-saas-owms": "http://gitlab.xxx.com/saas-back-end/qifu-saas-owms.git", "qifu-saas-owms-application": "http://gitlab.xxx.com/saas-back-end/qifu-saas-owms-application.git", "qifu-saas-pcs": "http://gitlab.xxx.com/saas-back-end/qifu-saas-pcs.git", "qifu-saas-portal": "http://gitlab.xxx.com/saas-back-end/qifu-saas-portal.git", "qifu-saas-quote": "http://gitlab.xxx.com/saas-back-end/qifu-saas-quote.git", "qifu-saas-s": "http://gitlab.xxx.com/saas-back-end/qifu-saas-s.git", "qifu-saas-scm": "http://gitlab.xxx.com/saas-back-end/qifu-saas-scm.git", "qifu-saas-server-application": "http://gitlab.xxx.com/saas-back-end/qifu-saas-server-application.git", "qifu-saas-stp": "http://gitlab.xxx.com/saas-back-end/qifu-saas-stp.git", "qifu-saas-tms": "http://gitlab.xxx.com/saas-back-end/qifu-saas-tms.git", "qifu-saas-uaa": "http://gitlab.xxx.com/saas-back-end/qifu-saas-uaa.git", "qifu-saas-wechat": "http://gitlab.xxx.com/saas-back-end/qifu-saas-wechat.git", "qifu-saas-wms": "http://gitlab.xxx.com/saas-back-end/qifu-saas-wms.git" ] def projects = getTargetProjects(projectMap) int maxParallel = params.MAX_PARALLEL.toInteger() def batches = projects.collate(maxParallel) batches.eachWithIndex &#123; batch, index -&gt; stage("Batch $&#123;index + 1&#125;") &#123; def parallelStages = [:] batch.each &#123; projectName -&gt; parallelStages[projectName] = &#123; stage("Scan $&#123;projectName&#125;") &#123; timeout(time: 60, unit: 'MINUTES') &#123; try &#123; scanSingleProject(projectMap, projectName) echo "✅ 项目 $&#123;projectName&#125; 扫描成功" &#125; catch (Exception e) &#123; echo "❌ 项目 $&#123;projectName&#125; 扫描失败: $&#123;e.toString()&#125;" currentBuild.result = 'FAILURE' // 更新失败项目列表 def currentFailed = env.FAILED_PROJECTS ?: "" def newFailed = currentFailed ? "$&#123;currentFailed&#125;,$&#123;projectName&#125;" : projectName env.FAILED_PROJECTS = newFailed &#125; &#125; &#125; &#125; &#125; parallel parallelStages &#125; &#125; &#125; &#125; &#125; &#125;post &#123; always &#123; script &#123; echo "所有项目扫描完成" sh ''' p=($(grep "cpu " /proc/stat)) sleep 1 c=($(grep "cpu " /proc/stat)) id=$((c[4]-p[4])) tot=$((c[1]+c[2]+c[3]+c[4]+c[5]+c[6]+c[7]+c[8]-p[1]-p[2]-p[3]-p[4]-p[5]-p[6]-p[7]-p[8])) echo "最终CPU使用率: $((100*(tot-id)/tot))%" ''' sh 'echo "最终内存使用率: $(free | grep Mem | awk \'&#123;print \$3/\$2 * 100.0&#125;\')%"' &#125; &#125; failure &#123; script &#123; echo "有项目扫描失败，发送微信通知" // 从环境变量获取失败项目列表 def failedProjects = env.FAILED_PROJECTS ?: "无" // 使用 echo 列出失败项目 if (failedProjects != "无") &#123; echo "===== 扫描失败的项目列表 =====" failedProjects.split(',').each &#123; project -&gt; echo "❌ $&#123;project&#125;" &#125; echo "=============================" &#125; else &#123; echo "没有项目被标记为失败" &#125; // 构建完整的通知消息 def fullMessage = """ &lt;font color="warning"&gt;SonarQube扫描失败通知&lt;/font&gt; &gt; 最大并行数: $&#123;params.MAX_PARALLEL&#125; &gt; 失败项目: $&#123;failedProjects&#125; &gt; 构建信息: $&#123;currentBuild.fullDisplayName&#125; &gt; [查看控制台]($&#123;env.BUILD_URL&#125;console) """ // 发送通知 qyWechatNotification( mentionedId: '', mentionedMobile: '15622091064', moreInfo: fullMessage, webhookUrl: env.WXBOT_URL ) &#125; &#125;&#125;&#125;def getTargetProjects(projectMap) &#123; params.SCAN_PROJECTS == 'ALL' ? projectMap.keySet() as List : [params.SCAN_PROJECTS]&#125;def scanSingleProject(projectMap, projectName) &#123; def gitUrl = projectMap[projectName] dir(projectName) &#123; deleteDir() checkout([$class: 'GitSCM', branches: [[name: 'test']], extensions: [], userRemoteConfigs: [[credentialsId: 'gitlab', url: gitUrl]]]) sh """ source /etc/profile export MAVEN_OPTS="-Xmx1024m -XX:MaxRAMPercentage=75.0" mvn clean package -P development -DskipTests -U -T 1C """ withSonarQubeEnv(env.SONAR_SERVER) &#123; sh """ source /etc/profile export MAVEN_OPTS="-Xmx1024m -XX:MaxRAMPercentage=75.0" mvn sonar:sonar \ -Dsonar.projectKey=$&#123;projectName&#125; \ -Dsonar.java.binaries=target/classes \ -Dsonar.sourceEncoding=UTF-8 \ -Dsonar.analysis.analysisMode=incremental \ -Dsonar.exclusions=**/*.md,**/*.png \ -T 1C """ &#125; &#125;&#125;其他配置sonarqube是用java启动的，默认-xmx有三个，都是512M，（search是es）12345#sonar.web.javaOpts=-Xmx512m -Xms128m -XX:+HeapDumpOnOutOfMemoryError#sonar.ce.javaOpts=-Xmx512m -Xms128m -XX:+HeapDumpOnOutOfMemoryError#sonar.search.javaOpts=-Xmx512m -Xms512m -XX:MaxDirectMemorySize=256m -XX:+HeapDumpOnOutOfMemoryError需要改的话可以在sonar.properties修改然后重启SonarQube服务：jenkins的插件sonarscanner需要修改-xmx的话可以在JVM Options加：问题扫描前端项目时，特别慢，排查到是因为node执行的eslint特别消耗CPU，CPU飙到99：]]></content>
      <categories>
        <category>技术</category>
        <category>SonarQube</category>
      </categories>
      <tags>
        <tag>SonarQube</tag>
        <tag>jenkins</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ES数据迁移]]></title>
    <url>%2F2025%2F089b445ae1.html</url>
    <content type="text"><![CDATA[公司最近需要把uat环境数据迁移到生产环境，其中有一个就是es数据迁移，在这里记录下迁移办法。快照模式迁移因为安装es集群的时候也安装了kibana，可以通过kibana图形界面去备份快照，之前的文章有写过（点击查看），从A集群把备份的快照数据拷贝到新的集群快照数据保存目录下，然后通过kibana把快照进行还原就行，对还原进行设置：还原就行：ElasticSearch-dump模式安装安装node注：node 版本不低于 v10.0.0**12wget https://nodejs.org/dist/v14.17.3/node-v14.17.3-linux-x64.tar.xz -O /opt/node-v14.17.3-linux-x64.tar.xztar -xvf /opt/node-v14.17.3-linux-x64.tar.xz配置环境变量1234567vim ~/.bashrc# 追加以下内容#node export NODE_HOME=/opt/node-v14.17.3-linux-x64export PATH=$NODE_HOME/bin:$PATH# 刷新source ~/.bashrc查看是否出现版本1234[root@localhost ~]# node -vv14.17.3[root@localhost ~]# npm -v6.14.13安装 Elasticdump1npm install elasticdump出现安装成功提示：12+ elasticdump@6.72.0added 112 packages from 198 contributors and audited 112 packages in 19.171s安装成功后会在当前目录生成node_modules目录，里面包含 elasticdump 主目录bin目录下面有两个可执行文件elasticdump（单索引操作）、multielasticdump（多索引操作）为了方便使用最好配置个环境变量1234567vim ~/.bashrc# 追加以下内容#node export DUMP_HOME=/root/node_modules/elasticdumpexport PATH=$DUMP_HOME/bin:$PATH# 刷新source ~/.bashrc使用elasticdump 使用方法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118# Copy an index from production to staging with analyzer and mapping:elasticdump \ --input=http://production.es.com:9200/my_index \ --output=http://staging.es.com:9200/my_index \ --type=analyzerelasticdump \ --input=http://production.es.com:9200/my_index \ --output=http://staging.es.com:9200/my_index \ --type=mappingelasticdump \ --input=http://production.es.com:9200/my_index \ --output=http://staging.es.com:9200/my_index \ --type=data# Backup index data to a file:elasticdump \ --input=http://production.es.com:9200/my_index \ --output=/data/my_index_mapping.json \ --type=mappingelasticdump \ --input=http://production.es.com:9200/my_index \ --output=/data/my_index.json \ --type=data# Backup and index to a gzip using stdout:elasticdump \ --input=http://production.es.com:9200/my_index \ --output=$ \ | gzip &gt; /data/my_index.json.gz# Backup the results of a query to a fileelasticdump \ --input=http://production.es.com:9200/my_index \ --output=query.json \ --searchBody="&#123;\"query\":&#123;\"term\":&#123;\"username\": \"admin\"&#125;&#125;&#125;" # Specify searchBody from a fileelasticdump \ --input=http://production.es.com:9200/my_index \ --output=query.json \ --searchBody=@/data/searchbody.json # Copy a single shard data:elasticdump \ --input=http://es.com:9200/api \ --output=http://es.com:9200/api2 \ --input-params="&#123;\"preference\":\"_shards:0\"&#125;"# Backup aliases to a fileelasticdump \ --input=http://es.com:9200/index-name/alias-filter \ --output=alias.json \ --type=alias# Import aliases into ESelasticdump \ --input=./alias.json \ --output=http://es.com:9200 \ --type=alias# Backup templates to a fileelasticdump \ --input=http://es.com:9200/template-filter \ --output=templates.json \ --type=template# Import templates into ESelasticdump \ --input=./templates.json \ --output=http://es.com:9200 \ --type=template# Split files into multiple partselasticdump \ --input=http://production.es.com:9200/my_index \ --output=/data/my_index.json \ --fileSize=10mb# Import data from S3 into ES (using s3urls)elasticdump \ --s3AccessKeyId "$&#123;access_key_id&#125;" \ --s3SecretAccessKey "$&#123;access_key_secret&#125;" \ --input "s3://$&#123;bucket_name&#125;/$&#123;file_name&#125;.json" \ --output=http://production.es.com:9200/my_index# Export ES data to S3 (using s3urls)elasticdump \ --s3AccessKeyId "$&#123;access_key_id&#125;" \ --s3SecretAccessKey "$&#123;access_key_secret&#125;" \ --input=http://production.es.com:9200/my_index \ --output "s3://$&#123;bucket_name&#125;/$&#123;file_name&#125;.json"# Import data from MINIO (s3 compatible) into ES (using s3urls)elasticdump \ --s3AccessKeyId "$&#123;access_key_id&#125;" \ --s3SecretAccessKey "$&#123;access_key_secret&#125;" \ --input "s3://$&#123;bucket_name&#125;/$&#123;file_name&#125;.json" \ --output=http://production.es.com:9200/my_index --s3ForcePathStyle true --s3Endpoint https://production.minio.co# Export ES data to MINIO (s3 compatible) (using s3urls)elasticdump \ --s3AccessKeyId "$&#123;access_key_id&#125;" \ --s3SecretAccessKey "$&#123;access_key_secret&#125;" \ --input=http://production.es.com:9200/my_index \ --output "s3://$&#123;bucket_name&#125;/$&#123;file_name&#125;.json" --s3ForcePathStyle true --s3Endpoint https://production.minio.co# Import data from CSV file into ES (using csvurls)elasticdump \ # csv:// prefix must be included to allow parsing of csv files # --input "csv://$&#123;file_path&#125;.csv" \ --input "csv:///data/cars.csv" --output=http://production.es.com:9200/my_index \ --csvSkipRows 1 # used to skip parsed rows (this does not include the headers row) --csvDelimiter ";" # default csvDelimiter is ','multielasticdump 使用方法12345678910111213141516# backup ES indices &amp; all their type to the es_backup foldermultielasticdump \ --direction=dump \ --match='^.*$' \ --input=http://production.es.com:9200 \ --output=/tmp/es_backup# Only backup ES indices ending with a prefix of `-index` (match regex). # Only the indices data will be backed up. All other types are ignored.# NB: analyzer &amp; alias types are ignored by defaultmultielasticdump \ --direction=dump \ --match='^.*-index$'\ --input=http://production.es.com:9200 \ --ignoreType='mapping,settings,template' \ --output=/tmp/es_backup常用参数：12345--direction dump/load 导出/导入--ignoreType 被忽略的类型，data,mapping,analyzer,alias,settings,template--includeType 包含的类型，data,mapping,analyzer,alias,settings,template--suffix 加前缀，es6-$&#123;index&#125;--prefix 加后缀，$&#123;index&#125;-backup-2018-03-13实战源es地址：http://192.168.1.140:9200源es索引名：source_index目标es地址：http://192.168.1.141:9200目标es索引名：target_index迁移在线迁移直接将两个ES的数据同步单索引：123456789elasticdump \ --input=http://192.168.1.140:9200/source_index \ --output=http://192.168.1.141:9200/target_index \ --type=mappingelasticdump \ --input=http://192.168.1.140:9200/source_index \ --output=http://192.168.1.141:9200/target_index \ --type=data \ --limit=2000 # 每次操作的objects数量，默认100，数据量大的话，可以调大加快迁移速度离线迁移单索引：将源es索引数据导出为json文件，然后再导入目标es1234567891011121314151617181920# 导出elasticdump \ --input=http://192.168.1.140:9200/source_index \ --output=/data/source_index_mapping.json \ --type=mappingelasticdump \ --input=http://192.168.1.140:9200/source_index \ --output=/data/source_index.json \ --type=data \ --limit=2000# 导入elasticdump \ --input=/data/source_index_mapping.json \ --output=http://192.168.1.141:9200/source_index \ --type=mappingelasticdump \ --input=/data/source_index.json \ --output=http://192.168.1.141:9200/source_index \ --type=data \ --limit=2000全索引：12345678910111213141516# 导出multielasticdump \ --direction=dump \ --match='^.*$' \ --input=http://192.168.1.140:9200 \ --output=/tmp/es_backup \ --includeType='data,mapping' \ --limit=2000# 导入multielasticdump \ --direction=load \ --match='^.*$' \ --input=/tmp/es_backup \ --output=http://192.168.1.141:9200 \ --includeType='data,mapping' \ --limit=2000 \备份单索引：将es索引备份成gz文件，减少储存压力：12345elasticdump \ --input=http://192.168.1.140:9200/source_index \ --output=$ \ --limit=2000 \ | gzip &gt; /data/source_index.json.gzES命令查看集群健康：1curl -X GET "http://localhost:9200/_cat/health?v"查看集群节点：1curl -X GET "http://localhost:9200/_cat/nodes?v"列出所有索引：1curl -X GET "http://localhost:9200/_cat/indices?v"]]></content>
      <categories>
        <category>技术</category>
        <category>es</category>
      </categories>
      <tags>
        <tag>es</tag>
        <tag>elasticdump</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RocketMQ命令行操作]]></title>
    <url>%2F2025%2F0869c0894f.html</url>
    <content type="text"><![CDATA[Rocketmq有带图形界面的控制台，这次公司环境迁移，需要把jl环境的topic及消费组加到另一个Rocketmq集群，如果在web控制台一个个点太费时间了，可以通过命令行批量添加。场景一如果是两个Rocketmq集群，先从A集群导出topic及消费组列表，再到B集群批量创建。比如我们公司是以%前面来区别不同的环境的，A集群是以jl%开头：获取topic列表1./mqadmin topicList -n $NAMESRV_ADDR | grep "^jl%" &gt; jl_topics.txt获取消费组列表1./mqadmin consumerProgress -n $NAMESRV_ADDR | grep "jl%" | awk '&#123;print $1&#125;' | sort | uniq &gt; jl_consumer_groups.txt把列表传到B集群的机器，然后编写批量创建jltest%开头的脚本：批量创建topic1234567891011121314151617181920#!/bin/bash# 设置 RocketMQ 环境ROCKETMQ_HOME=/home/rocketmq/rocketmq-5.0.0NAMESRV_ADDR=rocketmqnamesrv:9876# 读取 jl_topics.txt 中的主题while IFS= read -r topic; do # 替换前缀 jl% 为 jltest% new_topic="$&#123;topic/jl%/jltest%&#125;" echo "创建主题: $new_topic (基于 $topic)" ./mqadmin updateTopic -n $NAMESRV_ADDR -t "$new_topic" -c DefaultCluster -r 8 -w 8 sleep 0.5done &lt; jl_topics.txtecho "all jltest% topic create done"批量创建消费组12345678910111213141516171819#!/bin/bash# 设置 RocketMQ 环境ROCKETMQ_HOME=/home/rocketmq/rocketmq-5.0.0NAMESRV_ADDR=rocketmqnamesrv:9876# 读取 jl_consumer_groups.txt 中的消费者组while IFS= read -r consumer_group; do # 替换前缀 jl% 为 jltest% new_consumer_group="$&#123;consumer_group/jl%/jltest%&#125;" echo "创建消费者组: $new_consumer_group (基于 $consumer_group)" ./mqadmin updateSubGroup -n $NAMESRV_ADDR -c DefaultCluster -d true -g "$new_consumer_group" sleep 0.5done &lt; jl_consumer_groups.txtecho "所有 jltest% 消费者组创建完成"场景二如果不是两个集群，而是同一个集群，需要从jl环境创建一份到jltest环境，可以用如下的脚本：完整脚本123456789101112131415161718192021222324252627282930313233343536373839#!/bin/bash# 设置 RocketMQ 环境ROCKETMQ_HOME=/home/rocketmq/rocketmq-5.0.0NAMESRV_ADDR=rocketmq-name-server-service.qifu-middleware:9876cd $ROCKETMQ_HOME/binecho "开始获取 jl% 开头的主题..."./mqadmin topicList -n $NAMESRV_ADDR | grep "^jl%" &gt; jl_topics.txtecho "找到 $(wc -l &lt; jl_topics.txt) 个 jl% 主题"echo "开始获取 jl% 开头的消费者组..."./mqadmin consumerProgress -n $NAMESRV_ADDR | grep "jl%" | awk '&#123;print $1&#125;' | sort | uniq &gt; jl_consumer_groups.txtecho "找到 $(wc -l &lt; jl_consumer_groups.txt) 个 jl% 消费者组"echo "开始创建 jltest% 主题..."while IFS= read -r topic; do new_topic="$&#123;topic/jl%/jltest%&#125;" echo "创建主题: $new_topic" ./mqadmin updateTopic -n $NAMESRV_ADDR -t "$new_topic" -c DefaultCluster -r 8 -w 8 sleep 0.3done &lt; jl_topics.txtecho "开始创建 jltest% 消费者组..."while IFS= read -r consumer_group; do new_consumer_group="$&#123;consumer_group/jl%/jltest%&#125;" echo "创建消费者组: $new_consumer_group" ./mqadmin updateSubGroup -n $NAMESRV_ADDR -c DefaultCluster -d true -g "$new_consumer_group" sleep 0.3done &lt; jl_consumer_groups.txtecho "验证创建结果..."echo "创建的 jltest% 主题:"./mqadmin topicList -n $NAMESRV_ADDR | grep "^jltest%" | head -10echo "创建的 jltest% 消费者组:"./mqadmin consumerProgress -n $NAMESRV_ADDR | grep "jltest%" | awk '&#123;print $1&#125;' | sort | uniq | head -10echo "批量复制完成!"详细的Rocket MQ命令参数可参考：15 RocketMQ 常用命令实战]]></content>
      <categories>
        <category>技术</category>
        <category>RocketMQ</category>
      </categories>
      <tags>
        <tag>RocketMQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker部署certimateSSL证书管理]]></title>
    <url>%2F2025%2F080.html</url>
    <content type="text"><![CDATA[之前有写过一篇文章使用certbot申请免费ssl证书，最近发现certimate有提供web界面，而且集成了证书申请，证书部署，证书续订，监控告警等功能，都可以通过web界面点几下就可以完成，比较适合小白。开源certimate项目地址：https://github.com/certimate-go/certimate具体功能可以查看下项目地址，说下跟certbot的区别：优缺点：部署docker-compose.yml:12345678910111213141516version: "3.0"services: certimate: image: docker.1ms.run/certimate/certimate:latest container_name: certimate ports: - 8090:8090 dns: - 223.5.5.5 - 8.8.8.8 volumes: - /etc/localtime:/etc/localtime:ro - /etc/timezone:/etc/timezone:ro - ./data:/app/pb_data - ./cert:/etc/ssl/certimate/ restart: unless-stoppedps：申请泛域名证书会调用api去添加txt域名解析，dns的配置官方文档没有加，我不加的话，申请证书的时候会报错：启动容器：1docker-compose up -d启动完成后通过浏览器访问：访问：http://服务器IP:8090账号：`admin@certimate.fun密码：1234567890`域名证书申请工作流选择标准模板开始：申请：部署：通知： 保存，执行：执行成功后就会生成证书：把工作流启动，就会按照你设置的时间去跑任务，证书到期前就会自动续期：]]></content>
      <categories>
        <category>技术</category>
        <category>域名证书</category>
      </categories>
      <tags>
        <tag>免费证书</tag>
        <tag>certimate</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[K8S集群证书更新]]></title>
    <url>%2F2025%2F076a4a6c72.html</url>
    <content type="text"><![CDATA[通常情况下，使用 KubeKey 部署的 Kubernetes 集群是不会遇到证书过期问题的，KubeKey 在部署时会自动配置一个定时任务，定期检查集群中所有证书的有效期，系统会监控证书状态，当发现任何证书的剩余有效期低于 30 天时，就会触发自动更新流程。为了实现这个自动化的证书更新机制，KubeKey 在系统中配置了以下三个关键组件：脚本文件文件路径功能说明k8s-certs-renew.service/etc/systemd/system/k8s-certs-renew.service系统服务单元文件,用于执行证书更新脚本k8s-certs-renew.timer/etc/systemd/system/k8s-certs-renew.timer定时器单元,设置为每周一凌晨3点自动执行更新k8s-certs-renew.sh/usr/local/bin/kube-scripts/k8s-certs-renew.sh证书更新主脚本,使用 kubeadm certs renew all 命令更新所有证书备份脚本k8s-certs-renew.sh原始内容：123456789101112131415161718192021#!/bin/bashkubeadmCerts='/usr/local/bin/kubeadm certs'getCertValidDays() &#123; local earliestExpireDate; earliestExpireDate=$($&#123;kubeadmCerts&#125; check-expiration | grep -o "[A-Za-z]\&#123;3,4\&#125;\s\w\w,\s[0-9]\&#123;4,\&#125;\s\w*:\w*\s\w*\s*" | xargs -I &#123;&#125; date -d &#123;&#125; +%s | sort | head -n 1) local today; today="$(date +%s)" echo -n $(( ($earliestExpireDate - $today) / (24 * 60 * 60) ))&#125;echo "## Expiration before renewal ##"$&#123;kubeadmCerts&#125; check-expirationif [ $(getCertValidDays) -lt 30 ]; then echo "## Renewing certificates managed by kubeadm ##" $&#123;kubeadmCerts&#125; renew all echo "## Restarting control plane pods managed by kubeadm ##" $(which crictl | grep crictl) pods --namespace kube-system --name 'kube-scheduler-*|kube-controller-manager-*|kube-apiserver-*|etcd-*' -q | /usr/bin/xargs $(which crictl | grep crictl) rmp -f echo "## Updating /root/.kube/config ##" cp /etc/kubernetes/admin.conf /root/.kube/configfiecho "## Waiting for apiserver to be up again ##"until printf "" 2&gt;&gt;/dev/null &gt;&gt;/dev/tcp/127.0.0.1/6443; do sleep 1; doneecho "## Expiration after renewal ##"$&#123;kubeadmCerts&#125; check-expiration我的K8S集群不是通过KubeKey部署的，这里说一下更新k8s证书的流程。收到告警通知，证书即将过期：查看证书到期时间1kubeadm certs check-expiration备份集群证书关键信息操作有风险，备份是王道！请务必在任何改动前，完整备份现有环境的配置文件和证书。请在每个 Control 节点上执行以下操作：创建备份目录1mkdir /root/ksp-backup备份原有信息1cp -a /etc/kubernetes /root/ksp-backup/备份 ssl1cp -a /etc/ssl/etcd/ /root/ksp-backup/etcd-ssl-bak-`date +%Y-%H-%M`备份etcd 数据1cp -a /var/lib/etcd /root/ksp-backup/etcd-bak-`date +%Y-%H-%M`更新证书1kubeadm certs renew all更新完成后会提示重启组件生效：1Done renewing certificates. You must restart the kube-apiserver, kube-controller-manager, kube-scheduler and etcd, so that they can use the new certificates.重启组件获取控制平面组件列表：1ll /etc/kubernetes/manifests/重启所有控制平面组件（推荐顺序）,先重启 etcd（如果多节点需逐个重启）:123sudo mv /etc/kubernetes/manifests/etcd.yaml /tmp/ &amp;&amp; \sleep 20 &amp;&amp; \sudo mv /tmp/etcd.yaml /etc/kubernetes/manifests/再重启 API Server123sudo mv /etc/kubernetes/manifests/kube-apiserver.yaml /tmp/ &amp;&amp; \sleep 20 &amp;&amp; \sudo mv /tmp/kube-apiserver.yaml /etc/kubernetes/manifests/最后重启其他组件12345for comp in controller-manager scheduler; do sudo mv /etc/kubernetes/manifests/kube-$&#123;comp&#125;.yaml /tmp/ &amp;&amp; \ sleep 10 &amp;&amp; \ sudo mv /tmp/kube-$&#123;comp&#125;.yaml /etc/kubernetes/manifests/done]]></content>
      <categories>
        <category>技术</category>
        <category>K8S</category>
      </categories>
      <tags>
        <tag>证书</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[K8S部署seata-server集群]]></title>
    <url>%2F2025%2F068c94af43.html</url>
    <content type="text"><![CDATA[Seata分TC、TM和RM三个角色，TC（Server端）为单独服务端部署，TM和RM（Client端）由业务系统集成。 Seata-server 的高可用依赖于注册中心、配置中心和数据库来实现。Server端存储模式（store.mode）现有file、db、redis三种（后续将引入raft,mongodb），file模式无需改动，直接启动即可，下面专门讲下db启动步骤。注： file模式为单机模式，全局事务会话信息内存中读写并持久化本地文件root.data，性能较高;db模式为高可用模式，全局事务会话信息通过db共享，相应性能差些;redis模式Seata-Server 1.3及以上版本支持,性能较高,存在事务信息丢失风险,请提前配置合适当前场景的redis持久化配置.下面主要介绍一下基于nacos注册中心和mysql 数据库的集群高可用的搭建和实现。原理和架构图seata-server 的高可用的实现，主要基于db和注册中心，通过db获取全局事务，实现多实例事务共享。通过注册中心来实现seata-server多实例的动态管理。架构图原理图如 下：部署环境部署步骤1.创建seata_server库：1create database seata_server character set utf8mb4 collate utf8mb4_general_ci;创建表：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687-- -------------------------------- The script used when storeMode is 'db' ---------------------------------- the table to store GlobalSession dataCREATE TABLE IF NOT EXISTS `global_table`( `xid` VARCHAR(128) NOT NULL, `transaction_id` BIGINT, `status` TINYINT NOT NULL, `application_id` VARCHAR(256), `transaction_service_group` VARCHAR(32), `transaction_name` VARCHAR(128), `timeout` INT, `begin_time` BIGINT, `application_data` VARCHAR(2000), `gmt_create` DATETIME, `gmt_modified` DATETIME, PRIMARY KEY (`xid`), KEY `idx_status_gmt_modified` (`status` , `gmt_modified`), KEY `idx_transaction_id` (`transaction_id`)) ENGINE = InnoDB DEFAULT CHARSET = utf8mb4; -- the table to store BranchSession dataCREATE TABLE IF NOT EXISTS `branch_table`( `branch_id` BIGINT NOT NULL, `xid` VARCHAR(128) NOT NULL, `transaction_id` BIGINT, `resource_group_id` VARCHAR(32), `resource_id` VARCHAR(256), `branch_type` VARCHAR(8), `status` TINYINT, `client_id` VARCHAR(64), `application_data` VARCHAR(2000), `gmt_create` DATETIME(6), `gmt_modified` DATETIME(6), PRIMARY KEY (`branch_id`), KEY `idx_xid` (`xid`)) ENGINE = InnoDB DEFAULT CHARSET = utf8mb4; -- the table to store lock dataCREATE TABLE IF NOT EXISTS `lock_table`( `row_key` VARCHAR(128) NOT NULL, `xid` VARCHAR(128), `transaction_id` BIGINT, `branch_id` BIGINT NOT NULL, `resource_id` VARCHAR(256), `table_name` VARCHAR(32), `pk` VARCHAR(36), `status` TINYINT NOT NULL DEFAULT '0' COMMENT '0:locked ,1:rollbacking', `gmt_create` DATETIME, `gmt_modified` DATETIME, PRIMARY KEY (`row_key`), KEY `idx_status` (`status`), KEY `idx_branch_id` (`branch_id`), KEY `idx_xid` (`xid`)) ENGINE = InnoDB DEFAULT CHARSET = utf8mb4; CREATE TABLE IF NOT EXISTS `distributed_lock`( `lock_key` CHAR(20) NOT NULL, `lock_value` VARCHAR(20) NOT NULL, `expire` BIGINT, primary key (`lock_key`)) ENGINE = InnoDB DEFAULT CHARSET = utf8mb4; INSERT INTO `distributed_lock` (lock_key, lock_value, expire) VALUES ('AsyncCommitting', ' ', 0);INSERT INTO `distributed_lock` (lock_key, lock_value, expire) VALUES ('RetryCommitting', ' ', 0);INSERT INTO `distributed_lock` (lock_key, lock_value, expire) VALUES ('RetryRollbacking', ' ', 0);INSERT INTO `distributed_lock` (lock_key, lock_value, expire) VALUES ('TxTimeoutCheck', ' ', 0); -- for AT mode you must to init this sql for you business database. the seata server not need it.CREATE TABLE IF NOT EXISTS `undo_log`( `branch_id` BIGINT NOT NULL COMMENT 'branch transaction id', `xid` VARCHAR(128) NOT NULL COMMENT 'global transaction id', `context` VARCHAR(128) NOT NULL COMMENT 'undo_log context,such as serialization', `rollback_info` LONGBLOB NOT NULL COMMENT 'rollback info', `log_status` INT(11) NOT NULL COMMENT '0:normal status,1:defense status', `log_created` DATETIME(6) NOT NULL COMMENT 'create datetime', `log_modified` DATETIME(6) NOT NULL COMMENT 'modify datetime', UNIQUE KEY `ux_undo_log` (`xid`, `branch_id`)) ENGINE = InnoDB AUTO_INCREMENT = 1 DEFAULT CHARSET = utf8mb4 COMMENT ='AT transaction mode undo table';ALTER TABLE `undo_log` ADD INDEX `ix_log_created` (`log_created`);添加nacos配置：配置文件如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546store.mode=db#-----db-----store.db.datasource=druidstore.db.dbType=mysql# 需要根据mysql的版本调整driverClassName# mysql8及以上版本对应的driver：com.mysql.cj.jdbc.Driver# mysql8以下版本的driver：com.mysql.jdbc.Driverstore.db.driverClassName=com.mysql.jdbc.Driverstore.db.url=jdbc:mysql://mysql:3306/seata_server?useUnicode=true&amp;characterEncoding=utf8&amp;connectTimeout=1000&amp;socketTimeout=3000&amp;autoReconnect=true&amp;useSSL=falsestore.db.user= rootstore.db.password= qifu@2023# 数据库初始连接数store.db.minConn=1# 数据库最大连接数store.db.maxConn=200# 获取连接时最大等待时间 默认5000，单位毫秒store.db.maxWait=5000# 全局事务表名 默认global_tablestore.db.globalTable=global_table# 分支事务表名 默认branch_tablestore.db.branchTable=branch_table# 全局锁表名 默认lock_tablestore.db.lockTable=lock_tablestore.db.distributedLockTable=distributed_lock# 查询全局事务一次的最大条数 默认100store.db.queryLimit=100 # undo保留天数 默认7天,log_status=1（附录3）和未正常清理的undoserver.undo.logSaveDays=7# undo清理线程间隔时间 默认86400000，单位毫秒server.undo.logDeletePeriod=86400000# 二阶段提交重试超时时长 单位ms,s,m,h,d,对应毫秒,秒,分,小时,天,默认毫秒。默认值-1表示无限重试# 公式: timeout&gt;=now-globalTransactionBeginTime,true表示超时则不再重试# 注: 达到超时时间后将不会做任何重试,有数据不一致风险,除非业务自行可校准数据,否者慎用server.maxCommitRetryTimeout=-1# 二阶段回滚重试超时时长server.maxRollbackRetryTimeout=-1# 二阶段提交未完成状态全局事务重试提交线程间隔时间 默认1000，单位毫秒server.recovery.committingRetryPeriod=1000# 二阶段异步提交状态重试提交线程间隔时间 默认1000，单位毫秒server.recovery.asynCommittingRetryPeriod=1000# 二阶段回滚状态重试回滚线程间隔时间 默认1000，单位毫秒server.recovery.rollbackingRetryPeriod=1000# 超时状态检测重试线程间隔时间 默认1000，单位毫秒，检测出超时将全局事务置入回滚会话管理器server.recovery.timeoutRetryPeriod=1000seata-server.yaml文件：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138apiVersion: v1kind: ConfigMapmetadata: name: seata-ha-server-config namespace: qifu-developdata: application.yml: | server: # web管理的端口 port: 7091 spring: application: name: seata-server logging: config: classpath:logback-spring.xml file: path: $&#123;user.home&#125;/logs/seata extend: logstash-appender: destination: 127.0.0.1:4560 kafka-appender: bootstrap-servers: 127.0.0.1:9092 topic: logback_to_logstash console: # web端管理的用户名和密码 user: username: seata password: seata seata: config: type: nacos nacos: # nacos的地址 k8s内部域名需要在后面加端口号 server-addr: nacos-headless.qifu.svc.cluster.local:8848 namespace: qifu-develop group: SEATA_GROUP username: nacos password: nacos # 创建的配置文件名称 data-id: seataServer.properties registry: type: nacos nacos: application: seata-server server-addr: nacos-headless.qifu.svc.cluster.local:8848 group: SEATA_GROUP namespace: qifu-develop cluster: default username: nacos password: nacos #store: # mode: db # db: # datasource: druid # dbType: mysql # driverClassName: com.mysql.jdbc.Driver # url: jdbc:mysql:/192.168.1.50:3306/seata?useUnicode=true&amp;characterEncoding=utf8&amp;connectTimeout=1000&amp;socketTimeout=3000&amp;autoReconnect=true&amp;useSSL=false # user: root # password: root # seata 服务暴露的地址 server: service-port: 8091 security: secretKey: SeataSecretKey0c382ef121d778043159209298fd40bf3850a017 tokenValidityInMilliseconds: 1800000 ignore: urls: /,/**/*.css,/**/*.js,/**/*.html,/**/*.map,/**/*.svg,/**/*.png,/**/*.ico,/console-fe/public/**,/api/v1/auth/login ---apiVersion: v1kind: Servicemetadata: name: seata-ha-server namespace: qifu-develop labels: app.kubernetes.io/name: seata-ha-server #annotations: # service.beta.kubernetes.io/alibaba-cloud-loadbalancer-id: lb-2zeabcdefghijklmn # service.beta.kubernetes.io/alicloud-loadbalancer-force-override-listeners: 'true'spec: #type: LoadBalancer type: NodePort ports: - port: 8091 protocol: TCP targetPort: 8091 name: http - port: 7091 protocol: TCP targetPort: 7091 name: web selector: app.kubernetes.io/name: seata-ha-server---apiVersion: apps/v1kind: StatefulSetmetadata: name: seata-ha-server namespace: qifu-develop labels: app.kubernetes.io/name: seata-ha-serverspec: serviceName: seata-ha-server replicas: 3 selector: matchLabels: app.kubernetes.io/name: seata-ha-server template: metadata: labels: app.kubernetes.io/name: seata-ha-server spec: containers: - name: seata-ha-server image: swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/seataio/seata-server:1.7.1 imagePullPolicy: IfNotPresent ports: - name: http containerPort: 8091 protocol: TCP - name: web containerPort: 7091 protocol: TCP resources: limits: cpu: "1" memory: 2Gi requests: cpu: 200m memory: 1Gi volumeMounts: - mountPath: /seata-server/resources/application.yml name: seata-config subPath: application.yml volumes: - name: seata-config configMap: name: seata-ha-server-config部署服务：1kubectl apply -f seata-server.yamlnacos的服务列表：通过nodeport访问web界面：遇到的问题问题：在k8s部署好seata服务后，本地连接会报错，连不上seata服务：这个是因为本地是访问不了k8s集群的pod，即使把服务通过nodeport暴露出来也不行：查看nacos的seata服务会发现ip是pod ip：解决办法：启动seata服务时指定ip为node主机的ip：修改yaml文件，添加环境变量：12345env: - name: SEATA_IP value: 10.168.2.234 - name: SEATA_PORT value: '30180'如图：服务启动后查看nacos的seata服务ip：再写个service对象把端口暴露出来即可：现在就可以在本地连接seata服务啦。]]></content>
      <categories>
        <category>技术</category>
        <category>K8S</category>
      </categories>
      <tags>
        <tag>seata-server集群</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[K8S创建Redis集群并迁移数据]]></title>
    <url>%2F2025%2F06b3e106e6.html</url>
    <content type="text"><![CDATA[公司之前生产环境一直用的是单节点的Redis，生产环境版本是6.2.6，开发需要使用PEXPIRETIME命令，这个命令是7.0版本之后才有的，所以趁着这个机会，升级Redis版本顺便把单节点转成集群。说明k8s部署redis集群推荐7.0+版本，这里是7.4.0，因为当使用 redis-cli 7.0前的版本组建集群时只能使用ip端口，而不能使用pod的名称由于redis重启pod的ip会变化，集群会失效，k8s环境需要配置cluster-announce-ip通告地址，否则某些场景集群会失败看过网上一些k8s部署redis集群的文章在某些场景下多少都有点问题，比如升级、不正当重启等情况都可能导致集群失效，容易踩坑本例中部署6节点3主3从的集群。对升级、重启、高可用都进行了测试，均正常要求使用nfs作为redis的存储，需要准备好存储类，本例是nfs-client本例操作都在test命名空间，请改成你自己的命名空间redis的密码这里为hello_redis@234，请改成你自己的部署准备redis-cluster-sts.yaml，如下内容123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146# 1. redis的无头服务---apiVersion: v1kind: Servicemetadata: name: redis-cluster-headlessspec: clusterIP: None selector: app: redis-cluster ports: - port: 6379 protocol: TCP targetPort: 6379 name: redis - port: 16379 protocol: TCP targetPort: 16379 name: election# 2.redis集群的配置信息，redis的密码改成你自己的---apiVersion: v1kind: ConfigMapmetadata: name: redis-cluster-cmdata: redis-cluster.conf: | bind 0.0.0.0 port 6379 daemonize no protected-mode no dir /data cluster-announce-bus-port 16379 cluster-enabled yes cluster-node-timeout 15000 cluster-config-file /data/nodes.conf requirepass hello_redis@234 masterauth hello_redis@234# 3.部署StatefulSet的reids集群---apiVersion: apps/v1kind: StatefulSetmetadata: name: redis-cluster-stsspec: selector: matchLabels: app: redis-cluster serviceName: redis-cluster-headless replicas: 6 template: metadata: labels: app: redis-cluster spec: affinity: # 反亲和软策略，尽量不要让pod在同一节点 podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - podAffinityTerm: labelSelector: matchExpressions: - key: app operator: In values: - redis-cluster topologyKey: kubernetes.io/hostname weight: 100 containers: - name: redis-cluster image: docker-0.unsee.tech/redis:7.4.0 imagePullPolicy: IfNotPresent env: - name: TZ value: Asia/Shanghai - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name # 上面的service名称 - name: POD_SERVICE_NAME value: "redis-cluster-headless" args: - /etc/redis/redis-cluster.conf # 节点的通告地址，因为重启后pod的ip发生了变化，redis集群必须配置 # 可以是pod的ip，强烈建议是节点的短名称或长名称，但是不能超过46个字符 # 这里使用的是节点的短名称 - --cluster-announce-ip "$(POD_NAME).$(POD_SERVICE_NAME)" # - --cluster-announce-ip $(POD_IP) ports: - name: redis containerPort: 6379 protocol: TCP - name: election containerPort: 16379 protocol: TCP resources: requests: cpu: "0.5" memory: "1Gi" limits: cpu: "1" memory: "2Gi" # 存活探针 livenessProbe: failureThreshold: 2 tcpSocket: port: redis initialDelaySeconds: 16 periodSeconds: 3 successThreshold: 1 timeoutSeconds: 1 # 就绪探针 readinessProbe: failureThreshold: 2 tcpSocket: port: redis initialDelaySeconds: 16 periodSeconds: 3 successThreshold: 1 timeoutSeconds: 1 volumeMounts: - name: redis-conf mountPath: /etc/redis - name: claim mountPath: /data volumes: - name: redis-conf configMap: name: redis-cluster-cm items: - key: redis-cluster.conf path: redis-cluster.conf volumeClaimTemplates: - metadata: name: claim spec: accessModes: ["ReadWriteMany"] storageClassName: nfs-client volumeMode: Filesystem resources: requests: storage: 10Gi创建上面的资源1kubectl -n test apply -f redis-cluster-sts.yaml查看pod已经起来了12345678# kubectl -n midd get pod NAME READY STATUS RESTARTS AGEredis-cluster-sts-0 1/1 Running 0 125mredis-cluster-sts-1 1/1 Running 0 125mredis-cluster-sts-2 1/1 Running 0 125mredis-cluster-sts-3 1/1 Running 0 126mredis-cluster-sts-4 1/1 Running 0 126mredis-cluster-sts-5 1/1 Running 0 111m开始使用redis-cli创建集群，会组建3主3从的集群，然后分配槽位。会提示你输入yes就行123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263redis-cli这里使用6个pod的短域名建集群，即pod名称.svc名称# kubectl -n test exec -it \redis-cluster-sts-0 \ -- redis-cli -a hello_redis@234 \--cluster create \--cluster-replicas 1 \redis-cluster-sts-0.redis-cluster-headless:6379 \redis-cluster-sts-1.redis-cluster-headless:6379 \redis-cluster-sts-2.redis-cluster-headless:6379 \redis-cluster-sts-3.redis-cluster-headless:6379 \redis-cluster-sts-4.redis-cluster-headless:6379 \redis-cluster-sts-5.redis-cluster-headless:6379Warning: Using a password with '-a' or '-u' option on the command line interface may not be safe.&gt;&gt;&gt; Performing hash slots allocation on 6 nodes...Master[0] -&gt; Slots 0 - 5460Master[1] -&gt; Slots 5461 - 10922Master[2] -&gt; Slots 10923 - 16383Adding replica redis-cluster-sts-4.redis-cluster-headless:6379 to redis-cluster-sts-0.redis-cluster-headless:6379Adding replica redis-cluster-sts-5.redis-cluster-headless:6379 to redis-cluster-sts-1.redis-cluster-headless:6379Adding replica redis-cluster-sts-3.redis-cluster-headless:6379 to redis-cluster-sts-2.redis-cluster-headless:6379M: 37818a6e8e5a31235e72c77e4daf5dba5bbc9e1f redis-cluster-sts-0.redis-cluster-headless:6379 slots:[0-5460] (5461 slots) masterM: 21d7a21ef992f52c36ce4c1ed2be0d1a20102800 redis-cluster-sts-1.redis-cluster-headless:6379 slots:[5461-10922] (5462 slots) masterM: 1f0478896e6f5164bbd87e89f38b0ee5d3db0560 redis-cluster-sts-2.redis-cluster-headless:6379 slots:[10923-16383] (5461 slots) masterS: ed6237e68494d834a33420453e58fa47de9eeeaf redis-cluster-sts-3.redis-cluster-headless:6379 replicates 1f0478896e6f5164bbd87e89f38b0ee5d3db0560S: 5c5fa8090096d12bb3c9616893405d027c84c1d5 redis-cluster-sts-4.redis-cluster-headless:6379 replicates 37818a6e8e5a31235e72c77e4daf5dba5bbc9e1fS: e6bfa3ce7ddf65252e3d07a82fa13d3a18f3af7b redis-cluster-sts-5.redis-cluster-headless:6379 replicates 21d7a21ef992f52c36ce4c1ed2be0d1a20102800Can I set the above configuration? (type 'yes' to accept): yes&gt;&gt;&gt; Nodes configuration updated&gt;&gt;&gt; Assign a different config epoch to each node&gt;&gt;&gt; Sending CLUSTER MEET messages to join the clusterWaiting for the cluster to join&gt;&gt;&gt; Performing Cluster Check (using node redis-cluster-sts-0.redis-cluster-headless:6379)M: 37818a6e8e5a31235e72c77e4daf5dba5bbc9e1f redis-cluster-sts-0.redis-cluster-headless:6379 slots:[0-5460] (5461 slots) master 1 additional replica(s)M: 21d7a21ef992f52c36ce4c1ed2be0d1a20102800 redis-cluster-sts-1.redis-cluster-headless:6379 slots:[5461-10922] (5462 slots) master 1 additional replica(s)S: ed6237e68494d834a33420453e58fa47de9eeeaf redis-cluster-sts-3.redis-cluster-headless:6379 slots: (0 slots) slave replicates 1f0478896e6f5164bbd87e89f38b0ee5d3db0560S: 5c5fa8090096d12bb3c9616893405d027c84c1d5 redis-cluster-sts-4.redis-cluster-headless:6379 slots: (0 slots) slave replicates 37818a6e8e5a31235e72c77e4daf5dba5bbc9e1fM: 1f0478896e6f5164bbd87e89f38b0ee5d3db0560 redis-cluster-sts-2.redis-cluster-headless:6379 slots:[10923-16383] (5461 slots) master 1 additional replica(s)S: e6bfa3ce7ddf65252e3d07a82fa13d3a18f3af7b redis-cluster-sts-5.redis-cluster-headless:6379 slots: (0 slots) slave replicates 21d7a21ef992f52c36ce4c1ed2be0d1a20102800[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered.查看集群状态、测试数据读写，进入任意一个pod执行123456789101112131415161718192021222324252627282930313233343536373839# kubectl -n test exec -it redis-cluster-sts-0 -- bashroot@redis-cluster-sts-0:/data# redis-cli -a hello_redis@234 -c Warning: Using a password with '-a' or '-u' option on the command line interface may not be safe.# CLUSTER NODES 查看集群节点，正常127.0.0.1:6379&gt; CLUSTER NODESed6237e68494d834a33420453e58fa47de9eeeaf redis-cluster-sts-3.redis-cluster-headless:6379@16379 slave 1f0478896e6f5164bbd87e89f38b0ee5d3db0560 0 1733304225719 3 connected37818a6e8e5a31235e72c77e4daf5dba5bbc9e1f redis-cluster-sts-0.redis-cluster-headless:6379@16379 myself,master - 0 1733294745577 1 connected 0-54605c5fa8090096d12bb3c9616893405d027c84c1d5 redis-cluster-sts-4.redis-cluster-headless:6379@16379 slave 37818a6e8e5a31235e72c77e4daf5dba5bbc9e1f 0 1733304227000 1 connected21d7a21ef992f52c36ce4c1ed2be0d1a20102800 redis-cluster-sts-1.redis-cluster-headless:6379@16379 master - 0 1733304226000 2 connected 5461-109221f0478896e6f5164bbd87e89f38b0ee5d3db0560 redis-cluster-sts-2.redis-cluster-headless:6379@16379 master - 0 1733304227000 3 connected 10923-16383e6bfa3ce7ddf65252e3d07a82fa13d3a18f3af7b redis-cluster-sts-5.redis-cluster-headless:6379@16379 slave 21d7a21ef992f52c36ce4c1ed2be0d1a20102800 0 1733304227729 2 connected# CLUSTER INFO 查看集群信息：cluster_state:ok表示集群正常127.0.0.1:6379&gt; CLUSTER INFOcluster_state:okcluster_slots_assigned:16384cluster_slots_ok:16384cluster_slots_pfail:0cluster_slots_fail:0cluster_known_nodes:6cluster_size:3cluster_current_epoch:7cluster_my_epoch:1cluster_stats_messages_ping_sent:9439cluster_stats_messages_pong_sent:9440cluster_stats_messages_sent:18879cluster_stats_messages_ping_received:9440cluster_stats_messages_pong_received:9439cluster_stats_messages_fail_received:1cluster_stats_messages_received:18880total_cluster_links_buffer_limit_exceeded:0# 写入，读取key， 正常127.0.0.1:6379&gt; set boo foo127.0.0.1:6379&gt; get boo"foo"每个节点都有noeds.conf文件，包含了集群节点的主从关系，它是redis自己维护的，一般不需要去修改。它们内容应该是一致的（排序可能不一样），如果不一致说明集群有问题，需要排查1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# kubectl -n test exec -it redis-cluster-sts-0 -- cat /data/nodes.confed6237e68494d834a33420453e58fa47de9eeeaf redis-cluster-sts-3.redis-cluster-headless:6379@16379,,tls-port=0,shard-id=a8ae7e1cf170794d0b089d9cf94230d7d9adef17 slave 1f0478896e6f5164bbd87e89f38b0ee5d3db0560 0 1733308120285 3 connected21d7a21ef992f52c36ce4c1ed2be0d1a20102800 redis-cluster-sts-1.redis-cluster-headless:6379@16379,,tls-port=0,shard-id=fa67a2b4a6644b7d71c0b3d7dfea54bb814fbf6f master - 0 1733308121000 2 connected 5461-109225c5fa8090096d12bb3c9616893405d027c84c1d5 redis-cluster-sts-4.redis-cluster-headless:6379@16379,,tls-port=0,shard-id=10133e70314181c4d23d1558381740014c590c22 slave 37818a6e8e5a31235e72c77e4daf5dba5bbc9e1f 0 1733308120000 1 connected1f0478896e6f5164bbd87e89f38b0ee5d3db0560 redis-cluster-sts-2.redis-cluster-headless:6379@16379,,tls-port=0,shard-id=a8ae7e1cf170794d0b089d9cf94230d7d9adef17 master - 0 1733308121290 3 connected 10923-1638337818a6e8e5a31235e72c77e4daf5dba5bbc9e1f redis-cluster-sts-0.redis-cluster-headless:6379@16379,,tls-port=0,shard-id=10133e70314181c4d23d1558381740014c590c22 myself,master - 0 1733308028173 1 connected 0-5460e6bfa3ce7ddf65252e3d07a82fa13d3a18f3af7b redis-cluster-sts-5.redis-cluster-headless:6379@16379,,tls-port=0,shard-id=fa67a2b4a6644b7d71c0b3d7dfea54bb814fbf6f slave 21d7a21ef992f52c36ce4c1ed2be0d1a20102800 0 1733308122296 2 connectedvars currentEpoch 8 lastVoteEpoch 7# kubectl -n test exec -it redis-cluster-sts-1 -- cat /data/nodes.conf5c5fa8090096d12bb3c9616893405d027c84c1d5 redis-cluster-sts-4.redis-cluster-headless:6379@16379,,tls-port=0,shard-id=10133e70314181c4d23d1558381740014c590c22 slave 37818a6e8e5a31235e72c77e4daf5dba5bbc9e1f 0 1733308120871 1 connected21d7a21ef992f52c36ce4c1ed2be0d1a20102800 redis-cluster-sts-1.redis-cluster-headless:6379@16379,,tls-port=0,shard-id=fa67a2b4a6644b7d71c0b3d7dfea54bb814fbf6f myself,master - 0 1733308043534 2 connected 5461-1092237818a6e8e5a31235e72c77e4daf5dba5bbc9e1f redis-cluster-sts-0.redis-cluster-headless:6379@16379,,tls-port=0,shard-id=10133e70314181c4d23d1558381740014c590c22 master - 0 1733308121877 1 connected 0-5460ed6237e68494d834a33420453e58fa47de9eeeaf redis-cluster-sts-3.redis-cluster-headless:6379@16379,,tls-port=0,shard-id=a8ae7e1cf170794d0b089d9cf94230d7d9adef17 slave 1f0478896e6f5164bbd87e89f38b0ee5d3db0560 0 1733308119865 3 connected1f0478896e6f5164bbd87e89f38b0ee5d3db0560 redis-cluster-sts-2.redis-cluster-headless:6379@16379,,tls-port=0,shard-id=a8ae7e1cf170794d0b089d9cf94230d7d9adef17 master - 0 1733308121000 3 connected 10923-16383e6bfa3ce7ddf65252e3d07a82fa13d3a18f3af7b redis-cluster-sts-5.redis-cluster-headless:6379@16379,,tls-port=0,shard-id=fa67a2b4a6644b7d71c0b3d7dfea54bb814fbf6f slave 21d7a21ef992f52c36ce4c1ed2be0d1a20102800 0 1733308122884 2 connectedvars currentEpoch 8 lastVoteEpoch 0# kubectl -n test exec -it redis-cluster-sts-2 -- cat /data/nodes.conf1f0478896e6f5164bbd87e89f38b0ee5d3db0560 redis-cluster-sts-2.redis-cluster-headless:6379@16379,,tls-port=0,shard-id=a8ae7e1cf170794d0b089d9cf94230d7d9adef17 myself,master - 0 1733308059003 3 connected 10923-16383e6bfa3ce7ddf65252e3d07a82fa13d3a18f3af7b redis-cluster-sts-5.redis-cluster-headless:6379@16379,,tls-port=0,shard-id=fa67a2b4a6644b7d71c0b3d7dfea54bb814fbf6f slave 21d7a21ef992f52c36ce4c1ed2be0d1a20102800 0 1733308125906 2 connected37818a6e8e5a31235e72c77e4daf5dba5bbc9e1f redis-cluster-sts-0.redis-cluster-headless:6379@16379,,tls-port=0,shard-id=10133e70314181c4d23d1558381740014c590c22 master - 0 1733308124000 1 connected 0-5460ed6237e68494d834a33420453e58fa47de9eeeaf redis-cluster-sts-3.redis-cluster-headless:6379@16379,,tls-port=0,shard-id=a8ae7e1cf170794d0b089d9cf94230d7d9adef17 slave 1f0478896e6f5164bbd87e89f38b0ee5d3db0560 0 1733308124901 3 connected21d7a21ef992f52c36ce4c1ed2be0d1a20102800 redis-cluster-sts-1.redis-cluster-headless:6379@16379,,tls-port=0,shard-id=fa67a2b4a6644b7d71c0b3d7dfea54bb814fbf6f master - 0 1733308123894 2 connected 5461-109225c5fa8090096d12bb3c9616893405d027c84c1d5 redis-cluster-sts-4.redis-cluster-headless:6379@16379,,tls-port=0,shard-id=10133e70314181c4d23d1558381740014c590c22 slave 37818a6e8e5a31235e72c77e4daf5dba5bbc9e1f 0 1733308123000 1 connectedvars currentEpoch 8 lastVoteEpoch 0# kubectl -n test exec -it redis-cluster-sts-3 -- cat /data/nodes.confe6bfa3ce7ddf65252e3d07a82fa13d3a18f3af7b redis-cluster-sts-5.redis-cluster-headless:6379@16379,,tls-port=0,shard-id=fa67a2b4a6644b7d71c0b3d7dfea54bb814fbf6f slave 21d7a21ef992f52c36ce4c1ed2be0d1a20102800 0 1733308124088 2 connected21d7a21ef992f52c36ce4c1ed2be0d1a20102800 redis-cluster-sts-1.redis-cluster-headless:6379@16379,,tls-port=0,shard-id=fa67a2b4a6644b7d71c0b3d7dfea54bb814fbf6f master - 0 1733308122000 2 connected 5461-10922ed6237e68494d834a33420453e58fa47de9eeeaf redis-cluster-sts-3.redis-cluster-headless:6379@16379,,tls-port=0,shard-id=a8ae7e1cf170794d0b089d9cf94230d7d9adef17 myself,slave 1f0478896e6f5164bbd87e89f38b0ee5d3db0560 0 1733308069358 3 connected5c5fa8090096d12bb3c9616893405d027c84c1d5 redis-cluster-sts-4.redis-cluster-headless:6379@16379,,tls-port=0,shard-id=10133e70314181c4d23d1558381740014c590c22 slave 37818a6e8e5a31235e72c77e4daf5dba5bbc9e1f 0 1733308123000 1 connected1f0478896e6f5164bbd87e89f38b0ee5d3db0560 redis-cluster-sts-2.redis-cluster-headless:6379@16379,,tls-port=0,shard-id=a8ae7e1cf170794d0b089d9cf94230d7d9adef17 master - 0 1733308123082 3 connected 10923-1638337818a6e8e5a31235e72c77e4daf5dba5bbc9e1f redis-cluster-sts-0.redis-cluster-headless:6379@16379,,tls-port=0,shard-id=10133e70314181c4d23d1558381740014c590c22 master - 0 1733308122077 1 connected 0-5460vars currentEpoch 8 lastVoteEpoch 0# kubectl -n test exec -it redis-cluster-sts-4 -- cat /data/nodes.confe6bfa3ce7ddf65252e3d07a82fa13d3a18f3af7b redis-cluster-sts-5.redis-cluster-headless:6379@16379,,tls-port=0,shard-id=fa67a2b4a6644b7d71c0b3d7dfea54bb814fbf6f slave 21d7a21ef992f52c36ce4c1ed2be0d1a20102800 0 1733308119478 2 connected21d7a21ef992f52c36ce4c1ed2be0d1a20102800 redis-cluster-sts-1.redis-cluster-headless:6379@16379,,tls-port=0,shard-id=fa67a2b4a6644b7d71c0b3d7dfea54bb814fbf6f master - 0 1733308118760 2 connected 5461-109225c5fa8090096d12bb3c9616893405d027c84c1d5 redis-cluster-sts-4.redis-cluster-headless:6379@16379,,tls-port=0,shard-id=10133e70314181c4d23d1558381740014c590c22 myself,slave 37818a6e8e5a31235e72c77e4daf5dba5bbc9e1f 0 1733308084722 1 connected37818a6e8e5a31235e72c77e4daf5dba5bbc9e1f redis-cluster-sts-0.redis-cluster-headless:6379@16379,,tls-port=0,shard-id=10133e70314181c4d23d1558381740014c590c22 master - 0 1733308118000 1 connected 0-5460ed6237e68494d834a33420453e58fa47de9eeeaf redis-cluster-sts-3.redis-cluster-headless:6379@16379,,tls-port=0,shard-id=a8ae7e1cf170794d0b089d9cf94230d7d9adef17 slave 1f0478896e6f5164bbd87e89f38b0ee5d3db0560 0 1733308118000 3 connected1f0478896e6f5164bbd87e89f38b0ee5d3db0560 redis-cluster-sts-2.redis-cluster-headless:6379@16379,,tls-port=0,shard-id=a8ae7e1cf170794d0b089d9cf94230d7d9adef17 master - 0 1733308118000 3 connected 10923-16383vars currentEpoch 8 lastVoteEpoch 0# kubectl -n test exec -it redis-cluster-sts-5 -- cat /data/nodes.conf21d7a21ef992f52c36ce4c1ed2be0d1a20102800 redis-cluster-sts-1.redis-cluster-headless:6379@16379,,tls-port=0,shard-id=fa67a2b4a6644b7d71c0b3d7dfea54bb814fbf6f master - 0 1733308116847 2 connected 5461-109221f0478896e6f5164bbd87e89f38b0ee5d3db0560 redis-cluster-sts-2.redis-cluster-headless:6379@16379,,tls-port=0,shard-id=a8ae7e1cf170794d0b089d9cf94230d7d9adef17 master - 0 1733308118000 3 connected 10923-16383e6bfa3ce7ddf65252e3d07a82fa13d3a18f3af7b redis-cluster-sts-5.redis-cluster-headless:6379@16379,,tls-port=0,shard-id=fa67a2b4a6644b7d71c0b3d7dfea54bb814fbf6f myself,slave 21d7a21ef992f52c36ce4c1ed2be0d1a20102800 0 1733308095410 2 connecteded6237e68494d834a33420453e58fa47de9eeeaf redis-cluster-sts-3.redis-cluster-headless:6379@16379,,tls-port=0,shard-id=a8ae7e1cf170794d0b089d9cf94230d7d9adef17 slave 1f0478896e6f5164bbd87e89f38b0ee5d3db0560 0 1733308117871 3 connected5c5fa8090096d12bb3c9616893405d027c84c1d5 redis-cluster-sts-4.redis-cluster-headless:6379@16379,,tls-port=0,shard-id=10133e70314181c4d23d1558381740014c590c22 slave 37818a6e8e5a31235e72c77e4daf5dba5bbc9e1f 0 1733308118395 1 connected37818a6e8e5a31235e72c77e4daf5dba5bbc9e1f redis-cluster-sts-0.redis-cluster-headless:6379@16379,,tls-port=0,shard-id=10133e70314181c4d23d1558381740014c590c22 master - 0 1733308117000 1 connected 0-5460vars currentEpoch 8 lastVoteEpoch 0客户端如何连接集群内其它pod可以通过 svc或pod的FQDN访问通过svc访问12redis-cluster-headless:6379 # 同一个命名空间访问，短域名redis-cluster-headless.test.svc.cluster.local:6379 # 不同命名空间访问，长域名通过pod的FQDN访问，6个节点123456789101112131415# 同一个命名空间访问，短域名redis-cluster-sts-0.redis-cluster-headless:6379redis-cluster-sts-1.redis-cluster-headless:6379redis-cluster-sts-2.redis-cluster-headless:6379 redis-cluster-sts-3.redis-cluster-headless:6379redis-cluster-sts-4.redis-cluster-headless:6379redis-cluster-sts-5.redis-cluster-headless:6379# 不同命名空间访问，长域名redis-cluster-sts-0.redis-cluster-headless.test.svc.cluster.local:6379 redis-cluster-sts-1.redis-cluster-headless.test.svc.cluster.local:6379 redis-cluster-sts-2.redis-cluster-headless.test.svc.cluster.local:6379 redis-cluster-sts-3.redis-cluster-headless.test.svc.cluster.local:6379 redis-cluster-sts-4.redis-cluster-headless.test.svc.cluster.local:6379 redis-cluster-sts-5.redis-cluster-headless.test.svc.cluster.local:6379推荐第2种，写6个节点的主机名与端口，客户端的库保证高可用如：nacos配置可以修改成如下：123456789101112131415161718redis: timeout: 30000 lettuce: pool: max-active: 10 max-idle: 5 max-wait: 3000 min-idle: 2 password: hello_redis@234 cluster: nodes: - redis-cluster-sts-0.redis-cluster-headless.qifu-uat.svc.cluster.local:6379 - redis-cluster-sts-1.redis-cluster-headless.qifu-uat.svc.cluster.local:6379 - redis-cluster-sts-2.redis-cluster-headless.qifu-uat.svc.cluster.local:6379 - redis-cluster-sts-3.redis-cluster-headless.qifu-uat.svc.cluster.local:6379 - redis-cluster-sts-4.redis-cluster-headless.qifu-uat.svc.cluster.local:6379 - redis-cluster-sts-5.redis-cluster-headless.qifu-uat.svc.cluster.local:6379 max-redirects: 3各种测试对redis集群进行 缩容、升级、重启，验证redis集群是否正常重启重启redis集群。如下可以看到集群正常1234567891011121314151617181920212223242526# kubectl -n test rollout restart sts redis-cluster-sts statefulset.apps/redis-cluster-sts restarted# kubectl -n test exec -it redis-cluster-sts-0 -- redis-cli -a hello_redis@234 -c 127.0.0.1:6379&gt; CLUSTER INFO cluster_state:ok # 集群正常cluster_slots_assigned:16384cluster_slots_ok:16384cluster_slots_pfail:0cluster_slots_fail:0cluster_known_nodes:6cluster_size:3cluster_current_epoch:7cluster_my_epoch:1cluster_stats_messages_ping_sent:135cluster_stats_messages_pong_sent:85cluster_stats_messages_sent:220cluster_stats_messages_ping_received:85cluster_stats_messages_pong_received:135cluster_stats_messages_received:220total_cluster_links_buffer_limit_exceeded:0# 写入，读取key， 正常127.0.0.1:6379&gt; set boo foo127.0.0.1:6379&gt; get boo"foo"另外一种重启，先缩容为0个，然后扩容为6，可以看到集群依然正常12345678910111213141516171819202122232425262728kubectl -n test scale statefulset redis-cluster-sts --replicas 0 &amp;&amp; sleep 30kubectl -n midd scale statefulset redis-cluster-sts --replicas 6# 等待所有pod都正常启动, 查看状态# kubectl -n test exec -it redis-cluster-sts-0 -- redis-cli -a hello_redis@234 -c 127.0.0.1:6379&gt; CLUSTER INFO cluster_state:ok # 集群正常cluster_slots_assigned:16384cluster_slots_ok:16384cluster_slots_pfail:0cluster_slots_fail:0cluster_known_nodes:6cluster_size:3cluster_current_epoch:7cluster_my_epoch:1cluster_stats_messages_ping_sent:135cluster_stats_messages_pong_sent:85cluster_stats_messages_sent:220cluster_stats_messages_ping_received:85cluster_stats_messages_pong_received:135cluster_stats_messages_received:220total_cluster_links_buffer_limit_exceeded:0# 写入，读取key， 正常127.0.0.1:6379&gt; set boo foo127.0.0.1:6379&gt; get boo"foo"缩容现在6个节点，缩容为5个，这样模拟一个节点故障，可以看到集群正常12345678910111213141516171819202122232425# kubectl -n test scale statefulset redis-cluster-sts --replicas 5 # kubectl -n test exec -it redis-cluster-sts-0 -- redis-cli -a hello_redis@234 -c 127.0.0.1:6379&gt; CLUSTER INFO cluster_state:ok # 集群正常cluster_slots_assigned:16384cluster_slots_ok:16384cluster_slots_pfail:0cluster_slots_fail:0cluster_known_nodes:6cluster_size:3cluster_current_epoch:7cluster_my_epoch:1cluster_stats_messages_ping_sent:135cluster_stats_messages_pong_sent:85cluster_stats_messages_sent:220cluster_stats_messages_ping_received:85cluster_stats_messages_pong_received:135cluster_stats_messages_received:220total_cluster_links_buffer_limit_exceeded:0# 写入，读取key， 正常127.0.0.1:6379&gt; set boo foo127.0.0.1:6379&gt; get boo"foo"升级将redis版本升级，集群是否正常？在本例中将7.4.0升级到7.4.11kubectl -n test set image sts redis-cluster-sts redis-cluster=docker-0.unsee.tech/redis:7.4.1同理 查看集群状态，数据可以读写表示正常1234567891011121314151617181920212223127.0.0.1:6379&gt; CLUSTER INFO # 状态正常cluster_state:okcluster_slots_assigned:16384cluster_slots_ok:16384cluster_slots_pfail:0cluster_slots_fail:0cluster_known_nodes:6cluster_size:3cluster_current_epoch:7cluster_my_epoch:1cluster_stats_messages_ping_sent:9439cluster_stats_messages_pong_sent:9440cluster_stats_messages_sent:18879cluster_stats_messages_ping_received:9440cluster_stats_messages_pong_received:9439cluster_stats_messages_fail_received:1cluster_stats_messages_received:18880total_cluster_links_buffer_limit_exceeded:0# 写入，读取key， 正常127.0.0.1:6379&gt; set boo foo127.0.0.1:6379&gt; get boo"foo"其它k8s中部署redis集群最重要的就是配置--cluster-announce-ip通告地址，因为pod的变化的，重启后pod地址变化了导致集群失败强烈建议配置--cluster-announce-ip 为 pod的FQDN如下12345678910111213env: - name: TZ value: Asia/Shanghai - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_SERVICE_NAME value: "redis-cluster-headless"args: - /etc/redis/redis-cluster.conf # 配置为短域名 - --cluster-announce-ip "$(POD_NAME).$(POD_SERVICE_NAME)"不建议是pod的ip12345678910env: - name: TZ value: Asia/Shanghai - name: POD_IP valueFrom: fieldRef: fieldPath: status.podIPargs: - /etc/redis/redis-cluster.conf - --cluster-announce-ip $(POD_IP)这也是很多网上这样配置的。这个配置在 reids 升级（换镜像）、缩容为0在扩容为6时 集群失效，特别要注意所以推荐第1种方式，但是需要7.0x或以上的版本。至此，Redis三主三从集群就部署完成了。数据迁移使用Redis-shake迁移介绍redis-shake是阿里云Redis&amp;MongoDB团队开源的用于redis数据同步的工具，用于在两个 redis之 间同步数据的工具，满足用户非常灵活的同步、迁移需求。基本功能redis-shake是阿里基于redis-port基础上进行改进的一款产品。它支持解析、恢复、备份、同步四个功能。以下主要介绍同步sync。恢复restore：将RDB文件恢复到目的redis数据库。备份dump：将源redis的全量数据通过RDB文件备份起来。解析decode：对RDB文件进行读取，并以json格式解析存储。同步sync：支持源redis和目的redis的数据同步，支持全量和增量数据的迁移，支持从云下到阿里云云上的同步，也支持云下到云下不同环境的同步，支持单节点、主从版、集群版之间的互相同步。需要注意的是，如果源端是集群版，可以启动一个RedisShake，从不同的db结点进行拉取，同时源端不能开启move slot功能；对于目的端，如果是集群版，写入可以是1个或者多个db结点。同步rump：支持源redis和目的redis的数据同步，仅支持全量的迁移。采用scan和restore命令进行迁移，支持不同云厂商不同redis版本的迁移。基本原理redis-shake的基本原理就是模拟一个从节点加入源redis集群，首先进行全量拉取并回放，然后进行增量的拉取（通过psync命令）。如下图所示：如果源端是集群模式，只需要启动一个redis-shake进行拉取，同时不能开启源端的move slot操作。如果目的端是集群模式，可以写入到一个结点，然后再进行slot的迁移，当然也可以多对多写入。​ 目前，redis-shake到目的端采用单链路实现，对于正常情况下，这不会成为瓶颈，但对于极端情况，qps比较大的时候，此部分性能可能成为瓶颈，后续我们可能会计划对此进行优化。另外，redis-shake到目的端的数据同步采用异步的方式，读写分离在2个线程操作，降低因为网络时延带来的同步性能下降。迁移安装redis-shake12[root]# wget https://github.com/alibaba/RedisShake/releases/download/release-v2.1.1-20210903/release-v2.1.1-20210903.tar.gz[root]# tar -zxvf release-v2.1.1-20210903.tar.gz配置参数文件配置redis-shake.conf参数文件（修改的部分）123456789[root]# cd release-v2.1.1-20210903[root]# vi redis-shake.confsource.type = standalone #源端架构类型source.address = 127.0.0.1:6379 #源端IP:PORTsource.password_raw = Passwd@123 #源端密码target.type = cluster #目的端架构类型target.address = 10.150.57.13:6381;10.150.57.13:6382;10.150.57.13:6383 #目的端IP:PORT(Redis Cluster的Master或者Slave)target.password_raw = Passwd@123 #目的端密码key_exists = rewrite #如果目的端有同样的键值对，则覆盖迁移启动redis-shake1[root]# ./redis-shake.linux -type=sync -conf=redis-shake.confredis-full-check校验工具简介redis-full-check是阿里云Redis&amp;MongoDB团队开源的用于校验2个redis数据是否一致的工具，通常用于redis数据迁移（redis-shake）后正确性的校验。​ 支持：单节点、主从版、集群版、带proxy的云上集群版（阿里云）之间的同构或者异构对比，版本支持2.x-5.x。基本原理下图是基本的逻辑比较：​ redis-full-check通过全量对比源端和目的端的redis中的数据的方式来进行数据校验，其比较方式通过多轮次比较：每次都会抓取源和目的端的数据进行差异化比较，记录不一致的数据进入下轮对比。然后通过多伦比较不断收敛，减少因数据增量同步导致的源库和目的库的数据不一致。最后sqlite中存在的数据就是最终的差异结果。​ redis-full-check对比的方向是单向：抓取源库A的数据，然后检测是否位于B中，反向不会检测，也就是说，它检测的是源库是否是目的库的子集。如果希望对比双向，则需要对比2次，第一次以A为源库，B为目的库，第二次以B为源库，A为目的库。​ 下图是基本的数据流图，redis-full-check内部分为多轮比较，也就是黄色框所指示的部分。每次比较，会先抓取比较的key，第一轮是从源库中进行抓取，后面轮次是从sqlite3 db中进行抓取；抓取key之后是分别抓取key对应的field和value进行对比，然后将存在差异的部分存入sqlite3 db中，用于下次比较。不一致类型redis-full-check判断不一致的方式主要分为2类：key不一致和value不一致。key不一致key不一致主要分为以下几种情况：lack_target : key存在于源库，但不存在于目的库。type: key存在于源库和目的库，但是类型不一致。value: key存在于源库和目的库，且类型一致，但是value不一致。value不一致不同数据类型有不同的对比标准：string: value不同。hash: 存在field，满足下面3个条件之一：field存在于源端，但不存在与目的端。field存在于目的端，但不存在与源端。field同时存在于源和目的端，但是value不同。set/zset：与hash类似。list: 与hash类似。field冲突类型有以下几种情况（只存在于hash，set，zset，list类型key中）：lack_source: field存在于源端key，field不存在与目的端key。lack_target: field不存在与源端key，field存在于目的端key。value: field存在于源端key和目的端key，但是field对应的value不同。比较原理对比模式（comparemode）有三种可选：KeyOutline：只对比key值是否相等。ValueOutline：只对比value值的长度是否相等。FullValue：对比key值、value长度、value值是否相等。对比会进行comparetimes轮（默认comparetimes=3）比较：第一轮，首先找出在源库上所有的key，然后分别从源库和目的库抓取进行比较。第二轮开始迭代比较，只比较上一轮结束后仍然不一致的key和field。对于key不一致的情况，包括lack_source ，lack_target 和type，从源库和目的库重新取key、value进行比较。value不一致的string，重新比较key：从源和目的取key、value比较。value不一致的hash、set和zset，只重新比较不一致的field，之前已经比较且相同的filed不再比较。这是为了防止对于大key情况下，如果更新频繁，将会导致校验永远不通过的情况。value不一致的list，重新比较key：从源和目的取key、value比较。每轮之间会停止一定的时间（Interval）。对于hash，set，zset，list大key处理采用以下方式：len &lt;= 5192，直接取全量field、value进行比较，使用如下命令：hgetall，smembers，zrange 0 -1 withscores，lrange 0 -1。len &gt; 5192，使用hscan，sscan，zscan，lrange分批取field和value。校验安装GitHub：https://github.com/alibaba/RedisFullCheck安装redis-full-check1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950[root]# wget https://github.com/alibaba/RedisFullCheck/releases/download/release-v1.4.8-20200212/redis-full-check-1.4.8.tar.gz[root]# tar -zxvf redis-full-check-1.4.8.tar.gzredis-full-check-1.4.8/redis-full-check-1.4.8/redis-full-checkredis-full-check-1.4.8/ChangeLog[root]# redis-full-check-1.4.8[root]# ./redis-full-check --helpUsage: redis-full-check [OPTIONS]Application Options: -s, --source=SOURCE Set host:port of source redis. If db type is cluster, split by semicolon(;'), e.g., 10.1.1.1:1000;10.2.2.2:2000;10.3.3.3:3000. We also support auto-detection, so "master@10.1.1.1:1000" or "slave@10.1.1.1:1000" means choose master or slave. Only need to give a role in the master or slave. -p, --sourcepassword=Password Set source redis password --sourceauthtype=AUTH-TYPE useless for opensource redis, valid value:auth/adminauth (default: auth) --sourcedbtype= 0: db, 1: cluster 2: aliyun proxy, 3: tencent proxy (default: 0) --sourcedbfilterlist= db white list that need to be compared, -1 means fetch all, "0;5;15" means fetch db 0, 5, and 15 (default: -1) -t, --target=TARGET Set host:port of target redis. If db type is cluster, split by semicolon(;'), e.g., 10.1.1.1:1000;10.2.2.2:2000;10.3.3.3:3000. We also support auto-detection, so "master@10.1.1.1:1000" or "slave@10.1.1.1:1000" means choose master or slave. Only need to give a role in the master or slave. -a, --targetpassword=Password Set target redis password --targetauthtype=AUTH-TYPE useless for opensource redis, valid value:auth/adminauth (default: auth) --targetdbtype= 0: db, 1: cluster 2: aliyun proxy 3: tencent proxy (default: 0) --targetdbfilterlist= db white list that need to be compared, -1 means fetch all, "0;5;15" means fetch db 0, 5, and 15 (default: -1) -d, --db=Sqlite3-DB-FILE sqlite3 db file for store result. If exist, it will be removed and a new file is created. (default: result.db) --result=FILE store all diff result into the file, format is 'db diff-type key field' --comparetimes=COUNT Total compare count, at least 1. In the first round, all keys will be compared. The subsequent rounds of the comparison will be done on the previous results. (default: 3) -m, --comparemode= compare mode, 1: compare full value, 2: only compare value length, 3: only compare keys outline, 4: compare full value, but only compare value length when meets big key (default: 2) --id= used in metric, run id, useless for open source (default: unknown) --jobid= used in metric, job id, useless for open source (default: unknown) --taskid= used in metric, task id, useless for open source (default: unknown) -q, --qps= max batch qps limit: e.g., if qps is 10, full-check fetches 10 * $batch keys every second (default: 15000) --interval=Second The time interval for each round of comparison(Second) (default: 5) --batchcount=COUNT the count of key/field per batch compare, valid value [1, 10000] (default: 256) --parallel=COUNT concurrent goroutine number for comparison, valid value [1, 100] (default: 5) --log=FILE log file, if not specified, log is put to console --loglevel=LEVEL log level: 'debug', 'info', 'warn', 'error', default is 'info' --metric print metric in log --bigkeythreshold=COUNT -f, --filterlist=FILTER if the filter list isn't empty, all elements in list will be synced. The input should be split by '|'. The end of the string is followed by a * to indicate a prefix match, otherwise it is a full match. e.g.: 'abc*|efg|m*' matches 'abc', 'abc1', 'efg', 'm', 'mxyz', but 'efgh', 'p' aren't' --systemprofile=SYSTEM-PROFILE port that used to print golang inner head and stack message (default: 20445) -v, --versionHelp Options: -h, --help Show this help message参数解释：1234567891011121314151617181920212223242526-s, --source=SOURCE 源redis库地址（ip:port），如果是集群版，那么需要以分号（;）分割不同的db，只需要配置主或者从的其中之一。例如：10.1.1.1:1000;10.2.2.2:2000;10.3.3.3:3000。-p, --sourcepassword=Password 源redis库密码 --sourceauthtype=AUTH-TYPE 源库管理权限，开源reids下此参数无用。 --sourcedbtype= 源库的类别，0：db(standalone单节点、主从)，1: cluster（集群版），2: 阿里云 --sourcedbfilterlist= 源库需要抓取的逻辑db白名单，以分号（;）分割，例如：0;5;15表示db0,db5和db15都会被抓取-t, --target=TARGET 目的redis库地址（ip:port）-a, --targetpassword=Password 目的redis库密码 --targetauthtype=AUTH-TYPE 目的库管理权限，开源reids下此参数无用。 --targetdbtype= 参考sourcedbtype --targetdbfilterlist= 参考sourcedbfilterlist-d, --db=Sqlite3-DB-FILE 对于差异的key存储的sqlite3 db的位置，默认result.db --comparetimes=COUNT 比较轮数-m, --comparemode= 比较模式，1表示全量比较，2表示只对比value的长度，3只对比key是否存在，4全量比较的情况下，忽略大key的比较 --id= 用于打metric --jobid= 用于打metric --taskid= 用于打metric-q, --qps= qps限速阈值 --interval=Second 每轮之间的时间间隔 --batchcount=COUNT 批量聚合的数量 --parallel=COUNT 比较的并发协程数，默认5 --log=FILE log文件 --result=FILE 不一致结果记录到result文件中，格式：'db diff-type key field' --metric=FILE metric文件 --bigkeythreshold=COUNT 大key拆分的阈值，用于comparemode=4-f, --filterlist=FILTER 需要比较的key列表，以分号（;）分割。例如："abc*|efg|m*"表示对比'abc', 'abc1', 'efg', 'm', 'mxyz'，不对比'efgh', 'p'。-v, --version校验校验源端与目的端键值对12345678910111213141516171819202122232425262728293031[root]# ./redis-full-check --source=10.150.57.9:6379 --sourcepassword= --sourcedbtype=0 --target="10.150.57.13:6381;10.150.57.13:6382;10.150.57.13:6383" --targetpassword=Gaoyu@029 --targetdbtype=1 --comparemode=1 --qps=10 --batchcount=1000 --parallel=10[root@guizhou_hp-pop-10-150-57-9 redis-full-check-1.4.8]# ./redis-full-check --source=10.150.57.9:6379 --sourcepassword= --sourcedbtype=0 --target="10.150.57.13:6381;10.150.57.13:6382;10.150.57.13:6383" --targetpassword=Gaoyu@029 --targetdbtype=1 --comparemode=1 --qps=10 --batchcount=1000 --parallel=10[INFO 2021-12-03-10:47:25 main.go:65]: init log success[INFO 2021-12-03-10:47:25 main.go:168]: configuration: &#123;10.150.57.9:6379 auth 0 -1 10.150.57.13:6381;10.150.57.13:6382;10.150.57.13:6383 Gaoyu@029 auth 1 -1 result.db 3 1 unknown unknown unknown 10 5 1000 10 false 16384 20445 false&#125;[INFO 2021-12-03-10:47:25 main.go:170]: ---------[INFO 2021-12-03-10:47:25 full_check.go:238]: sourceDbType=0, p.sourcePhysicalDBList=[meaningless][INFO 2021-12-03-10:47:25 full_check.go:243]: db=0:keys=9[INFO 2021-12-03-10:47:25 full_check.go:253]: ---------------- start 1th time compare[INFO 2021-12-03-10:47:25 full_check.go:278]: start compare db 0[INFO 2021-12-03-10:47:25 scan.go:20]: build connection[source redis addr: [10.150.57.9:6379]][INFO 2021-12-03-10:47:26 full_check.go:203]: stat:times:1, db:0, dbkeys:9, finish:33%, finished:trueKeyScan:&#123;9 9 0&#125;KeyEqualInProcess|string|equal|&#123;9 9 0&#125;[INFO 2021-12-03-10:47:26 full_check.go:250]: wait 5 seconds before start[INFO 2021-12-03-10:47:31 full_check.go:253]: ---------------- start 2th time compare[INFO 2021-12-03-10:47:31 full_check.go:278]: start compare db 0[INFO 2021-12-03-10:47:31 full_check.go:203]: stat:times:2, db:0, finished:trueKeyScan:&#123;0 0 0&#125;[INFO 2021-12-03-10:47:31 full_check.go:250]: wait 5 seconds before start[INFO 2021-12-03-10:47:36 full_check.go:253]: ---------------- start 3th time compare[INFO 2021-12-03-10:47:36 full_check.go:278]: start compare db 0[INFO 2021-12-03-10:47:36 full_check.go:203]: stat:times:3, db:0, finished:trueKeyScan:&#123;0 0 0&#125;[INFO 2021-12-03-10:47:36 full_check.go:328]: --------------- finished! ----------------all finish successfully, totally 0 key(s) and 0 field(s) conflict校验完毕，没有键冲突。Redis集群监控redis-exporter安装redis-cluster-exporter.yaml：123456789101112131415161718192021222324252627282930313233343536373839404142apiVersion: apps/v1kind: Deploymentmetadata: name: redis-cluster-exporter namespace: kubesphere-monitoring-system labels: k8s-app: redis-cluster-exporterspec: selector: matchLabels: k8s-app: redis-cluster-exporter template: metadata: labels: k8s-app: redis-cluster-exporter spec: containers: - name: redis-cluster-exporter image: oliver006/redis_exporter:latest args: - '-redis.addr' - 'redis-cluster-headless.test.svc.cluster.local:6379' - '-redis.password' - 'Passwd@123' ports: - containerPort: 9121 name: http---apiVersion: v1kind: Servicemetadata: labels: k8s-app: redis-cluster-exporter name: redis-cluster-exporter namespace: kubesphere-monitoring-systemspec: ports: - name: http port: 9121 targetPort: http selector: k8s-app: redis-cluster-exporter查看redis-exporter容器日志：添加Prometheus配置123456789101112131415161718192021222324- job_name: 'redis-cluster-exporter-uat_target' static_configs: - targets: - redis://redis-cluster-sts-0.redis-cluster-headless.test.svc.cluster.local:6379 - redis://redis-cluster-sts-1.redis-cluster-headless.test.svc.cluster.local:6379 - redis://redis-cluster-sts-2.redis-cluster-headless.test.svc.cluster.local:6379 - redis://redis-cluster-sts-3.redis-cluster-headless.test.svc.cluster.local:6379 - redis://redis-cluster-sts-4.redis-cluster-headless.test.svc.cluster.local:6379 - redis://redis-cluster-sts-5.redis-cluster-headless.test.svc.cluster.local:6379 labels: env: uat cluster: uat-redis-cluster metrics_path: /scrape relabel_configs: - source_labels: [__address__] target_label: __param_target - source_labels: [__param_target] target_label: instance - target_label: __address__ replacement: redis-cluster-exporter.kubesphere-monitoring-system.svc.cluster.local:9121- job_name: 'redis-cluster-exporter-uat' static_configs: - targets: - redis-cluster-exporter.kubesphere-monitoring-system.svc.cluster.local:9121在Prometheus查看是否有数据收集：创建Grafana可视化数据导入仪表盘，id为763：此时的仪表盘还不可以按集群和角色去筛选数据，需要修改下变量：修改完成保存即可：或者使用json，创建一个仪表盘：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697989910010110210310410510610710810911011111211311411511611711811912012112212312412512612712812913013113213313413513613713813914014114214314414514614714814915015115215315415515615715815916016116216316416516616716816917017117217317417517617717817918018118218318418518618718818919019119219319419519619719819920020120220320420520620720820921021121221321421521621721821922022122222322422522622722822923023123223323423523623723823924024124224324424524624724824925025125225325425525625725825926026126226326426526626726826927027127227327427527627727827928028128228328428528628728828929029129229329429529629729829930030130230330430530630730830931031131231331431531631731831932032132232332432532632732832933033133233333433533633733833934034134234334434534634734834935035135235335435535635735835936036136236336436536636736836937037137237337437537637737837938038138238338438538638738838939039139239339439539639739839940040140240340440540640740840941041141241341441541641741841942042142242342442542642742842943043143243343443543643743843944044144244344444544644744844945045145245345445545645745845946046146246346446546646746846947047147247347447547647747847948048148248348448548648748848949049149249349449549649749849950050150250350450550650750850951051151251351451551651751851952052152252352452552652752852953053153253353453553653753853954054154254354454554654754854955055155255355455555655755855956056156256356456556656756856957057157257357457557657757857958058158258358458558658758858959059159259359459559659759859960060160260360460560660760860961061161261361461561661761861962062162262362462562662762862963063163263363463563663763863964064164264364464564664764864965065165265365465565665765865966066166266366466566666766866967067167267367467567667767867968068168268368468568668768868969069169269369469569669769869970070170270370470570670770870971071171271371471571671771871972072172272372472572672772872973073173273373473573673773873974074174274374474574674774874975075175275375475575675775875976076176276376476576676776876977077177277377477577677777877978078178278378478578678778878979079179279379479579679779879980080180280380480580680780880981081181281381481581681781881982082182282382482582682782882983083183283383483583683783883984084184284384484584684784884985085185285385485585685785885986086186286386486586686786886987087187287387487587687787887988088188288388488588688788888989089189289389489589689789889990090190290390490590690790890991091191291391491591691791891992092192292392492592692792892993093193293393493593693793893994094194294394494594694794894995095195295395495595695795895996096196296396496596696796896997097197297397497597697797897998098198298398498598698798898999099199299399499599699799899910001001100210031004100510061007100810091010101110121013101410151016101710181019102010211022102310241025102610271028102910301031103210331034103510361037103810391040104110421043104410451046104710481049105010511052105310541055105610571058105910601061106210631064106510661067106810691070107110721073107410751076107710781079108010811082108310841085108610871088108910901091109210931094109510961097109810991100110111021103110411051106110711081109111011111112111311141115111611171118111911201121112211231124112511261127112811291130113111321133113411351136113711381139114011411142114311441145114611471148114911501151115211531154115511561157115811591160116111621163116411651166116711681169117011711172117311741175117611771178117911801181118211831184118511861187118811891190119111921193119411951196119711981199120012011202120312041205120612071208120912101211121212131214121512161217121812191220122112221223122412251226122712281229123012311232123312341235123612371238123912401241124212431244124512461247124812491250125112521253125412551256125712581259126012611262126312641265126612671268126912701271127212731274127512761277127812791280128112821283128412851286128712881289129012911292129312941295129612971298129913001301130213031304130513061307130813091310131113121313131413151316131713181319132013211322132313241325132613271328132913301331133213331334133513361337133813391340134113421343134413451346134713481349135013511352135313541355135613571358135913601361136213631364136513661367136813691370137113721373137413751376137713781379138013811382138313841385138613871388138913901391139213931394139513961397139813991400140114021403140414051406140714081409141014111412141314141415141614171418141914201421142214231424142514261427142814291430143114321433143414351436143714381439144014411442144314441445144614471448144914501451145214531454145514561457145814591460146114621463146414651466146714681469147014711472147314741475147614771478147914801481148214831484148514861487148814891490149114921493149414951496149714981499150015011502150315041505150615071508150915101511151215131514151515161517151815191520152115221523152415251526152715281529153015311532153315341535153615371538153915401541154215431544154515461547154815491550155115521553155415551556155715581559156015611562156315641565156615671568156915701571157215731574157515761577157815791580158115821583158415851586158715881589159015911592159315941595159615971598159916001601160216031604160516061607160816091610161116121613161416151616161716181619162016211622162316241625162616271628162916301631163216331634163516361637163816391640164116421643164416451646164716481649165016511652165316541655165616571658165916601661166216631664166516661667166816691670167116721673167416751676167716781679168016811682168316841685168616871688168916901691169216931694169516961697169816991700170117021703170417051706170717081709171017111712171317141715171617171718171917201721172217231724172517261727172817291730173117321733173417351736173717381739174017411742174317441745174617471748174917501751175217531754175517561757175817591760&#123; "annotations": &#123; "list": [ &#123; "builtIn": 1, "datasource": "-- Grafana --", "enable": true, "hide": true, "iconColor": "rgba(0, 211, 255, 1)", "name": "Annotations &amp; Alerts", "type": "dashboard" &#125; ] &#125;, "editable": true, "gnetId": null, "graphTooltip": 0, "id": 66, "iteration": 1606455383511, "links": [], "panels": [ &#123; "cacheTimeout": null, "colorBackground": false, "colorValue": true, "colors": [ "#299c46", "rgba(237, 129, 40, 0.89)", "#d44a3a" ], "datasource": "prometheus", "description": "集群个数", "fieldConfig": &#123; "defaults": &#123; "custom": &#123;&#125; &#125;, "overrides": [] &#125;, "format": "none", "gauge": &#123; "maxValue": 100, "minValue": 0, "show": false, "thresholdLabels": false, "thresholdMarkers": true &#125;, "gridPos": &#123; "h": 3, "w": 5, "x": 0, "y": 0 &#125;, "id": 8, "interval": null, "links": [], "mappingType": 1, "mappingTypes": [ &#123; "name": "value to text", "value": 1 &#125;, &#123; "name": "range to text", "value": 2 &#125; ], "maxDataPoints": 100, "nullPointMode": "connected", "nullText": null, "postfix": "", "postfixFontSize": "50%", "prefix": "", "prefixFontSize": "50%", "rangeMaps": [ &#123; "from": "null", "text": "N/A", "to": "null" &#125; ], "sparkline": &#123; "fillColor": "rgba(31, 118, 189, 0.18)", "full": false, "lineColor": "rgb(31, 120, 193)", "show": false, "ymax": null, "ymin": null &#125;, "tableColumn": "", "targets": [ &#123; "expr": "count(count(redis_up&#123;cluster=~\"$cluster\"&#125;&gt;0) by (cluster))", "interval": "", "legendFormat": "", "refId": "A" &#125; ], "thresholds": "", "timeFrom": null, "timeShift": null, "title": "集群个数", "type": "singlestat", "valueFontSize": "100%", "valueMaps": [ &#123; "op": "=", "text": "N/A", "value": "null" &#125; ], "valueName": "current" &#125;, &#123; "cacheTimeout": null, "colorBackground": false, "colorValue": true, "colors": [ "#299c46", "rgba(237, 129, 40, 0.89)", "#d44a3a" ], "datasource": "prometheus", "description": "集群节点数量", "fieldConfig": &#123; "defaults": &#123; "custom": &#123;&#125; &#125;, "overrides": [] &#125;, "format": "none", "gauge": &#123; "maxValue": 100, "minValue": 0, "show": false, "thresholdLabels": false, "thresholdMarkers": true &#125;, "gridPos": &#123; "h": 3, "w": 5, "x": 5, "y": 0 &#125;, "id": 4, "interval": null, "links": [], "mappingType": 1, "mappingTypes": [ &#123; "name": "value to text", "value": 1 &#125;, &#123; "name": "range to text", "value": 2 &#125; ], "maxDataPoints": 100, "nullPointMode": "connected", "nullText": null, "postfix": "", "postfixFontSize": "50%", "prefix": "", "prefixFontSize": "50%", "rangeMaps": [ &#123; "from": "null", "text": "N/A", "to": "null" &#125; ], "sparkline": &#123; "fillColor": "rgba(31, 118, 189, 0.18)", "full": false, "lineColor": "rgb(31, 120, 193)", "show": false, "ymax": null, "ymin": null &#125;, "tableColumn": "", "targets": [ &#123; "expr": "count(redis_up&#123;cluster=~\"$cluster\"&#125;)", "interval": "", "legendFormat": "", "refId": "A" &#125; ], "thresholds": "", "timeFrom": null, "timeShift": null, "title": "集群节点数", "type": "singlestat", "valueFontSize": "100%", "valueMaps": [ &#123; "op": "=", "text": "N/A", "value": "null" &#125; ], "valueName": "current" &#125;, &#123; "cacheTimeout": null, "colorBackground": false, "colorValue": true, "colors": [ "#299c46", "rgba(237, 129, 40, 0.89)", "#d44a3a" ], "datasource": "prometheus", "description": "集群活跃节点数量", "fieldConfig": &#123; "defaults": &#123; "custom": &#123;&#125; &#125;, "overrides": [] &#125;, "format": "none", "gauge": &#123; "maxValue": 100, "minValue": 0, "show": false, "thresholdLabels": false, "thresholdMarkers": true &#125;, "gridPos": &#123; "h": 3, "w": 4, "x": 10, "y": 0 &#125;, "id": 6, "interval": null, "links": [], "mappingType": 1, "mappingTypes": [ &#123; "name": "value to text", "value": 1 &#125;, &#123; "name": "range to text", "value": 2 &#125; ], "maxDataPoints": 100, "nullPointMode": "connected", "nullText": null, "postfix": "", "postfixFontSize": "50%", "prefix": "", "prefixFontSize": "50%", "rangeMaps": [ &#123; "from": "null", "text": "N/A", "to": "null" &#125; ], "sparkline": &#123; "fillColor": "rgba(31, 118, 189, 0.18)", "full": false, "lineColor": "rgb(31, 120, 193)", "show": false, "ymax": null, "ymin": null &#125;, "tableColumn": "", "targets": [ &#123; "expr": "count(redis_up&#123;cluster=~\"$cluster\"&#125;&gt;0)", "interval": "", "legendFormat": "", "refId": "A" &#125; ], "thresholds": "", "timeFrom": null, "timeShift": null, "title": "集群活跃节点数", "type": "singlestat", "valueFontSize": "100%", "valueMaps": [ &#123; "op": "=", "text": "N/A", "value": "null" &#125; ], "valueName": "current" &#125;, &#123; "cacheTimeout": null, "colorBackground": false, "colorPrefix": false, "colorValue": true, "colors": [ "#299c46", "rgba(237, 129, 40, 0.89)", "#d44a3a" ], "datasource": "prometheus", "description": "主节点数量", "fieldConfig": &#123; "defaults": &#123; "custom": &#123;&#125; &#125;, "overrides": [] &#125;, "format": "none", "gauge": &#123; "maxValue": 100, "minValue": 0, "show": false, "thresholdLabels": false, "thresholdMarkers": true &#125;, "gridPos": &#123; "h": 3, "w": 5, "x": 14, "y": 0 &#125;, "id": 10, "interval": null, "links": [], "mappingType": 1, "mappingTypes": [ &#123; "name": "value to text", "value": 1 &#125;, &#123; "name": "range to text", "value": 2 &#125; ], "maxDataPoints": 100, "nullPointMode": "connected", "nullText": null, "postfix": "", "postfixFontSize": "50%", "prefix": "", "prefixFontSize": "50%", "rangeMaps": [ &#123; "from": "null", "text": "N/A", "to": "null" &#125; ], "sparkline": &#123; "fillColor": "rgba(31, 118, 189, 0.18)", "full": false, "lineColor": "rgb(31, 120, 193)", "show": false, "ymax": null, "ymin": null &#125;, "tableColumn": "", "targets": [ &#123; "expr": "count(redis_instance_info&#123;cluster=~\"$cluster\", role=\"master\"&#125;)", "interval": "", "legendFormat": "", "refId": "A" &#125; ], "thresholds": "", "timeFrom": null, "timeShift": null, "title": "主节点个数", "type": "singlestat", "valueFontSize": "100%", "valueMaps": [ &#123; "op": "=", "text": "N/A", "value": "null" &#125; ], "valueName": "current" &#125;, &#123; "cacheTimeout": null, "colorBackground": false, "colorValue": true, "colors": [ "#299c46", "rgba(237, 129, 40, 0.89)", "#d44a3a" ], "datasource": "prometheus", "description": "从节点数量", "fieldConfig": &#123; "defaults": &#123; "custom": &#123;&#125; &#125;, "overrides": [] &#125;, "format": "none", "gauge": &#123; "maxValue": 100, "minValue": 0, "show": false, "thresholdLabels": false, "thresholdMarkers": true &#125;, "gridPos": &#123; "h": 3, "w": 5, "x": 19, "y": 0 &#125;, "id": 12, "interval": null, "links": [], "mappingType": 1, "mappingTypes": [ &#123; "name": "value to text", "value": 1 &#125;, &#123; "name": "range to text", "value": 2 &#125; ], "maxDataPoints": 100, "nullPointMode": "connected", "nullText": null, "postfix": "", "postfixFontSize": "50%", "prefix": "", "prefixFontSize": "50%", "rangeMaps": [ &#123; "from": "null", "text": "N/A", "to": "null" &#125; ], "sparkline": &#123; "fillColor": "rgba(31, 118, 189, 0.18)", "full": false, "lineColor": "rgb(31, 120, 193)", "show": false, "ymax": null, "ymin": null &#125;, "tableColumn": "", "targets": [ &#123; "expr": "count(redis_instance_info&#123;cluster=~\"$cluster\", role=\"slave\"&#125;)", "interval": "", "legendFormat": "", "refId": "A" &#125; ], "thresholds": "", "timeFrom": null, "timeShift": null, "title": "从节点个数", "type": "singlestat", "valueFontSize": "100%", "valueMaps": [ &#123; "op": "=", "text": "N/A", "value": "null" &#125; ], "valueName": "current" &#125;, &#123; "aliasColors": &#123;&#125;, "bars": false, "dashLength": 10, "dashes": false, "datasource": "prometheus", "description": "Redus Cluster OPS指标", "fieldConfig": &#123; "defaults": &#123; "custom": &#123;&#125;, "links": [] &#125;, "overrides": [] &#125;, "fill": 1, "fillGradient": 0, "gridPos": &#123; "h": 8, "w": 12, "x": 0, "y": 3 &#125;, "hiddenSeries": false, "id": 16, "legend": &#123; "avg": false, "current": false, "max": false, "min": false, "show": true, "total": false, "values": false &#125;, "lines": true, "linewidth": 1, "nullPointMode": "null", "percentage": false, "pluginVersion": "7.1.0", "pointradius": 2, "points": false, "renderer": "flot", "seriesOverrides": [], "spaceLength": 10, "stack": false, "steppedLine": false, "targets": [ &#123; "expr": "ceil(sum(rate(redis_commands_processed_total&#123;cluster=~\"$cluster\",instance=~\"$master\"&#125;[$interval])) by (cluster))", "interval": "5s", "legendFormat": "&#123;&#123;cluster&#125;&#125;", "refId": "A" &#125; ], "thresholds": [], "timeFrom": null, "timeRegions": [], "timeShift": null, "title": "Redis Cluster OPS", "tooltip": &#123; "shared": true, "sort": 0, "value_type": "individual" &#125;, "type": "graph", "xaxis": &#123; "buckets": null, "mode": "time", "name": null, "show": true, "values": [] &#125;, "yaxes": [ &#123; "format": "short", "label": null, "logBase": 1, "max": null, "min": null, "show": true &#125;, &#123; "format": "short", "label": null, "logBase": 1, "max": null, "min": null, "show": true &#125; ], "yaxis": &#123; "align": false, "alignLevel": null &#125; &#125;, &#123; "aliasColors": &#123;&#125;, "bars": false, "dashLength": 10, "dashes": false, "datasource": "prometheus", "description": "Redis 内存使用大小值", "fieldConfig": &#123; "defaults": &#123; "custom": &#123;&#125;, "links": [] &#125;, "overrides": [] &#125;, "fill": 1, "fillGradient": 0, "gridPos": &#123; "h": 8, "w": 12, "x": 12, "y": 3 &#125;, "hiddenSeries": false, "id": 18, "legend": &#123; "avg": false, "current": false, "max": false, "min": false, "show": true, "total": false, "values": false &#125;, "lines": true, "linewidth": 1, "nullPointMode": "null", "percentage": false, "pluginVersion": "7.1.0", "pointradius": 2, "points": false, "renderer": "flot", "seriesOverrides": [], "spaceLength": 10, "stack": false, "steppedLine": false, "targets": [ &#123; "expr": "redis_memory_used_bytes&#123;instance=~\"$instance\"&#125; ", "interval": "5s", "legendFormat": "&#123;&#123;instance&#125;&#125;", "refId": "A" &#125; ], "thresholds": [], "timeFrom": null, "timeRegions": [], "timeShift": null, "title": "Redis 内存使用大小", "tooltip": &#123; "shared": true, "sort": 0, "value_type": "individual" &#125;, "type": "graph", "xaxis": &#123; "buckets": null, "mode": "time", "name": null, "show": true, "values": [] &#125;, "yaxes": [ &#123; "format": "bytes", "label": null, "logBase": 1, "max": null, "min": null, "show": true &#125;, &#123; "format": "short", "label": null, "logBase": 1, "max": null, "min": null, "show": true &#125; ], "yaxis": &#123; "align": false, "alignLevel": null &#125; &#125;, &#123; "aliasColors": &#123;&#125;, "bars": false, "dashLength": 10, "dashes": false, "datasource": "prometheus", "description": "Redis Server CPU使用率", "fieldConfig": &#123; "defaults": &#123; "custom": &#123;&#125;, "links": [] &#125;, "overrides": [] &#125;, "fill": 1, "fillGradient": 0, "gridPos": &#123; "h": 8, "w": 12, "x": 0, "y": 11 &#125;, "hiddenSeries": false, "id": 14, "legend": &#123; "avg": false, "current": false, "max": false, "min": false, "show": true, "total": false, "values": false &#125;, "lines": true, "linewidth": 1, "nullPointMode": "null", "percentage": false, "pluginVersion": "7.1.0", "pointradius": 2, "points": false, "renderer": "flot", "seriesOverrides": [], "spaceLength": 10, "stack": false, "steppedLine": false, "targets": [ &#123; "expr": "100* (rate(redis_cpu_sys_seconds_total&#123;instance=~\"$instance\"&#125;[$interval]) + rate(redis_cpu_user_seconds_total&#123;instance=~\"$instance\"&#125;[$interval]))", "interval": "5s", "legendFormat": "&#123;&#123;instance&#125;&#125;", "refId": "A" &#125; ], "thresholds": [], "timeFrom": null, "timeRegions": [], "timeShift": null, "title": "Redis CPU使用率", "tooltip": &#123; "shared": true, "sort": 0, "value_type": "individual" &#125;, "type": "graph", "xaxis": &#123; "buckets": null, "mode": "time", "name": null, "show": true, "values": [] &#125;, "yaxes": [ &#123; "format": "percent", "label": null, "logBase": 1, "max": null, "min": null, "show": true &#125;, &#123; "format": "short", "label": null, "logBase": 1, "max": null, "min": null, "show": true &#125; ], "yaxis": &#123; "align": false, "alignLevel": null &#125; &#125;, &#123; "aliasColors": &#123;&#125;, "bars": false, "dashLength": 10, "dashes": false, "datasource": "prometheus", "description": "Redis 内存使用率", "fieldConfig": &#123; "defaults": &#123; "custom": &#123;&#125;, "links": [] &#125;, "overrides": [] &#125;, "fill": 1, "fillGradient": 0, "gridPos": &#123; "h": 8, "w": 12, "x": 12, "y": 11 &#125;, "hiddenSeries": false, "id": 20, "legend": &#123; "avg": false, "current": false, "max": false, "min": false, "show": true, "total": false, "values": false &#125;, "lines": true, "linewidth": 1, "nullPointMode": "null", "percentage": false, "pluginVersion": "7.1.0", "pointradius": 2, "points": false, "renderer": "flot", "seriesOverrides": [], "spaceLength": 10, "stack": false, "steppedLine": false, "targets": [ &#123; "expr": "(redis_memory_used_bytes&#123;instance=~\"$instance\"&#125; / redis_config_maxmemory&#123;instance=~\"$instance\"&#125;) * 100", "interval": "5s", "legendFormat": "&#123;&#123;instance&#125;&#125;", "refId": "A" &#125; ], "thresholds": [], "timeFrom": null, "timeRegions": [], "timeShift": null, "title": "Redis 内存使用率", "tooltip": &#123; "shared": true, "sort": 0, "value_type": "individual" &#125;, "type": "graph", "xaxis": &#123; "buckets": null, "mode": "time", "name": null, "show": true, "values": [] &#125;, "yaxes": [ &#123; "format": "percent", "label": null, "logBase": 1, "max": null, "min": null, "show": true &#125;, &#123; "format": "short", "label": null, "logBase": 1, "max": null, "min": null, "show": true &#125; ], "yaxis": &#123; "align": false, "alignLevel": null &#125; &#125;, &#123; "aliasColors": &#123;&#125;, "bars": false, "dashLength": 10, "dashes": false, "datasource": "prometheus", "description": "各个节点系统CPU使用率", "fieldConfig": &#123; "defaults": &#123; "custom": &#123;&#125;, "links": [] &#125;, "overrides": [] &#125;, "fill": 1, "fillGradient": 0, "gridPos": &#123; "h": 8, "w": 12, "x": 0, "y": 19 &#125;, "hiddenSeries": false, "id": 24, "legend": &#123; "avg": false, "current": false, "max": false, "min": false, "show": true, "total": false, "values": false &#125;, "lines": true, "linewidth": 1, "nullPointMode": "null", "percentage": false, "pluginVersion": "7.1.0", "pointradius": 2, "points": false, "renderer": "flot", "seriesOverrides": [], "spaceLength": 10, "stack": false, "steppedLine": false, "targets": [ &#123; "expr": "topk(3,clamp_max((avg by (hostname,mode) ((clamp_max(rate(node_cpu_seconds_total&#123;hostname=~\"$host\",mode!=\"idle\"&#125;[$interval]),1)) or (clamp_max(irate(node_cpu_seconds_total&#123;hostname=~\"$host\",mode!=\"idle\"&#125;[5m]),1)) ))*100,100))", "hide": true, "interval": "", "legendFormat": "&#123;&#123;mode&#125;&#125;", "refId": "A" &#125;, &#123; "expr": "1 - avg by (hostname)(rate(node_cpu_seconds_total&#123;hostname=~\"$host\", mode=\"idle\"&#125;[$interval]))", "interval": "", "legendFormat": "&#123;&#123;hostname&#125;&#125;", "refId": "B" &#125; ], "thresholds": [], "timeFrom": null, "timeRegions": [], "timeShift": null, "title": "系统CPU使用率", "tooltip": &#123; "shared": true, "sort": 0, "value_type": "individual" &#125;, "type": "graph", "xaxis": &#123; "buckets": null, "mode": "time", "name": null, "show": true, "values": [] &#125;, "yaxes": [ &#123; "format": "percentunit", "label": null, "logBase": 1, "max": null, "min": null, "show": true &#125;, &#123; "format": "short", "label": null, "logBase": 1, "max": null, "min": null, "show": true &#125; ], "yaxis": &#123; "align": false, "alignLevel": null &#125; &#125;, &#123; "aliasColors": &#123;&#125;, "bars": false, "dashLength": 10, "dashes": false, "datasource": "prometheus", "description": "各个节点系统物理内存利用率", "fieldConfig": &#123; "defaults": &#123; "custom": &#123;&#125;, "links": [] &#125;, "overrides": [] &#125;, "fill": 1, "fillGradient": 0, "gridPos": &#123; "h": 8, "w": 12, "x": 12, "y": 19 &#125;, "hiddenSeries": false, "id": 30, "legend": &#123; "avg": false, "current": false, "max": false, "min": false, "show": true, "total": false, "values": false &#125;, "lines": true, "linewidth": 1, "nullPointMode": "null", "percentage": false, "pluginVersion": "7.1.0", "pointradius": 2, "points": false, "renderer": "flot", "seriesOverrides": [], "spaceLength": 10, "stack": false, "steppedLine": false, "targets": [ &#123; "expr": "(node_memory_Cached_bytes&#123;hostname=~\"$host\"&#125; + node_memory_Buffers_bytes&#123;hostname=~\"$host\"&#125; + node_memory_MemFree_bytes&#123;hostname=~\"$host\"&#125;)/node_memory_MemTotal_bytes&#123;hostname=~\"$host\"&#125;", "hide": true, "interval": "", "legendFormat": "&#123;&#123;hostname&#125;&#125;", "refId": "A" &#125;, &#123; "expr": "((node_memory_MemTotal_bytes&#123;hostname=~\"$host\"&#125; - (node_memory_MemAvailable_bytes&#123;hostname=~\"$host\"&#125; or (node_memory_MemFree_bytes&#123;hostname=~\"$host\"&#125; + node_memory_Buffers_bytes&#123;hostname=~\"$host\"&#125; + node_memory_Cached_bytes&#123;hostname=~\"$host\"&#125;)))*100 / node_memory_MemTotal_bytes&#123;hostname=~\"$host\"&#125;)", "hide": false, "interval": "", "legendFormat": "&#123;&#123;hostname&#125;&#125;", "refId": "B" &#125; ], "thresholds": [], "timeFrom": null, "timeRegions": [], "timeShift": null, "title": "系统物理内存利用率", "tooltip": &#123; "shared": true, "sort": 0, "value_type": "individual" &#125;, "type": "graph", "xaxis": &#123; "buckets": null, "mode": "time", "name": null, "show": true, "values": [] &#125;, "yaxes": [ &#123; "format": "percent", "label": null, "logBase": 1, "max": null, "min": null, "show": true &#125;, &#123; "format": "short", "label": null, "logBase": 1, "max": null, "min": null, "show": true &#125; ], "yaxis": &#123; "align": false, "alignLevel": null &#125; &#125;, &#123; "aliasColors": &#123;&#125;, "bars": true, "cacheTimeout": null, "dashLength": 10, "dashes": false, "datasource": "prometheus", "description": "各个节点系统磁盘使用率", "fieldConfig": &#123; "defaults": &#123; "custom": &#123;&#125;, "links": [] &#125;, "overrides": [] &#125;, "fill": 1, "fillGradient": 0, "gridPos": &#123; "h": 8, "w": 12, "x": 0, "y": 27 &#125;, "hiddenSeries": false, "id": 32, "legend": &#123; "avg": false, "current": false, "max": false, "min": false, "show": true, "total": false, "values": false &#125;, "lines": false, "linewidth": 1, "links": [], "nullPointMode": "null", "percentage": false, "pluginVersion": "7.1.0", "pointradius": 2, "points": false, "renderer": "flot", "seriesOverrides": [], "spaceLength": 10, "stack": false, "steppedLine": false, "targets": [ &#123; "expr": "topk(10,max((1 - node_filesystem_avail_bytes&#123;hostname=~\"$host\", fstype!~\"rootfs|selinuxfs|autofs|rpc_pipefs|tmpfs\"&#125; / node_filesystem_size_bytes&#123;hostname=~\"$host\", fstype!~\"rootfs|selinuxfs|autofs|rpc_pipefs|tmpfs\"&#125;)*100) by (hostname) &gt; 0)", "interval": "", "legendFormat": "&#123;&#123;hostname&#125;&#125;", "refId": "A" &#125; ], "thresholds": [], "timeFrom": null, "timeRegions": [], "timeShift": null, "title": "系统磁盘使用率", "tooltip": &#123; "shared": false, "sort": 0, "value_type": "individual" &#125;, "type": "graph", "xaxis": &#123; "buckets": null, "mode": "series", "name": null, "show": true, "values": [ "current" ] &#125;, "yaxes": [ &#123; "format": "percent", "label": null, "logBase": 1, "max": null, "min": null, "show": true &#125;, &#123; "format": "short", "label": null, "logBase": 1, "max": null, "min": null, "show": true &#125; ], "yaxis": &#123; "align": false, "alignLevel": null &#125; &#125;, &#123; "aliasColors": &#123;&#125;, "bars": false, "dashLength": 10, "dashes": false, "datasource": "prometheus", "description": "各个节点以及集群的 KEYS 总数", "fieldConfig": &#123; "defaults": &#123; "custom": &#123;&#125;, "links": [] &#125;, "overrides": [] &#125;, "fill": 1, "fillGradient": 0, "gridPos": &#123; "h": 8, "w": 12, "x": 12, "y": 27 &#125;, "hiddenSeries": false, "id": 22, "legend": &#123; "avg": false, "current": false, "max": false, "min": false, "show": true, "total": false, "values": false &#125;, "lines": true, "linewidth": 1, "nullPointMode": "null", "percentage": false, "pluginVersion": "7.1.0", "pointradius": 2, "points": false, "renderer": "flot", "seriesOverrides": [], "spaceLength": 10, "stack": false, "steppedLine": false, "targets": [ &#123; "expr": "sum(redis_db_keys&#123;instance=~\"$master\"&#125;) by (instance,db)", "interval": "", "legendFormat": "&#123;&#123;instance&#125;&#125;", "refId": "A" &#125;, &#123; "expr": "sum(redis_db_keys&#123;instance=~\"$master\"&#125;) by (cluster)", "interval": "", "legendFormat": "&#123;&#123;cluster&#125;&#125;_total", "refId": "B" &#125; ], "thresholds": [], "timeFrom": null, "timeRegions": [], "timeShift": null, "title": "KEYS 总数", "tooltip": &#123; "shared": true, "sort": 0, "value_type": "individual" &#125;, "type": "graph", "xaxis": &#123; "buckets": null, "mode": "time", "name": null, "show": true, "values": [] &#125;, "yaxes": [ &#123; "format": "short", "label": null, "logBase": 1, "max": null, "min": null, "show": true &#125;, &#123; "format": "short", "label": null, "logBase": 1, "max": null, "min": null, "show": true &#125; ], "yaxis": &#123; "align": false, "alignLevel": null &#125; &#125;, &#123; "aliasColors": &#123;&#125;, "bars": false, "dashLength": 10, "dashes": false, "datasource": "prometheus", "description": "各个节点活跃连接数", "fieldConfig": &#123; "defaults": &#123; "custom": &#123;&#125;, "links": [] &#125;, "overrides": [] &#125;, "fill": 1, "fillGradient": 0, "gridPos": &#123; "h": 8, "w": 12, "x": 0, "y": 35 &#125;, "hiddenSeries": false, "id": 26, "legend": &#123; "avg": false, "current": false, "max": false, "min": false, "show": true, "total": false, "values": false &#125;, "lines": true, "linewidth": 1, "nullPointMode": "null", "percentage": false, "pluginVersion": "7.1.0", "pointradius": 2, "points": false, "renderer": "flot", "seriesOverrides": [], "spaceLength": 10, "stack": false, "steppedLine": false, "targets": [ &#123; "expr": "redis_connected_clients&#123;instance=~\"$instance\"&#125;", "interval": "", "legendFormat": "&#123;&#123;hostip&#125;&#125;", "refId": "A" &#125; ], "thresholds": [], "timeFrom": null, "timeRegions": [], "timeShift": null, "title": "活跃连接数", "tooltip": &#123; "shared": true, "sort": 0, "value_type": "individual" &#125;, "type": "graph", "xaxis": &#123; "buckets": null, "mode": "time", "name": null, "show": true, "values": [] &#125;, "yaxes": [ &#123; "format": "short", "label": null, "logBase": 1, "max": null, "min": null, "show": true &#125;, &#123; "format": "short", "label": null, "logBase": 1, "max": null, "min": null, "show": true &#125; ], "yaxis": &#123; "align": false, "alignLevel": null &#125; &#125;, &#123; "aliasColors": &#123;&#125;, "bars": false, "dashLength": 10, "dashes": false, "datasource": "prometheus", "description": "各个节点内存碎片率", "fieldConfig": &#123; "defaults": &#123; "custom": &#123;&#125;, "links": [] &#125;, "overrides": [] &#125;, "fill": 1, "fillGradient": 0, "gridPos": &#123; "h": 8, "w": 12, "x": 12, "y": 35 &#125;, "hiddenSeries": false, "id": 28, "legend": &#123; "avg": false, "current": false, "max": false, "min": false, "show": true, "total": false, "values": false &#125;, "lines": true, "linewidth": 1, "nullPointMode": "null", "percentage": false, "pluginVersion": "7.1.0", "pointradius": 2, "points": false, "renderer": "flot", "seriesOverrides": [], "spaceLength": 10, "stack": false, "steppedLine": false, "targets": [ &#123; "expr": "redis_mem_fragmentation_ratio&#123;instance=~\"$instance\"&#125;", "interval": "", "legendFormat": "&#123;&#123;instance&#125;&#125;", "refId": "A" &#125; ], "thresholds": [], "timeFrom": null, "timeRegions": [], "timeShift": null, "title": "内存碎片率", "tooltip": &#123; "shared": true, "sort": 0, "value_type": "individual" &#125;, "type": "graph", "xaxis": &#123; "buckets": null, "mode": "time", "name": null, "show": true, "values": [] &#125;, "yaxes": [ &#123; "format": "short", "label": null, "logBase": 1, "max": null, "min": null, "show": true &#125;, &#123; "format": "short", "label": null, "logBase": 1, "max": null, "min": null, "show": true &#125; ], "yaxis": &#123; "align": false, "alignLevel": null &#125; &#125;, &#123; "aliasColors": &#123;&#125;, "bars": false, "dashLength": 10, "dashes": false, "datasource": "prometheus", "description": "各个节点 的负载", "fieldConfig": &#123; "defaults": &#123; "custom": &#123;&#125;, "links": [] &#125;, "overrides": [] &#125;, "fill": 1, "fillGradient": 0, "gridPos": &#123; "h": 8, "w": 12, "x": 0, "y": 43 &#125;, "hiddenSeries": false, "id": 34, "legend": &#123; "avg": false, "current": false, "max": false, "min": false, "show": true, "total": false, "values": false &#125;, "lines": true, "linewidth": 1, "nullPointMode": "null", "percentage": false, "pluginVersion": "7.1.0", "pointradius": 2, "points": false, "renderer": "flot", "seriesOverrides": [], "spaceLength": 10, "stack": false, "steppedLine": false, "targets": [ &#123; "expr": "node_load1&#123;hostname=~\"$host\"&#125;", "interval": "", "legendFormat": "&#123;&#123;hostname&#125;&#125;", "refId": "A" &#125; ], "thresholds": [], "timeFrom": null, "timeRegions": [], "timeShift": null, "title": "负载（Load）", "tooltip": &#123; "shared": true, "sort": 0, "value_type": "individual" &#125;, "type": "graph", "xaxis": &#123; "buckets": null, "mode": "time", "name": null, "show": true, "values": [] &#125;, "yaxes": [ &#123; "format": "short", "label": null, "logBase": 1, "max": null, "min": null, "show": true &#125;, &#123; "format": "short", "label": null, "logBase": 1, "max": null, "min": null, "show": true &#125; ], "yaxis": &#123; "align": false, "alignLevel": null &#125; &#125; ], "refresh": "5s", "schemaVersion": 26, "style": "dark", "tags": [], "templating": &#123; "list": [ &#123; "auto": false, "auto_count": 100, "auto_min": "1s", "current": &#123; "selected": false, "text": "1m", "value": "1m" &#125;, "hide": 0, "label": "interval", "name": "interval", "options": [ &#123; "selected": true, "text": "1m", "value": "1m" &#125;, &#123; "selected": false, "text": "10m", "value": "10m" &#125;, &#123; "selected": false, "text": "30m", "value": "30m" &#125;, &#123; "selected": false, "text": "1h", "value": "1h" &#125;, &#123; "selected": false, "text": "6h", "value": "6h" &#125;, &#123; "selected": false, "text": "12h", "value": "12h" &#125;, &#123; "selected": false, "text": "1d", "value": "1d" &#125; ], "query": "1m,10m,30m,1h,6h,12h,1d", "queryValue": "", "refresh": 2, "skipUrlSync": false, "type": "interval" &#125;, &#123; "allValue": null, "current": &#123; "selected": true, "text": "All", "value": [ "$__all" ] &#125;, "datasource": "prometheus", "definition": "label_values(redis_up,cluster)", "hide": 0, "includeAll": true, "label": "Cluster", "multi": true, "name": "cluster", "options": [], "query": "label_values(redis_up,cluster)", "refresh": 1, "regex": "", "skipUrlSync": false, "sort": 0, "tagValuesQuery": "", "tags": [], "tagsQuery": "", "type": "query", "useTags": false &#125;, &#123; "allValue": null, "current": &#123; "selected": true, "text": "All", "value": [ "$__all" ] &#125;, "datasource": "prometheus", "definition": "label_values(redis_instance_info,role)", "hide": 0, "includeAll": true, "label": "Role", "multi": true, "name": "role", "options": [], "query": "label_values(redis_instance_info,role)", "refresh": 1, "regex": "", "skipUrlSync": false, "sort": 0, "tagValuesQuery": "", "tags": [], "tagsQuery": "", "type": "query", "useTags": false &#125;, &#123; "allValue": null, "current": &#123; "selected": true, "text": "All", "value": [ "$__all" ] &#125;, "datasource": "prometheus", "definition": "label_values(redis_instance_info&#123;cluster=~\"$cluster\",role=~\"$role\"&#125;,instance)", "hide": 2, "includeAll": true, "label": "instance", "multi": true, "name": "instance", "options": [], "query": "label_values(redis_instance_info&#123;cluster=~\"$cluster\",role=~\"$role\"&#125;,instance)", "refresh": 1, "regex": "", "skipUrlSync": false, "sort": 0, "tagValuesQuery": "", "tags": [], "tagsQuery": "", "type": "query", "useTags": false &#125;, &#123; "allValue": null, "current": &#123; "selected": true, "text": "All", "value": [ "$__all" ] &#125;, "datasource": "prometheus", "definition": "label_values(redis_instance_info&#123;cluster=~\"$cluster\",role=\"master\"&#125;,instance)", "hide": 2, "includeAll": true, "label": "master", "multi": true, "name": "master", "options": [], "query": "label_values(redis_instance_info&#123;cluster=~\"$cluster\",role=\"master\"&#125;,instance)", "refresh": 1, "regex": "", "skipUrlSync": false, "sort": 0, "tagValuesQuery": "", "tags": [], "tagsQuery": "", "type": "query", "useTags": false &#125;, &#123; "allValue": null, "current": &#123; "selected": true, "text": "All", "value": [ "$__all" ] &#125;, "datasource": "prometheus", "definition": "label_values(redis_up&#123;cluster=~\"$cluster\"&#125;, hostip)", "hide": 2, "includeAll": true, "label": "host", "multi": true, "name": "host", "options": [], "query": "label_values(redis_up&#123;cluster=~\"$cluster\"&#125;, hostip)", "refresh": 1, "regex": "", "skipUrlSync": false, "sort": 0, "tagValuesQuery": "", "tags": [], "tagsQuery": "", "type": "query", "useTags": false &#125; ] &#125;, "time": &#123; "from": "now-5m", "to": "now" &#125;, "timepicker": &#123; "refresh_intervals": [ "1m", "5m", "15m", "30m", "1h", "2h", "1d" ] &#125;, "timezone": "", "title": "Redis_Cluster监控", "uid": "5FfBHG3Zzddsds", "version": 1&#125;]]></content>
      <categories>
        <category>技术</category>
        <category>K8S</category>
      </categories>
      <tags>
        <tag>Redis集群</tag>
        <tag>Redis数据迁移</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[购买海外VPS部署v2ray服务]]></title>
    <url>%2F2025%2F051a0de5eb.html</url>
    <content type="text"><![CDATA[公司之前翻墙一直是购买的梯子，由于泄密原因，决定自己购买海外VPS来部署v2ray服务，在此记录下整个过程。服务提供商网上用的比较多的是Vultr和搬瓦工Vultr稍微优惠点，而且如果ip被墙了，可以直接销毁vps，重新创建，就可以获得新的ip，相当于免费换ip，搬瓦工换ip是收费的，折算成人民币大概是五十多块。所以在此使用的是Vultr的服务，直接注册账号，充值之后选择对应配置，开通就行（这里有个坑，如果使用支付宝支付的话，国家选择中国，网络不可以使用代理，不然会报错，支付不了）流量伪装介绍直接使用tcp配置就相当于裸奔，估计没一会就得被墙，本文介绍的V2ray伪装便是将穿墙流量以常见的HTTPS/TLS包装，大大降低vps被墙或被干扰的可能性，在敏感时期提供稳如狗的上外网体验。关于伪装技术的选择，V2ray web+websocket+tls 和 V2ray web+http2+tls 常用来做对比。理论上http2省去了upgrade的请求，性能更好。但实际使用中两者没有明显区别，加之某些web服务器（例如Nginx）不支持后端服务器为http2，所以websocket的方式更流行。如果你要上http2，记得web服务器不能用Nginx，要用支持反代http2的Caddy等软件。理论上来说，证书不是必须的。但没有tls加持或不做加密，防火墙直接能看出来流量真实意图从而进行干扰，这也是为什么不建议伪装http流量的原因。本文给出的方法采用合法机构签发的证书对流量进行加密，不是做特征混淆得到的TLS流量，从而更难被检测和干扰。下文介绍流量伪装的配置步骤，演示域名为tlanyan.pp.ua，服务器为Linux(CentOS)，web服务器软件用Caddy，web+websocket+tls组合，最终效果为：http/https方式打开域名，显示正常的网页；V2Ray客户端请求特定的路径，例如https://itlanyan.com/awesomepath，能科学上网；浏览器直接请求https://itlanyan.com/awesomepath，返回”400 bad request”。即外部看起来完全是一个人畜无害的正规网站，特定手段请求特定网址才是科学上网的通道。服务部署准备域名一个域名，无备案要求，可以在DigitalPlat平台注册一个免费的二级域名，可以解析，但是不能转让，可以免费续期。cloudflare在DigitalPlat注册的免费域名不可以直接在上面添加解析，需要托管在cloudflare平台。在cloudflare添加一个域，选择免费的计划即可，会生成两个ns记录，添加到DigitalPlat平台申请的免费域名里即可。生效后状态为活动：V2ray服务部署登录vps服务器，初始化环境：12345systemctl stop firewalldsystemctl disable firewalldsetenforce 0 #临时修改/etc/selinux/config文件SELINUX=disabled ##永久配置安全组安全起见，登录Vultr平台添加安全组，只放开22,80,443端口。安装v2ray服务：1bash &lt;(wget -qO- -o- https://git.io/v2ray.sh)或者自己创建一个v2ray.sh脚本：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438#!/bin/bashauthor=233boy# github=https://github.com/233boy/v2ray# bash fonts colorsred='\e[31m'yellow='\e[33m'gray='\e[90m'green='\e[92m'blue='\e[94m'magenta='\e[95m'cyan='\e[96m'none='\e[0m'_red() &#123; echo -e $&#123;red&#125;$@$&#123;none&#125;; &#125;_blue() &#123; echo -e $&#123;blue&#125;$@$&#123;none&#125;; &#125;_cyan() &#123; echo -e $&#123;cyan&#125;$@$&#123;none&#125;; &#125;_green() &#123; echo -e $&#123;green&#125;$@$&#123;none&#125;; &#125;_yellow() &#123; echo -e $&#123;yellow&#125;$@$&#123;none&#125;; &#125;_magenta() &#123; echo -e $&#123;magenta&#125;$@$&#123;none&#125;; &#125;_red_bg() &#123; echo -e "\e[41m$@$&#123;none&#125;"; &#125;is_err=$(_red_bg 错误!)is_warn=$(_red_bg 警告!)err() &#123; echo -e "\n$is_err $@\n" &amp;&amp; exit 1&#125;warn() &#123; echo -e "\n$is_warn $@\n"&#125;# root[[ $EUID != 0 ]] &amp;&amp; err "当前非 $&#123;yellow&#125;ROOT用户.$&#123;none&#125;"# yum or apt-get, ubuntu/debian/centoscmd=$(type -P apt-get || type -P yum)[[ ! $cmd ]] &amp;&amp; err "此脚本仅支持 $&#123;yellow&#125;(Ubuntu or Debian or CentOS)$&#123;none&#125;."# systemd[[ ! $(type -P systemctl) ]] &amp;&amp; &#123; err "此系统缺少 $&#123;yellow&#125;(systemctl)$&#123;none&#125;, 请尝试执行:$&#123;yellow&#125; $&#123;cmd&#125; update -y;$&#123;cmd&#125; install systemd -y $&#123;none&#125;来修复此错误."&#125;# wget installed or noneis_wget=$(type -P wget)# x64case $(uname -m) inamd64 | x86_64) is_jq_arch=amd64 is_core_arch="64" ;;*aarch64* | *armv8*) is_jq_arch=arm64 is_core_arch="arm64-v8a" ;;*) err "此脚本仅支持 64 位系统..." ;;esacis_core=v2rayis_core_name=V2Rayis_core_dir=/etc/$is_coreis_core_bin=$is_core_dir/bin/$is_coreis_core_repo=v2fly/$is_core-coreis_conf_dir=$is_core_dir/confis_log_dir=/var/log/$is_coreis_sh_bin=/usr/local/bin/$is_coreis_sh_dir=$is_core_dir/shis_sh_repo=$author/$is_coreis_pkg="wget unzip"is_config_json=$is_core_dir/config.jsontmp_var_lists=( tmpcore tmpsh tmpjq is_core_ok is_sh_ok is_jq_ok is_pkg_ok)# tmp dirtmpdir=$(mktemp -u)[[ ! $tmpdir ]] &amp;&amp; &#123; tmpdir=/tmp/tmp-$RANDOM&#125;# set up varfor i in $&#123;tmp_var_lists[*]&#125;; do export $i=$tmpdir/$idone# load bash script.load() &#123; . $is_sh_dir/src/$1&#125;# wget add --no-check-certificate_wget() &#123; [[ $proxy ]] &amp;&amp; export https_proxy=$proxy wget --no-check-certificate $*&#125;# print a mesagemsg() &#123; case $1 in warn) local color=$yellow ;; err) local color=$red ;; ok) local color=$green ;; esac echo -e "$&#123;color&#125;$(date +'%T')$&#123;none&#125;) $&#123;2&#125;"&#125;# show help msgshow_help() &#123; echo -e "Usage: $0 [-f xxx | -l | -p xxx | -v xxx | -h]" echo -e " -f, --core-file &lt;path&gt; 自定义 $is_core_name 文件路径, e.g., -f /root/$&#123;is_core&#125;-linux-64.zip" echo -e " -l, --local-install 本地获取安装脚本, 使用当前目录" echo -e " -p, --proxy &lt;addr&gt; 使用代理下载, e.g., -p http://127.0.0.1:2333" echo -e " -v, --core-version &lt;ver&gt; 自定义 $is_core_name 版本, e.g., -v v5.4.1" echo -e " -h, --help 显示此帮助界面\n" exit 0&#125;# install dependent pkginstall_pkg() &#123; cmd_not_found= for i in $*; do [[ ! $(type -P $i) ]] &amp;&amp; cmd_not_found="$cmd_not_found,$i" done if [[ $cmd_not_found ]]; then pkg=$(echo $cmd_not_found | sed 's/,/ /g') msg warn "安装依赖包 &gt;$&#123;pkg&#125;" $cmd install -y $pkg &amp;&gt;/dev/null if [[ $? != 0 ]]; then [[ $cmd =~ yum ]] &amp;&amp; yum install epel-release -y &amp;&gt;/dev/null $cmd update -y &amp;&gt;/dev/null $cmd install -y $pkg &amp;&gt;/dev/null [[ $? == 0 ]] &amp;&amp; &gt;$is_pkg_ok else &gt;$is_pkg_ok fi else &gt;$is_pkg_ok fi&#125;# download filedownload() &#123; case $1 in core) link=https://github.com/$&#123;is_core_repo&#125;/releases/latest/download/$&#123;is_core&#125;-linux-$&#123;is_core_arch&#125;.zip [[ $is_core_ver ]] &amp;&amp; link="https://github.com/$&#123;is_core_repo&#125;/releases/download/$&#123;is_core_ver&#125;/$&#123;is_core&#125;-linux-$&#123;is_core_arch&#125;.zip" name=$is_core_name tmpfile=$tmpcore is_ok=$is_core_ok ;; sh) link=https://github.com/$&#123;is_sh_repo&#125;/releases/latest/download/code.zip name="$is_core_name 脚本" tmpfile=$tmpsh is_ok=$is_sh_ok ;; jq) link=https://github.com/jqlang/jq/releases/download/jq-1.7.1/jq-linux-$is_jq_arch name="jq" tmpfile=$tmpjq is_ok=$is_jq_ok ;; esac msg warn "下载 $&#123;name&#125; &gt; $&#123;link&#125;" if _wget -t 3 -q -c $link -O $tmpfile; then mv -f $tmpfile $is_ok fi&#125;# get server ipget_ip() &#123; export "$(_wget -4 -qO- https://one.one.one.one/cdn-cgi/trace | grep ip=)" &amp;&gt;/dev/null [[ -z $ip ]] &amp;&amp; export "$(_wget -6 -qO- https://one.one.one.one/cdn-cgi/trace | grep ip=)" &amp;&gt;/dev/null&#125;# check background tasks statuscheck_status() &#123; # dependent pkg install fail [[ ! -f $is_pkg_ok ]] &amp;&amp; &#123; msg err "安装依赖包失败" msg err "请尝试手动安装依赖包: $cmd update -y; $cmd install -y $pkg" is_fail=1 &#125; # download file status if [[ $is_wget ]]; then [[ ! -f $is_core_ok ]] &amp;&amp; &#123; msg err "下载 $&#123;is_core_name&#125; 失败" is_fail=1 &#125; [[ ! -f $is_sh_ok ]] &amp;&amp; &#123; msg err "下载 $&#123;is_core_name&#125; 脚本失败" is_fail=1 &#125; [[ ! -f $is_jq_ok ]] &amp;&amp; &#123; msg err "下载 jq 失败" is_fail=1 &#125; else [[ ! $is_fail ]] &amp;&amp; &#123; is_wget=1 [[ ! $is_core_file ]] &amp;&amp; download core &amp; [[ ! $local_install ]] &amp;&amp; download sh &amp; [[ $jq_not_found ]] &amp;&amp; download jq &amp; get_ip wait check_status &#125; fi # found fail status, remove tmp dir and exit. [[ $is_fail ]] &amp;&amp; &#123; exit_and_del_tmpdir &#125;&#125;# parameters checkpass_args() &#123; while [[ $# -gt 0 ]]; do case $1 in online) err "如果想要安装旧版本, 请转到: https://github.com/233boy/v2ray/tree/old" ;; -f | --core-file) [[ -z $2 ]] &amp;&amp; &#123; err "($1) 缺少必需参数, 正确使用示例: [$1 /root/$is_core-linux-64.zip]" &#125; || [[ ! -f $2 ]] &amp;&amp; &#123; err "($2) 不是一个常规的文件." &#125; is_core_file=$2 shift 2 ;; -l | --local-install) [[ ! -f $&#123;PWD&#125;/src/core.sh || ! -f $&#123;PWD&#125;/$is_core.sh ]] &amp;&amp; &#123; err "当前目录 ($&#123;PWD&#125;) 非完整的脚本目录." &#125; local_install=1 shift 1 ;; -p | --proxy) [[ -z $2 ]] &amp;&amp; &#123; err "($1) 缺少必需参数, 正确使用示例: [$1 http://127.0.0.1:2333 or -p socks5://127.0.0.1:2333]" &#125; proxy=$2 shift 2 ;; -v | --core-version) [[ -z $2 ]] &amp;&amp; &#123; err "($1) 缺少必需参数, 正确使用示例: [$1 v1.8.1]" &#125; is_core_ver=v$&#123;2#v&#125; shift 2 ;; -h | --help) show_help ;; *) echo -e "\n$&#123;is_err&#125; ($@) 为未知参数...\n" show_help ;; esac done [[ $is_core_ver &amp;&amp; $is_core_file ]] &amp;&amp; &#123; err "无法同时自定义 $&#123;is_core_name&#125; 版本和 $&#123;is_core_name&#125; 文件." &#125;&#125;# exit and remove tmpdirexit_and_del_tmpdir() &#123; rm -rf $tmpdir [[ ! $1 ]] &amp;&amp; &#123; msg err "哦豁.." msg err "安装过程出现错误..." echo -e "反馈问题) https://github.com/$&#123;is_sh_repo&#125;/issues" echo exit 1 &#125; exit&#125;# mainmain() &#123; # check old version [[ -f $is_sh_bin &amp;&amp; -d $is_core_dir/bin &amp;&amp; -d $is_sh_dir &amp;&amp; -d $is_conf_dir ]] &amp;&amp; &#123; err "检测到脚本已安装, 如需重装请使用$&#123;green&#125; $&#123;is_core&#125; reinstall $&#123;none&#125;命令." &#125; # check parameters [[ $# -gt 0 ]] &amp;&amp; pass_args $@ # show welcome msg clear echo echo "........... $is_core_name script by $author .........." echo # start installing... msg warn "开始安装..." [[ $is_core_ver ]] &amp;&amp; msg warn "$&#123;is_core_name&#125; 版本: $&#123;yellow&#125;$is_core_ver$&#123;none&#125;" [[ $proxy ]] &amp;&amp; msg warn "使用代理: $&#123;yellow&#125;$proxy$&#123;none&#125;" # create tmpdir mkdir -p $tmpdir # if is_core_file, copy file [[ $is_core_file ]] &amp;&amp; &#123; cp -f $is_core_file $is_core_ok msg warn "$&#123;yellow&#125;$&#123;is_core_name&#125; 文件使用 &gt; $is_core_file$&#123;none&#125;" &#125; # local dir install sh script [[ $local_install ]] &amp;&amp; &#123; &gt;$is_sh_ok msg warn "$&#123;yellow&#125;本地获取安装脚本 &gt; $PWD $&#123;none&#125;" &#125; timedatectl set-ntp true &amp;&gt;/dev/null [[ $? != 0 ]] &amp;&amp; &#123; msg warn "$&#123;yellow&#125;\e[4m提醒!!! 无法设置自动同步时间, 可能会影响使用 VMess 协议.$&#123;none&#125;" &#125; # install dependent pkg install_pkg $is_pkg &amp; # jq if [[ $(type -P jq) ]]; then &gt;$is_jq_ok else jq_not_found=1 fi # if wget installed. download core, sh, jq, get ip [[ $is_wget ]] &amp;&amp; &#123; [[ ! $is_core_file ]] &amp;&amp; download core &amp; [[ ! $local_install ]] &amp;&amp; download sh &amp; [[ $jq_not_found ]] &amp;&amp; download jq &amp; get_ip &#125; # waiting for background tasks is done wait # check background tasks status check_status # test $is_core_file if [[ $is_core_file ]]; then unzip -qo $is_core_ok -d $tmpdir/testzip &amp;&gt;/dev/null [[ $? != 0 ]] &amp;&amp; &#123; msg err "$&#123;is_core_name&#125; 文件无法通过测试." exit_and_del_tmpdir &#125; for i in $&#123;is_core&#125; geoip.dat geosite.dat; do [[ ! -f $tmpdir/testzip/$i ]] &amp;&amp; is_file_err=1 &amp;&amp; break done [[ $is_file_err ]] &amp;&amp; &#123; msg err "$&#123;is_core_name&#125; 文件无法通过测试." exit_and_del_tmpdir &#125; fi # get server ip. [[ ! $ip ]] &amp;&amp; &#123; msg err "获取服务器 IP 失败." exit_and_del_tmpdir &#125; # create sh dir... mkdir -p $is_sh_dir # copy sh file or unzip sh zip file. if [[ $local_install ]]; then cp -rf $PWD/* $is_sh_dir else unzip -qo $is_sh_ok -d $is_sh_dir fi # create core bin dir mkdir -p $is_core_dir/bin # copy core file or unzip core zip file if [[ $is_core_file ]]; then cp -rf $tmpdir/testzip/* $is_core_dir/bin else unzip -qo $is_core_ok -d $is_core_dir/bin fi # add alias echo "alias $is_core=$is_sh_bin" &gt;&gt;/root/.bashrc # core command ln -sf $is_sh_dir/$is_core.sh $is_sh_bin # jq [[ $jq_not_found ]] &amp;&amp; mv -f $is_jq_ok /usr/bin/jq # chmod chmod +x $is_core_bin $is_sh_bin /usr/bin/jq # create log dir mkdir -p $is_log_dir # show a tips msg msg ok "生成配置文件..." # create systemd service load systemd.sh is_new_install=1 install_service $is_core &amp;&gt;/dev/null # create condf dir mkdir -p $is_conf_dir load core.sh # create a tcp config add tcp # remove tmp dir and exit. exit_and_del_tmpdir ok&#125;# start.main $@安装完成执行了上面的安装命令，并且没有错误提示的话，就能看到类似下面的图片此时可以复制 URL 到相关软件 (例如 v2rayN) 去测试一下是否正常使用，记得安全组要放开对应端口，但是这个是没有伪装的，很容易被墙，所以我们不用这个，继续下面的步骤。打开 BBR 优化1v2ray bbr开启流量伪装1v2ray add ws复制链接到客户端测试一下是否正常使用至此，v2ray服务就部署完成了。流量伪装+CDN即使使用了流量伪装，还是可能会被封ip，可以利用 CDN 中转，cloudflare就有免费的，在cloudflare配置域名解析的时候打开代理（小云朵）即可。这时流量传递的顺序是这样的：主要实现就是两点：一、借助 V2Ray 代理，将我们的流量被伪装成网站流量二、利用 CDN 中转 V2Ray 的 WebSocket 流量这样，GFW 只知道你与 CDN 之间的联系，不知道 VPS 的实际地址，并且 CDN 会有很多 IP 地址，GFW 也不会随意封这些 IP，毕竟也有很多正规网站在使用。优点：不怕ip被封，因为CDN的ip是在海外的，即使我们的vps ip已经被封了也可以使用。缺点：网速会慢很多。测试ip端口是否被墙网站1：https://www.toolsdaquan.com/ipcheck/#google_vignette网站2：https://tcp.ping.pe/V2ray客户端下载地址：https://itlanyan.com/v2ray-clients-download/后续使用流量伪装方式也不是百分百万无一失，还是有可能被墙，如果ip被墙的话，我们就需要更换ip（重建vps），但是如果重建服务器就需要重新配置v2ray服务，那生成的配置也会变化，每次都得把生成的链接给同事们去替换的话就很麻烦，所以这里提供两种方式，无需客户端修改配置。方式一：v2ray服务部署好之后，登录vultr平台打个快照，如果原ip被墙，就使用此快照去创建一个新的服务器，新服务器的v2ray服务配置不会更改，只需要登录到cloudflare平台把域名解析记录修改为新的ip即可。快照是收费的，0.05美元/G/月。方式二：搞个个人订阅，一般情况下 ss, ssr, v2ray 之类的分享链接去掉前缀以后都是一个 base64 的字符串而且一般我们常见的订阅链接，里面的内容虽然一看都是乱码。但是它其实也是一条或多条分享链接再进行一次 base64 编码后的字符串，比如 ss, ssr, v2ray, clash, qv2ray。假设分享链接为:1vmess://eyJob3N0IjoiamlrZS1henNnMDIuZGRucy1vbmx5Lnh5eiIsInBhdGgiOiIvamlrZSIsInRscyI6IiIsInZlcmlmeV9jZXJ0Ijp0cnVlLCJhZGQiOiJqaWtlLWF6c2cwMi5kZG5zLW9ubHkueHl6IiwicG9ydCI6MzA1MDMsImFpZCI6MiwibmV0Ijoid3MiLCJoZWFkZXJUeXBlIjoibm9uZSIsInYiOiIyIiwidHlwZSI6InZtZXNzIiwicHMiOiJWNC3mlrDliqDlnaHlvq7ova/kupEwMXx2MnJheSIsInJlbWFyayI6IlY0LeaWsOWKoOWdoeW+rui9r+S6kTAxfHYycmF5IiwiaWQiOiJlZDYyMDE2ZS1lM2NhLTM0N2YtOGY0Ni1iYzczMzRmYjliYmYiLCJjbGFzcyI6MX0=把分享链接去掉前缀保存到一个文件中:通过使用 base64 -d 对原本的链接进行解密，我们可以获得如下信息可以看到解码后就是v2ray的节点配置信息。所以我们可以把分享链接保存到一个文件file1里，对文件进行加密：1base64 -w 0 file1然后把加密后的内容保存到另一个文件file2中并把file2放到一个web服务器上作为订阅链接，在客户端配置这个订阅链接就可以拉取节点配置信息。如果ip被墙了，在新的服务器上配置了新的v2ray的话，会有新的配置信息：把新的分享链接进行加密，然后把加密后的内容替换web服务器的file2的内容，然后在客户端更新订阅就可以更新为新的节点配置。也可以在在线网站OSCHINA对内容进行加解密。]]></content>
      <categories>
        <category>技术</category>
        <category>V2ray</category>
      </categories>
      <tags>
        <tag>V2ray</tag>
        <tag>Vultr</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka相关操作]]></title>
    <url>%2F2025%2F0586a17973.html</url>
    <content type="text"><![CDATA[记录下kafka相关的命令。查看topic1/opt/kafka/bin/kafka-topics.sh --list --bootstrap-server kafka_ip:kafka_port创建topic1/opt/kafka/bin/kafka-topics.sh --create --bootstrap-server kafka_ip:kafka_port --replication-factor 2 --partitions 1 --topic topic_name删除topic1/opt/kafka/bin/kafka-topics.sh --delete --bootstrap-server kafka_ip:kafka_port --topic topic_name查看topic 详细信息1/opt/kafka/bin/kafka-topics.sh --bootstrap-server kafka_ip:kafka_port --describe --topic topic_name修改topic副本数修改前一个副本：编写修改副本计划12345&#123;"version":1,"partitions":[&#123;"topic":"uat.qifu_saas_owms.tables","partition":0,"replicas":[0,1]&#125;]&#125;注：如果topic有多个分区，每个分区都要写上，replicas后面接的是broker_id，想加几个副本就在后面写上对应的id，删除副本就把对应的id去掉，如：1234567891011&#123;"version":1,"partitions":[&#123;"topic":"test","partition":0,"replicas":[0,1]&#125;,&#123;"topic":"test","partition":1,"replicas":[0,1]&#125;,&#123;"topic":"test","partition":2,"replicas":[0,1]&#125;,&#123;"topic":"test","partition":3,"replicas":[0,1]&#125;,&#123;"topic":"test","partition":4,"replicas":[0,1]&#125;,&#123;"topic":"test","partition":5,"replicas":[0,1]&#125;,&#123;"topic":"test","partition":6,"replicas":[0,1]&#125;]&#125;执行修改：1/opt/kafka/bin/kafka-reassign-partitions.sh --bootstrap-server kafka_ip:kafka_port --reassignment-json-file replication.json --execute修改后副本数：修改分区数据（只能增加）1/opt/kafka/bin/kafka-topics.sh --bootstrap-server kafka_ip:kafka_port --alter --topic test --partitions 4创建生产者1/opt/kafka/bin/kafka-console-producer.sh --broker-list kafka_ip:kafka_port --topic test创建消费者1/opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server kafka_ip:kafka_port --topic test --from-beginning创建消费者组1/opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server kafka_ip:kafka_port --topic test --consumer.config /opt/kafka/config/consumer.properties列出消费者群组1/opt/kafka/bin/kafka-consumer-groups.sh --bootstrap-server kafka_ip:kafka_port --list查看消息积压1/opt/kafka/bin/kafka-consumer-groups.sh --bootstrap-server kafka_ip:kafka_port --group owms-consumer3 --describe]]></content>
      <categories>
        <category>技术</category>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>消息队列</tag>
        <tag>kafka</tag>
        <tag>kafka命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Grafana-oncall企微告警问题]]></title>
    <url>%2F2025%2F04ab4e863e.html</url>
    <content type="text"><![CDATA[oncall平台部署完之后，我们一般会接入各个平台的告警信息，比如我接入了Skywalking，Graylog和Prometheus的告警。告警@对应负责人员如果通过邮件告警的话，没啥问题，但是如果想通过企业微信机器人告警的话，因为不同平台告警的消息格式不一样，这里我通过PrometheusAlert平台来做不同来源的告警模板：对应的平台配置对应的模板就行，但是如果我想通过企业微信告警并且@对应负责人：有两种办法：方法一@放到机器人URL后面每个用户创建一个webhook，在机器人URL后面加上&amp;at=chenmingchang然后在用户信息里选择对应的webhook：方法二@放到消息体里，只需创建一个个人webhook，然后全部用户都引用这个webhook把@放到消息体里，修改prometheusalert对应模板：方法一如果有n个人同时值班，会发送n条告警信息，每条@一个人；方法二如果有n个人同时值班，会发送n条告警信息，每条@n个人；多种告警来源配置企微告警Grafana-oncall平台每个用户只能配置一个webhook，Skywalking，Graylog和Prometheus由于告警消息格式不一致，无法一个webhook告警同时告多种格式。解决：通过中转webhook把告警消息格式化为同一种或者把多个告警模板放到合并到一个模板里，这里选择把三个告警模板合并到一起：]]></content>
      <categories>
        <category>技术</category>
        <category>Oncall</category>
      </categories>
      <tags>
        <tag>grafana</tag>
        <tag>oncall</tag>
        <tag>企微告警</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[oncall平台部署及使用]]></title>
    <url>%2F2025%2F044e4419ae.html</url>
    <content type="text"><![CDATA[Grafana OnCall 是 Grafana Labs 推出的一款开源事件响应与排班调度工具，可以帮助团队管理和跟踪故障处理情况，提高 SRE 团队的工作效率，更快地解决事件。可以自动路由警报到指定的值班团队和 ChatOps 频道，根据预定义的升级策略、时间表和通知偏好进行处理。Oncall平台一般都要钱购买的，开源的不多，Granfana-Oncall是开源的，但是网上的资料比较少，官网写的也不是很详细，自己摸索了好几天，遇到不少坑。部署通过docker-compose部署docker-compose.yaml:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124x-environment: &amp;oncall-environment DATABASE_TYPE: sqlite3 BROKER_TYPE: redis BASE_URL: $DOMAIN SECRET_KEY: $SECRET_KEY FEATURE_PROMETHEUS_EXPORTER_ENABLED: $&#123;FEATURE_PROMETHEUS_EXPORTER_ENABLED:-false&#125; PROMETHEUS_EXPORTER_SECRET: $&#123;PROMETHEUS_EXPORTER_SECRET:-&#125; REDIS_URI: redis://redis:6379/0 DJANGO_SETTINGS_MODULE: settings.hobby CELERY_WORKER_QUEUE: "default,critical,long,slack,telegram,webhook,retry,celery,grafana" CELERY_WORKER_CONCURRENCY: "1" CELERY_WORKER_MAX_TASKS_PER_CHILD: "100" CELERY_WORKER_SHUTDOWN_INTERVAL: "65m" CELERY_WORKER_BEAT_ENABLED: "True" GRAFANA_API_URL: http://grafana:3000 TZ: Asia/Shanghaiservices: engine: image: docker-0.unsee.tech/grafana/oncall restart: always ports: - "8080:8080" command: sh -c "uwsgi --ini uwsgi.ini" environment: *oncall-environment volumes: - oncall_data:/var/lib/oncall depends_on: oncall_db_migration: condition: service_completed_successfully redis: condition: service_healthy celery: image: docker-0.unsee.tech/grafana/oncall restart: always command: sh -c "./celery_with_exporter.sh" environment: *oncall-environment volumes: - oncall_data:/var/lib/oncall depends_on: oncall_db_migration: condition: service_completed_successfully redis: condition: service_healthy oncall_db_migration: image: docker-0.unsee.tech/grafana/oncall command: python manage.py migrate --noinput environment: *oncall-environment volumes: - oncall_data:/var/lib/oncall depends_on: redis: condition: service_healthy redis: image: docker-0.unsee.tech/redis:5.0 restart: always expose: - 6379 volumes: - redis_data:/data deploy: resources: limits: memory: 500m cpus: "0.5" healthcheck: test: ["CMD", "redis-cli", "ping"] timeout: 5s interval: 5s retries: 10# prometheus:# image: prom/prometheus# hostname: prometheus# restart: always# ports:# - "9090:9090"# volumes:# - ./prometheus.yml:/etc/prometheus/prometheus.yml# - prometheus_data:/prometheus# profiles:# - with_prometheus grafana: image: "docker-0.unsee.tech/grafana/$&#123;GRAFANA_IMAGE:-grafana:latest&#125;" restart: always ports: - "3000:3000" environment: GF_FEATURE_TOGGLES_ENABLE: externalServiceAccounts GF_SECURITY_ADMIN_USER: $&#123;GRAFANA_USER:-admin&#125; GF_SECURITY_ADMIN_PASSWORD: $&#123;GRAFANA_PASSWORD:-admin&#125; GF_PLUGINS_ALLOW_LOADING_UNSIGNED_PLUGINS: grafana-oncall-app GF_INSTALL_PLUGINS: grafana-oncall-app GF_AUTH_MANAGED_SERVICE_ACCOUNTS_ENABLED: true TZ: Asia/Shanghai volumes: - grafana_data:/var/lib/grafana - ./grafana.ini:/etc/grafana/grafana.ini deploy: resources: limits: memory: 500m cpus: "0.5" profiles: - with_grafana# configs:# - source: grafana.ini# target: /etc/grafana/grafana.inivolumes: grafana_data: prometheus_data: oncall_data: redis_data:#configs:# grafana.ini:# content: |# [feature_toggles]# accessControlOnCall = false配置环境变量：.env文件：12345678910DOMAIN=http://10.168.2.236:8080# 如果您想使用现有的 grafana，请删除下面的“with_grafana”# 添加下面的“with_prometheus”以选择性地为 oncall 指标启用本地 prometheus# 例如 COMPOSE_PROFILES=with_grafana,with_prometheusCOMPOSE_PROFILES=with_grafana# 为 prometheus 导出器指标设置身份验证令牌：PROMETHEUS_EXPORTER_SECRET=my_random_prometheus_secret# 确保启用 /metrics 端点：FEATURE_PROMETHEUS_EXPORTER_ENABLED=TrueSECRET_KEY=my_random_secret_must_be_more_than_32_characters_longGrafana配置文件：grafana.ini文件：12345678910[feature_toggles]accessControlOnCall = false [smtp]enabled = truehost = smtp.exmail.qq.com:465user = chenmingchang@keyfil.compassword = *************from_address = chenmingchang@keyfil.comfrom_name = chenmingchang启动：1docker-compose pull &amp;&amp; docker-compose up -d启动完成后安装Grafana-oncall插件：12curl -X POST 'http://admin:admin@localhost:3000/api/plugins/grafana-oncall-app/settings' -H "Content-Type: application/json" -d '&#123;"enabled":true, "jsonData":&#123;"stackId":5, "orgId":100, "onCallApiUrl":"http://engine:8080", "grafanaUrl":"http://grafana:3000"&#125;&#125;'curl -X POST 'http://admin:admin@localhost:3000/api/plugins/grafana-oncall-app/resources/plugin/install'登录：10.168.2.236:3000，账号密码为admin/admin配置集成：skywalking的告警通过webhook的类型，Altermanager直接用Altermanager的类型就行：点击集成可以查看对应的Endpoint，配置到对应的skywalking和Altermanager配置文件里：skywalking：在alarm-settings.yml文件最后添加12webhooks: - http://10.168.2.236:8080/integrations/v1/webhook/0DIaZCp09dNem7g4P30PSR4KO/Altermanager：alertmanager.yaml文件1234567891011121314151617181920212223242526272829303132333435363738394041424344454647global: resolve_timeout: "5m"templates: - '/etc/alertmanager/config/*.tmpl'inhibit_rules:- equal: - "namespace" - "alertname" source_match: severity: "critical" target_match_re: severity: "warning|info"- equal: - "namespace" - "alertname" source_match: severity: "warning" target_match_re: severity: "info"receivers:- name: "default" webhook_configs: - send_resolved: true url: "http://10.168.2.236:3000"- name: "oncall" webhook_configs: - send_resolved: true url: "http://10.168.2.236:8080/integrations/v1/alertmanager/tDZlvyPGfD2Uk2HvtlX0mS8XR/"route: group_by: - "namespace" - "alertname" - "env" - "instance" - "type" - "group" - "job" - "cluster" - "app" group_interval: "10m" group_wait: "30s" receiver: "default" repeat_interval: "10m" routes: - match_re: severity: "warning|critical" receiver: "oncall"集成配置完之后配置升级链：触发告警之后执行的步骤：可以在集成详情里引用不同的升级链：配置Outgoing webhooks：发送到prometheusalert平台对应的模板，再发送到企微： 配置模板内容：Skywalking到grafana-oncall再到企微：配置排班表：配置发送邮箱：触发告警时会按照升级链的配置执行相应的步骤： Prometheus配置文件添加指标导出：123456- job_name: prometheus metrics_path: /metrics/ authorization: credentials: my_random_prometheus_secret static_configs: - targets: ["10.168.2.236:8080"] 通过k8s部署需要先安装好helm添加仓库：1helm repo add grafana https://grafana.github.io/helm-charts安装：123456helm install \ --wait \ --set base_url=grafana-oncall.keyfil.com \ --set grafana."grafana\.ini".server.domain=grafana-oncall.keyfil.com \ release-oncall \ grafana/oncall -n oncall -f values.yaml自定义values.yaml文件，可按需修改：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690691692693694695696697698699700701702703704705706707708709710711712713714715716717718719720721722723724725726727728729730731732733734735736737738739740# Values for configuring the deployment of Grafana OnCall # Set the domain name Grafana OnCall will be installed on.# If you want to install grafana as a part of this release make sure to configure grafana.grafana.ini.server.domain toobase_url: grafana-oncall.qifu.combase_url_protocol: http ## Optionally specify an array of imagePullSecrets.## Secrets must be manually created in the namespace.## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/## e.g:## imagePullSecrets:## - name: myRegistryKeySecretNameimagePullSecrets: [] image: # Grafana OnCall docker image repository repository: docker-0.unsee.tech/grafana/oncall tag: pullPolicy: IfNotPresent # Whether to create additional service for external connections# ClusterIP service is always createdservice: enabled: false type: LoadBalancer port: 8080 annotations: &#123;&#125; # Engine pods configurationengine: replicaCount: 1 resources: &#123;&#125; # limits: # cpu: 100m # memory: 128Mi # requests: # cpu: 100m # memory: 128Mi # Labels for engine pods podLabels: &#123;&#125; ## Deployment update strategy ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy updateStrategy: rollingUpdate: maxSurge: 25% maxUnavailable: 0 type: RollingUpdate ## Affinity for pod assignment ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity affinity: &#123;&#125; ## Node labels for pod assignment ## ref: https://kubernetes.io/docs/user-guide/node-selection/ nodeSelector: &#123;&#125; ## Tolerations for pod assignment ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/ tolerations: [] ## Topology spread constraints for pod assignment ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/ topologySpreadConstraints: [] ## Priority class for the pods ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/ priorityClassName: "" # Extra containers which runs as sidecar extraContainers: "" # extraContainers: | # - name: cloud-sql-proxy # image: gcr.io/cloud-sql-connectors/cloud-sql-proxy:2.1.2 # args: # - --private-ip # - --port=5432 # - example:europe-west3:grafana-oncall-db # Extra volume mounts for the main app container extraVolumeMounts: [] # - mountPath: /mnt/postgres-tls # name: postgres-tls # - mountPath: /mnt/redis-tls # name: redis-tls # Extra volumes for the pod extraVolumes: [] # - name: postgres-tls # configMap: # name: my-postgres-tls # defaultMode: 0640 # - name: redis-tls # configMap: # name: my-redis-tls # defaultMode: 0640 detached_integrations_service: enabled: false type: LoadBalancer port: 8080 annotations: &#123;&#125; # Integrations pods configurationdetached_integrations: enabled: false replicaCount: 1 resources: &#123;&#125; # limits: # cpu: 100m # memory: 128Mi # requests: # cpu: 100m # memory: 128Mi ## Deployment update strategy ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy updateStrategy: rollingUpdate: maxSurge: 25% maxUnavailable: 0 type: RollingUpdate ## Affinity for pod assignment ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity affinity: &#123;&#125; ## Node labels for pod assignment ## ref: https://kubernetes.io/docs/user-guide/node-selection/ nodeSelector: &#123;&#125; ## Tolerations for pod assignment ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/ tolerations: [] ## Topology spread constraints for pod assignment ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/ topologySpreadConstraints: [] ## Priority class for the pods ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/ priorityClassName: "" # Extra containers which runs as sidecar extraContainers: "" # extraContainers: | # - name: cloud-sql-proxy # image: gcr.io/cloud-sql-connectors/cloud-sql-proxy:2.1.2 # args: # - --private-ip # - --port=5432 # - example:europe-west3:grafana-oncall-db # Extra volume mounts for the container extraVolumeMounts: [] # - mountPath: /mnt/postgres-tls # name: postgres-tls # - mountPath: /mnt/redis-tls # name: redis-tls # Extra volumes for the pod extraVolumes: [] # - name: postgres-tls # configMap: # name: my-postgres-tls # defaultMode: 0640 # - name: redis-tls # configMap: # name: my-redis-tls # defaultMode: 0640 # Celery workers pods configurationcelery: replicaCount: 1 worker_queue: "default,critical,long,slack,telegram,webhook,celery,grafana,retry" worker_concurrency: "1" worker_max_tasks_per_child: "100" worker_beat_enabled: "True" ## Restart of the celery workers once in a given interval as an additional precaution to the probes ## If this setting is enabled TERM signal will be sent to celery workers ## It will lead to warm shutdown (waiting for the tasks to complete) and restart the container ## If this setting is set numbers of pod restarts will increase ## Comment this line out if you want to remove restarts worker_shutdown_interval: "65m" livenessProbe: enabled: true initialDelaySeconds: 30 periodSeconds: 300 timeoutSeconds: 10 resources: &#123;&#125; # limits: # cpu: 100m # memory: 128Mi # requests: # cpu: 100m # memory: 128Mi # Labels for celery pods podLabels: &#123;&#125; ## Affinity for pod assignment ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity affinity: &#123;&#125; ## Node labels for pod assignment ## ref: https://kubernetes.io/docs/user-guide/node-selection/ nodeSelector: &#123;&#125; ## Tolerations for pod assignment ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/ tolerations: [] ## Topology spread constraints for pod assignment ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/ topologySpreadConstraints: [] ## Priority class for the pods ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/ priorityClassName: "" # Extra containers which runs as sidecar extraContainers: "" # extraContainers: | # - name: cloud-sql-proxy # image: gcr.io/cloud-sql-connectors/cloud-sql-proxy:2.1.2 # args: # - --private-ip # - --port=5432 # - example:europe-west3:grafana-oncall-db # Extra volume mounts for the main container extraVolumeMounts: [] # - mountPath: /mnt/postgres-tls # name: postgres-tls # - mountPath: /mnt/redis-tls # name: redis-tls # Extra volumes for the pod extraVolumes: [] # - name: postgres-tls # configMap: # name: my-postgres-tls # defaultMode: 0640 # - name: redis-tls # configMap: # name: my-redis-tls # defaultMode: 0640 # Telegram polling pod configurationtelegramPolling: enabled: false resources: &#123;&#125; # limits: # cpu: 100m # memory: 128Mi # requests: # cpu: 100m # memory: 128Mi # Labels for telegram-polling pods podLabels: &#123;&#125; # Extra volume mounts for the main container extraVolumeMounts: [] # - mountPath: /mnt/postgres-tls # name: postgres-tls # - mountPath: /mnt/redis-tls # name: redis-tls # Extra volumes for the pod extraVolumes: [] # - name: postgres-tls # configMap: # name: my-postgres-tls # defaultMode: 0640 # - name: redis-tls # configMap: # name: my-redis-tls # defaultMode: 0640 oncall: # this is intended to be used for local development. In short, it will mount the ./engine dir into # any backend related containers, to allow hot-reloading + also run the containers with slightly modified # startup commands (which configures the hot-reloading) devMode: false # Override default MIRAGE_CIPHER_IV (must be 16 bytes long) # For existing installation, this should not be changed. # mirageCipherIV: 1234567890abcdef # oncall secrets secrets: # Use existing secret. (secretKey and mirageSecretKey is required) existingSecret: "" # The key in the secret containing secret key secretKey: "" # The key in the secret containing mirage secret key mirageSecretKey: "" # Slack configures the Grafana Oncall Slack ChatOps integration. slack: # Enable the Slack ChatOps integration for the Oncall Engine. enabled: false # clientId configures the Slack app OAuth2 client ID. # api.slack.com/apps/&lt;yourApp&gt; -&gt; Basic Information -&gt; App Credentials -&gt; Client ID clientId: ~ # clientSecret configures the Slack app OAuth2 client secret. # api.slack.com/apps/&lt;yourApp&gt; -&gt; Basic Information -&gt; App Credentials -&gt; Client Secret clientSecret: ~ # signingSecret - configures the Slack app signature secret used to sign # requests comming from Slack. # api.slack.com/apps/&lt;yourApp&gt; -&gt; Basic Information -&gt; App Credentials -&gt; Signing Secret signingSecret: ~ # Use existing secret for clientId, clientSecret and signingSecret. # clientIdKey, clientSecretKey and signingSecretKey are required existingSecret: "" # The key in the secret containing OAuth2 client ID clientIdKey: "" # The key in the secret containing OAuth2 client secret clientSecretKey: "" # The key in the secret containing the Slack app signature secret signingSecretKey: "" # OnCall external URL redirectHost: ~ telegram: enabled: false token: ~ webhookUrl: ~ # Use existing secret. (tokenKey is required) existingSecret: "" # The key in the secret containing Telegram token tokenKey: "" smtp: enabled: true host: ~ port: ~ username: ~ password: ~ tls: ~ ssl: ~ fromEmail: ~ exporter: enabled: false authToken: ~ twilio: # Twilio account SID/username to allow OnCall to send SMSes and make phone calls accountSid: "" # Twilio password to allow OnCall to send SMSes and make calls authToken: "" # Number from which you will receive calls and SMS # (NOTE: must be quoted, otherwise would be rendered as float value) phoneNumber: "" # SID of Twilio service for number verification. You can create a service in Twilio web interface. # twilio.com -&gt; verify -&gt; create new service verifySid: "" # Twilio API key SID/username to allow OnCall to send SMSes and make phone calls apiKeySid: "" # Twilio API key secret/password to allow OnCall to send SMSes and make phone calls apiKeySecret: "" # Use existing secret for authToken, phoneNumber, verifySid, apiKeySid and apiKeySecret. existingSecret: "" # Twilio password to allow OnCall to send SMSes and make calls # The key in the secret containing the auth token authTokenKey: "" # The key in the secret containing the phone number phoneNumberKey: "" # The key in the secret containing verify service sid verifySidKey: "" # The key in the secret containing api key sid apiKeySidKey: "" # The key in the secret containing the api key secret apiKeySecretKey: "" # Phone notifications limit (the only non-secret value). # TODO: rename to phoneNotificationLimit limitPhone: # Whether to run django database migrations automaticallymigrate: enabled: true # TTL can be unset by setting ttlSecondsAfterFinished: "" ttlSecondsAfterFinished: 20 # use a helm hook to manage the migration job useHook: false annotations: &#123;&#125; ## Affinity for pod assignment ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity affinity: &#123;&#125; ## Node labels for pod assignment ## ref: https://kubernetes.io/docs/user-guide/node-selection/ nodeSelector: &#123;&#125; ## Tolerations for pod assignment ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/ tolerations: [] # Extra containers which runs as sidecar extraContainers: "" # extraContainers: | # - name: cloud-sql-proxy # image: gcr.io/cloud-sql-connectors/cloud-sql-proxy:2.1.2 # args: # - --private-ip # - --port=5432 # - example:europe-west3:grafana-oncall-db resources: &#123;&#125; # limits: # cpu: 100m # memory: 128Mi # requests: # cpu: 100m # memory: 128Mi # Extra volume mounts for the main container extraVolumeMounts: [] # - mountPath: /mnt/postgres-tls # name: postgres-tls # - mountPath: /mnt/redis-tls # name: redis-tls # Extra volumes for the pod extraVolumes: [] # - name: postgres-tls # configMap: # name: my-postgres-tls # defaultMode: 0640 # - name: redis-tls # configMap: # name: my-redis-tls # defaultMode: 0640 # Sets environment variables with name capitalized and prefixed with UWSGI_,# and dashes are substituted with underscores.# see more: https://uwsgi-docs.readthedocs.io/en/latest/Configuration.html#environment-variables# Set null to disable all UWSGI environment variablesuwsgi: listen: 128 # Additional env variables to add to deploymentsenv: &#123;&#125; # Enable ingress object for external access to the resourcesingress: enabled: true # className: "" annotations: kubernetes.io/ingress.class: "nginx" # cert-manager.io/issuer: "letsencrypt-prod" tls: - hosts: - "&#123;&#123; .Values.base_url &#125;&#125;" secretName: certificate-tls # Extra paths to prepend to the host configuration. If using something # like an ALB ingress controller, you may want to configure SSL redirects extraPaths: [] # - path: /* # backend: # serviceName: ssl-redirect # servicePort: use-annotation ## Or for k8s &gt; 1.19 # - path: /* # pathType: Prefix # backend: # service: # name: ssl-redirect # port: # name: use-annotation # Whether to install ingress controlleringress-nginx: enabled: false # Install cert-manager as a part of the releasecert-manager: enabled: false # Instal CRD resources installCRDs: true webhook: timeoutSeconds: 30 # cert-manager tries to use the already used port, changing to another one # https://github.com/cert-manager/cert-manager/issues/3237 # https://cert-manager.io/docs/installation/compatibility/ securePort: 10260 # Fix self-checks https://github.com/jetstack/cert-manager/issues/4286 podDnsPolicy: None podDnsConfig: nameservers: - 8.8.8.8 - 1.1.1.1 database: # can be either mysql or postgresql type: mysql # MySQL is included into this release for the convenience.# It is recommended to host it separately from this release# Set mariadb.enabled = false and configure externalMysqlmariadb: enabled: true image: repository: docker-0.unsee.tech/bitnami/mariadb tag: 10.11.4-debian-11-r0 auth: database: oncall existingSecret: primary: extraEnvVars: - name: MARIADB_COLLATE value: utf8mb4_unicode_ci - name: MARIADB_CHARACTER_SET value: utf8mb4 secondary: extraEnvVars: - name: MARIADB_COLLATE value: utf8mb4_unicode_ci - name: MARIADB_CHARACTER_SET value: utf8mb4 # Make sure to create the database with the following parameters:# CREATE DATABASE oncall CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;externalMysql: host: port: db_name: user: password: # Use an existing secret for the mysql password. existingSecret: # The key in the secret containing the mysql username usernameKey: # The key in the secret containing the mysql password passwordKey: # Extra options (see example below) # Reference: https://pymysql.readthedocs.io/en/latest/modules/connections.html options: # options: &gt;- # ssl_verify_cert=true # ssl_verify_identity=true # ssl_ca=/mnt/mysql-tls/ca.crt # ssl_cert=/mnt/mysql-tls/client.crt # ssl_key=/mnt/mysql-tls/client.key # PostgreSQL is included into this release for the convenience.# It is recommended to host it separately from this release# Set postgresql.enabled = false and configure externalPostgresqlpostgresql: enabled: false auth: database: oncall existingSecret: # Make sure to create the database with the following parameters:# CREATE DATABASE oncall WITH ENCODING UTF8;externalPostgresql: host: port: db_name: user: password: # Use an existing secret for the database password existingSecret: # The key in the secret containing the database password passwordKey: # Extra options (see example below) # Reference: https://www.postgresql.org/docs/current/libpq-connect.html#LIBPQ-PARAMKEYWORDS options: # options: &gt;- # sslmode=verify-full # sslrootcert=/mnt/postgres-tls/ca.crt # sslcert=/mnt/postgres-tls/client.crt # sslkey=/mnt/postgres-tls/client.key # RabbitMQ is included into this release for the convenience.# It is recommended to host it separately from this release# Set rabbitmq.enabled = false and configure externalRabbitmqrabbitmq: enabled: true image: repository: docker-0.unsee.tech/bitnami/rabbitmq tag: 3.12.0-debian-11-r0 auth: existingPasswordSecret: broker: type: rabbitmq externalRabbitmq: host: port: user: password: protocol: vhost: # Use an existing secret for the rabbitmq password existingSecret: # The key in the secret containing the rabbitmq password passwordKey: "" # The key in the secret containing the rabbitmq username usernameKey: username # Redis is included into this release for the convenience.# It is recommended to host it separately from this releaseredis: enabled: true image: repository: docker-0.unsee.tech/bitnami/redis tag: 6.2.7-debian-11-r11 auth: existingSecret: externalRedis: protocol: host: port: database: username: password: # Use an existing secret for the redis password existingSecret: # The key in the secret containing the redis password passwordKey: # SSL options ssl_options: enabled: false # CA certificate ca_certs: # Client SSL certs certfile: keyfile: # SSL verification mode: "cert_none" | "cert_optional" | "cert_required" cert_reqs: # Grafana is included into this release for the convenience.# It is recommended to host it separately from this releasegrafana: enabled: true grafana.ini: server: domain: grafana-oncall.qifu.com root_url: "%(protocol)s://%(domain)s/grafana/" serve_from_sub_path: true feature_toggles: enable: externalServiceAccounts accessControlOnCall: false env: GF_AUTH_MANAGED_SERVICE_ACCOUNTS_ENABLED: true persistence: enabled: true # Disable psp as PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+ rbac: pspEnabled: false plugins: - grafana-oncall-app extraVolumes: - name: provisioning configMap: name: helm-testing-grafana-plugin-provisioning extraVolumeMounts: - name: provisioning mountPath: /etc/grafana/provisioning/plugins/grafana-oncall-app-provisioning.yaml subPath: grafana-oncall-app-provisioning.yaml externalGrafana: # Example: https://grafana.mydomain.com url: nameOverride: ""fullnameOverride: "" serviceAccount: # Specifies whether a service account should be created create: true # Annotations to add to the service account annotations: &#123;&#125; # The name of the service account to use. # If not set and create is true, a name is generated using the fullname template name: "" podAnnotations: &#123;&#125; podSecurityContext: &#123;&#125; # fsGroup: 2000 securityContext: &#123;&#125; # capabilities: # drop: # - ALL # readOnlyRootFilesystem: true # runAsNonRoot: true # runAsGroup: 2000 # runAsUser: 1000 init: securityContext: &#123;&#125; # allowPrivilegeEscalation: false # capabilities: # drop: # - ALL # privileged: false # readOnlyRootFilesystem: true # runAsGroup: 2000 # runAsNonRoot: true # runAsUser: 1000 resources: &#123;&#125; # limits: # cpu: 100m # memory: 128Mi # requests: # cpu: 100m # memory: 128Mi ui: # this is intended to be used for local development. In short, it will spin up an additional container # running the plugin frontend, such that hot reloading can be enabled enabled: false image: repository: oncall/ui tag: dev # Additional env vars for the ui container env: &#123;&#125; prometheus: enabled: false # extraScrapeConfigs: | # - job_name: 'oncall-exporter' # metrics_path: /metrics/ # static_configs: # - targets: # - oncall-dev-engine.default.svc.cluster.local:8080遇到的问题oncall模板识别payload错误配置完之后由于Skywalking发送的payload如下：12345678910111213[ &#123; "scopeId": 1, "scope": "SERVICE", "name": "qifu-saas-gateway-test", "id0": "cWlmdS1zYWFzLWdhdGV3YXktdGVzdA==.1", "id1": "", "ruleName": "service_resp_time_percentile_rule", "alarmMessage": "最近3分钟的服务 qifu-saas-gateway-test 的响应时间百分比超过1秒", "tags": [], "startTime": 1744275620189 &#125; ]会导致template识别报错：所以我用python写一个中转节点程序来格式化Skywalking原始告警payload并把告警时间戳转为CST时间，整体就是Skywalking发送告警到中转节点，格式化之后再把告警信息转发到Granfana-oncall，skywalking-tra.py：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455# -*- coding: utf-8 -*-from flask import Flask, requestimport requestsfrom datetime import datetime, timezone, timedelta app = Flask(__name__)#Granfana Oncall集成的Endpoint：TARGET_URL = "http://10.168.2.236:8080/integrations/v1/webhook/0DIaZCp09dNem7g4P30PSR4KO/" def convert_timestamp_to_cst(timestamp_ms): """将毫秒时间戳转换为中国标准时间 (UTC+8) 的字符串""" try: # 转换为秒（保留小数部分） timestamp_sec = timestamp_ms / 1000.0 # 创建UTC+8时区 cst_timezone = timezone(timedelta(hours=8)) # 生成datetime对象并转换时区 dt = datetime.fromtimestamp(timestamp_sec, tz=timezone.utc).astimezone(cst_timezone) # 格式化为字符串（示例：2024-01-01 12:34:56） return dt.strftime("%Y-%m-%d %H:%M:%S") except Exception as e: print(f"时间转换失败: &#123;str(e)&#125;") return None @app.route('/alert', methods=['POST'])def handle_alert(): original_data = request.json # 遍历每个告警项，添加转换后的时间 for alert in original_data: timestamp_ms = alert.get("startTime") if timestamp_ms: formatted_time = convert_timestamp_to_cst(timestamp_ms) if formatted_time: # 添加新字段（保留原时间戳） alert["startTimeUTC8"] = formatted_time converted_data = &#123;"alters": original_data&#125; try: response = requests.post( TARGET_URL, json=converted_data, headers=&#123;'Content-Type': 'application/json'&#125;, timeout=10 ) response.raise_for_status() return "Forward success", 200 except Exception as e: print(f"转发失败: &#123;str(e)&#125;") return "Forward failed", 500 if __name__ == '__main__': app.run(host='0.0.0.0', port=5000)安装好依赖：1pip3 install flask requests datetime运行：1python3 skywaliking-tra.py修改Skywalking的配置文件webhook为中转节点程序：alarm-settings.yml：12webhooks: - http://10.168.2.126:5000/alert重启Skywalking服务以生效。现在的payload信息grafana-oncall可以识别了：由于修改了payload的格式，所以对应的PrometheusAlert的模板也要修改，不然会导致识别不到字段从而发送失败Skywalking到中转节点到grafana-oncall再到企微：URL错误发现邮件告警的链接之前是在服务器配置的url，本地会打不开：配置了Granfana-oncall的对外访问链接之后，可以修改URL为对外访问的URL：123curl -X POST 'http://admin:admin@localhost:3000/api/plugins/grafana-oncall-app/settings' -H "Content-Type: application/json" -d '&#123;"enabled":true, "jsonData":&#123;"stackId":5, "orgId":100, "onCallApiUrl":"http://engine:8080", "grafanaUrl":"https://oncall.example.com"&#125;&#125;' curl -X POST 'http://admin:admin@localhost:3000/api/plugins/grafana-oncall-app/resources/plugin/install'个性化配置告警内容简介美化因为我们是配置了通过邮件方式发送告警给值班人员，发送的内容如下：告警内容有些字段我们是不需要的，我们可以通过配置模板来实现美化简介告警内容：模板配置如下： 看一下效果：根据服务名路由告警现在可以通过创建不同的团队，创建对应的团队的升级链，然后通过配置集成的路由来实现对应的告警发给对应的升级链：比如我配置的模板，只要payload.alters的name字段包含qifu-saas-cbl-application或qifu-saas-gateway或qifu-saas-tms就发送到ops的升级链： 配置完效果：拆分告警组skywalking默认同一条告警会包含多组告警信息：正常情况没啥问题，但是在这里我们需要根据服务名来区别，不同的人员接收到他们自己的告警信息，如果合并在同一条告警里，可能只有1组告警是需要我负责的，但是现在3组信息我都看到了，所以在这里对webhook进行改造，把告警信息进行拆分，每条告警只包含一组信息：skywalking-tra.py：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# -*- coding: utf-8 -*-from flask import Flask, requestimport requestsfrom datetime import datetime, timezone, timedelta app = Flask(__name__) TARGET_URL = "http://172.28.81.143:8080/integrations/v1/webhook/crzkWCbF2KNGLQmEsLd8X47Pp/" def convert_timestamp_to_cst(timestamp_ms): """将毫秒时间戳转换为中国标准时间 (UTC+8) 的字符串""" try: timestamp_sec = timestamp_ms / 1000.0 cst_timezone = timezone(timedelta(hours=8)) dt = datetime.fromtimestamp(timestamp_sec, tz=timezone.utc).astimezone(cst_timezone) return dt.strftime("%Y-%m-%d %H:%M:%S") except Exception as e: print(f"时间转换失败: &#123;str(e)&#125;") return None @app.route('/alert', methods=['POST'])def handle_alert(): # 获取原始告警数据（注意：Skywalking 的告警数据格式是[...]） original_data = request.json # 提取告警数组 alerts = original_data # 关键修改：获取数组 # 遍历每个告警项，逐个转发 for alert in alerts: # 添加转换后的时间 timestamp_ms = alert.get("startTime") if timestamp_ms: formatted_time = convert_timestamp_to_cst(timestamp_ms) if formatted_time: alert["startTimeUTC8"] = formatted_time # 直接修改单个告警对象 # 构造单个告警的请求体（保持原结构，但 alters 只包含当前告警） single_alert_payload = &#123;"alters": [alert]&#125; # 关键修改：单个告警包装成数组 try: # 转发单个告警 response = requests.post( TARGET_URL, json=single_alert_payload, # 发送单个告警 headers=&#123;'Content-Type': 'application/json'&#125;, timeout=10 ) response.raise_for_status() print(f"告警转发成功: &#123;alert.get('name')&#125;") except Exception as e: print(f"告警转发失败（&#123;alert.get('name')&#125;）: &#123;str(e)&#125;") # 可以选择继续处理后续告警（不 return，继续循环） return "所有告警处理完成", 200 # 统一返回成功（即使部分失败） if __name__ == '__main__': app.run(host='0.0.0.0', port=5000)测试告警信息是否分开发送：1234curl -X POST http://localhost:5000/alert -H "Content-Type: application/json" -d '[ &#123;"alarmMessage": "测试告警3", "startTime": 1744869502595&#125;, &#123;"alarmMessage": "测试告警4", "startTime": 1744869502595&#125;]'优化端点类型的告警oncall平台通过skywalking告警payload的name字段的值来路由给对应的负责人员，正常的告警没有问题，会通过我们编写的路由模板来匹配给对应的升级链：但是如果是端点类型的告警，会出现两个服务名称：这种情况是qifu-saas-gateway服务去请求qifu-saas-bc服务的接口响应时间过长，需要告警给负责qifu-saas-bc服务的人员，所以还需要对webhook进行改造：思路：新添加一个routeName字段，如果name字段不包含to字符，routeName就等于name，如果包含to字符（包含空格），就以in为分隔符，取最后一列：skywalking-tra.py：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768# -*- coding: utf-8 -*-from flask import Flask, requestimport requestsfrom datetime import datetime, timezone, timedelta app = Flask(__name__) TARGET_URL = "http://172.28.81.143:8080/integrations/v1/webhook/crzkWCbF2KNGLQmEsLd8X47Pp/" def convert_timestamp_to_cst(timestamp_ms): """将毫秒时间戳转换为中国标准时间 (UTC+8) 的字符串""" try: timestamp_sec = timestamp_ms / 1000.0 cst_timezone = timezone(timedelta(hours=8)) dt = datetime.fromtimestamp(timestamp_sec, tz=timezone.utc).astimezone(cst_timezone) return dt.strftime("%Y-%m-%d %H:%M:%S") except Exception as e: print(f"时间转换失败: &#123;str(e)&#125;") return None @app.route('/alert', methods=['POST'])def handle_alert(): # 获取原始告警数据（注意：Skywalking 的告警数据格式是 &#123;"alters": [...]&#125;） original_data = request.json # 提取告警数组 alerts = original_data # 关键修改：获取数组 # 遍历每个告警项，逐个转发 for alert in alerts: # 添加转换后的时间 timestamp_ms = alert.get("startTime") if timestamp_ms: formatted_time = convert_timestamp_to_cst(timestamp_ms) if formatted_time: alert["startTimeUTC8"] = formatted_time # 直接修改单个告警对象 # ========================= 新增逻辑：添加 routeName 字段 ========================= name = alert.get("name", "") route_name = name # 默认值 if " to " in name: # 如果包含 " to "（注意前后空格） # 分割字符串（示例：A in B to C in D → ["A", "B to C", "D"]） parts = name.split(" in ") if len(parts) &gt;= 1: route_name = parts[-1].strip() # 取最后一个部分（如 "D"） # 添加字段到告警数据 alert["routeName"] = route_name # ========================= 新增逻辑结束 ========================= # 构造单个告警的请求体（保持原结构，但 alters 只包含当前告警） single_alert_payload = &#123;"alters": [alert]&#125; # 关键修改：单个告警包装成数组 try: # 转发单个告警 response = requests.post( TARGET_URL, json=single_alert_payload, # 发送单个告警 headers=&#123;'Content-Type': 'application/json'&#125;, timeout=10 ) response.raise_for_status() print(f"告警转发成功: &#123;alert.get('name')&#125;") except Exception as e: print(f"告警转发失败（&#123;alert.get('name')&#125;）: &#123;str(e)&#125;") return "所有告警处理完成", 200 # 统一返回成功（即使部分失败） if __name__ == '__main__': app.run(host='0.0.0.0', port=5000)然后再修改路由模板，根据routeName来路由：Graylog日志告警抑制Graylog有ERROR日志的话就会触发告警，但是会有多个相同的告警频繁触发，导致告警很多，通过接入一个中转webhook对告警信息的app，namespace和message字段的类名+行号来生成一个告警ID，十分钟内如果告警ID相同的话，就不再重复发送告警：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576# -*- coding: utf-8 -*-from flask import Flask, request, jsonifyimport reimport threadingimport requestsfrom datetime import datetime, timedeltafrom expiringdict import ExpiringDictapp = Flask(__name__)# 目标 Grafana 地址GRAFANA_WEBHOOK_URL = "http://172.28.81.143:8080/integrations/v1/webhook/766yLECYaHhZj9kO9bfR6GlVV/"# 缓存配置：10分钟过期，最大存储1000个键（避免内存溢出）ALERT_CACHE = ExpiringDict(max_len=1000, max_age_seconds=600)# 线程锁对象alert_lock = threading.Lock()def extract_class_name(message): """ 从日志消息中提取Java类名+行号（例如：com.xxx.xxx:行号） 匹配格式：[com.xxx.xxx:行号] """ match = re.search(r'\[([a-zA-Z0-9_.]+):(\d+)\]', message) if match: return f"&#123;match.group(1)&#125;:&#123;match.group(2)&#125;" # 格式：类名:行号 else: return "unknown:0" # 兜底值 """ 从日志消息中提取Java类名（例如：com.xxx.xxx） 匹配格式：[com.xxx.xxx:行号] return match.group(1) if match else "unknown" """def generate_alert_id(payload): try: backlog = payload.get("backlog", [&#123;&#125;]) fields = backlog[0].get("fields", &#123;&#125;) app = fields.get("app", "unknown_app") namespace = fields.get("namespace", "unknown_namespace") message = backlog[0].get("message", "") class_name = extract_class_name(message) return f"&#123;app&#125;_&#123;namespace&#125;_&#123;class_name&#125;" except Exception as e: # 防止意外崩溃，返回异常兜底标识 return f"&#123;app&#125;_&#123;namespace&#125;_error"@app.route('/webhook', methods=['POST'])def handle_webhook(): try: payload = request.json alert_id = generate_alert_id(payload) # 使用线程锁包裹整个处理逻辑 with alert_lock: # 原子操作锁 # 检查是否已存在且未过期 if alert_id in ALERT_CACHE: print(f"告警已抑制: &#123;alert_id&#125;") return jsonify(&#123;"status": "suppressed"&#125;), 200 # 转发到 Grafana response = requests.post(GRAFANA_WEBHOOK_URL, json=payload) if response.status_code == 200: ALERT_CACHE[alert_id] = True # 记录到缓存 print(f"告警已转发: &#123;alert_id&#125;") return jsonify(&#123;"status": "forwarded"&#125;), 200 else: return jsonify(&#123;"error": "forward failed"&#125;), 500 except Exception as e: print(f"处理异常: &#123;str(e)&#125;") return jsonify(&#123;"error": str(e)&#125;), 500if __name__ == '__main__': app.run(host='0.0.0.0', port=5001, debug=False)注：有些ERROR日志很长，通过邮箱告警的话没问题，但是如果想通过企微告警，会有长度限制（4096字节），而且消息太长也不美观，所以可以通过中转webhook，对消息进行截取：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990# -*- coding: utf-8 -*-from flask import Flask, request, jsonifyimport reimport threadingimport requestsfrom datetime import datetime, timedeltafrom expiringdict import ExpiringDict app = Flask(__name__) # 目标 Grafana 地址GRAFANA_WEBHOOK_URL = "http://172.28.81.143:8080/integrations/v1/webhook/766yLECYaHhZj9kO9bfR6GlVV/" # 缓存配置：10分钟过期，最大存储1000个键（避免内存溢出）ALERT_CACHE = ExpiringDict(max_len=1000, max_age_seconds=600) # 线程锁对象alert_lock = threading.Lock() def extract_class_name(message): """ 从日志消息中提取Java类名+行号（例如：com.xxx.xxx:行号） 匹配格式：[com.xxx.xxx:行号] """ match = re.search(r'\[([a-zA-Z0-9_.]+):(\d+)\]', message) if match: return f"&#123;match.group(1)&#125;:&#123;match.group(2)&#125;" # 格式：类名:行号 else: return "unknown:0" # 兜底值 """ 从日志消息中提取Java类名（例如：com.xxx.xxx） 匹配格式：[com.xxx.xxx:行号] return match.group(1) if match else "unknown" """def generate_alert_id(payload): try: backlog = payload.get("backlog", [&#123;&#125;]) fields = backlog[0].get("fields", &#123;&#125;) app = fields.get("app", "unknown_app") namespace = fields.get("namespace", "unknown_namespace") message = backlog[0].get("message", "") class_name = extract_class_name(message) return f"&#123;app&#125;_&#123;namespace&#125;_&#123;class_name&#125;" except Exception as e: # 防止意外崩溃，返回异常兜底标识 return f"&#123;app&#125;_&#123;namespace&#125;_error"def truncate_message(message, max_length=300): """ 截取消息以确保其长度不超过最大限制 """ if len(message) &gt; max_length: return message[:max_length] + f"...\n[消息截断 总长度:&#123;len(message)&#125; &gt; 限制:&#123;max_length&#125;]" return message @app.route('/webhook', methods=['POST'])def handle_webhook(): try: payload = request.json alert_id = generate_alert_id(payload) # 截取 message 字段 backlog = payload.get("backlog", [&#123;&#125;]) if backlog: backlog[0]['message'] = truncate_message(backlog[0].get("message", "")) # 使用线程锁包裹整个处理逻辑 with alert_lock: # 原子操作锁 # 检查是否已存在且未过期 if alert_id in ALERT_CACHE: print(f"告警已抑制: &#123;alert_id&#125;") return jsonify(&#123;"status": "suppressed"&#125;), 200 # 转发到 Grafana response = requests.post(GRAFANA_WEBHOOK_URL, json=payload) if response.status_code == 200: ALERT_CACHE[alert_id] = True # 记录到缓存 print(f"告警已转发: &#123;alert_id&#125;") return jsonify(&#123;"status": "forwarded"&#125;), 200 else: return jsonify(&#123;"error": "forward failed"&#125;), 500 except Exception as e: print(f"处理异常: &#123;str(e)&#125;") return jsonify(&#123;"error": str(e)&#125;), 500 if __name__ == '__main__': app.run(host='0.0.0.0', port=5001, debug=False)优化跳转链接时间范围之前的Graylog告警点击告警链接取的值是当前时间的前5分钟，如果超过了5分钟才看到告警，此时点击链接跳转就找不到对应的日志了，还得手动去调时间范围，所以再改造webhook，生成两个新的字段，取值为告警时间的前后一分钟：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144# -*- coding: utf-8 -*-from flask import Flask, request, jsonifyimport reimport threadingimport requestsfrom datetime import datetime, timedeltafrom expiringdict import ExpiringDict app = Flask(__name__) # 目标 Grafana 地址GRAFANA_WEBHOOK_URL = "http://172.28.81.143:8080/integrations/v1/webhook/766yLECYaHhZj9kO9bfR6GlVV/" # 缓存配置：10分钟过期，最大存储1000个键（避免内存溢出）ALERT_CACHE = ExpiringDict(max_len=1000, max_age_seconds=600) # 线程锁对象alert_lock = threading.Lock() def extract_class_name(message): """ 从日志消息中提取Java类名+行号（例如：com.xxx.xxx:行号） 匹配格式：[com.xxx.xxx:行号] """ match = re.search(r'\[([a-zA-Z0-9_.]+):(\d+)\]', message) if match: return f"&#123;match.group(1)&#125;:&#123;match.group(2)&#125;" # 格式：类名:行号 else: return "unknown:0" # 兜底值 """ 从日志消息中提取Java类名（例如：com.xxx.xxx） 匹配格式：[com.xxx.xxx:行号] return match.group(1) if match else "unknown" """def generate_alert_id(payload): try: backlog = payload.get("backlog", [&#123;&#125;]) fields = backlog[0].get("fields", &#123;&#125;) app = fields.get("app", "unknown_app") namespace = fields.get("namespace", "unknown_namespace") message = backlog[0].get("message", "") class_name = extract_class_name(message) return f"&#123;app&#125;_&#123;namespace&#125;_&#123;class_name&#125;" except Exception as e: # 防止意外崩溃，返回异常兜底标识 return f"&#123;app&#125;_&#123;namespace&#125;_error" def process_timestamp(timestamp_str): """处理时间戳生成前后1分钟范围""" try: dt = datetime.strptime(timestamp_str, "%Y-%m-%dT%H:%M:%S.%fZ") # 生成时间范围 before = dt - timedelta(minutes=1) after = dt + timedelta(minutes=1) # URL编码时间格式 def encode_time(t): return t.strftime("%Y-%m-%dT%H:%M:%S.%fZ").replace(":", "%3A") return &#123; "before": encode_time(before)[:-4] + "Z", # 保持3位毫秒 "after": encode_time(after)[:-4] + "Z" &#125; except Exception as e: print(f"时间处理失败: &#123;str(e)&#125;") return &#123;"before": "", "after": ""&#125; def truncate_message(message, max_length=500): """ 截取消息以确保其长度不超过最大限制 """ if len(message) &gt; max_length: return message[:max_length] + f"...\n[消息截断 总长度:&#123;len(message)&#125; &gt; 限制:&#123;max_length&#125;]" return message def extract_description(message): """提取description字段""" try: match = re.search(r'\[[a-zA-Z0-9_.]+:\d+\]\s*-\s*(.*)', message) if match: return match.group(1)[:180] # 截取180个字符 else: return "无法提取描述信息" except Exception as e: return f"提取描述信息失败: &#123;e&#125;" @app.route('/webhook', methods=['POST'])def handle_webhook(): try: payload = request.json # 处理每条日志 if 'backlog' in payload: for log_entry in payload['backlog']: # 1. 消息截断 if 'message' in log_entry: log_entry['message'] = truncate_message(log_entry['message']) # 2. 添加时间范围字段 if 'timestamp' in log_entry: time_range = process_timestamp(log_entry['timestamp']) if not log_entry.get('fields'): log_entry['fields'] = &#123;&#125; log_entry['fields'].update(&#123; "before_timestamp": time_range['before'], "after_timestamp": time_range['after'] &#125;) # 3. 添加描述字段 if 'message' in log_entry: log_entry['fields']['description'] = extract_description(log_entry['message']) alert_id = generate_alert_id(payload) # 使用线程锁包裹整个处理逻辑 with alert_lock: # 原子操作锁 # 检查是否已存在且未过期 if alert_id in ALERT_CACHE: print(f"告警已抑制: &#123;alert_id&#125;") return jsonify(&#123;"status": "suppressed"&#125;), 200 # 转发到 Grafana response = requests.post(GRAFANA_WEBHOOK_URL, json=payload) if response.status_code == 200: ALERT_CACHE[alert_id] = True # 记录到缓存 print(f"告警已转发: &#123;alert_id&#125;") return jsonify(&#123;"status": "forwarded"&#125;), 200 else: return jsonify(&#123;"error": "forward failed"&#125;), 500 except Exception as e: print(f"处理异常: &#123;str(e)&#125;") return jsonify(&#123;"error": str(e)&#125;), 500 if __name__ == '__main__': app.run(host='0.0.0.0', port=5001, debug=False)Graylog的MySQL慢日志告警添加了Graylog收集的MySQL慢日志告警，直接发送的payload还是有message过长，时间长告警链接跳转找不到对应日志得到问题，所以也通过中转webhook来处理payload，格式化为我们需要的格式之后再发送到oncall平台（添加了告警时间的前后一分钟字段，message过长截断以及处理慢日志里存在的###被Markdown解析为标题的问题）：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# -*- coding: utf-8 -*-from flask import Flask, request, jsonifyimport requestsfrom datetime import datetime, timedeltaimport html # 用于HTML转义（可选） app = Flask(__name__)TARGET_URL = "http://172.28.81.143:8080/integrations/v1/webhook/766yLECYaHhZj9kO9bfR6GlVV/" def process_timestamps(payload): """处理payload添加时间范围字段""" for entry in payload.get('backlog', []): fields = entry.setdefault('fields', &#123;&#125;) if 'timestamp' in entry: try: original_time = datetime.strptime(entry['timestamp'], "%Y-%m-%dT%H:%M:%S.%fZ") before_time = original_time - timedelta(minutes=1) after_time = original_time + timedelta(minutes=1) def format_graylog_time(t): return t.strftime("%Y-%m-%dT%H:%M:%S.%fZ").replace(":", "%3A") fields.update(&#123; "before_timestamp": format_graylog_time(before_time)[:23] + "Z", "after_timestamp": format_graylog_time(after_time)[:23] + "Z" &#125;) except Exception as e: print(f"时间处理失败: &#123;str(e)&#125;") fields.update(&#123; "before_timestamp": "invalid_time", "after_timestamp": "invalid_time" &#125;) @app.route('/graylog-webhook', methods=['POST'])def graylog_handler(): try: payload = request.json process_timestamps(payload) # 处理message字段：转义特殊字符并截断 for entry in payload.get('backlog', []): if 'message' in entry: # 转义#防止Markdown格式解析 message = entry['message'].replace('#', '\\#') # 可选：HTML转义（根据告警系统支持情况） # message = html.escape(message) # 截断超过500字符的部分 if len(message) &gt; 500: message = message[:500] + "..." entry['message'] = message resp = requests.post(TARGET_URL, json=payload, timeout=5) return jsonify(&#123;"status": "forwarded", "code": resp.status_code&#125;), resp.status_code except Exception as e: return jsonify(&#123;"error": str(e)&#125;), 500 if __name__ == '__main__': app.run(host='0.0.0.0', port=5002)告警模板对应的告警模板也需要需改nohup启动日志缺失问题通过以下命令启动：1/usr/bin/python3 /usr/local/grafana-oncall/graylog-suppressed.py控制台的输出日志为：通过以下命令启动：1nohup /usr/bin/python3 /usr/local/grafana-oncall/graylog-suppressed.py &gt;&gt; /usr/local/grafana-oncall/graylog_nohup.out 2&gt;&amp;1 &amp;graylog_nohup.out文件的内容只有：这是因为通过 nohup 后台运行时，标准输出变为块缓冲（block-buffered），默认只在缓冲区满或程序退出时才会写入文件，导致日志延迟或丢失。解决办法：在启动命令中添加 -u 参数，强制 Python 使用无缓冲模式：1nohup /usr/bin/python3 -u /usr/local/grafana-oncall/graylog-suppressed.py &gt;&gt; /usr/local/grafana-oncall/graylog_nohup.out 2&gt;&amp;1 &amp;]]></content>
      <categories>
        <category>技术</category>
        <category>Oncall</category>
      </categories>
      <tags>
        <tag>grafana</tag>
        <tag>oncall</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[javamelody监控图表中文乱码]]></title>
    <url>%2F2025%2F031c702fda.html</url>
    <content type="text"><![CDATA[JavaMelody是一款能够监测Java或Java EE应用程序的服务，JavaMelody能够很好的反映我们系统的各种性能指标，并提供很好的图形界面，其指标还能反应Java内存和Java CPU使用情况、用户Session数量、JDBC连接数、http请求、sql请求、jsp页面与业务接口方法（EJB3、Spring、 Guice）的执行数量，平均执行时间，错误百分比等。java程序集成JavaMelody就不说了，网上一大堆，说说部署完之后的问题。配置了javamelody监控之后发现图表中文乱码：排查：研发在本地跑的时候是正常的，放到容器里跑乱码，应该是缺少字体原因，网上搜到很多教程都是说Linux服务器缺少字体，在windows把微软雅黑的中文字体复制到java的家目录下的jre/lib/fonts/fallback目录下，试过了这种方法，还是不行。解决：java服务都是在openjdk11的容器里跑的，在windows把宋体复制到容器的/usr/share/fonts/zh_CN（默认没有这个目录，自行创建），重新打一个镜像，用新镜像运行java服务，问题解决：ps：fc-list命令可查看服务字体：]]></content>
      <categories>
        <category>技术</category>
        <category>Java</category>
      </categories>
      <tags>
        <tag>javamelody</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用certbot续费letsencrypt证书触发告警问题]]></title>
    <url>%2F2025%2F03acc9418.html</url>
    <content type="text"><![CDATA[我们是使用cerbot来申请letsencrypt证书的，letsencrypt证书有效期只有3个月，过期前需要续期，通过certbot命令去定时续期，但是每次续期都会触发阿里云的AK告警。问题：周末收到短信告警，AK存在异常调用。排查过程：1.看着告警的时间点，发现是我通过certbot更新证书的时间点，更新证书的操作会调用alidns的api去添加和删除一个txt域名解析记录以供验证，怀疑是这个操作触发的，登录阿里云控制台，查看AK操作事件记录：都执行成功了，按理说应该不会告警，所以提工单问阿里云的客服，客服说告警跟更新证书的操作没有关系，是因为调用了DescribeEndpoints的api接口，执行失败了导致的告警，登录阿里云控制台，查看AK操作事件记录，确实有Location类型（地理位置相关的产品）的DescribeEndpoints执行失败了，但是我没有调用过DescribeEndpoints的api接口啊，问阿里云的客服也不清楚。2.阿里云客服说告警跟我更新证书的操作没有关系，但是我看这个告警时间点，调用DescribeEndpoints的api接口的时间点和我执行更新证书的操作时间点是一致的，而且有6条调用DescribeEndpoints的api接口的记录，跟我调用alidns的次数是一样的，应该就是这个操作触发的告警，继续提工单问阿里云工程师，有了新说法，说没有对外开放DescribeEndpoints的接口，导致的执行失败，说调用逻辑是从certbot程序发起的但是我如果单独执行cerbot命令是不会触发调用DescribeEndpoints的api接口，只有加上调用alidns的api接口去添加txt解析等操作才会触发调用DescribeEndpoints的api接口，所以问题应该跟cerbot程序无关，alidns的api接口如下，也确实没有发现有调用DescribeEndpoints的api接口的操作：3.查到网上有文章说只要通过阿里云的cli工具去调用api，都会先去调DescribeEndpoints接口去获取Endpoint：尝试显式指定Endpoint看是否还会触发DescribeEndpoints接口：到阿里云控制台查看AK事件操作记录，发现没有去调用DescribeEndpoints接口了。解决办法：修改alidns的脚本，指定Endpoint参数：12345678910111213141516171819202122232425262728#!/bin/bash if ! command -v aliyun &gt;/dev/null; then echo "错误: 你需要先安装 aliyun 命令行工具 https://help.aliyun.com/document_detail/121541.html。" 1&gt;&amp;2 exit 1fi ALIYUN_DNS_ENDPOINT="alidns.cn-hangzhou.aliyuncs.com" if [ $# -eq 0 ]; then aliyun --endpoint $ALIYUN_DNS_ENDPOINT alidns AddDomainRecord \ --DomainName $CERTBOT_DOMAIN \ --RR "_acme-challenge" \ --Type "TXT" \ --Value $CERTBOT_VALIDATION /bin/sleep 20else RecordId=$(aliyun --endpoint $ALIYUN_DNS_ENDPOINT alidns DescribeDomainRecords \ --DomainName $CERTBOT_DOMAIN \ --RRKeyWord "_acme-challenge" \ --Type "TXT" \ --ValueKeyWord $CERTBOT_VALIDATION \ | grep "RecordId" \ | grep -Eo "[0-9]+") aliyun --endpoint $ALIYUN_DNS_ENDPOINT alidns DeleteDomainRecord \ --RecordId $RecordIdfi]]></content>
      <categories>
        <category>技术</category>
        <category>域名证书</category>
      </categories>
      <tags>
        <tag>cerbot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s部署Debezium及kafka]]></title>
    <url>%2F2025%2F032620f80c.html</url>
    <content type="text"><![CDATA[Debezium 是一组分布式服务，用于捕获数据库中的更改（通过读取数据库日志的方式来完成数据增删改的记录），以便您的应用程序可以看到这些更改并做出响应。Debezium 将每个数据库表中的所有行级更改记录在更改事件流中，应用程序只需读取这些流，即可按更改事件发生的顺序查看更改事件。目前debezium有三种部署方式：Kafka Connect 模式Debezium 作为一个 Kafka Connect 的 Source Connector 运行，将数据库的变更事件发送到 Kafka 中。适用场景：适用于需要将数据库变更事件持久化到 Kafka，并且需要 Kafka Connect 提供的分布式、可扩展和容错能力的场景。主要特点：可以利用 Kafka 的可靠性和容错性，支持高吞吐量和低延迟的数据传输。Debezium Server 模式Debezium Server 是一个独立的应用程序，它可以将数据库的变更事件流式传输到各种消息传递基础设施，如 Amazon Kinesis、Google Cloud Pub/Sub 或 Apache Pulsar。适用场景：适用于需要将数据库变更事件发送到非 Kafka 的消息队列或流处理系统的场景。主要特点：提供了更多的灵活性，可以支持多种不同的消息传递基础设施。Embedded Engine 模式在这种模式下，Debezium 不通过 Kafka Connect 运行，而是作为一个库嵌入到自定义的 Java 应用程序中。适用场景：适用于需要在应用程序内部直接消费数据库变更事件，而不希望通过 Kafka 进行中转的场景。主要特点：减少了对外部系统的依赖，适合于轻量级的应用程序或微服务架构。Debezium特点：简单易上手快速稳定，可以扩展，可以通过kafka构建能够监控多种数据库 mysql pgsql等等下面介绍基于kafka connector部署 Debeziunm：Strimzi简化了Kafka在Kubernetes上的部署和管理：安装Strimzi Operator12345# 添加Strimzi Helm仓库helm repo add strimzi https://strimzi.io/charts/ # 安装Strimzi Operatorhelm install strimzi-kafka strimzi/strimzi-kafka-operator -n kafka --create-namespace这个是添加的最新的仓库，我需要安装的是历史版本（0.33.0）的，所以把strimzi-kafka-operator-helm-3-chart-0.33.0.tgz下载到本地安装：1helm install strimzi-kafka ./strimzi-kafka-operator-helm-3-chart-0.33.0.tgz -n qifu-develop安装完成后会有这个pod：ps：卸载命令：1helm uninstall strimzi-kafka -n qifu-develop部署kafka集群：debezium-cluster.yaml文件：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748apiVersion: kafka.strimzi.io/v1beta2kind: Kafkametadata: name: debezium-cluster namespace: qifu-developspec: kafka: version: 3.2.0 replicas: 1 listeners: - name: plain port: 9092 type: internal tls: false - name: tls port: 9093 type: internal tls: true authentication: type: tls - name: external port: 9094 type: nodeport tls: false storage: type: jbod volumes: - id: 0 type: persistent-claim size: 100Gi deleteClaim: false class: nfs-storage-node04 config: #开发环境配置，生产环境去掉此配置 offsets.topic.replication.factor: 1 transaction.state.log.replication.factor: 1 transaction.state.log.min.isr: 1 default.replication.factor: 1 min.insync.replicas: 1 zookeeper: replicas: 1 storage: type: persistent-claim size: 20Gi deleteClaim: false class: nfs-storage-node04 entityOperator: topicOperator: &#123;&#125; userOperator: &#123;&#125;部署：1kubectl apply -f debezium-cluster.yaml部署完成后会有3个pod：构建包含Debezium插件的kafka Connect镜像创建Dockerfile，将Debezium插件添加到Kafka Connect：12345FROM quay.io/strimzi/kafka:0.33.0-kafka-3.2.0USER root:rootRUN mkdir -p /opt/kafka/plugins/debeziumRUN curl -L https://repo1.maven.org/maven2/io/debezium/debezium-connector-mysql/2.3.4.Final/debezium-connector-mysql-2.3.4.Final-plugin.tar.gz | tar xz -C /opt/kafka/plugins/debezium/USER 1001构建并推送镜像到容器仓库：12docker build -t harbor.qifu.com/qifu-develop/debezium-connect:2.3.4.Final .docker push harbor.qifu.com/qifu-develop/debezium-connect:2.3.4.Final部署Kafka Connect创建KafkaConnect资源（kafka-connect.yaml）：12345678910111213141516171819202122apiVersion: kafka.strimzi.io/v1beta2kind: KafkaConnectmetadata: name: debezium-connect-cluster namespace: qifu-develop annotations: strimzi.io/use-connector-resources: "true"spec: version: 3.2.0 image: harbor.qifu.com/qifu-develop/debezium-connect:2.3.4.Final replicas: 1 bootstrapServers: debezium-cluster-kafka-bootstrap:9092 config: config.providers: secrets config.providers.secrets.class: io.strimzi.kafka.KubernetesSecretConfigProvider group.id: connect-cluster offset.storage.topic: connect-offsets config.storage.topic: connect-configs status.storage.topic: connect-status config.storage.replication.factor: -1 #开发环境配置，生产环境去掉此配置 offset.storage.replication.factor: -1 #开发环境配置，生产环境去掉此配置 status.storage.replication.factor: -1 #开发环境配置，生产环境去掉此配置部署：1kubectl apply -f kafka-connect.yaml部署完成后会有以下pod创建Debezium连接器创建连接器配置（mysql-connector-test.yaml）：1234567891011121314151617181920212223242526272829303132apiVersion: kafka.strimzi.io/v1beta2kind: KafkaConnectormetadata: name: mysql-connector-test namespace: qifu-develop labels: strimzi.io/cluster: debezium-connect-clusterspec: class: io.debezium.connector.mysql.MySqlConnector tasksMax: 1 config: database.hostname: mysql.qifu.svc.cluster.local #监控的数据库连接地址 database.port: 3306 database.user: root database.password: ****** database.server.id: 184055 #每个连接器的ID需要唯一# database.server.name: test topic.prefix: test #主题前缀 #监控的数据库： database.include.list: qifu_saas_oms,qifu_saas_owms,qifu_saas_inventory,qifu_saas_aggregation #监控的数据表： table.include.list: qifu_saas_oms.o_order,qifu_saas_oms.o_order_task,qifu_saas_oms.o_order_company,qifu_saas_oms.o_outbound_order,qifu_saas_oms.o_outbound_order_detail,qifu_saas_oms.o_outbound_order_extra_info,qifu_saas_oms.o_outbound_order_pack_cargo,qifu_saas_oms.o_entry_order,qifu_saas_oms.o_entry_pack_cargo,qifu_saas_oms.o_entry_pack_cargo_detail,qifu_saas_owms.entry_plan,qifu_saas_owms.outbound_order,qifu_saas_owms.outbound_order_box_info,qifu_saas_owms.outbound_order_detail,qifu_saas_owms.outbound_order_extension,qifu_saas_owms.putaway_order,qifu_saas_owms.putaway_order_detail,qifu_saas_owms.putaway_order_detail_batch,qifu_saas_inventory.item_loc_inventory_flow,qifu_saas_inventory.inventory_batch,qifu_saas_aggregation.base_warehouse,qifu_saas_aggregation.logistics_package schema.history.internal.kafka.bootstrap.servers: debezium-cluster-kafka-bootstrap:9092 #kafka连接地址 schema.history.internal.kafka.topic: schema-changes.inventory-test include.schema.changes: true include.query: true #kafka消息中记录sql语句，需mysql开启binlog_rows_query_log_events provide.transaction.metadata: true#以下为配置每个库对应一个主题，不配置的话默认是一个表对应一个主题 transforms: Reroute transforms.Reroute.type: io.debezium.transforms.ByLogicalTableRouter transforms.Reroute.topic.regex: (test.qifu_saas_oms|test.qifu_saas_owms|test.qifu_saas_inventory|test.qifu_saas_aggregation)\..* transforms.Reroute.topic.replacement: $1.tables注：快照模式默认是initial，全量同步历史数据到kafka，如果希望不同步历史数据的话可以加上以下配置：1snapshot.mode: schema_only部署：1kubectl apply -f mysql-connector-test.yaml验证：可以到debezium-connect-cluster-connect容器里执行以下命令查看连接器：1curl -i -X GET -H "Accept:application/json" localhost:8083/connectors/mysql-connector-test或者使用以下命令查看：1kubectl describe KafkaConnector mysql-connector-test -n qifu-develop查看Kafka主题中的变更事件：1kubectl -n qifu-develop run kafka-consumer -ti --image=quay.io/strimzi/kafka:0.33.0-kafka-3.2.0 --rm=true --restart=Never -- bin/kafka-console-consumer.sh --bootstrap-server debezium-cluster-kafka-bootstrap:9092 --topic test.qifu_saas_oms.tables登录kafka容器查看主题列表：1bin/kafka-topics.sh --bootstrap-server debezium-cluster-kafka-bootstrap:9092 --list启动一个消费者，消费主题：1bin/kafka-console-consumer.sh --bootstrap-server debezium-cluster-kafka-bootstrap:9092 --topic test.qifu_saas_oms.tables查看消费群组详情：1bin/kafka-consumer-groups.sh --bootstrap-server debezium-cluster-kafka-bootstrap:9092 --list查看消费群组消息积压：1bin/kafka-consumer-groups.sh --bootstrap-server debezium-cluster-kafka-bootstrap:9092 --describe --group consumer-oms修改连接器配置之后需要修改mysql连接器配置的话，可以到debezium-connect-cluster-connect容器里执行以下命令修改：123456789101112131415161718192021curl -X PUT -H "Accept:application/json" -H "Content-Type:application/json" http://localhost:8083/connectors/mysql-connector-test/config \-d '&#123;"connector.class": "io.debezium.connector.mysql.MySqlConnector","database.hostname": "mysql.qifu.svc.cluster.local","database.port": "3306","database.user": "root","database.password": "*****","database.server.id": "184055","database.include.list": "qifu_saas_oms,qifu_saas_owms,qifu_saas_inventory,qifu_saas_aggregation","table.include.list": "qifu_saas_oms.o_order,qifu_saas_oms.o_order_task,qifu_saas_oms.o_order_company,qifu_saas_oms.o_outbound_order,qifu_sas_oms.o_outbound_order_extra_info,qifu_saas_oms.o_outbound_order_pack_cargo,qifu_saas_oms.o_entry_order,qifu_saas_oms.o_entry_pack_cargo,qifu_saas_oms.o_entry_pack_cargo_detail,qifu_saas_owms.entry_plan,qifu_saas_owms.outbound_order,qifu_saas_owms.outbound_order_box_info,qifu_saas_owms.outbound_owms.putaway_order,qifu_saas_owms.putaway_order_detail,qifu_saas_owms.putaway_order_detail_batch,qifu_saas_inventory.item_loc_inventory_flow,qifu_saas_inventory.inventory_batch,qifu_saas_aggregation.base_warehouse,qifu_saas_aggregation.logistics_package","schema.history.internal.kafka.topic": "schema-changes.inventory-test","schema.history.internal.kafka.bootstrap.servers": "debezium-cluster-kafka-bootstrap:9092","topic.prefix": "test","include.schema.changes": "true","include.query":"true","provide.transaction.metadata": "true","transforms": "Reroute","transforms.Reroute.type": "io.debezium.transforms.ByLogicalTableRouter","transforms.Reroute.topic.regex": "(test.qifu_saas_oms|test.qifu_saas_owms|test.qifu_saas_inventory|test.qifu_saas_aggregation)\\..*","transforms.Reroute.topic.replacement": "$1.tables"&#125;'删除连接器：1curl -i -X DELETE localhost:8083/connectors/mysql-connector-test/重启连接器：1curl -X POST http://localhost:8083/connectors/mysql-connector-test/restart检查连接器状态：1curl http://localhost:8083/connectors/mysql-connector-test/status或者修改mysql-connector-test.yaml文件之后再部署：1kubectl apply -f mysql-connector-test.yaml删除连接器：1kubectl delete -f mysql-connector-test.yaml遇到的问题问题1遇到开发环境部署mysql连接器之后同步完数据后报错’performance_schema.session_status’ doesn’t exist，然后重复一直同步的问题。到数据库里查看performance_schema库，确实没有session_status表，其他环境的数据库都有。解决办法：在开发环境数据库执行以下命令：1mysql_upgrade -u root -p --force然后就会重新生成performance_schema.session_status，需要重启数据库生效。问题2相同配置，UAT环境没问题，上生产的时候报错：123456789101112131415162025-06-09 10:04:43,710 ERROR [mysql-connector-to-etm|task-0] WorkerSourceTask&#123;id=mysql-connector-to-etm-0&#125; Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask) [task-thread-mysql-connector-to-etm-0] org.apache.kafka.connect.errors.ConnectException: Unrecoverable exception from producer send callback at org.apache.kafka.connect.runtime.WorkerSourceTask.maybeThrowProducerSendException(WorkerSourceTask.java:343) at org.apache.kafka.connect.runtime.WorkerSourceTask.prepareToSendRecord(WorkerSourceTask.java:135) at org.apache.kafka.connect.runtime.AbstractWorkerSourceTask.sendRecords(AbstractWorkerSourceTask.java:408) at org.apache.kafka.connect.runtime.AbstractWorkerSourceTask.execute(AbstractWorkerSourceTask.java:364) at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226) at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281) at org.apache.kafka.connect.runtime.AbstractWorkerSourceTask.run(AbstractWorkerSourceTask.java:79) at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238) at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264) at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) at java.base/java.lang.Thread.run(Thread.java:840) Caused by: org.apache.kafka.common.errors.RecordTooLargeException: The message is 2238388 bytes when serialized which is larger than 1048576, which is the value of the max.request.size configuration.生产者发数据默认限制1M，超过了限制，需要修改一下debezium连接器和kafka的限制大小：kafka-connect.yaml：1producer.max.request.size: 3145728debezium-cluster.yaml：12345message.max.bytes: 3145728replica.fetch.max.bytes: 6291456socket.send.buffer.bytes: 3145728socket.receive.buffer.bytes: 3145728socket.request.max.bytes: 3145728修改完之后依次重启kafka，debezium连接器和mysql连接器使配置生效。问题3mysql连接器配置里加了include.query: true，并且mysql开启binlog_rows_query_log_events之后，会记录具体的sql语句，其他字段的中文正常显示，但是query字段的中文会乱码：问了Debezium社区的人，他们说3.0的版本没有这个问题，于是用docker临时启动了服务来测试：zookeeper：1docker run -it --rm --name zookeeper -p 2181:2181 -p 2888:2888 -p 3888:3888 --security-opt seccomp=unconfined quay.io/debezium/zookeeper:3.0kafka：1docker run -it --rm --name kafka -p 9092:9092 --security-opt seccomp=unconfined --link zookeeper:zookeeper quay.io/debezium/kafka:3.0mysql：1docker run -it --rm --name mysql -p 3306:3306 -v /root/my.cnf:/etc/my.cnf --security-opt seccomp=unconfined -e LANG=en_US.UTF-8 -e LC_ALL=en_US.UTF-8 -e MYSQL_ROOT_PASSWORD=debezium -e MYSQL_USER=mysqluser -e MYSQL_PASSWORD=mysqlpw quay.io/debezium/example-mysql:3.0connect：1docker run -it --rm --name connect -p 8083:8083 --security-opt seccomp=unconfined -e GROUP_ID=1 -e CONFIG_STORAGE_TOPIC=my_connect_configs -e OFFSET_STORAGE_TOPIC=my_connect_offsets -e STATUS_STORAGE_TOPIC=my_connect_statuses --link kafka:kafka --link mysql:mysql quay.io/debezium/connect:3.0注册mysql连接器：1curl -i -X POST -H "Accept:application/json" -H "Content-Type:application/json" localhost:8083/connectors/ -d '&#123; "name": "dev-connector", "config": &#123; "connector.class": "io.debezium.connector.mysql.MySqlConnector", "tasks.max": "1", "database.hostname": "10.168.2.187", "database.port": "30336", "database.user": "root", "database.password": "******", "database.server.id": "184059", "topic.prefix": "dev", "database.include.list": "dev_qifu_saas_aggregation", "table.include.list": "dev_qifu_saas_aggregation.base_warehouse_area", "schema.history.internal.kafka.bootstrap.servers": "kafka:9092", "schema.history.internal.kafka.topic": "schemahistory.inventory-dev", "include.schema.changes": "true", "include.query":"true", "provide.transaction.metadata": "true" &#125; &#125;'查看消费内容：发现确实可以正常显示query字段的中文，研发说这个不影响，就没在k8s上面升级版本，在此记录下docker部署的3.0版本各组件的版本情况，以便以后k8s部署的Debezium服务如果需要升级，可直接安装对应版本：zookeeper：3.8.4kafka：3.9.0Debezium：3.0.8.final更新由于有新的需求，研发告知中文乱码的问题需要修复，按照之前排查的思路，在k8s把对应组件容器版本修改为对应3.0的版本，发现还是乱码，我们k8s进入容器的命令一般是使用1kubectl exec -it debezium-connect-cluster-connect-8d787f9c7-v25h7 -n qifu-develop -- /bin/bash查看编码是utf8：1java -XshowSettings:properties -version | grep encoding之前排查编码问题也一直没看出啥问题，直到一次偶然，使用以下命令进入容器：1kubectl exec -it debezium-connect-cluster-connect-8d787f9c7-v25h7 -n qifu-develop sh发现编码竟然不是utf81java -XshowSettings:properties -version | grep encoding至此，发现确实是编码问题，重新构建一个镜像，以新镜像启动容器即可修改Dockerfile文件：123456789FROM quay.io/strimzi/kafka:0.33.0-kafka-3.2.0USER root:rootENV LANG=C.utf8 \ LC_ALL=C.utf8 \ LANGUAGE=en_USRUN echo 'export LANG=C.utf8 LC_ALL=C.utf8' &gt;&gt; /etc/profileRUN mkdir -p /opt/kafka/plugins/debeziumRUN curl -L https://repo1.maven.org/maven2/io/debezium/debezium-connector-mysql/3.0.8.Final/debezium-connector-mysql-3.0.8.Final-plugin.tar.gz | tar xz -C /opt/kafka/plugins/debezium/USER 1001构建并推送镜像到容器仓库：12docker build -t harbor.qifu.com/qifu-develop/debezium-connect:2.3.4.Final_bug_fix .docker push harbor.qifu.com/qifu-develop/debezium-connect:2.3.4.Final_bug_fix问题4添加新表后，如果新表修改了数据触发同步到kafka，由于历史主题没有新表的元数据，连接器就会报错：12345678910111213141516171819202025-06-24 10:42:42,079 ERROR [mysql-connector-to-etm|task-0] Encountered change event 'Event&#123;header=EventHeaderV4&#123;timestamp=1750728336000, eventType=TABLE_MAP, serverId=1, headerLength=19, dataLength=162, nextPosition=998903090, flags=0&#125;, data=TableMapEventData&#123;tableId=10742, database='qifu_saas_owms', table='outbound_order', columnTypes=8, 8, 8, 8, 8, 15, 15, 15, 3, -10, 8, 15, 15, 3, 1, 15, 8, 15, 15, 8, 15, 15, 15, 3, 15, 15, 15, 1, 3, 8, 8, 8, -10, 8, 8, 8, -10, 15, 15, 15, 8, 3, 8, 3, 3, 15, 3, 8, 8, 3, 3, 15, 8, 15, 8, 8, 15, 8, 8, 3, columnMetadata=0, 0, 0, 0, 0, 384, 512, 512, 0, 1556, 0, 384, 512, 0, 0, 512, 0, 512, 512, 0, 512, 512, 800, 0, 200, 800, 800, 0, 0, 0, 0, 0, 788, 0, 0, 0, 532, 900, 1020, 1020, 0, 0, 0, 0, 0, 3072, 0, 0, 0, 0, 0, 1020, 0, 128, 0, 0, 128, 0, 0, 0, columnNullability=&#123;2, 3, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 55, 56, 57, 59&#125;, eventMetadata=null&#125;&#125;' at offset &#123;transaction_id=6f54a8d9-71a4-11ef-bfb2-00163e105ad2:85818558, file=mysql-bin.000113, pos=998902025, gtids=6f54a8d9-71a4-11ef-bfb2-00163e105ad2:1-85818557, server_id=1, event=2&#125; for table qifu_saas_owms.outbound_order whose schema isn't known to this connector. One possible cause is an incomplete database schema history topic. Take a new snapshot in this case. Use the mysqlbinlog tool to view the problematic event: mysqlbinlog --start-position=998902909 --stop-position=998903090 --verbose mysql-bin.000113 (io.debezium.connector.binlog.BinlogStreamingChangeEventSource) [blc-mysql.qifu.svc.cluster.local:3306] 2025-06-24 10:42:42,081 ERROR [mysql-connector-to-etm|task-0] Error during binlog processing. Last offset stored = &#123;transaction_id=6f54a8d9-71a4-11ef-bfb2-00163e105ad2:85818558, file=mysql-bin.000113, pos=998902025, gtids=6f54a8d9-71a4-11ef-bfb2-00163e105ad2:1-85818557, server_id=1, event=2&#125;, binlog reader near position = mysql-bin.000113/998902909 (io.debezium.connector.binlog.BinlogStreamingChangeEventSource) [blc-mysql.qifu.svc.cluster.local:3306] 2025-06-24 10:42:42,082 ERROR [mysql-connector-to-etm|task-0] Producer failure (io.debezium.pipeline.ErrorHandler) [blc-mysql.qifu.svc.cluster.local:3306] io.debezium.DebeziumException: Error processing binlog event at io.debezium.connector.binlog.BinlogStreamingChangeEventSource.handleEvent(BinlogStreamingChangeEventSource.java:591) at io.debezium.connector.binlog.BinlogStreamingChangeEventSource.lambda$execute$17(BinlogStreamingChangeEventSource.java:209) at com.github.shyiko.mysql.binlog.BinaryLogClient.notifyEventListeners(BinaryLogClient.java:1281) at com.github.shyiko.mysql.binlog.BinaryLogClient.listenForEventPackets(BinaryLogClient.java:1103) at com.github.shyiko.mysql.binlog.BinaryLogClient.connect(BinaryLogClient.java:657) at com.github.shyiko.mysql.binlog.BinaryLogClient$7.run(BinaryLogClient.java:959) at java.base/java.lang.Thread.run(Thread.java:840) Caused by: io.debezium.DebeziumException: Encountered change event for table qifu_saas_owms.outbound_ordeOne possible causer whose schema isn't known to this connector at io.debezium.connector.binlog.BinlogStreamingChangeEventSource.informAboutUnknownTableIfRequired(BinlogStreamingChangeEventSource.java:996) at io.debezium.connector.binlog.BinlogStreamingChangeEventSource.informAboutUnknownTableIfRequired(BinlogStreamingChangeEventSource.java:1048) at io.debezium.connector.binlog.BinlogStreamingChangeEventSource.handleUpdateTableMetadata(BinlogStreamingChangeEventSource.java:797) at io.debezium.connector.binlog.BinlogStreamingChangeEventSource.lambda$execute$4(BinlogStreamingChangeEventSource.java:178) at io.debezium.connector.binlog.BinlogStreamingChangeEventSource.handleEvent(BinlogStreamingChangeEventSource.java:571) ... 6 more解决办法：方法1(需要新版本的Debezium，旧版本执行这个命令会报错）：此方法相当于初始化，会再把所有数据同步一遍，数据量大的话耗时很长：停止连接器：1curl -X PUT http://127.0.0.1:8083/connectors/mysql-connector-to-etm/stop重置连接器偏移量：1curl -X DELETE http://127.0.0.1:8083/connectors/mysql-connector-to-etm/offsets方法2：1.删除历史主题或重命名历史主题（schema.history.internal.kafka.topic）2.配置文件修改模式为恢复模式：1snapshot.mode: schema_only_recovery3.更新连接器配置：1kubectl apply -f mysql-connector-to-etm.yaml问题5连接器报错：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263io.debezium.DebeziumException: Error reading MySQL variables: Communications link failure The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server. at io.debezium.connector.binlog.jdbc.BinlogConnectorConnection.querySystemVariables(BinlogConnectorConnection.java:514) at io.debezium.connector.binlog.jdbc.BinlogConnectorConnection.readSystemVariables(BinlogConnectorConnection.java:496) at io.debezium.connector.binlog.jdbc.BinlogConnectorConnection.isTableIdCaseSensitive(BinlogConnectorConnection.java:332) at io.debezium.connector.mysql.MySqlConnectorTask.start(MySqlConnectorTask.java:98) at io.debezium.connector.common.BaseSourceTask.startIfNeededAndPossible(BaseSourceTask.java:403) at io.debezium.connector.common.BaseSourceTask.poll(BaseSourceTask.java:313) at org.apache.kafka.connect.runtime.WorkerSourceTask.poll(WorkerSourceTask.java:305) at org.apache.kafka.connect.runtime.WorkerSourceTask.execute(WorkerSourceTask.java:249) at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188) at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:243) at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264) at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) at java.base/java.lang.Thread.run(Thread.java:833)Caused by: com.mysql.cj.jdbc.exceptions.CommunicationsException: Communications link failure The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server. at com.mysql.cj.jdbc.exceptions.SQLError.createCommunicationsException(SQLError.java:165) at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:55) at com.mysql.cj.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:837) at com.mysql.cj.jdbc.ConnectionImpl.&lt;init&gt;(ConnectionImpl.java:420) at com.mysql.cj.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:238) at com.mysql.cj.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:180) at io.debezium.jdbc.JdbcConnection.lambda$patternBasedFactory$1(JdbcConnection.java:246) at io.debezium.jdbc.JdbcConnection$ConnectionFactoryDecorator.connect(JdbcConnection.java:132) at io.debezium.jdbc.JdbcConnection.establishConnection(JdbcConnection.java:920) at io.debezium.jdbc.JdbcConnection.connection(JdbcConnection.java:904) at io.debezium.jdbc.JdbcConnection.connection(JdbcConnection.java:898) at io.debezium.jdbc.JdbcConnection.query(JdbcConnection.java:553) at io.debezium.jdbc.JdbcConnection.query(JdbcConnection.java:501) at io.debezium.connector.binlog.jdbc.BinlogConnectorConnection.querySystemVariables(BinlogConnectorConnection.java:502) ... 14 moreCaused by: com.mysql.cj.exceptions.CJCommunicationsException: Communications link failure The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server. at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77) at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499) at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480) at com.mysql.cj.exceptions.ExceptionFactory.createException(ExceptionFactory.java:52) at com.mysql.cj.exceptions.ExceptionFactory.createException(ExceptionFactory.java:95) at com.mysql.cj.exceptions.ExceptionFactory.createException(ExceptionFactory.java:140) at com.mysql.cj.exceptions.ExceptionFactory.createCommunicationsException(ExceptionFactory.java:156) at com.mysql.cj.protocol.a.NativeSocketConnection.connect(NativeSocketConnection.java:79) at com.mysql.cj.NativeSession.connect(NativeSession.java:142) at com.mysql.cj.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:961) at com.mysql.cj.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:825) ... 25 moreCaused by: java.net.ConnectException: Connection refused at java.base/sun.nio.ch.Net.pollConnect(Native Method) at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672) at java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(NioSocketImpl.java:549) at java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:597) at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327) at java.base/java.net.Socket.connect(Socket.java:633) at com.mysql.cj.protocol.StandardSocketFactory.connect(StandardSocketFactory.java:144) at com.mysql.cj.protocol.a.NativeSocketConnection.connect(NativeSocketConnection.java:53) ... 28 more解决：mysql连接器配置添加以下配置：123456connect.timeout.ms: 30000connect.retry.interval.ms: 5000connect.max.attempts: 10database.ssl.mode: DISABLEDdatabase.connection.pool.size: 5database.connection.pool.min.idle: 2更新mysql连接器配置：1kubectl apply -f mysql-connector-test.yaml问题6连接器报错：1234567891011121314151617java.lang.IllegalStateException: The database schema history couldn't be recovered. Consider to increase the value for schema.history.internal.kafka.recovery.poll.interval.ms at io.debezium.storage.kafka.history.KafkaSchemaHistory.recoverRecords(KafkaSchemaHistory.java:312) at io.debezium.relational.history.AbstractSchemaHistory.recover(AbstractSchemaHistory.java:100) at io.debezium.relational.history.SchemaHistory.recover(SchemaHistory.java:192) at io.debezium.relational.HistorizedRelationalDatabaseSchema.recover(HistorizedRelationalDatabaseSchema.java:72) at io.debezium.schema.HistorizedDatabaseSchema.recover(HistorizedDatabaseSchema.java:40) at io.debezium.connector.common.BaseSourceTask.validateAndLoadSchemaHistory(BaseSourceTask.java:151) at io.debezium.connector.mysql.MySqlConnectorTask.start(MySqlConnectorTask.java:134) at io.debezium.connector.common.BaseSourceTask.start(BaseSourceTask.java:253) at org.apache.kafka.connect.runtime.WorkerSourceTask.initializeAndStart(WorkerSourceTask.java:226) at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:186) at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:243) at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264) at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) at java.base/java.lang.Thread.run(Thread.java:833)解决办法:添加超时时间字段：1schema.history.internal.kafka.recovery.poll.interval.ms: 5000]]></content>
      <categories>
        <category>技术</category>
        <category>K8S</category>
      </categories>
      <tags>
        <tag>kafka</tag>
        <tag>Debezium</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux机器压测脚本]]></title>
    <url>%2F2025%2F03d01d51b6.html</url>
    <content type="text"><![CDATA[工作中我们可能会遇到需要模拟高并发的情况，就是我们说的压测，以下通过一些shell脚本用于模拟 OOM（内存耗尽）、CPU 高负载、网络高吞吐 和 磁盘高 IO 压力。工具依赖安装确保系统中已安装必要工具：123456# Debian/Ubuntusudo apt-get install stress iperf3# CentOS/RHELsudo yum install epel-releasesudo yum install stress iperf3模拟内存耗尽（OOM）1234567891011121314#!/bin/bash# 文件名：stress_oom.sh# 作用：持续分配内存直到触发 OOM# 设置内存分配块大小（单位：MB，默认 100MB）MEM_BLOCK_SIZE=100echo "[+] 开始内存压力测试，按 Ctrl+C 终止..."while true; do # 分配内存并追加到数组中 echo "分配内存块: $&#123;MEM_BLOCK_SIZE&#125;MB" stress --vm 1 --vm-bytes $&#123;MEM_BLOCK_SIZE&#125;M --vm-keep &amp; sleep 1done使用方式：12chmod +x stress_oom.sh./stress_oom.sh终止条件：系统触发 OOM Killer 或手动终止。模拟 CPU 高负载123456789#!/bin/bash# 文件名：stress_cpu.sh# 作用：让所有 CPU 核心满载# 设置压测时间（单位：秒，默认 600 秒）DURATION=600echo "[+] 开始 CPU 压力测试，持续 $&#123;DURATION&#125; 秒..."stress --cpu $(nproc) --timeout $&#123;DURATION&#125;使用方式：12chmod +x stress_cpu.sh./stress_cpu.sh说明：使用 stress 工具压满所有 CPU 核心。模拟网络高吞吐12345678910111213#!/bin/bash# 文件名：stress_network.sh# 作用：生成网络流量（需安装 iperf3）# 参数配置DURATION=300 # 测试时间（秒）SERVER_IP="192.168.1.100" # 替换为目标服务器 IPecho "[+] 启动 iperf3 服务端（在目标机器执行）："echo " iperf3 -s"echo "[+] 开始网络压力测试，目标 IP: $&#123;SERVER_IP&#125;..."iperf3 -c $&#123;SERVER_IP&#125; -t $&#123;DURATION&#125; -P 10 # 10 个并行连接使用方式：在目标机器启动服务端：1iperf3 -s在压测机器运行：12chmod +x stress_network.sh./stress_network.sh模拟磁盘高 IO12345678910111213141516#!/bin/bash# 文件名：stress_disk.sh# 作用：生成磁盘读写压力# 参数配置DURATION=300 # 测试时间（秒）IO_THREADS=4 # 并发 IO 线程数FILE_SIZE="10G" # 单个测试文件大小echo "[+] 开始磁盘压力测试，持续 $&#123;DURATION&#125; 秒..."stress --io $&#123;IO_THREADS&#125; --timeout $&#123;DURATION&#125; &amp;# 同时使用 dd 写入大文件dd if=/dev/urandom of=/tmp/stress_disk.tmp bs=1M count=10240 oflag=direct &amp;sleep $&#123;DURATION&#125;killall stress dd # 结束后清理rm -f /tmp/stress_disk.tmp使用方式：12chmod +x stress_disk.sh./stress_disk.sh注意事项风险提示：OOM 测试可能导致系统崩溃或进程被杀死。磁盘测试可能损坏硬件，避免在无冗余的硬盘上执行。网络测试需确保目标 IP 可达且有足够带宽。监控资源：12345# 实时监控工具htop # CPU/内存iotop # 磁盘 IOiftop # 网络流量终止测试：按 Ctrl+C 终止脚本，或使用 killall stress dd iperf3。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>压测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RocketMQ使用教程]]></title>
    <url>%2F2025%2F0396717edc.html</url>
    <content type="text"><![CDATA[之前一直用的是kafka，新公司使用的是RocketMQ，之前没接触过RocketMQ，导致告警出现消息积压，不知怎么去排查问题，趁着这次排查问题，顺便熟悉了下RocketMQ的操作。先来熟悉下RocketMQ的控制台可视化界面：使用教程可视化界面更改后如下：主题（Topic）1、假如说生产者发送了10条消息，此时消费者服务没有启动，因此就无法被消费，那么此时的积压的10条消息在哪看呢？2、点击主题(Topic)，找到我们发消息时的Topic，然后点击CONSUMER管理选项如下：3、生产者发送10条消息，看下上面的差值会有什么变化上图可看出消息积压的数量4、如果消息消费失败，可以重置消费位点： 5、 如果消息不重要，可以跳过消息积压：消息（Message）1、 接下来，如何才能看出一条消息是否被消费呢?2、进入消息3、选择对应的topic，进行搜索4、选择一条消息，点击MESSAGE DETAIL，进入之后往下拉，如下：通过trackType可以看出是否被消费12345NOT_ONLINE 代表该Consumer没有运行CONSUMED 代表该消息已经被消费NOT_CONSUME_YET 还没被消费UNKNOW_EXCEPTION 报错CONSUMED_BUT_FILTERED 消费了，但是被过滤了，一般是被tag过滤了而上图中trackType的值为NOT_ONLINE，说明消费者没有运行，也就是未被消费问题排查告警群出现告警，有消息积压：到Grafana查看，发现确实有消息积压：到RocketMQ控制台查看消息消费情况，发现告警的消费组不在线：导致消息积压：到消费者查询这个群组查不到：研发告知创建了新的消费者群组，旧的没用就删掉了，但是没删掉对应群组的重试，死信主题： 所以导致了消息积压：把对应消费组的重试主题删除后，再查看，发现消息积压没了： 总结：重试主题会在消费组消费失败时自动创建（死信主题需手动创建），当消息在重试主题中达到最大重试次数仍失败时，RocketMQ 会将消息转移到死信主题，防止消息无限循环重试。旧的消费组不使用了，需要删除的时候，记得查询一下是否有对应的重试，死信主题，把对应的重试，死信主题也删掉。]]></content>
      <categories>
        <category>技术</category>
        <category>RocketMQ</category>
      </categories>
      <tags>
        <tag>RocketMQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s-pod重启时生成dump文件]]></title>
    <url>%2F2025%2F03ba513123.html</url>
    <content type="text"><![CDATA[k8s环境，有个服务频繁重启，经过排查日志和事件，确认是由于OOM导致服务重启，为了方便研发定位OOM的具体原因，需要在OOM发生时自动生成内存快照（Heap Dump），以供后续研发分析。 方法一：JVM参数JVM是提供了一些参数，能在发生OOM时自动生成Heap Dump：-XX:+HeapDumpOnOutOfMemoryError-XX:HeapDumpPath=/app/oom/java_heapdump.hprof然而，目前存在问题：多次重启会导致heap dump文件覆盖，例如，服务发生了两次OOM，第二次生成的heap dump会覆盖第一次的。dump文件保存在容器内，如果没有把目录挂载到宿主机的话，容器重启文件丢失。所以如果只是单次分析OOM的原因，可以使用JVM的参数，在java启动参数里加上 -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/app/oom/java_heapdump.hprof，再把容器内的/app/oom/目录挂载到node上即可。12345678volumes: - name: dump-file hostPath: path: /opt/dump/oom type: '' volumeMounts: - name: dump-file mountPath: /app/oom方法二：JVM参数+脚本为了解决以上文件覆盖以及文件丢失的问题，可以写一个脚本，在发生OOM时把dump文件传到sso或者minio，在这里使用自己部署的minio，具体方法如下：部署minio服务端这里使用docker-compose来部署：docker-compose.yaml：1234567891011121314151617version: '3'services: minio: image: registry.cn-shenzhen.aliyuncs.com/hxlk8s/minio:RELEASE.2023-03-20T20-16-18Z hostname: "minio" ports: - 9000:9000 # api 端口 - 9001:9001 # 控制台端口 environment: MINIO_ACCESS_KEY: admin #管理后台用户名 MINIO_SECRET_KEY: admin123 #管理后台密码，最小8个字符 volumes: - ./data:/data #映射当前目录下的data目录至容器内/data目录 - ./config:/root/.minio/ #映射配置目录 command: server --console-address ':9001' /data #指定容器中的目录 /data privileged: true restart: always启动：1docker-compose up -d使用NGINX反向代理：123456789101112131415server &#123; listen 8080; server_name minio.qifu.com; location / &#123; proxy_pass http://10.168.2.236:9001; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection "upgrade"; &#125;&#125;访问web界面：编写shell脚本编写shell脚本，在pod发生OOM时，执行脚本，把dump文件上传到minio，并告警到企微群：oom.sh：123456789101112131415161718192021#！/bin/bashMINIO_URL="http://10.168.2.236:9000" MINIO_USER="oomdump"MINIO_PASSWORD="0TYyt2p7.U12454"SW_AGENT_NAME=$(cat /app/config/bootstrap.yml | grep name | head -n 1 | cut -d : -f 2 | xargs)MINIO_DUMPS_PATH=minio/oomdumps/test/$SW_AGENT_NAME/dump-$(date +'%Y-%m-%d_%H%M%S').hprofmc alias set minio $MINIO_URL $MINIO_USER $MINIO_PASSWORDmc cp /app/dump.hprof $MINIO_DUMPS_PATHwx_alert()&#123; echo "发送OOM告警中!" msg="## &lt;font color=red size=22&gt;【JVM OOM告警】&lt;/font&gt; \n -----\n **应用**: *$&#123;1&#125;* \n **环境**: *test* \n **时间**: *$(date '+%Y-%m-%d %H:%M:%S')* \n 测试环境的【$&#123;1&#125;】发生OOM，相关dump文件已上传到minio的【'$&#123;2&#125;'】路径下，请及时查看！！ \n &lt;@chenmingchang&gt;" webHookUrl="https://qyapi.weixin.qq.com/cgi-bin/webhook/send?key=********" content='&#123;"msgtype": "markdown","markdown": &#123;"content": "'$msg'","mentioned_mobile_list":['$3']&#125;&#125;' curl -H 'Content-Type: application/json' -d "$content" $webHookUrl echo "成功发送OOM告警!"&#125;wx_alert "$SW_AGENT_NAME" "$MINIO_DUMPS_PATH"添加JVM参数启动应用时需要添加以下参数：1234-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/opt/run/1.hprof -XX:+ExitOnOutOfMemoryError -XX:OnOutOfMemoryError=/opt/run/oom.sh参数解释：1234567-XX:+HeapDumpOnOutOfMemoryError ： 当发生内存溢出错误（Out of Memory Error）时，将会生成一个堆转储文件（Heap Dump）。堆转储文件是一个二进制文件，记录了 JVM 堆中对象的详细信息，可以用于分析内存使用情况和调试。使用该选项启用堆转储功能。-XX:HeapDumpPath=/opt/run/1.hprof ： 指定堆转储文件的保存路径和文件名。在这个示例中，堆转储文件将保存在 /opt/run 目录下，并命名为 1.hprof 。-XX:+ExitOnOutOfMemoryError ： 当发生内存溢出错误时，JVM 将直接退出。通常情况下，JVM 在发生内存溢出错误后会尝试继续执行程序，但可能会导致未定义行为或进一步的错误。使用该选项可以使 JVM 在出现内存溢出错误时立即退出。-XX:OnOutOfMemoryError=/opt/run/oom.sh ： 当发生内存溢出错误时，执行指定的 shell 脚本。在这个示例中，指定的脚本是 /opt/run/oom.sh ，可以根据需要自定义这个脚本。这个特性可以让在内存溢出错误发生时执行一些自定义的操作，例如发送通知、记录日志等。下载minio客户端123wget https://dl.min.io/client/mc/release/linux-amd64/mcchmod +x mcmv mc /usr/local/sbin/测试让开发写个死循环去请求，触发OOM：告警：dump文件：方法三：preStop钩子为了解决k8s重启时无法导出heap dump和二次覆盖的问题，我们可以通过配置preStop钩子，在容器停止前生成内存快照。相关命令如下：获取进程ID为10的程序的堆栈信息：1jstack -F 10 &gt;&gt; /logs/thread.dump生成堆内存快照：1jmap -dump:format=b,file=/usr/src/logs/dump.hprof 10通过这样的优化，既能避免heap dump被覆盖，又能在k8s重启时生成有用的内存快照，帮助排查问题。命令：1jstack -F $(jps |grep -v Jps | awk '&#123;print $1&#125;') | tee -a /usr/src/logs/thread.dump &amp;&amp; jmap -dump:format=b,file=/usr/src/logs/$(date +'%Y-%m-%d_%H%M%S').hprof $(jps |grep -v Jps | awk '&#123;print $1&#125;')"pod的yaml文件配置加上以下配置：1234lifecycle: preStop: exec: command: ["/bin/sh", "-c", "jstack -F $(jps |grep -v Jps | awk '&#123;print $1&#125;') | tee -a /usr/src/logs/thread.dump &amp;&amp; jmap -dump:format=b,file=/usr/src/logs/$(date +'%Y-%m-%d_%H%M%S').hprof $(jps |grep -v Jps | awk '&#123;print $1&#125;')"]验证手动执行kubectl delete pod，可以查看到生成的文件：两个命令的区别：]]></content>
      <categories>
        <category>技术</category>
        <category>K8S</category>
      </categories>
      <tags>
        <tag>OOM</tag>
        <tag>dump</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux系统中负载、磁盘IO问题排查]]></title>
    <url>%2F2025%2F02ba513123.html</url>
    <content type="text"><![CDATA[CPU利用率：显示的是程序在运行期间实时占用的CPU百分比CPU负载：显示的是一段时间内正在使用和等待使用CPU的平均任务数。CPU利用率高，并不意味着负载就一定大。举例来说：如果我有一个程序它需要一直使用CPU的运算功能，那么此时CPU的使用率可能达到100%，但是CPU的工作负载则是趋近于“1”，因为CPU仅负责一个工作嘛！如果同时执行这样的程序两个呢？CPU的使用率还是100%，但是工作负载则变成2了。所以也就是说，当CPU的工作负载越大，代表CPU必须要在不同的工作之间进行频繁的工作切换。我们知道：平均负载是指单位时间内，处在可执行状态和不可中断睡眠状态的进程的平均数。也就是说，它包括了处在执行态，阻塞态和就绪态的进程。CPU使用率是指在单位时间内CPU处于非空闲状态的时间比，反映了CPU的繁忙程度。例如：单核CPU单位时间内非空闲态运行时间为0.8s，那么他的CPU使用率为80%；双核CPU单位时间内非空闲态运行时间分别为0.4s和0.6s，那么它的CPU使用率为（0.4+0.6）/2*100%=50%我们再举个更生动的例子： 有一家银行，他只有一个业务窗口，每次只能接待一个人（单核CPU）。有一天一共有五个人来了，那么就会出现一人在办理手续，其余四人在等待的情况（CPU负载为5） 我们约定在业务窗口的那个人只有真正在办理业务才算是真正使用（CPU使用率）如下图了解了负载与CPU使用率的关系之后，我们来聊聊什么情况下会导致负载上升以及平均负载和CPU使用率的关系CPU 密集型进程，使用大量 CPU 会导致平均负载升高，此时这两者是一致的；I/O 密集型进程，等待 I/O 也会导致平均负载升高，但 CPU 使用率不一定很高；大量等待 CPU 的进程调度也会导致平均负载升高，此时的 CPU 使用率也会比较高。Load分析：情况1：CPU高、Load高1.通过top命令查找占用CPU最高的进程PID；2.通过top -Hp PID查找占用CPU最高的线程TID;3.对于java程序，使用jstack打印线程堆栈信息（可联系业务进行排查定位）；4.通过printf %x tid打印出最消耗CPU线程的十六进制；5.在堆栈信息中查看该线程的堆栈信息；情况2：CPU低、Load高（此情况出现几率很大）1.通过top命令查看CPU等待IO时间，即%wa；2.通过iostat -d -x -m 1 10查看磁盘IO情况；(安装命令 yum install -y sysstat)3.通过sar -n DEV 1 10查看网络IO情况；4.通过如下命令查找占用IO的程序:1ps -e -L h o state,cmd | awk '&#123;if($1=="R"||$1=="D")&#123;print $0&#125;&#125;' | sort | uniq -c | sort -k 1nrCPU高、Load高情况分析使用 vmstat 查看系统纬度的 CPU 负载可以通过 vmstat 从系统维度查看 CPU 资源的使用情况格式：vmstat -n 1 表示结果一秒刷新一次123456[root@k8s-master01 ~]# vmstat -n 1procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 0 0 0 24718776 290068 13277172 0 0 2 398 0 0 3 1 96 1 0 0 0 0 24716664 290068 13275600 0 0 0 4564 10581 23766 2 1 96 1 0 0 0 0 24717244 290068 13275728 0 0 12 1324 9613 21440 2 1 96 0 0返回结果中的主要数据列说明：123456r： 表示系统中 CPU 等待处理的线程。由于 CPU 每次只能处理一个线程，所以，该数值越大，通常表示系统运行越慢。b： 表示阻塞的进程,这个不多说，进程阻塞，大家懂的。us： 用户CPU时间，我曾经在一个做加密解密很频繁的服务器上，可以看到us接近100,r运行队列达到80(机器在做压力测试，性能表现不佳)。sy： 系统CPU时间，如果太高，表示系统调用时间长，例如是IO操作频繁。wa：IO 等待消耗的 CPU 时间百分比。该值较高时，说明 IO 等待比较严重，这可能磁盘大量作随机访问造成的，也可能是磁盘性能出现了瓶颈。id：处于空闲状态的 CPU 时间百分比。如果该值持续为 0，同时 sy 是 us 的两倍，则通常说明系统则面临着 CPU 资源的短缺。使用 top 查看进程纬度的 CPU 负载可以通过 top 从进程纬度来查看其 CPU、内存等资源的使用情况:默认界面上第三行会显示当前 CPU 资源的总体使用情况，下方会显示各个进程的资源占用情况。可以直接在界面输入大小字母 P，来使监控结果按 CPU 使用率倒序排列，进而定位系统中占用 CPU 较高的进程。最后，根据系统日志和程序自身相关日志，对相应进程做进一步排查分析，以判断其占用过高 CPU 的原因。strace命令分析1strace -tt -T -v -f -e trace=file -o /data/log/strace.log -s 1024 -p 2348912345678-tt：在每行输出的前面，显示毫秒级别的时间-T：显示每次系统调用所花费的时间-v：对于某些相关调用，把完整的环境变量，文件 stat 结构等打出来。-f：跟踪目标进程，以及目标进程创建的所有子进程-e：控制要跟踪的事件和跟踪行为，比如指定要跟踪的系统调用名称-o：把 strace 的输出单独写到指定的文件-s：当系统调用的某个参数是字符串时，最多输出指定长度的内容，默认是 32 个字节-p：指定要跟踪的进程 pid，要同时跟踪多个 pid，重复多次 -p 选项即可。CPU低、Load高情况分析问题描述：Linux 系统没有业务程序运行，通过 top 观察，CPU 很空闲，但是 load average 却非常高。处理办法：load average 是对 CPU 负载的评估，其值越高，说明其任务队列越长，处于等待执行的任务越多。出现此种情况时，可能是由于僵死进程导致的。可以通过指令ps -axjf查看是否存在 D 状态进程。D 状态是指不可中断的睡眠状态。该状态的进程无法被 kill，也无法自行退出。只能通过恢复其依赖的资源或者重启系统来解决。等待I/O的进程通过处于uninterruptible sleep或D状态，通过给出这些信息我们就可以简单的查找出处在wait状态的进程1ps -eo state,pid,cmd | grep "^D"; echo "----"查找占用IO的程序：1ps -e -L h o state,cmd | awk '&#123;if($1=="R"||$1=="D")&#123;print $0&#125;&#125;' | sort | uniq -c | sort -k 1nr案例分析磁盘I/O， %util特别高环境复现环境配置：本次测试使用128C_512G_4TSSD服务器配置，MySQL版本为8.0.27场景模拟：使用sysbench创建5个表，每个表2亿条数据，执行产生笛卡尔积查询的sql语句，产生io,可以模拟业务压力。系统层面底层故障排查1shell&gt; sysbench --test=/usr/local/share/sysbench/oltp_insert.lua --mysql-host=XXX --mysql-port=3306 --mysql-user=pcms --mysql-password=abc123 --mysql-db=sysbench --percentile=99 --table-size=2000000000 --tables=5 --threads=1000 prepare使用sysbench进行模拟高并发1shell&gt; sysbench --test=/usr/local/share/sysbench/oltp_write_only.lua --mysql-host=xxx --mysql-port=3306 --mysql-user=pcms --mysql-password=abc123 --mysql-db=sysbench --percentile=99 --table-size=2000000000 --tables=5 --threads=1000 --max-time=60000 --report-interval=1 --threads=1000 --max-requests=0 --mysql-ignore-errors=all run执行笛卡尔积sql语句1mysql&gt; select SQL_NO_CACHE b.id,a.k from sbtest_a a left join sbtest_b b on a.id=b.id group by a.k order by b.c desc;检查当前服务器状态123456shell&gt; toptop - 19:49:05 up 10 days, 8:16, 2 users, load average: 72.56, 40.21, 17.08Tasks: 1288 total, 1 running, 586 sleeping, 0 stopped, 0 zombie%Cpu(s): 19.7 us, 4.2 sy, 0.0 ni, 75.9 id, 1.0 wa, 0.0 hi, 0.2 si, 0.0 stKiB Mem : 53542118+total, 23667507+free, 22735366+used, 71392448 buff/cacheKiB Swap: 0 total, 0 free, 0 used. 23128006+avail Mem由上可知：目前一分钟负载为72.56，且呈上升趋势，并且存在io压力查看当前各个磁盘设备的io情况12345678shell&gt; iostat -m -x 1Linux 4.14.0-115.el7a.0.1.aarch64 (mysql-4) 01/08/2022 _aarch64_ (128 CPU)Device: rrqm/s wrqm/s r/s w/s rMB/s wMB/s avgrq-sz avgqu-sz await r_await w_await svctm %utilsda 0.00 0.00 14213.00 27430.00 222.08 465.15 33.80 5.39 0.13 0.14 0.12 0.02 86.00sdb 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00dm-0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00dm-1 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00dm-2 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00由上可知：目前有多块物理磁盘，sda磁盘的io压力较大检查sda磁盘当前的io读写情况12345678910shell&gt; iostat -d /dev/sda -m -x 1Linux 4.14.0-115.el7a.0.1.aarch64 (mysql-4) 01/08/2022 _aarch64_ (128 CPU)Device: rrqm/s wrqm/s r/s w/s rMB/s wMB/s avgrq-sz avgqu-sz await r_await w_await svctm %utilsda 0.00 0.17 7.86 81.23 0.29 3.96 97.88 0.23 2.53 0.22 2.76 0.04 0.33Device: rrqm/s wrqm/s r/s w/s rMB/s wMB/s avgrq-sz avgqu-sz await r_await w_await svctm %utilsda 0.00 0.00 21109.00 42839.00 329.81 710.90 33.33 19.47 0.30 0.16 0.37 0.02 96.00Device: rrqm/s wrqm/s r/s w/s rMB/s wMB/s avgrq-sz avgqu-sz await r_await w_await svctm %utilsda 0.00 0.00 19287.00 41404.00 301.36 692.29 33.53 15.73 0.26 0.18 0.30 0.02 93.00Device: rrqm/s wrqm/s r/s w/s rMB/s wMB/s avgrq-sz avgqu-sz await r_await w_await svctm %utilsda 0.00 0.00 22135.00 43044.00 345.86 1165.18 47.48 100.87 1.55 0.20 2.24 0.01 97.00由上可知：目前sda磁盘的压力比较大，每秒写入比每秒读差距较大，证明目前有大量的io写入检查sda磁盘中哪个应用程序占用的io比较高12345678shell&gt; pidstat -d 1Linux 4.14.0-115.el7a.0.1.aarch64 (mysql-4) 01/08/2022 _aarch64_ (128 CPU)08:01:43 PM UID PID kB_rd/s kB_wr/s kB_ccwr/s Command08:01:44 PM 1000 73739 62018.35 171346.79 0.00 mysqld08:01:44 PM UID PID kB_rd/s kB_wr/s kB_ccwr/s Command08:01:45 PM 1000 73739 145328.00 435216.00 0.00 mysqld08:01:45 PM UID PID kB_rd/s kB_wr/s kB_ccwr/s Command08:01:46 PM 1000 73739 141488.00 433232.00 0.00 mysqld由上可知：占用io高的应用程序是mysql，且pid为73739分析应用程序中哪一个线程占用的io比较高123456shell&gt; pidstat -dt -p 73739 1 执行两三秒即可Average: 1000 - 73823 0.00 233133.98 0.00 |__mysqldAverage: 1000 - 74674 0.00 174291.26 0.00 |__mysqld11:56:18 PM 1000 - 74770 124928.00 74688.00 0.00 |__mysqld11:56:17 PM 1000 - 74770 124603.77 73358.49 0.00 |__mysqldAverage: 1000 - 74770 124761.17 74003.88 0.00 |__mysqld由上可知：74770这个线程占用的io比较高分析这个线程在干什么？123456789101112131415Shell&gt; perf trace -t 74770 -o /tmp/tmp_aa.pstraceShell&gt; cat /tmp/tmp_aa.pstrace2850.656 ( 1.915 ms): futex(uaddr: 0x653ae9c4, op: WAIT|PRIVATE_FLAG, val: 1) = 02852.572 ( 0.001 ms): futex(uaddr: 0x653ae990, op: WAKE|PRIVATE_FLAG, val: 1) = 02852.601 ( 0.001 ms): clock_gettime(which_clock: MONOTONIC, tp: 0xfff7bd470f68) = 02852.690 ( 0.040 ms): write(fd: 159, buf: 0xd7a30020, count: 65536) = 655362852.796 ( 0.001 ms): clock_gettime(which_clock: MONOTONIC, tp: 0xfff7bd470f68) = 02852.798 ( 0.001 ms): clock_gettime(which_clock: MONOTONIC, tp: 0xfff7bd470f58) = 02852.939 ( 0.001 ms): clock_gettime(which_clock: MONOTONIC, tp: 0xfff7bd470f38) = 02852.950 ( 0.001 ms): clock_gettime(which_clock: MONOTONIC, tp: 0xfff7bd470f68) = 02852.977 ( 0.001 ms): clock_gettime(which_clock: MONOTONIC, tp: 0xfff7bd470f68) = 02853.029 ( 0.035 ms): write(fd: 64, buf: 0xcd51e020, count: 65536) = 655362853.164 ( 0.001 ms): clock_gettime(which_clock: MONOTONIC, tp: 0xfff7bd470f68) = 02853.167 ( 0.001 ms): clock_gettime(which_clock: MONOTONIC, tp: 0xfff7bd470f58) = 02853.302 ( 0.001 ms): clock_gettime(which_clock: MONOTONIC, tp: 0xfff7bd470f38) = 0由上可知：目前这个线程在写入多个文件，fd为文件句柄，文件句柄号有64、159查看这个文件句柄是什么1234shell&gt; lsof -p 73739|grep 159umysqld 73739 mysql 159u REG 8,0 212143246 7046482357 /mysql/mysqldata/16320fff-5fd5-4c47-889a-a9e1a8591d0d/tmp/#7046482357 (deleted)[root@mysql-4 ~]# lsof -p 73739|grep 64umysqld 73739 mysql 64u REG 8,0 211872724 6979323031 /mysql/mysqldata/16320fff-5fd5-4c47-889a-a9e1a8591d0d/tmp/#6979323031 (deleted)由上可知：这个线程在大量的写入临时文件分析MySQL应用程序查看当前的会话列表12mysql&gt; select * from information_schema.processlist where command !='sleep';| 9 | pcms | 172.16.76.12:57596 | sysbench | Query | 67 | executing | select SQL_NO_CACHE b.id,a.k from sbtest_a a left join sbtest_b b on a.id=b.id group by a.k order by b.c desc | 66477 | 0 | 0 |由上可知：目前这个sql已经执行了67s，且此sql使用了group by和order by，必然会产生io通过线程号查询会话123456789101112131415161718192021mysql&gt; select * from threads where thread_os_id=74770\G;*************************** 1. row ***************************THREAD_ID: 95NAME: thread/sql/one_connectionTYPE: FOREGROUNDPROCESSLIST_ID: 9PROCESSLIST_USER: pcmsPROCESSLIST_HOST: 172.16.76.12PROCESSLIST_DB: sysbenchPROCESSLIST_COMMAND: QueryPROCESSLIST_TIME: 91PROCESSLIST_STATE: NULLPROCESSLIST_INFO: select SQL_NO_CACHE b.id,a.k from sbtest_a a left join sbtest_b b on a.id=b.id group by a.k order by b.c descPARENT_THREAD_ID: 1ROLE: NULLINSTRUMENTED: YESHISTORY: YESCONNECTION_TYPE: TCP/IPTHREAD_OS_ID: 74770RESOURCE_GROUP: USR_default1 row in set (0.00 sec)由上可知：通过查询threads表可以进行验证，该线程在频繁创建临时表的原因就来源于此sql查看该sql语句的执行计划，进行进一步认证12345678910111213141516171819202122232425262728mysql&gt; explain select SQL_NO_CACHE b.id,a.k from sbtest_a a left join sbtest_b b on a.id=b.id group by a.k order by b.c desc\G;*************************** 1. row ***************************id: 1select_type: SIMPLEtable: apartitions: NULLtype: ALLpossible_keys: NULLkey: NULLkey_len: NULLref: NULLrows: 1filtered: 100.00Extra: Using temporary; Using filesort*************************** 2. row ***************************id: 1select_type: SIMPLEtable: bpartitions: NULLtype: eq_refpossible_keys: PRIMARYkey: PRIMARYkey_len: 4ref: sysbench.a.idrows: 1filtered: 100.00Extra: NULL2 rows in set, 2 warnings (0.00 sec)由上可知：该sql的执行计划用到了临时表及临时文件，符合查看全局状态进一步进行确认12345678mysql&gt; show global status like '%tmp%';+-------------------------+-------+| Variable_name | Value |+-------------------------+-------+| Created_tmp_disk_tables | 3 || Created_tmp_files | 22 || Created_tmp_tables | 8 |+-------------------------+-------+多执行几次，可以看出tmp_files和tmp_disk_tables的值在增长，证明在大量的创建临时文件及磁盘临时表，符合该线程的行为故障处理通过上述的一系列排查，我们已经分析出来：目前sda磁盘的io使用率最高，且mysqld程序占用的最多。通过排查有一个线程在频繁的创建临时表或临时文件且通过登录mysql排查会话及线程视图可以找到是由某一个慢sql导致的。查看此慢sql的执行计划也会创建临时表和临时文件符合我们之前排查的预期。此时我们就需要针对此慢sql进行优化。慢sql优化完成后可以进行io的继续观察，看io是否有下降。代码分析我们可以使用pstack进行跟踪线程号，获取当前的线程堆栈信息。切记pstack会调用gdb进行debug调试1234567891011121314151617181920212223242526272829303132333435363738394041shell&gt; pstack 74770 &gt;/tmp/74770.pstackThread 1 (process 74770):#0 ha_innobase::general_fetch (this=0xea654228, buf=0xea662028 "\212t\317\030\002", direction=1, match_mode=0 ) at /builds/naiwei.fang/percona-server/storage/innobase/handler/ha_innodb.cc:11159#1 0x0000000000d9913c in handler::ha_rnd_next (this=0xea654228, buf=0xea662028 "\212t\317\030\002") at /builds/naiwei.fang/percona-server/sql/handler.cc:3173#2 0x0000000000f77db0 in TableScanIterator::Read (this=0xd256d5e8) at /builds/naiwei.fang/percona-server/sql/row_iterator.h:208#3 0x000000000124c714 in WriteRowsToChunks (xxhash_seed=899339, write_to_build_chunk=true, write_rows_with_null_in_join_key=false, join_key_buffer=0xd01fdb98, tables_to_get_rowid_for=0, chunks=0xd01fdb58, join_conditions=..., tables=..., iterator=0xd256d5e8, thd=0xdb888000) at /builds/naiwei.fang/percona-server/sql/hash_join_iterator.cc:282#4 HashJoinIterator::BuildHashTable (this=this@entry=0xd01fd028) at /builds/naiwei.fang/percona-server/sql/hash_join_iterator.cc:495#5 0x000000000124c8ac in Init (this=0xd01fd028) at /builds/naiwei.fang/percona-server/sql/hash_join_iterator.cc:203#6 HashJoinIterator::Init (this=0xd01fd028) at /builds/naiwei.fang/percona-server/sql/hash_join_iterator.cc:145#7 0x00000000010eca14 in Query_expression::ExecuteIteratorQuery (this=0xdec3a8b8, thd=thd@entry=0xdb888000) at /builds/naiwei.fang/percona-server/sql/sql_union.cc:1224#8 0x00000000010ecccc in Query_expression::execute (this=this@entry=0xdec3a8b8, thd=thd@entry=0xdb888000) at/builds/naiwei.fang/percona-server/sql/sql_union.cc:1284#9 0x0000000001083db0 in Sql_cmd_dml::execute_inner (this=0xd256bcb0, thd=0xdb888000) at /builds/naiwei.fang/percona-server/sql/sql_select.cc:791#10 0x000000000108cac8 in Sql_cmd_dml::execute (this=0xd256bcb0, thd=0xdb888000) at /builds/naiwei.fang/percona-server/sql/sql_select.cc:575#11 0x00000000010384e8 in mysql_execute_command (thd=thd@entry=0xdb888000, first_level=first_level@entry=true)at /builds/naiwei.fang/percona-server/sql/sql_parse.cc:4677#12 0x000000000103b314 in dispatch_sql_command (thd=thd@entry=0xdb888000, parser_state=parser_state@entry=0xfff7bd4735b0, update_userstat=update_userstat@entry=false) at /builds/naiwei.fang/percona-server/sql/sql_parse.cc:5273#13 0x000000000103ccf0 in dispatch_command (thd=thd@entry=0xdb888000, com_data=0xffffb467c4d0, com_data@entry=0xfff7bd474640, command=COM_QUERY) at /builds/naiwei.fang/percona-server/sql/sql_parse.cc:1938#14 0x000000000103da40 in do_command (thd=thd@entry=0xdb888000) at /builds/naiwei.fang/percona-server/sql/sql_parse.cc:1386#15 0x0000000001152ca8 in handle_connection (arg=arg@entry=0xda53ab10) at /builds/naiwei.fang/percona-server/sql/conn_handler/connection_handler_per_thread.cc:307#16 0x00000000022bc3ec in pfs_spawn_thread (arg=&lt;optimized out&gt;) at /builds/naiwei.fang/percona-server/storage/perfschema/pfs.cc:2899#17 0x0000ffffb43c7c48 in start_thread () from /lib64/libpthread.so.0#18 0x0000ffffb3c0f600 in thread_start () from /lib64/libc.so.6linux服务器磁盘IO持续飙高排查问题发现12告警通知服务器的磁盘IO持续飙高，我们首先登录服务器，安装工具yum -y install sysstat 进行安装iostat命令123456789101112131415cpu属性值说明：%user：CPU处在用户模式下的时间百分比。%nice：CPU处在带NICE值的用户模式下的时间百分比。%system：CPU处在系统模式下的时间百分比。%iowait：CPU等待输入输出完成时间的百分比。如该值过高，表示硬盘存在I/O瓶颈%steal：管理程序维护另一个虚拟处理器时，虚拟CPU的无意识等待时间百分比。%idle：CPU空闲时间百分比。如该值高，表示CPU较空闲，如该值高但系统响应慢时，可能是CPU等待分配内存，应加大内存容量。如该值持续低于10，表明CPU处理能力相对较低，系统中最需要解决的资源是CPU tps：该设备每秒的传输次数kB_read/s：每秒从设备（drive expressed）读取的数据量；kB_wrtn/s：每秒向设备（drive expressed）写入的数据量；kB_read： 读取的总数据量；kB_wrtn：写入的总数量数据量；12345678910111213141516171819由上图可知，vdb磁盘的 %util【IO】几乎都在100%，原因是频繁的读取数据造成的 Device：设备名称rrqm/s：每秒合并到设备的读请求数。即delta(rmerge)/s wrqm/s：每秒合并到设备的写入请求数。即delta(wmerge)/s r/s：每秒完成的读I/O设备次数。即delta(rio)/s w/s：每秒完成的写I/0设备次数。即delta(wio)/s rkB/s: 每秒读K字节数。是 rsect/s 的一半，因为每扇区大小为512字节。(需要计算)wkB/s: 每秒写K字节数。是 wsect/s 的一半。(需要计算)avgrq-sz：平均每次设备I/O操作的数据量(扇区为单位)。即delta(rsec+wsec)/delta(rio+wio) avgqu-sz：平均每次发送给设备的I/O队列长度。await：平均每次IO请求等待时间。(包括等待队列时间和处理时间，毫秒为单位)r_await：平均每次IO读请求等待时间。(包括等待队列时间和处理时间，毫秒为单位)w_await：平均每次IO写请求等待时间。(包括等待队列时间和处理时间，毫秒为单位)svctm：平均每次设备I/O操作的处理时间(毫秒)。警告！不要再相信这个字段值，这个字段将在将来的sysstat版本中删除。 %util：一秒中有百分之多少的时间用于I/O操作，或者说一秒中有多少时间I/O队列是非空的。当该值接近100%时，设备饱和发生。 rsec/s (rkB/s, rMB/s)：每秒读取设备的扇区数(千字节、兆字节)。每扇区大小为512字节wsec/s (wkB/s, wMB/s)：每秒写入设备的扇区数(千字节、兆字节)。每扇区大小为512字节使用iotop命令找到IO占用高的进程12iotop -oP展示I/O统计，每秒更新一次可看到比较详细信息，如：进程号，磁盘读取量，磁盘写入量，IO百分比，涉及到的命令是什么等等pidstat 命令12pidstat -d 1 检查哪个程序占用了IO可见其中 mongod 命令占用了大量的读IO，之后可根据 PID 查看相关进程信息]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>磁盘IO</tag>
        <tag>CPU负载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s-pod生命周期钩子函数]]></title>
    <url>%2F2025%2F025914afe5.html</url>
    <content type="text"><![CDATA[pod从开始创建到终止退出的时间范围称为Pod生命周期pod生命周期生命周期包含以下几个重要流程：1234567初始化容器（initContainers）创建主容器（containers）是必须的操作容器启动后钩子启动探测存活性探测就绪性探测容器停止前钩子pod在整个生命周期的过程中总会处于以下几个状态：123456Pending：创建了pod资源并存入etcd中，但尚未完成调度。ContainerCreating：Pod 的调度完成，被分配到指定 Node 上。处于容器创建的过程中。通常是在拉取镜像的过程中。Running：Pod 包含的所有容器都已经成功创建，并且成功运行起来。Succeeded：Pod中的所有容器都已经成功终止并且不会被重启Failed：所有容器都已经终止，但至少有一个容器终止失败，也就是说容器返回了非0值的退出状态或已经被系统终止。Unknown：因为某些原因无法取得 Pod 的状态。这种情况通常是因为与 Pod 所在主机通信失败。pod生命周期的重要行为1234567891011121314151、在启动任何容器之前，先创建pause基础容器，它初始化Pod的环境并为后续加⼊的容器提供共享的名称空间。2.初始化容器（initcontainer）：一个pod可以拥有任意数量的init容器。init容器是按照顺序以此执行的，并且仅当最后一个init容器执行完毕才会去启动主容器。3.生命周期钩子：pod允许定义两种类型的生命周期钩子，启动后(post-start)钩子和停止前(pre-stop)钩子这些生命周期钩子是基于每个容器来指定的，和init容器不同的是，init容器是应用到整个pod。而这些钩子是针对容器的，是在容器启动后和停止前执行的。4.容器探测：对Pod健康状态诊断。分为三种： Startupprobe、Livenessprobe(存活性探测)， Readinessprobe(就绪性检测)Startup（启动探测）:探测容器是否正常运行Liveness(存活性探测)：判断容器是否处于runnning状态，根据重启策略决定是否重启容器Readiness(就绪性检测)：判断容器是否准备就绪并对外提供服务，将容器设置为不可用，不接受service转发的请求三种探针用于Pod检测：ExecAction：在容器中执行一个命令，并根据返回的状态码进行诊断，只有返回0为成功TCPSocketAction：通过与容器的某TCP端口尝试建立连接HTTPGetAction：通过向容器IP地址的某指定端口的path发起HTTP GET请求容器的重启策略123456定义是否重启Pod对象Always：但凡Pod对象终止就重启，默认设置OnFailure：仅在Pod出现错误时才重启Never：从不注：一旦Pod绑定到一个节点上，就不会被重新绑定到另一个节点上，要么重启，要么终止pod的终止过程终止过程主要分为如下几个步骤：123456789(1)用户发出删除 pod 命令：kubectl delete pods ，kubectl delete -f yaml(2)Pod 对象随着时间的推移更新，在宽限期（默认情况下30秒），pod 被视为“dead”状态(3)将 pod 标记为“Terminating”状态(4)第三步同时运行，监控到 pod 对象为“Terminating”状态的同时启动 pod 关闭过程(5)第三步同时进行，endpoints 控制器监控到 pod 对象关闭，将pod与service匹配的 endpoints 列表中删除(6)如果 pod 中定义了 preStop 钩子处理程序，则 pod 被标记为“Terminating”状态时以同步的方式启动执行；若宽限期结束后，preStop 仍未执行结束，第二步会重新执行并额外获得一个2秒的小宽限期(7)Pod 内对象的容器收到 TERM 信号(8)宽限期结束之后，若存在任何一个运行的进程，pod 会收到 SIGKILL 信号(9)Kubelet 请求 API Server 将此 Pod 资源宽限期设置为0从而完成删除操作初始化容器（init）12spec字段下有个initContainers字段(初始化容器)，Init容器就是做初始化工作的容器。可以有一个或多个，如果多个按照定义的顺序依次执行，先执行初始化容器1，再执行初始化容器2等，等初始化容器执行完具体操作之后初始化容器就退出了，只有所有的初始化容器执行完后，主容器才启动。由于一个Pod里容器存储卷是共享的，所以Init Container里产生的数据可以被主容器使用到，Init Container可以在多种K8S资源里被使用到，如Deployment、DaemonSet, StatefulSet、Job等，但都是在Pod启动时，在主容器启动前执行，做初始化工作。初始化容器与主容器区别是121、初始化容器不支持 Readinessprobe,因为它们必须在Pod就绪之前运行完成2、每个Init容器必须运行成功,下一个才能够运行初始化容器应用：[root@pengfei-master1 pod]# cat init-1.yaml1234567891011121314151617181920212223242526272829303132apiVersion: v1kind: Podmetadata: name: init-busybox labels: app: init-busyboxspec: initContainers: - name: install image: busybox imagePullPolicy: IfNotPresent command: - wget - "-O" - "/tmp/index.html" - "https://www.baidu.com" volumeMounts: - name: workdir mountPath: /tmp containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent ports: - containerPort: 80 volumeMounts: - name: workdir mountPath: /usr/share/nginx/html dnsPolicy: Default volumes: - name: workdir emptyDir: &#123;&#125;创建pod并查看pod：1234[root@pengfei-master1 pod]# kubectl apply -f init-1.yaml[root@pengfei-master1 pod]# kubectl get pods -owideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESinit-busybox 1/1 Running 0 16h 10.244.225.88 pengfei-node2 &lt;none&gt; &lt;none&gt;测试pod1[root@pengfei-master1 pod]# curl 10.244.225.88123456789101112[root@pengfei-master1 pod]# kubectl exec -it init-busybox /bin/bashkubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.Defaulted container &quot;nginx&quot; out of: nginx, install (init)root@init-busybox:/# lsbin dev docker-entrypoint.sh home lib64 mnt proc run srv tmp varboot docker-entrypoint.d etc lib media opt root sbin sys usrroot@init-busybox:/# cd /usr/share/nginxroot@init-busybox:/usr/share/nginx# lshtmlroot@init-busybox:/usr/share/nginx# cd htmlroot@init-busybox:/usr/share/nginx/html# lsindex.html主容器121、容器钩子初始化容器启动之后，开始启动主容器，在主容器启动之后有一个post start hook（容器启动后钩子）和pre stop hook（容器结束前钩子），无论启动后还是结束前所做的事我们可以把它放到这两个钩子，这个钩子就表示用户可以用它来钩住一些命令，非必须选项postStart：该钩子在容器被创建后立刻执行，如果该钩子对应的探测执行失败，则该容器会被杀死，并根据该容器的重启策略决定是否要重启该容器，这个钩子不需要传递任何参数。preStop：该钩子在容器被删除前执行，主要用于释放资源和优雅关闭程序容器钩子：postStart和preStoppostStart：容器创建之后立刻执行，用于资源部署、环境准备等。preStop：在容器被终止前执行，用于优雅关闭应用程序、通知其他系统等postStart和preStop用法1234567891011121314151617181920......containers:- image: sample:v2 name: war lifecycle： postStart: exec: command: - “cp” - “/sample.war” - “/app” prestop: httpGet: host: monitor.com path: /waring port: 8080 scheme: HTTP......#以上示例中，定义了一个Pod，包含一个JAVA的web应用容器，其中设置了PostStart和PreStop回调函数。即在容器创建成功后，复制/sample.war到/app文件夹中。而在容器终止之前，发送HTTP请求到http://monitor.com:8080/waring，即向监控系统发送警告。测试启动和停止钩子1234567891011121314151617181920212223[root@pengfei-master1 pod]# cat &gt;pre-start.yaml&gt;&gt;EOFapiVersion: v1kind: Podmetadata: name: pre-start-demo1 labels: app: pre-start-demo1spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent lifecycle: postStart: exec: command: [&quot;/bin/bash&quot;,&quot;-c&quot;, &quot;echo &apos;test&apos; &gt;/usr/share/nginx/html/index.html&quot;] preStop: exec: command: [&quot;/bin/bash&quot;,&quot;-c&quot;,&quot;nginx -s stop&quot;] ports: - containerPort: 80 name: http EOF查看pod12345[root@pengfei-master1 pod]# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESpre-start-demo1 1/1 Running 0 61s 10.244.128.88 pengfei-node1 &lt;none&gt; &lt;none&gt;[root@pengfei-master1 pod]# curl 10.244.128.88test优雅的删除资源对象当用户请求删除含有pod的资源对象时（如RC、deployment等），K8S为了让应用程序优雅关闭（即让应用程序完成正在处理的请求后，再关闭软件），K8S提供两种信息通知：12345678910111213141516171）默认：K8S通知node执行docker stop命令，docker会先向容器中PID为1的进程发送系统信号SIGTERM，然后等待容器中的应用程序终止执行，如果等待时间达到设定的超时时间，或者默认超时时间（30s），会继续发送SIGKILL的系统信号强行kill掉进程。2）使用pod生命周期（利用PreStop回调函数），它执行在发送终止信号之前。默认情况下，所有的删除操作的优雅退出时间都在30秒以内。kubectl delete命令支持--grace-period=的选项，以运行用户来修改默认值。0表示删除立即执行，并且立即从API中删除pod。在节点上，被设置了立即结束的的pod，仍然会给一个很短的优雅退出时间段，才会开始被强制杀死。如下:...... spec: containers: - name: nginx-demo image: centos:nginx lifecycle: preStop: exec: # nginx -s quit gracefully terminate while SIGTERM triggers a quick exit command: [&quot;/usr/local/nginx/sbin/nginx&quot;,&quot;-s&quot;,&quot;quit&quot;] ports: - name: http containerPort: 80 ......总结：pod在整个生命周期中有非常多的用户行为：12341、初始化容器完成初始化2、主容器启动后可以做启动后钩子3、主容器结束前可以做结束前钩子4、在主容器运行中可以做一些健康检测，如startupprobe、livenessprobe，readnessprobe实战：通过钩子优雅停机背景1在 Kubernetes 中，每次微服务的代码发布都意味着创建新版本的 pod 并删除旧 pod，如果部署不够优雅的话，可能出现如下两个问题：正在处理请求的pod被删除，在请求没有做幂等处理的情况下，就会出现数据重复、数据错误，亦或导致分布式系统数据不一致；Kubernetes 将流量路由到已被删除的 pod，导致处理请求失败造成用户体验不佳。所以，为了让代码发布的部署过程不影响业务的正常运行和用户无感知，我们需要实现容器的优雅停机。容器的生命周期钩子1在介绍优雅停机之前，我们先来了解下k8s的容器都有哪些生命周期钩子？作用是什么？要怎么使用？Kubernetes的容器有两种生命周期钩子（Lifecycle Hooks）：PostStart这个钩子会在容器被创建后立即执行，但无法保证会在容器的起始点 ENTRYPOINT之前执行，如果执行时间太长，将会阻止Pod状态进入running，可用于数据初始化、容器启动回调等场景。如果需要保证在应用程序启动前就要执行完的任务，可以考虑放在初始化容器( Init Containers)中去实现。PreStop这个钩子会在容器被结束前执行，执行期间Pod状态为 Terminating，运行时间受终止宽限期（ terminationGracePeriodSeconds）约束，超出宽限期Pod将被强制杀死，可用于容器回收前的数据清理、优雅停机等场景。1上述的两个钩子（PostStart 和 PreStop）都有四种类型，分别为：exec、httpGet、tcpSocket 和 sleep。由于这四种钩子类型在 PostStart 和 PreStop 中的使用方法一致，下面以 PreStop 为例介绍这四种钩子类型的使用方法exec（执行shell指令，可以是指令或shell脚本， 退出状态码为 0则为成功）1234567891011# shell指令模式lifecycle: preStop: exec: command: ["/bin/sh", "-c", "echo 'Container is stopping'"]# shell脚本模式lifecycle: preStop: exec: command: ["/bin/sh", "-c", "/data/scripts/preStop.sh"]httpGet（执行http get请求，响应状态码在[200,400)区间则为成功1234567lifecycle: preStop: httpGet: path: /shutdown # 请求的uri port: 8080 # 端口 host: api.yilingyi.com # 主机域名，不加该字段将请求Pod本身 scheme: HTTP # http协议，默认值HTTP，支持HTTP、HTTPStcpSocket（执行tcp socket请求， TCP连接成功建立则为成功）1234lifecycle: preStop: tcpSocket: port: 8080sleep（将容器暂停5秒，Kubernetes 1.30的新特性 PodLifecycleSleepAction，待验证）1234lifecycle: preStop: sleep: seconds: 5请注意，如果 PostStart 或 PreStop 回调失败，容器将被杀死，所以回调处理的程序应尽量轻量级及把控好执行的时间。微服务优雅停机实现1本次将以k8s + SpringBoot + Nacos作为案例，介绍在实际业务场景中如何实现微服务的优雅停机，从而实现代码发布时的零宕机。首先，先看看pod的默认删除过程：12345678910111. Kube-apiserver接收到pod的删除请求，在Etcd上更新pod的状态为Terminating；2. Kubelet 清理节点上容器相关的资源，如存储、网络；3. Kubelet向容器发送SIGTERM，如果容器内进程没有任何配置，则容器立即退出。4. 如果容器在默认的 30 秒内没有退出，Kubelet 将发送 SIGKILL 并强制其退出。 可以看出，在没有配置优雅停机之前，pod的删除相当暴力，所以为了更加优雅，我们加入了preStop hook，和将终止宽限期延长，具体实现如下：1. preStop hook做了两件事情： 1）nacos反注册（也称 实例注销），确保在实例关闭期间不会再有新的请求被路由到该实例。 2) sleep 35s，nacos客户端的实例缓存为30s，30s后会重新拉取实例信息，超时为10s，一般不用10s这么长，所以我们设置为35s。2. springboot开启优雅停机后，最大等待时间为30s。3. terminationGracePeriodSeconds默认为30s，远小于preStop和springboot的时间之和，所以我们需要将其调大，我这里设置的是60s。4. 其实在terminationGracePeriodSeconds耗尽后，k8s还给了一个2s的额外宽限期，最后才执行SIGKILL。操作步骤12345678 在SpringBoot &gt; 2.3.0的版本后支持应用程序优雅停机，需要在java微服务的配置中设置如下两个属性，这一步很重要！！！server: # 默认值immediate:即立即关闭，graceful:即优雅停机 shutdown: gracefulspring: lifecycle: # 优雅停机最大等待时间，默认30s timeout-per-shutdown-phase: 30s在微服务的yaml文件加上优雅停机的配置：通过env定义POD_IP获取当前Pod的ip，传递给preStop进行nacos反注册。1234567891011121314151617181920212223apiVersion: v1kind: Podmetadata: name: sre-yilingyispec: containers: - name: sre-yilingyi image: 'sre/yilingyi:1.0.0' env: - name: POD_IP valueFrom: fieldRef: apiVersion: v1 fieldPath: status.podIP lifecycle: preStop: exec: command: - /bin/sh - '-c' - &gt; curl -s --connect-timeout 10 -m 20 -X POST "http://nacos.yilingyi.com:8848/nacos/v1/ns/instance?port=8080&amp;healthy=true&amp;ip=$&#123;POD_IP&#125;&amp;weight=1&amp;enabled=false&amp;serviceName=sre-yilingyi&amp;encoding=GBK&amp;namespaceId=production" &amp;&amp; sleep 35 terminationGracePeriodSeconds: 60至此，完成微服务的优雅停机配置。]]></content>
      <categories>
        <category>技术</category>
        <category>K8S</category>
      </categories>
      <tags>
        <tag>K8S</tag>
        <tag>钩子</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s探针详解]]></title>
    <url>%2F2025%2F02d2fb66f3.html</url>
    <content type="text"><![CDATA[k8s提供了3种Probe，分别是startupProbe，readinessProbe和livenessProbe。POD状态Pod 常见的状态Pending：挂起，我们在请求创建pod时，条件不满足，调度没有完成，没有任何一个节点能满足调度条件。已经创建了但是没有适合它运行的节点叫做挂起，这其中也包含集群为容器创建网络，或者下载镜像的过程。Running：Pod内所有的容器都已经被创建，且至少一个容器正在处于运行状态、正在启动状态或者重启状态。Succeeded：Pod中所以容器都执行成功后退出，并且没有处于重启的容器。Failed：Pod中所以容器都已退出，但是至少还有一个容器退出时为失败状态。Unknown：未知状态，所谓pod是什么状态是apiserver和运行在pod节点的kubelet进行通信获取状态信息的，如果节点之上的kubelet本身出故障，那么apiserver就连不上kubelet，得不到信息了，就会看UnknownPod重启策略Always: 只要容器失效退出就重新启动容器。OnFailure: 当容器以非正常(异常)退出后才自动重新启动容器。Never: 无论容器状态如何，都不重新启动容器。如果pod的restartpolicy没有设置，那么默认值是Always。Pod常见状态转换场景就绪、存活两种探针K8S 提供了3种探针readinessProbelivenessProbestartupProbe（这个1.16版本增加的）探针介绍在 Kubernetes 中 Pod 是最小的计算单元，而一个 Pod 又由多个容器组成，相当于每个容器就是一个应用，应用在运行期间，可能因为某也意外情况致使程序挂掉。那么如何监控这些容器状态稳定性，保证服务在运行期间不会发生问题，发生问题后进行重启等机制，就成为了重中之重的事情，考虑到这点 kubernetes 推出了活性探针机制。有了存活性探针能保证程序在运行中如果挂掉能够自动重启，但是还有个经常遇到的问题，比如说，在Kubernetes 中启动Pod，显示明明Pod已经启动成功，且能访问里面的端口，但是却返回错误信息。还有就是在执行滚动更新时候，总会出现一段时间，Pod对外提供网络访问，但是访问却发生404，这两个原因，都是因为Pod已经成功启动，但是 Pod 的的容器中应用程序还在启动中导致，考虑到这点Kubernetes推出了就绪性探针机制。livenessProbelivenessProbe：存活性探针，用于判断容器是不是健康，如果不满足健康条件，那么 Kubelet 将根据 Pod 中设置的 restartPolicy （重启策略）来判断，Pod 是否要进行重启操作。LivenessProbe按照配置去探测 ( 进程、或者端口、或者命令执行后是否成功等等)，来判断容器是不是正常。如果探测不到，代表容器不健康（可以配置连续多少次失败才记为不健康），则 kubelet 会杀掉该容器，并根据容器的重启策略做相应的处理。如果未配置存活探针，则默认容器启动为通过（Success）状态。即探针返回的值永远是 Success。即Success后pod状态是RUNINGreadinessProbereadinessProbe : 就绪性探针，用于判断容器内的程序是否存活（或者说是否健康），只有程序(服务)正常， 容器开始对外提供网络访问（启动完成并就绪）。容器启动后按照readinessProbe配置进行探测，无问题后结果为成功即状态为 Success。pod的READY状态为 true，从0/1变为1/1。如果失败继续为0/1，状态为 false。若未配置就绪探针，则默认状态容器启动后为Success。对于此pod、此pod关联的Service资源、EndPoint 的关系也将基于 Pod 的 Ready 状态进行设置，如果 Pod 运行过程中 Ready 状态变为 false，则系统自动从 Service资源 关联的 EndPoint 列表中去除此pod，届时service资源接收到GET请求后，kube-proxy将一定不会把流量引入此pod中，通过这种机制就能防止将流量转发到不可用的 Pod 上。如果 Pod 恢复为 Ready 状态，将再会被加回 Endpoint 列表。kube-proxy也将有概率通过负载机制会引入流量到此pod中。就绪、存活两种探针的区别ReadinessProbe 和 livenessProbe 可以使用相同探测方式，只是对 Pod 的处置方式不同：12readinessProbe 当检测失败后，将 Pod 的 IP:Port 从对应的 EndPoint 列表中删除。livenessProbe 当检测失败后，将杀死容器并根据 Pod 的重启策略来决定作出对应的措施。就绪、存活两种探针的使用方法目前 LivenessProbe 和 ReadinessProbe 两种探针都支持下面三种探测方法：123ExecAction：在容器中执行指定的命令，如果执行成功，退出码为 0 则探测成功。HTTPGetAction：通过容器的IP地址、端口号及路径调用 HTTP Get方法，如果响应的状态码大于等于200且小于400，则认为容器 健康。TCPSocketAction：通过容器的 IP 地址和端口号执行 TCP 检 查，如果能够建立 TCP 连接，则表明容器健康。探针探测结果有以下值：Success：表示通过检测。Failure：表示未通过检测。Unknown：表示检测没有正常进行。LivenessProbe 和 ReadinessProbe 两种探针的相关属性探针(Probe)有许多可选字段，可以用来更加精确的控制Liveness和Readiness两种探针的行为(Probe)：initialDelaySeconds：容器启动后要等待多少秒后就探针开始工作，单位“秒”，默认是 0 秒，最小值是 0periodSeconds：执行探测的时间间隔（单位是秒），默认为 10s，单位“秒”，最小值是 1timeoutSeconds：探针执行检测请求后，等待响应的超时时间，默认为 1s，单位“秒”，最小值是 1successThreshold：探针检测失败后认为成功的最小连接成功次数，默认为 1，在 Liveness 探针中必须为 1，最小值为 1。failureThreshold：探测失败的重试次数，重试一定次数后将认为失败，在 readiness 探针中，Pod会被标记为未就绪，默认为 3，最小值为 1注：initialDelaySeconds在readinessProbe其实可以不用配置，不配置默认pod刚启动，开始进行readinessProbe探测，但那又怎么样，除了startupProbe，readinessProbe、livenessProbe运行在pod的整个生命周期，刚启动的时候readinessProbe检测失败了，只不过显示READY状态一直是0/1，readinessProbe失败并不会导致重启pod，只有startupProbe、livenessProbe失败才会重启pod。而等到多少s后，真正服务启动后，检查success成功后，READY状态自然正常。探针使用示例LivenessProbe 探针使用示例通过exec方式做健康探测[root@localhost ~]# vi liveness-exec.yaml123456789101112131415161718192021apiVersion: v1kind: Podmetadata: name: liveness-exec labels: app: livenessspec: containers: - name: liveness image: busybox args: #创建测试探针探测的文件 - /bin/sh - -c - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600 livenessProbe: initialDelaySeconds: 10 #延迟检测时间 periodSeconds: 5 #检测时间间隔 exec: #使用命令检查 command: #指令，类似于运行命令sh - cat #sh 后的第一个内容，直到需要输入空格，变成下一行 - /tmp/healthy #由于不能输入空格，需要另外声明，结果为sh cat"空格"/tmp/healthy解释整体意思：容器在初始化后，执行（/bin/sh -c &quot;touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600&quot;）首先创建一个 /tmp/healthy 文件，然后执行睡眠命令，睡眠 30 秒，到时间后执行删除 /tmp/healthy 文件命令。而设置的存活探针检检测方式为执行 shell 命令，用 cat 命令输出 healthy 文件的内容，如果能成功执行这条命令一次(默认successThreshold:1)，存活探针就认为探测成功，由于没有配置(failureThreshold、timeoutSeconds)，所以执行（cat /tmp/healthy）并只等待1s，如果1s内执行后返回失败，探测失败。在前 30 秒内，由于文件存在，所以存活探针探测时执行 cat /tmp/healthy 命令成功执行。30 秒后 healthy 文件被删除，所以执行命令失败，Kubernetes 会根据 Pod 设置的重启策略来判断，是否重启 Pod。通过HTTP方式做健康探测httpGet探测方式有如下可选的控制字段:scheme: 用于连接host的协议，默认为HTTP。host：要连接的主机名，默认为Pod IP，可以在http request head中设置host头部。port：容器上要访问端口号或名称。path：http服务器上的访问URI。httpHeaders：自定义HTTP请求headers，HTTP允许重复headers。[root@localhost ~]# vi liveness-http.yaml # 通过httpGet访问url1234567891011121314151617181920apiVersion: v1kind: Podmetadata: name: liveness-http labels: test: livenessspec: containers: - name: liveness image: mydlqclub/springboot-helloworld:0.0.1 livenessProbe: failureThreshold: 5 #检测失败5次表示未就绪 initialDelaySeconds: 20 #延迟加载时间 periodSeconds: 10 #重试时间间隔 timeoutSeconds: 5 #超时时间设置 successThreshold: 2 #检查成功为2次表示就绪 httpGet: scheme: HTTP port: 8081 path: /actuator/health解释整体意思：上面 Pod 中启动的容器是一个 SpringBoot 应用，其中引用了 Actuator 组件，提供了 /actuator/health 健康检查地址，在pod启动后，初始化等待20s后，livenessProbe开始工作，去请求HTTP://podIP:8081/actuator/health 接口，类似于curl -I HTTP://podIP:8081/actuator/health接口,考虑到请求会有延迟(curl -I后一直出现假死状态)，所以给这次请求操作一直持续5s，如果5s内访问返回数值在&gt;=200且&lt;=400代表第一次检测success，如果是其他的数值，或者5s后还是假死状态，执行类似（ctrl+c）中断，并反回failure失败。等待10s后，再一次的去请求HTTP://podIP:8081/actuator/health接口。如果有连续的2次都是success，代表无问题。如果期间有连续的5次都是failure，代表有问题，直接重启pod，此操作会伴随pod的整个生命周期通过自定义HTTP请求headerslivenessProbe: failureThreshold: 5 #检测失败5次表示未就绪 initialDelaySeconds: 20 #延迟加载时间 periodSeconds: 10 #重试时间间隔 timeoutSeconds: 5 #超时时间设置 successThreshold: 2 #检查成功为2次表示就绪 httpGet: port: 8080 path: /health httpHeaders: - name: end-user - value: Jason 通过TCP方式做健康探测[root@localhost ~]# vi liveness-tcp.yaml123456789101112131415apiVersion: v1kind: Podmetadata: name: liveness-tcp labels: app: livenessspec: containers: - name: liveness image: nginx livenessProbe: initialDelaySeconds: 15 periodSeconds: 20 tcpSocket: port: 80解释整体意思：TCP 检查方式和 HTTP 检查方式非常相似，在容器启动 initialDelaySeconds 参数设定的时间后，kubelet 将发送第一个 livenessProbe 探针，尝试连接容器的 80 端口，类似于telnet 80端口，如果连接失败则将杀死 Pod 重启容器。ReadinessProbe 探针使用示例Pod 的ReadinessProbe 探针使用方式和 LivenessProbe 探针探测方法一样，也是支持三种，只是一个是用于探测应用的存活，一个是判断是否对外提供流量的条件。这里用一个 Springboot 项目，设置 ReadinessProbe 探测 SpringBoot 项目的 8081 端口下的 /actuator/health 接口，如果探测成功则代表内部程序以及启动，就开放对外提供接口访问，否则内部应用没有成功启动，暂不对外提供访问，直到就绪探针探测成功。顺便书写service资源查看调度规则。[root@localhost ~]# vi readiness-exec.yaml12345678910111213141516171819202122232425262728293031323334353637383940414243apiVersion: v1kind: Servicemetadata: name: springboot labels: app: springbootspec: type: NodePort ports: - name: server port: 8080 targetPort: 8080 nodePort: 31180 - name: management port: 8081 targetPort: 8081 nodePort: 31181 selector: app: springboot---apiVersion: v1kind: Podmetadata: name: springboot labels: app: springbootspec: containers: - name: springboot image: mydlqclub/springboot-helloworld:0.0.1 ports: - name: server containerPort: 8080 - name: management containerPort: 8081 readinessProbe: initialDelaySeconds: 20 periodSeconds: 5 timeoutSeconds: 10 httpGet: scheme: HTTP port: 8081 path: /actuator/healthReadinessProbe + LivenessProbe 配合使用示例一般程序中需要设置两种探针结合使用，并且也要结合实际情况，来配置初始化检查时间和检测间隔，下面列一个简单的 SpringBoot 项目的 Deployment 例子。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061apiVersion: v1kind: Servicemetadata: name: springboot labels: app: springbootspec: type: NodePort ports: - name: server port: 8080 targetPort: 8080 nodePort: 31180 - name: management port: 8081 targetPort: 8081 nodePort: 31181 selector: app: springboot---apiVersion: apps/v1kind: Deploymentmetadata: name: springboot labels: app: springbootspec: replicas: 1 selector: matchLabels: app: springboot template: metadata: name: springboot labels: app: springboot spec: containers: - name: readiness image: mydlqclub/springboot-helloworld:0.0.1 ports: - name: server containerPort: 8080 - name: management containerPort: 8081 readinessProbe: initialDelaySeconds: 20 periodSeconds: 5 timeoutSeconds: 10 httpGet: scheme: HTTP port: 8081 path: /actuator/health livenessProbe: initialDelaySeconds: 30 periodSeconds: 10 timeoutSeconds: 5 httpGet: scheme: HTTP port: 8081 path: /actuator/health注意terminationGracePeriodSeconds不能用于就绪态探针（readinessProbe），如果将它应用于readinessProbe将会被APIserver接口所拒绝123456789----------------------------------livenessProbe:httpGet: path: /healthz port: liveness-portfailureThreshold: 1 periodSeconds: 60 terminationGracePeriodSeconds: 60 # 宽限时间60s----------------------------------startupProbe探针startupProbe探针介绍k8s 在1.16版本后增加startupProbe探针，主要解决在复杂的程序中readinessProbe、livenessProbe探针无法更好的判断程序是否启动、是否存活。进而引入startupProbe探针为readinessProbe、livenessProbe探针服务。startupProbe探针与另两种区别如果三个探针同时存在，先执行startupProbe探针，其他两个探针将会被暂时禁用，直到pod满足startupProbe探针配置的条件，其他2个探针启动，如果不满足按照规则重启容器另外两种探针在容器启动后，会按照配置，直到容器消亡才停止探测，而startupProbe探针只是在容器启动后按照配置满足一次后，不在进行后续的探测。startupProbe探针方法、属性startupProbe探针的使用方法跟 ReadinessProbe 和 livenessProbe 相同，对 Pod 的处置跟livenessProbe 方式相同，失败重启。ExecAction：在容器中执行指定的命令，如果执行成功，退出码为 0 则探测成功。HTTPGetAction：通过容器的IP地址、端口号及路径调用 HTTP Get方法，如果响应的状态码大于等于200且小于400，则认为容器 健康。TCPSocketAction：通过容器的 IP 地址和端口号执行 TCP 检 查，如果能够建立 TCP 连接，则表明容器健康。探针探测结果有以下值：Success：表示通过检测。Failure：表示未通过检测。Unknown：表示检测没有正常进行。startupProbe探针属性跟 ReadinessProbe 和 livenessProbe 相同initialDelaySeconds：容器启动后要等待多少秒后就探针开始工作，单位“秒”，默认是 0 秒，最小值是 0periodSeconds：执行探测的时间间隔（单位是秒），默认为 10s，单位“秒”，最小值是 1timeoutSeconds：探针执行检测请求后，等待响应的超时时间，默认为 1s，单位“秒”，最小值是 1successThreshold：探针检测失败后认为成功的最小连接成功次数，默认为 1s，在 Liveness 探针中必须为 1s，最小值为 1s。failureThreshold：探测失败的重试次数，重试一定次数后将认为失败，在 readiness 探针中，Pod会被标记为未就绪，默认为 3s，最小值为 1s注：在startupProbe执行完之后，其他2种探针的所有配置才全部启动，相当于容器刚启动的时候，所以其他2种探针如果配置了initialDelaySeconds，建议不要给太长。startupProbe使用场景startupProbe官方的解释：可以定义一个启动探针，该探针将推迟所有其他探针，直到 Pod 完成启动为止。startupProbe 启动探针存在的意义是不是： 如果服务A启动需要1分钟 ，我们存活探针探测的时候设置的是initialDelaySeconds 10s后开始探测，然后她探测的时候发现服务不正常，然后就开始重启Pod陷入死循环，但是如果意义在这个地方，那我们可以把探测时间调整大一点，failureThreshold 把这个也多设置几次就行了啊。 为什么还要单独的设置一个satrtupProbe呢？startupProbe的存在意义startupProbe 和 livenessProbe 最大的区别就是startupProbe在探测成功之后就不会继续探测了，而livenessProbe在pod的生命周期中一直在探测。如果没有startupProbe探针的话我们只设置livenessProbe探针会存在如下问题： 一个服务如果前期启动需要很长时间，那么它后面死亡未被发现的时间就越长，为什么会这么说呢？假设我们一个服务A启动完成需要2分钟，那么我们如下定义livenessProbe1234567livenessProbe: httpGet: path: /test prot: 80failureThreshold: 1initialDelay：5periodSeconds: 5如果我们这样定义的话，那pod 5s就会根据重启策略进行一次重启，这个时候你会发现pod一直会陷入死循环，那我们可以按照上面的猜想把配置改成这样1234567livenessProbe: httpGet: path: /test prot: 80failureThreshold: 6initialDelay：40periodSeconds: 5你肯定会说你看这样不就行了吗？这样的话pod就不会陷入死循环能启动起来了，确实这样pod能够启动起来了，但是你有没有考虑过这样一个问题，当我们启动完成之后，在后期的探测中，你需要6*5=30s才能发现这个pod不可用，这个时候你的服务已经停止运行了30s你才发现，这在生产中有可能是不会被原谅的。还有就是这边只是我们假设一个服务A需要1分钟才能起来，但是在实际生产中你如何定义这些值呢？？？针对上面这两个问题引入startupProbe之后都解决了123456789101112131415livenessProbe: httpGet: path: /test prot: 80failureThreshold: 1initialDelay：5periodSeconds: 5startupProbe: httpGet: path: /test prot: 80failureThreshold: 60initialDelay：5periodSeconds: 5我们这样设置之后，由于startupProbe探针的存在，程序有60x5s=300s的启动时间，一旦startupProbe探针探测成功之后，就会被livenessProbe接管，这样在运行中出问题livenessProbe就能在1x5=5s内发现。如果启动探测是5分钟内还没有探测成功，则接受Pod的重启策略进行重启。]]></content>
      <categories>
        <category>技术</category>
        <category>K8S</category>
      </categories>
      <tags>
        <tag>K8S</tag>
        <tag>探针</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jenkins使用不同版本软件的方法]]></title>
    <url>%2F2025%2F02dc15b3b5.html</url>
    <content type="text"><![CDATA[27da93e20b455c1ffb6ad82f6a745658009007ef004c721eda37d7ffb722c4daea3bc7836e41038b95b3afefc3557302b67fc2c3063bff1f7dec745efcb0a22355d2827f6529543aec31f5ba02917a006de45be0dfd44096d78dd79c505ed1f45f30d1262ec1f8aec24d0f49f7da9ddfeac46e3d08a5da2864c9274a429b28b861dc6ec5d941ec69cdb34277f9852b300db1bcafed4bb69ec29e8d2f14347abbe035bd1e97d7c44d4dd0b78439b79d3278a0eaa0ca8169620f76f462bdc4f0d7d2c361fe3b973301097f6f6b1559cee09dafbd7849290e0b6c3011219443e81e8890e5fa58371d074f7b538cb60cc5c920fd7cfe460dcd666679cd4815b0f3c8be4a7809c67957c5ab2c7156becfb57ac9b4265b66f6d601f4621c41f62a5ae227e2beeb5fc0ec27fbf6e68a21d412a189caa4371c9d47cd8e14fa5b0f8546b8d5400ea870761ee814b3c7ffa3e65c0204ee5b78c199c1a8c007d903925698e084936917438bb2d21aee838b5f0dfcd8b25ae45f58c8b006774559c2fb299e76c98a5674e6b56cb26ac2fc95e553a24f63af193a1736b381f6e0d6f4d0354e5bad879aa3455b38b52c118dcd6497c8441941a9c70db35955a29bcd6f2476c3b9a3b02d6a02dfe3540f3b2688aff3553bfc45879ac431cdf1839554b487ea50a61082d108f3536398bca15c99d60516fe5bb12e5e22e7eb63c06814196fd7a25c6d4009384d59d1db57229551ff05e7e1b77220a27a0ea037c420621600edc13975a7b167094da6ee4fb407bd5d0178a04ab24b99cb77db7438cc9b1406de4ccacfff4140cef5214b8fbbb2b5290df75a1cbce7d26b3ca55babd4bb9b5058b49ad55c827933a4db03a0f166fcedd5bfd4de84472f3dec77c43227bd789d474ae02654a2e6836ed50ecaec9f9db5a3a558340f716a7c0d6aa6c7a3374b8a60c18305b07af8862450c3c42ac22dc7cdd9fb7ff2543ccd87249bb10d0a089f67d3fae236d8ac191e358d4682d0f788b7ac1a4251dea58e264dc01aa5a88080d6976be2674e1cb7771687a864c8145ecd7305f4f0239af95ce7793fb5b63efa38ff3509fbdc9737dc54b7bdb467e3be20a52cc164f292d00ccfeaf1c2c035c1f4f222a0b941c72c60823c0ff554deda10a22d4f93b8d6893f844810db375aa97082e3c18b7d4e948fbd5bd187a55229e21e044f16bfb1028d1a18c4bc66a235f3641deefb71bd53747af43fd5911bffffad09c49f2777d65e14078e94108676c5a46d7c3584cc2733fbb247452ab2824d144a9bbfb17f5c0358aa35fd0ec5a748c1024fc30d749bf5a174efb202945534fb0b40368a522838f2bd50b38d86e07cdd0637e48bebc3411b1d4766e0e1bbb601df66006e0d2cf8a38382fc661e077dd239184b3585dc4653abf2934bc5575a6a85004b89b11fa0f0cef210b85e1c7e64a8fad1dfd03f599f939de6f85eed6065434d4f3b3948ae99f57f672b3f5d86d86637abe64c480bdc24ad53ea1fc52dfe5e239bf0f51aecc310ad507b5aaa03d890f1cadb448443b16411c912919edcba2421d9cfa35f6aae6fc861f083114a0720f1a1df5721cde73e81b3959cd0725841d0bb5efa2b8729373891366de6ec18c8e022534ef72bb5cbc70d93231954a311075e904e185e0882161f3607a65a4fe5d2e9091fc0bd014a29d29dc4abe260a5825fb0357bb2e9ca4cf8ffb6d8eff5c0b75d9b9fd0f4b8431b799723742426d0d77e8563659c7b7bbe879b3e13ed3ae1589d44f81302a59745a528d656f171aa02edff94fec7411cea3ff184a5f1ed2e280219dae61daa17feaa5fad8e326090faff6fef15278dfcf9322b604b321c1d4be044a0ad2755b02892f8bfe18b9483f0c1d26dc1690390014342caa28e49865ec181e6433022dd1ae5b9e3ba90835948e0a0d37b46b19c5fd5027306069ea505435498ccde9178464ccaadd30a901f6dac6cd2f9838d8d3d946f4bca4b178ad8df626c9acc93bee0ce97fac11fc50d8efcdc98d724b3a5775b160f41bb43b23fe98c3b7d55d9b4a1b79c0e2c3baa01d8f83f2605325e7e231ee0ba182909cdccb1d055d978ec820708c3e7d31e1a26be55f17bd6a4e765059d5526f9a2c115c3a5ffc5dd183c337f95de38d88ffeb57dd6dcdedfb295839efb18929e9df52443f298a2fad8332b6d06e1c4299b9c0b5f163deb924b8697cb093f0bb0278f517c5c3b86a725a65302f913da8752465993968664bdbd47d5196e777121677409c466fa28de0c0aefa7827d0037edb75b7a62caac7a8525b06167db67ded949aaefb19443fa465daf5ade5d1dc10c22566a17729d4a07931108e4866a1e6e1cf481d91fec6999669ea0cc86fdb9859518 您好, 请在此输入密码]]></content>
      <categories>
        <category>技术</category>
        <category>Jenkins</category>
      </categories>
      <tags>
        <tag>Jenkins</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[通过jenkins批量合并分支代码到主分支]]></title>
    <url>%2F2025%2F02f9c764ea.html</url>
    <content type="text"><![CDATA[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 您好, 请在此输入密码]]></content>
      <categories>
        <category>技术</category>
        <category>Jenkins</category>
      </categories>
      <tags>
        <tag>Jenkins</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[通过jenkins更新client包到maven仓库]]></title>
    <url>%2F2025%2F028a58dc5c.html</url>
    <content type="text"><![CDATA[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 您好, 请在此输入密码]]></content>
      <categories>
        <category>技术</category>
        <category>Jenkins</category>
      </categories>
      <tags>
        <tag>Jenkins</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[通过jenkins更新k8s的微服务]]></title>
    <url>%2F2025%2F02812174e2.html</url>
    <content type="text"><![CDATA[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 您好, 请在此输入密码]]></content>
      <categories>
        <category>技术</category>
        <category>Jenkins</category>
      </categories>
      <tags>
        <tag>Jenkins</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[通过jenkins创建k8s服务]]></title>
    <url>%2F2025%2F02fc4a1ca1.html</url>
    <content type="text"><![CDATA[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 您好, 请在此输入密码]]></content>
      <categories>
        <category>技术</category>
        <category>Jenkins</category>
      </categories>
      <tags>
        <tag>Jenkins</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL备份恢复单库或单表]]></title>
    <url>%2F2024%2F127fcc32a7.html</url>
    <content type="text"><![CDATA[MySQL 逻辑备份工具最常用的就是 mysqldump 了，一般我们都是备份整个实例或部分业务库恢复场景可能就比较多了，比如恢复某个库或某个表等，那么如何从全备中恢复单库或单表，这其中又有哪些隐藏的坑呢？如何恢复单库或单表每个数据库实例中都不止一个库，一般备份都是备份整个实例，但恢复需求又是多种多样的，比如说我想只恢复某个库或某张表，这个时候应该怎么操作呢？如果你的实例数据量不大，可以在另外一个环境恢复出整个实例，然后再单独备份出所需库或表用来恢复。不过这种方法不够灵活，并且只适用数据量比较少的情况。其实从全备中恢复单库还是比较方便的，有个 --one-database 参数可以指定单库恢复，下面来具体演示下：12345678910111213141516171819202122# 查看及备份所有库mysql&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || mysql || performance_schema || sbtest || sys || testdb || testdb2 |+--------------------+mysqldump -uroot -pxxxx -R -E --single-transaction --all-databases &gt; all_db.sql# 删除testdb库 并进行单库恢复mysql&gt; drop database testdb;Query OK, 36 rows affected (2.06 sec)# 恢复前 testdb库不存在的话要手动新建mysql -uroot -pxxxx --one-database testdb &lt; all_db.sql除了上述方法外，恢复单库或单表还可以采用手动筛选的方法。这个时候 Linux 下大名鼎鼎的 sed 和 grep 命令就派上用场了，我们可以利用这两个命令从全备中筛选出单库或单表的语句，筛选方法如下：123456# 从全备中恢复单库sed -n '/^-- Current Database: `testdb`/,/^-- Current Database: `/p' all_db.sql &gt; testdb.sql# 筛选出单表语句cat all_db.sql | sed -e '/./&#123;H;$!d;&#125;' -e 'x;/CREATE TABLE `test_tb`/!d;q' &gt; /tmp/test_tb_info.sql cat all_db.sql | grep --ignore-case 'insert into `test_tb`' &gt; /tmp/test_tb_data.sql小心有坑对于上述手动筛选来恢复单库或单表的方法，看起来简单方便，其实隐藏着一个小坑，下面我们来具体演示下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768# 备份整个实例mysqldump -uroot -pxxxx -R -E --single-transaction --all-databases &gt; all_db.sql# 手动备份下test_tb 然后删除test_tbmysql&gt; create table test_tb_bak like test_tb;Query OK, 0 rows affected (0.03 sec)mysql&gt; insert into test_tb_bak select * from test_tb;Query OK, 4 rows affected (0.02 sec)Records: 4 Duplicates: 0 Warnings: 0mysql&gt; drop table test_tb;Query OK, 0 rows affected (0.02 sec)# 从全备中筛选test_db建表及插数据语句cat all_db.sql | sed -e '/./&#123;H;$!d;&#125;' -e 'x;/CREATE TABLE `test_tb`/!d;q' &gt; test_tb_info.sql cat all_db.sql | grep --ignore-case 'insert into `test_tb`' &gt; test_tb_data.sql# 查看得到的语句 貌似没问题cat test_tb_info.sqlDROP TABLE IF EXISTS `test_tb`;/*!40101 SET @saved_cs_client = @@character_set_client */;/*!40101 SET character_set_client = utf8 */;CREATE TABLE `test_tb` ( `inc_id` int(11) NOT NULL AUTO_INCREMENT COMMENT '自增主键', `col1` int(11) NOT NULL, `col2` varchar(20) DEFAULT NULL, `col_dt` datetime DEFAULT NULL, `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间', `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '修改时间', PRIMARY KEY (`inc_id`)) ENGINE=InnoDB AUTO_INCREMENT=5 DEFAULT CHARSET=utf8 COMMENT='测试表';/*!40101 SET character_set_client = @saved_cs_client */;cat test_tb_data.sqlINSERT INTO `test_tb` VALUES (1,1001,'dsfs','2020-08-04 12:12:36','2020-09-17 06:19:27','2020-09-17 06:19:27'),(2,1002,'vfsfs','2020-09-04 12:12:36','2020-09-17 06:19:27','2020-09-17 06:19:27'),(3,1003,'adsfsf',NULL,'2020-09-17 06:19:27','2020-09-17 06:19:27'),(4,1004,'walfd','2020-09-17 14:19:27','2020-09-17 06:19:27','2020-09-18 07:52:13');# 执行恢复单表操作mysql -uroot -pxxxx testdb &lt; test_tb_info.sqlmysql -uroot -pxxxx testdb &lt; test_tb_data.sql# 查看恢复数据 并和备份表比对mysql&gt; select * from test_tb;+--------+------+--------+---------------------+---------------------+---------------------+| inc_id | col1 | col2 | col_dt | create_time | update_time |+--------+------+--------+---------------------+---------------------+---------------------+| 1 | 1001 | dsfs | 2020-08-04 12:12:36 | 2020-09-17 06:19:27 | 2020-09-17 06:19:27 || 2 | 1002 | vfsfs | 2020-09-04 12:12:36 | 2020-09-17 06:19:27 | 2020-09-17 06:19:27 || 3 | 1003 | adsfsf | NULL | 2020-09-17 06:19:27 | 2020-09-17 06:19:27 || 4 | 1004 | walfd | 2020-09-17 14:19:27 | 2020-09-17 06:19:27 | 2020-09-18 07:52:13 |+--------+------+--------+---------------------+---------------------+---------------------+4 rows in set (0.00 sec)mysql&gt; select * from test_tb_bak;+--------+------+--------+---------------------+---------------------+---------------------+| inc_id | col1 | col2 | col_dt | create_time | update_time |+--------+------+--------+---------------------+---------------------+---------------------+| 1 | 1001 | dsfs | 2020-08-04 12:12:36 | 2020-09-17 14:19:27 | 2020-09-17 14:19:27 || 2 | 1002 | vfsfs | 2020-09-04 12:12:36 | 2020-09-17 14:19:27 | 2020-09-17 14:19:27 || 3 | 1003 | adsfsf | NULL | 2020-09-17 14:19:27 | 2020-09-17 14:19:27 || 4 | 1004 | walfd | 2020-09-17 14:19:27 | 2020-09-17 14:19:27 | 2020-09-18 15:52:13 |+--------+------+--------+---------------------+---------------------+---------------------+4 rows in set (0.00 sec)如果你仔细观察的话，会发现恢复出来的数据有问题，貌似时间不太对，你再仔细看看，是不是有的时间差了8小时！详细探究下来，我们发现 timestamp 类型字段的时间数据恢复有问题，准确来讲备份文件中记录的是0时区，而我们系统一般采用东八区，所以才会出现误差8小时的问题。那么你会问了，为什么全部恢复不会出问题呢？问的好，我们看下备份文件就知道了。12345678910111213141516171819202122232425262728293031# 备份文件开头-- MySQL dump 10.13 Distrib 5.7.23, for Linux (x86_64)---- Host: localhost Database:-- -------------------------------------------------------- Server version 5.7.23-log/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;/*!40101 SET NAMES utf8 */;/*!40103 SET @OLD_TIME_ZONE=@@TIME_ZONE */;/*!40103 SET TIME_ZONE='+00:00' */; 注意上面两行/*!40014 SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0 */;/*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */;/*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE='NO_AUTO_VALUE_ON_ZERO' */;/*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */;# 备份文件结尾/*!40103 SET TIME_ZONE=@OLD_TIME_ZONE */;/*!40101 SET SQL_MODE=@OLD_SQL_MODE */;/*!40014 SET FOREIGN_KEY_CHECKS=@OLD_FOREIGN_KEY_CHECKS */;/*!40014 SET UNIQUE_CHECKS=@OLD_UNIQUE_CHECKS */;/*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */;/*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */;/*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */;/*!40111 SET SQL_NOTES=@OLD_SQL_NOTES */;-- Dump completed on 2020-09-18 15:56:40仔细看备份文件，你会发现 mysqldump 备份出来的文件中，首先会将会话时区改为0，结尾处再改回原时区。这就代表着，备份文件中记录的时间戳数据都是以0时区为基础的。如果直接执行筛选出的SQL，就会造成0时区的时间戳插入的东八区的系统中，显然会造成时间相差8小时的问题。看到这里，不知道你是否看懂了呢，可能有过备份恢复经验的同学好理解些。解决上述问题的方法也很简单，那就是在执行SQL文件前，更改当前会话时区为0，再次来演示下：12345678910111213141516171819202122232425262728293031323334# 清空test_db表数据mysql&gt; truncate table test_tb;Query OK, 0 rows affected (0.02 sec)# 文件开头增加时区声明vim test_tb_data.sqlset session TIME_ZONE='+00:00';INSERT INTO `test_tb` VALUES (1,1001,'dsfs','2020-08-04 12:12:36','2020-09-17 06:19:27','2020-09-17 06:19:27'),(2,1002,'vfsfs','2020-09-04 12:12:36','2020-09-17 06:19:27','2020-09-17 06:19:27'),(3,1003,'adsfsf',NULL,'2020-09-17 06:19:27','2020-09-17 06:19:27'),(4,1004,'walfd','2020-09-17 14:19:27','2020-09-17 06:19:27','2020-09-18 07:52:13');# 执行恢复并比对 发现数据正确mysql&gt; select * from test_tb;+--------+------+--------+---------------------+---------------------+---------------------+| inc_id | col1 | col2 | col_dt | create_time | update_time |+--------+------+--------+---------------------+---------------------+---------------------+| 1 | 1001 | dsfs | 2020-08-04 12:12:36 | 2020-09-17 14:19:27 | 2020-09-17 14:19:27 || 2 | 1002 | vfsfs | 2020-09-04 12:12:36 | 2020-09-17 14:19:27 | 2020-09-17 14:19:27 || 3 | 1003 | adsfsf | NULL | 2020-09-17 14:19:27 | 2020-09-17 14:19:27 || 4 | 1004 | walfd | 2020-09-17 14:19:27 | 2020-09-17 14:19:27 | 2020-09-18 15:52:13 |+--------+------+--------+---------------------+---------------------+---------------------+4 rows in set (0.00 sec)mysql&gt; select * from test_tb_bak;+--------+------+--------+---------------------+---------------------+---------------------+| inc_id | col1 | col2 | col_dt | create_time | update_time |+--------+------+--------+---------------------+---------------------+---------------------+| 1 | 1001 | dsfs | 2020-08-04 12:12:36 | 2020-09-17 14:19:27 | 2020-09-17 14:19:27 || 2 | 1002 | vfsfs | 2020-09-04 12:12:36 | 2020-09-17 14:19:27 | 2020-09-17 14:19:27 || 3 | 1003 | adsfsf | NULL | 2020-09-17 14:19:27 | 2020-09-17 14:19:27 || 4 | 1004 | walfd | 2020-09-17 14:19:27 | 2020-09-17 14:19:27 | 2020-09-18 15:52:13 |+--------+------+--------+---------------------+---------------------+---------------------+4 rows in set (0.00 sec)总结：我们在网络中很容易搜索出恢复单库或单表的方法，大多都有提到上述利用 sed 、grep 命令来手动筛选的方法。但大部分文章都未提及可能出现的问题，如果你的表字段有timestamp 类型，用这种方法要格外注意。无论面对哪种恢复需求，我们都要格外小心，不要造成越恢复越糟糕的情况，最好有个空实例演练下，然后再进行恢复。]]></content>
      <categories>
        <category>技术</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysqldump备份（全量+增量）]]></title>
    <url>%2F2024%2F12378b45cf.html</url>
    <content type="text"><![CDATA[在日常运维工作中，对mysql数据库的备份是万分重要的，以防在数据库表丢失或损坏情况出现，可以及时恢复数据。线上数据库备份场景：每周日执行一次全量备份，然后每天下午1点执行MySQLdump增量备份.下面对这种备份方案详细说明下：MySQLdump增量备份配置执行增量备份的前提条件是MySQL打开binlog日志功能，在my.cnf中加入1log-bin=/opt/Data/MySQL-bin“log-bin=”后的字符串为日志记载目录，一般建议放在不同于MySQL数据目录的磁盘上。123456789-----------------------------------------------------------------------------------mysqldump &gt; 导出数据mysql &lt; 导入数据 （或者使用source命令导入数据，导入前要先切换到对应库下） 注意一个细节：若是mysqldump导出一个库的数据，导出文件为a.sql，然后mysql导入这个数据到新的空库下。如果新库名和老库名不一致，那么需要将a.sql文件里的老库名改为新库名，这样才能顺利使用mysql命令导入数据（如果使用source命令导入就不需要修改a.sql文件了）。-----------------------------------------------------------------------------------MySQLdump增量备份假定星期日下午1点执行全量备份，适用于MyISAM存储引擎。1mysqldump --lock-all-tables --flush-logs --master-data=2 -u root -p test &gt; backup_sunday_1_PM.sql对于InnoDB将–lock-all-tables替换为–single-transaction–flush-logs为结束当前日志，生成新日志文件；–master-data=2 选项将会在输出SQL中记录下完全备份后新日志文件的名称用于日后恢复时参考，例如输出的备份SQL文件中含有：1CHANGE MASTER TO MASTER_LOG_FILE=’MySQL-bin.000002′, MASTER_LOG_POS=106;MySQLdump增量备份其他说明：如果MySQLdump加上–delete-master-logs 则清除以前的日志，以释放空间。但是如果服务器配置为镜像的复制主服务器，用MySQLdump –delete-master-logs删掉MySQL二进制日志很危险，因为从服务器可能还没有完全处理该二进制日志的内容，在这种情况下，使用 PURGE MASTER LOGS更为安全。每日定时使用 MySQLadmin flush-logs来创建新日志，并结束前一日志写入过程。并把前一日志备份，例如上例中开始保存数据目录下的日志文件 MySQL-bin.000002 , …恢复完全备份1mysql -u root -p &lt; backup_sunday_1_PM.sql恢复增量备份1mysqlbinlog MySQL-bin.000002 … | mysql -u root -p注意此次恢复过程亦会写入日志文件，如果数据量很大，建议先关闭日志功能参数解释：12345678910111213141516171819202122232425262728293031323334353637383940414243444546--compatible=name它告诉 MySQLdump，导出的数据将和哪种数据库或哪个旧版本的 MySQL 服务器相兼容。值可以为 ansi、MySQL323、MySQL40、postgresql、oracle、mssql、db2、maxdb、no_key_options、no_tables_options、no_field_options 等，要使用几个值，用逗号将它们隔开。当然了，它并不保证能完全兼容，而是尽量兼容。--complete-insert，-c导出的数据采用包含字段名的完整 INSERT 方式，也就是把所有的值都写在一行。这么做能提高插入效率，但是可能会受到 max_allowed_packet 参数的影响而导致插入失败。因此，需要谨慎使用该参数，至少我不推荐。--default-character-set=charset指定导出数据时采用何种字符集，如果数据表不是采用默认的 latin1 字符集的话，那么导出时必须指定该选项，否则再次导入数据后将产生乱码问题。--disable-keys告诉 MySQLdump 在 INSERT 语句的开头和结尾增加 /*!40000 ALTER TABLE table DISABLE KEYS */; 和 /*!40000 ALTER TABLE table ENABLE KEYS */; 语句，这能大大提高插入语句的速度，因为它是在插入完所有数据后才重建索引的。该选项只适合 MyISAM 表。--extended-insert = true|false默认情况下，MySQLdump 开启 --complete-insert 模式，因此不想用它的的话，就使用本选项，设定它的值为 false 即可。--hex-blob使用十六进制格式导出二进制字符串字段。如果有二进制数据就必须使用本选项。影响到的字段类型有 BINARY、VARBINARY、BLOB。--lock-all-tables，-x在开始导出之前，提交请求锁定所有数据库中的所有表，以保证数据的一致性。这是一个全局读锁，并且自动关闭 --single-transaction 和 --lock-tables 选项。--lock-tables它和 --lock-all-tables 类似，不过是锁定当前导出的数据表，而不是一下子锁定全部库下的表。本选项只适用于 MyISAM 表，如果是 Innodb 表可以用 --single-transaction 选项。--no-create-info，-t只导出数据，而不添加 CREATE TABLE 语句。--no-data，-d不导出任何数据，只导出数据库表结构。mysqldump --no-data --databases mydatabase1 mydatabase2 mydatabase3 &gt; test.dump将只备份表结构。--databases指示主机上要备份的数据库。--opt这只是一个快捷选项，等同于同时添加 --add-drop-tables --add-locking --create-option --disable-keys --extended-insert --lock-tables --quick --set-charset 选项。本选项能让 MySQLdump 很快的导出数据，并且导出的数据能很快导回。该选项默认开启，但可以用 --skip-opt 禁用。注意，如果运行 MySQLdump 没有指定 --quick 或 --opt 选项，则会将整个结果集放在内存中。如果导出大数据库的话可能会出现问题。--quick，-q该选项在导出大表时很有用，它强制 MySQLdump 从服务器查询取得记录直接输出而不是取得所有记录后将它们缓存到内存中。--routines，-R导出存储过程以及自定义函数。--single-transaction该选项在导出数据之前提交一个 BEGIN SQL语句，BEGIN 不会阻塞任何应用程序且能保证导出时数据库的一致性状态。它只适用于事务表，例如 InnoDB 和 BDB。本选项和 --lock-tables 选项是互斥的，因为 LOCK TABLES 会使任何挂起的事务隐含提交。要想导出大表的话，应结合使用 --quick 选项。--triggers同时导出触发器。该选项默认启用，用 --skip-triggers 禁用它。跨主机备份使用下面的命令可以将host1上的sourceDb复制到host2的targetDb，前提是host2主机上已经创建targetDb数据库：1mysqldump --host=host1 --opt sourceDb| mysql --host=host2 -C targetDb-C 指示主机间的数据传输使用数据压缩结合Linux的cron命令实现定时备份比如需要在每天凌晨1:30备份某个主机上的所有数据库并压缩dump文件为gz格式：130 1 * * * mysqldump -u root -pPASSWORD --all-databases | gzip &gt; /mnt/disk2/database_`date '+%m-%d-%Y'`.sql.gz一个完整的Shell脚本备份MySQL数据库示例。比如备份数据库opspc：12345678[root@loacalhost ~]# vim /root/backup.sh#!bin/bashecho "Begin backup mysql database"mysqldump -u root -ppassword opspc &gt; /home/backup/mysqlbackup-`date +%Y-%m-%d`.sqlecho "Your database backup successfully completed"[root@loacalhost ~]# crontab -e30 1 * * * /bin/bash -x /root/backup.sh &gt; /dev/null 2&gt;&amp;1mysqldump全量备份+mysqlbinlog二进制日志增量备份1）从mysqldump备份文件恢复数据会丢失掉从备份点开始的更新数据，所以还需要结合mysqlbinlog二进制日志增量备份。首先确保已开启binlog日志功能。在my.cnf中包含下面的配置以启用二进制日志：12[mysqld]log-bin=mysql-bin2）mysqldump命令必须带上–flush-logs选项以生成新的二进制日志文件：1mysqldump --single-transaction --flush-logs --master-data=2 &gt; backup.sql其中参数–master-data=[0|1|2]0: 不记录1：记录为CHANGE MASTER语句2：记录为注释的CHANGE MASTER语句mysqldump全量+增量备份方案的具体操作可参考下面两篇文档：数据库误删除后的数据恢复操作说明解说mysql之binlog日志以及利用binlog日志恢复数据-———————————————————————————————————————————————————-shell脚本下面分享一下自己用过的mysqldump全量和增量备份脚本应用场景1）增量备份在周一到周六凌晨3点，会复制mysql-bin.00000到指定目录；2）全量备份则使用mysqldump将所有的数据库导出，每周日凌晨3点执行，并会删除上周留下的mysq-bin.00000，然后对mysql的备份操作会保留在bak.log文件中。shell脚本1全量备份脚本（假设mysql登录密码为123456；注意脚本中的命令路径）：1vim /root/Mysql-FullyBak.sh12345678910111213141516171819#!/bin/bash# Program# use mysqldump to Fully backup mysql data per week!# History# PathBakDir=/home/mysql/backupLogFile=/home/mysql/backup/bak.logDate=`date +%Y%m%d`Begin=`date +"%Y年%m月%d日 %H:%M:%S"`cd $BakDirDumpFile=$Date.sqlGZDumpFile=$Date.sql.tgz/usr/local/mysql/bin/mysqldump -uroot -p123456 --quick --events --all-databases --flush-logs --delete-master-logs --single-transaction &gt; $DumpFile/bin/tar -zvcf $GZDumpFile $DumpFile/bin/rm $DumpFileLast=`date +"%Y年%m月%d日 %H:%M:%S"`echo 开始:$Begin 结束:$Last $GZDumpFile succ &gt;&gt; $LogFilecd $BakDir/daily/bin/rm -f ./*增量备份脚本（脚本中mysql的数据存放路径是/home/mysql/data，具体根据自己的实际情况进行调整）1vim /root/Mysql-DailyBak.sh1234567891011121314151617181920212223242526272829303132333435#!/bin/bash# Program# use cp to backup mysql data everyday!# History# PathBakDir=/home/mysql/backup/daily #增量备份时复制mysql-bin.00000*的目标目录，提前手动创建这个目录BinDir=/home/mysql/data #mysql的数据目录LogFile=/home/mysql/backup/bak.logBinFile=/home/mysql/data/mysql-bin.index #mysql的index文件路径，放在数据目录下的/usr/local/mysql/bin/mysqladmin -uroot -p123456 flush-logs #这个是用于产生新的mysql-bin.00000*文件Counter=`wc -l $BinFile |awk '&#123;print $1&#125;'`NextNum=0#这个for循环用于比对$Counter,$NextNum这两个值来确定文件是不是存在或最新的for file in `cat $BinFile`do base=`basename $file` #basename用于截取mysql-bin.00000*文件名，去掉./mysql-bin.000005前面的./ NextNum=`expr $NextNum + 1` if [ $NextNum -eq $Counter ] then echo $base skip! &gt;&gt; $LogFile else dest=$BakDir/$base if(test -e $dest) #test -e用于检测目标文件是否存在，存在就写exist!到$LogFile去 then echo $base exist! &gt;&gt; $LogFile else cp $BinDir/$base $BakDir echo $base copying &gt;&gt; $LogFile fi fidoneecho `date +"%Y年%m月%d日 %H:%M:%S"` $Next Bakup succ! &gt;&gt; $LogFile设置crontab任务执行备份脚本，先执行的是增量备份脚本，然后执行的是全量备份脚本：123[root@localhost ~]# crontab -e0 3 * * 0 /bin/bash -x /root/Mysql-FullyBak.sh &gt;/dev/null 2&gt;&amp;1 #每个星期日凌晨3:00执行完全备份脚本0 3 * * 1-6 /bin/bash -x /root/Mysql-DailyBak.sh &gt;/dev/null 2&gt;&amp;1 #周一到周六凌晨3:00做增量备份手动执行上面两个脚本，测试下备份效果12345678[root@localhost backup]# pwd/home/mysql/backup[root@localhost backup]# mkdir daily[root@localhost backup]# lltotal 4drwxr-xr-x. 2 root root 4096 Nov 29 11:29 daily[root@localhost backup]# ll daily/total 0先执行增量备份脚本1234567891011121314[root@localhost backup]# sh /root/Mysql-DailyBak.sh[root@localhost backup]# lltotal 8-rw-r--r--. 1 root root 121 Nov 29 11:29 bak.logdrwxr-xr-x. 2 root root 4096 Nov 29 11:29 daily[root@localhost backup]# ll daily/total 8-rw-r-----. 1 root root 152 Nov 29 11:29 mysql-binlog.000030-rw-r-----. 1 root root 152 Nov 29 11:29 mysql-binlog.000031[root@localhost backup]# cat bak.logmysql-binlog.000030 copyingmysql-binlog.000031 copyingmysql-binlog.000032 skip!2016年11月29日 11:29:32 Bakup succ!然后执行全量备份脚本123456789101112131415[root@localhost backup]# sh /root/Mysql-FullyBak.sh20161129.sql[root@localhost backup]# lltotal 152-rw-r--r--. 1 root root 145742 Nov 29 11:30 20161129.sql.tgz-rw-r--r--. 1 root root 211 Nov 29 11:30 bak.logdrwxr-xr-x. 2 root root 4096 Nov 29 11:30 daily[root@localhost backup]# ll daily/total 0[root@localhost backup]# cat bak.logmysql-binlog.000030 copyingmysql-binlog.000031 copyingmysql-binlog.000032 skip!2016年11月29日 11:29:32 Bakup succ!开始:2016年11月29日 11:30:38 结束:2016年11月29日 11:30:38 20161129.sql.tgz succ不足以上脚本执行全备的时候会把binlog日志清掉，如果是主从集群，从节点还没有完成复制就把日志清掉的话，有风险。如果不想清掉binlog日志，可以把–delete-master-logs参数去掉，但是这样会把所有的binlog日志都备份。需要恢复时，先把全量备份恢复，然后查看全量备份sql文件的CHANGE MASTER所在行会记录binlog文件名及pos点，恢复增量备份的时候就只恢复此binlog之后的binlog日志就行。shell脚本2123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242#!/bin/bash# 配置参数BACKUP_DIR="/data/backups/mysql"MYSQL_USER="backup_user"MYSQL_PASSWORD="your_password"MYSQL_HOST="localhost"MYSQL_PORT="3306"LOG_FILE="/var/log/mysql_backup.log" # 脚本执行日志文件DATABASES="database1 database2" # 指定要备份的数据库# 指定binlog路径（根据你的MySQL配置修改）BINLOG_DIR="/var/lib/mysql"BINLOG_PREFIX="mysql-bin" # 常见的binlog前缀# 或者如果知道完整路径模式，可以直接指定# BINLOG_PATH_PATTERN="/var/lib/mysql/mysql-bin"# 位置记录文件LAST_BINLOG_FILE="$BACKUP_DIR/last_binlog.position"# 日志函数log() &#123; echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1" | tee -a $LOG_FILE&#125;# 记录binlog位置函数record_binlog_position() &#123; mysql -h$MYSQL_HOST -P$MYSQL_PORT -u$MYSQL_USER -p$MYSQL_PASSWORD -B -N -e "SHOW MASTER STATUS" | awk '&#123;print $1, $2&#125;' &gt; "$LAST_BINLOG_FILE" log "记录binlog位置: $(cat $LAST_BINLOG_FILE)"&#125;# 获取需要备份的binlog文件列表get_binlogs_to_backup() &#123; # 读取上次备份的binlog位置 if [ ! -f "$LAST_BINLOG_FILE" ]; then log "未找到上次备份记录，需要全量备份" return 1 fi local last_binlog_file last_binlog_pos read last_binlog_file last_binlog_pos &lt; "$LAST_BINLOG_FILE" log "上次备份位置: $last_binlog_file $last_binlog_pos" # 获取当前binlog信息 local current_binlog_info=$(mysql -h$MYSQL_HOST -P$MYSQL_PORT -u$MYSQL_USER -p$MYSQL_PASSWORD -B -N -e "SHOW MASTER STATUS") local current_binlog_file=$(echo "$current_binlog_info" | awk '&#123;print $1&#125;') local current_binlog_pos=$(echo "$current_binlog_info" | awk '&#123;print $2&#125;') # 获取所有binlog文件列表 local all_binlogs=$(mysql -h$MYSQL_HOST -P$MYSQL_PORT -u$MYSQL_USER -p$MYSQL_PASSWORD -B -N -e "SHOW BINARY LOGS" | awk '&#123;print $1&#125;') local binlogs_to_backup="" local found_last_file=false for binlog in $all_binlogs; do # 如果找到上次备份的文件，标记并从下一个文件开始 if [ "$binlog" = "$last_binlog_file" ]; then found_last_file=true continue fi # 如果是上次备份之后的文件，且不是当前正在写入的文件 if [ "$found_last_file" = true ] || [ -z "$last_binlog_file" ]; then if [ "$binlog" != "$current_binlog_file" ]; then binlogs_to_backup="$binlogs_to_backup $binlog" fi fi done echo "$binlogs_to_backup"&#125;# 根据binlog文件名获取完整路径get_binlog_path() &#123; local binlog_name=$1 # 直接拼接路径 echo "$&#123;BINLOG_DIR&#125;/$&#123;binlog_name&#125;"&#125;# 正确的增量备份函数incremental_backup() &#123; local backup_time=$(date '+%Y%m%d_%H%M%S') local backup_file="$BACKUP_DIR/inc/inc_backup_$&#123;backup_time&#125;.tar.gz" log "开始增量备份..." # 刷新日志 mysql -h$MYSQL_HOST -P$MYSQL_PORT -u$MYSQL_USER -p$MYSQL_PASSWORD -e "FLUSH LOGS;" 2&gt;&gt; $LOG_FILE # 获取需要备份的binlog文件列表 local binlogs_to_backup=$(get_binlogs_to_backup) if [ -z "$binlogs_to_backup" ]; then log "没有新的binlog需要备份" # 仍然记录当前位置（可能是第一次增量备份） record_binlog_position return 0 fi log "需要备份的binlog文件: $binlogs_to_backup" # 创建临时目录 local temp_dir=$(mktemp -d) local backup_success=true local backed_up_files="" # 复制binlog文件 for binlog in $binlogs_to_backup; do local binlog_path=$(get_binlog_path "$binlog") if [ -f "$binlog_path" ] &amp;&amp; [ -r "$binlog_path" ]; then # 验证binlog文件完整性 if mysqlbinlog --verify-binlog-checksum "$binlog_path" &gt;/dev/null 2&gt;&amp;1; then cp "$binlog_path" "$temp_dir/" backed_up_files="$backed_up_files $binlog" log "成功备份: $binlog" else log "错误: binlog文件损坏: $binlog" backup_success=false break fi else log "错误: 找不到或无法读取binlog文件: $binlog_path" backup_success=false break fi done # 如果备份成功，压缩文件 if [ "$backup_success" = true ] &amp;&amp; [ -n "$backed_up_files" ]; then if tar -czf "$backup_file" -C "$temp_dir" . 2&gt;&gt; $LOG_FILE; then # 验证压缩文件 if tar -tzf "$backup_file" &gt;/dev/null 2&gt;&amp;1; then # 记录新的binlog位置 record_binlog_position log "增量备份成功: $backup_file" # 记录备份信息 echo "备份时间: $backup_time" &gt; "$&#123;backup_file&#125;.info" echo "Binlog文件: $backed_up_files" &gt;&gt; "$&#123;backup_file&#125;.info" # 清理旧的增量备份（保留最近14天） find "$BACKUP_DIR/inc" -name "inc_backup_*.tar.gz" -mtime +14 -delete find "$BACKUP_DIR/inc" -name "inc_backup_*.info" -mtime +14 -delete else log "错误: 备份文件验证失败" rm -f "$backup_file" fi else log "错误: 压缩备份文件失败" fi else log "增量备份失败或没有文件需要备份" fi # 清理临时目录 rm -rf "$temp_dir"&#125;# 全量备份函数full_backup() &#123; local backup_time=$(date '+%Y%m%d_%H%M%S') local backup_file="$BACKUP_DIR/full/full_backup_$&#123;backup_time&#125;.sql" local tar_file="$BACKUP_DIR/full/full_backup_$&#123;backup_time&#125;.tar.gz" log "开始全量备份..." # 执行全量备份 mysqldump -h$MYSQL_HOST -P$MYSQL_PORT -u$MYSQL_USER -p$MYSQL_PASSWORD \ --single-transaction \ --routines \ --triggers \ --events \ --master-data=2 \ --flush-logs \ --databases $DATABASES &gt; "$backup_file" 2&gt;&gt; $LOG_FILE if [ $? -eq 0 ]; then # 压缩备份文件 tar -czf "$tar_file" -C "$(dirname "$backup_file")" "$(basename "$backup_file")" rm -f "$backup_file" # 记录binlog位置 record_binlog_position log "全量备份完成: $tar_file" # 清理旧的全量备份（保留最近4周） find "$BACKUP_DIR/full" -name "full_backup_*.tar.gz" -mtime +28 -delete else log "错误: 全量备份失败" exit 1 fi&#125;# 主函数main() &#123; local day_of_week=$(date '+%u') # 1=周一, 7=周日 # 创建备份目录 mkdir -p "$BACKUP_DIR"/&#123;full,inc&#125; case $day_of_week in 7) # 周日 - 全量备份 full_backup ;; *) # 周一到周六 - 增量备份 # 检查是否有全量备份基础 if [ ! -f "$LAST_BINLOG_FILE" ]; then log "警告: 没有找到全量备份记录，先执行全量备份" full_backup else incremental_backup fi ;; esac&#125;# 配置检查函数check_config() &#123; # 检查binlog目录是否存在 if [ ! -d "$BINLOG_DIR" ]; then log "错误: binlog目录不存在: $BINLOG_DIR" log "请检查MySQL配置中的log_bin_basename或datadir" exit 1 fi # 检查目录中是否有binlog文件 local binlog_count=$(find "$BINLOG_DIR" -name "$&#123;BINLOG_PREFIX&#125;.*" -type f | wc -l) if [ "$binlog_count" -eq 0 ]; then log "警告: 在 $BINLOG_DIR 中未找到binlog文件" log "请确认BINLOG_PREFIX配置是否正确（当前: $BINLOG_PREFIX）" fi&#125;# 执行前的配置检查check_config# 执行主函数main "$@"因为每天都会刷新日志，会导致binlog文件很多，如果需要清理的话可以修改my.cnf文件（需要重启服务生效）：123[mysqld]# 自动清理30天前的binlogexpire_logs_days = 30或者在脚本里添加清除的函数：1234567891011121314151617# 清理一个月前的binlog函数cleanup_old_binlogs() &#123; log "开始清理一个月前的binlog文件..." # 方法1: 使用PURGE BINARY LOGS命令（推荐） mysql -h$MYSQL_HOST -P$MYSQL_PORT -u$MYSQL_USER -p$MYSQL_PASSWORD -e "PURGE BINARY LOGS BEFORE NOW() - INTERVAL 30 DAY;" if [ $? -eq 0 ]; then log "成功清理一个月前的binlog文件" # 可选：验证清理结果 local remaining_binlogs=$(mysql -h$MYSQL_HOST -P$MYSQL_PORT -u$MYSQL_USER -p$MYSQL_PASSWORD -B -N -e "SHOW BINARY LOGS" | wc -l) log "剩余binlog文件数量: $remaining_binlogs" else log "错误: 清理binlog失败" fi&#125;再在主函数里执行完全量备份函数之后执行清理函数：12345678910111213141516171819202122main() &#123; local day_of_week=$(date '+%u') # 1=周一, 7=周日 # 创建备份目录 mkdir -p "$BACKUP_DIR"/&#123;full,inc&#125; case $day_of_week in 7) # 周日 - 全量备份 full_backup ;; *) # 周一到周六 - 增量备份 # 检查是否有全量备份基础 if [ ! -f "$LAST_BINLOG_FILE" ]; then log "警告: 没有找到全量备份记录，先执行全量备份" full_backup cleanup_old_binlogs else incremental_backup fi ;; esac&#125;设置crontab定时任务10 1 * * * /bin/bash mysql_backup.sh &gt;&gt; /var/log/mysql_backup.log 2&gt;&amp;1]]></content>
      <categories>
        <category>技术</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL查询调优]]></title>
    <url>%2F2024%2F12b2f7a67.html</url>
    <content type="text"><![CDATA[mysql查询为什么会慢，遇到这种问题，我们一般会想到是因为索引。那除开索引之外，还有哪些因素会导致数据库查询变慢呢？有哪些操作，可以提升mysql的查询能力呢？今天来聊聊会导致数据库查询变慢的场景有哪些，并给出原因和解决方案。数据库查询流程先来看下，一条查询语句下来，会经历哪些流程。比如我们有一张数据库表123456789TABLE `user` ( `id` int(10) unsigned NOT NULL AUTO_INCREMENT COMMENT &apos;主键&apos;, `name` varchar(100) NOT NULL DEFAULT &apos;&apos; COMMENT &apos;名字&apos;, `age` int(11) NOT NULL DEFAULT &apos;0&apos; COMMENT &apos;年龄&apos;, `gender` int(8) NOT NULL DEFAULT &apos;0&apos; COMMENT &apos;性别&apos;, PRIMARY KEY (`id`), KEY `idx_age` (`age`), KEY `idx_gender` (`gender`)) ENGINE=InnoDB DEFAULT CHARSET=utf8;我们平常写的应用代码（go或C++之类的），这时候就叫客户端了。客户端底层会带着账号密码，尝试向mysql建立一条TCP长链接。mysql的连接管理模块会对这条连接进行管理。建立连接后，客户端执行一条查询sql语句。比如：1select * from user where gender = 1 and age = 100;客户端会将sql语句通过网络连接给mysql。mysql收到sql语句后，会在分析器中先判断下SQL语句有没有语法错误，比如select，如果少打一个l，写成slect，则会报错You have an error in your SQL syntax;。接下来是优化器，在这里会根据一定的规则选择该用什么索引。之后，才是通过执行器去调用存储引擎的接口函数。存储引擎类似于一个个组件，它们才是mysql真正获取一行行数据并返回数据的地方，存储引擎是可以替换更改的，既可以用不支持事务的MyISAM，也可以替换成支持事务的Innodb。这个可以在建表的时候指定。比如1CREATE TABLE `user` ( ...) ENGINE=InnoDB;现在最常用的是InnoDB。我们就重点说这个。InnoDB中，因为直接操作磁盘会比较慢，所以加了一层内存提提速，叫buffer pool，这里面，放了很多内存页，每一页16KB，有些内存页放的是数据库表里看到的那种一行行的数据，有些则是放的索引信息。查询SQL到了InnoDB中。会根据前面优化器里计算得到的索引，去查询相应的索引页，如果不在buffer pool里则从磁盘里加载索引页。再通过索引页加速查询，得到数据页的具体位置。如果这些数据页不在buffer pool中，则从磁盘里加载进来。这样我们就得到了我们想要的一行行数据。最后将得到的数据结果返回给客户端。慢查询分析如果上面的流程比较慢的话，我们可以通过开启profiling看到流程慢在哪。12345678910mysql&gt; set profiling=ON;Query OK, 0 rows affected, 1 warning (0.00 sec)mysql&gt; show variables like 'profiling';+---------------+-------+| Variable_name | Value |+---------------+-------+| profiling | ON |+---------------+-------+1 row in set (0.00 sec)然后正常执行sql语句。这些SQL语句的执行时间都会被记录下来，此时你想查看有哪些语句被记录下来了，可以执行 show profiles;1234567891011mysql&gt; show profiles;+----------+------------+---------------------------------------------------+| Query_ID | Duration | Query |+----------+------------+---------------------------------------------------+| 1 | 0.06811025 | select * from user where age&gt;=60 || 2 | 0.00151375 | select * from user where gender = 2 and age = 80 || 3 | 0.00230425 | select * from user where gender = 2 and age = 60 || 4 | 0.00070400 | select * from user where gender = 2 and age = 100 || 5 | 0.07797650 | select * from user where age!=60 |+----------+------------+---------------------------------------------------+5 rows in set, 1 warning (0.00 sec)关注下上面的query_id，比如select * from user where age&gt;=60对应的query_id是1，如果你想查看这条SQL语句的具体耗时，那么可以执行以下的命令。123456789101112131415161718192021mysql&gt; show profile for query 1;+----------------------+----------+| Status | Duration |+----------------------+----------+| starting | 0.000074 || checking permissions | 0.000010 || Opening tables | 0.000034 || init | 0.000032 || System lock | 0.000027 || optimizing | 0.000020 || statistics | 0.000058 || preparing | 0.000018 || executing | 0.000013 || Sending data | 0.067701 || end | 0.000021 || query end | 0.000015 || closing tables | 0.000014 || freeing items | 0.000047 || cleaning up | 0.000027 |+----------------------+----------+15 rows in set, 1 warning (0.00 sec)通过上面的各个项，大家就可以看到具体耗时在哪。比如从上面可以看出Sending data的耗时最大，这个是指执行器开始查询数据并将数据发送给客户端的耗时，因为我的这张表符合条件的数据有好几万条，所以这块耗时最大，也符合预期。一般情况下，我们开发过程中，耗时大部分时候都在Sending data阶段，而这一阶段里如果慢的话，最容易想到的还是索引相关的原因。索引相关原因索引相关的问题，一般能用explain命令帮助分析。通过它能看到用了哪些索引，大概会扫描多少行之类的信息。mysql会在优化器阶段里看下选择哪个索引，查询速度会更快。一般主要考虑几个因素，比如：选择这个索引大概要扫描多少行（rows）为了把这些行取出来，需要读多少个16kb的页走普通索引需要回表，主键索引则不需要，回表成本大不大？回到show profile中提到的sql语句，我们使用explain select * from user where age&gt;=60 分析一下。上面的这条语句，使用的type为ALL，意味着是全表扫描，possible_keys是指可能用得到的索引，这里可能使用到的索引是为age建的普通索引，但实际上数据库使用的索引是在key那一列，是NULL。也就是说这句sql不走索引，全表扫描。这个是因为数据表里，符合条件的数据行数（rows）太多，如果使用age索引，那么需要将它们从age索引中读出来，并且age索引是普通索引，还需要回表找到对应的主键才能找到对应的数据页。算下来还不如直接走主键划算。于是最终选择了全表扫描。当然上面只是举了个例子，实际上，mysql执行sql时，不用索引或者用的索引不符合我们预期这件事经常发生，索引失效的场景有很多，比如用了不等号，隐式转换等。聊两个生产中容易遇到的问题吧。索引不符合预期实际开发中有些情况比较特殊，比如有些数据库表一开始数据量小，索引少，执行sql时，确实使用了符合你预期的索引。但随时时间边长，开发的人变多了，数据量也变大了，甚至还可能会加入一些其他重复多余的索引，就有可能出现用着用着，用到了不符合你预期的其他索引了。从而导致查询突然变慢。这种问题，也好解决，可以通过force index。比如通过explain可以看出，加了force index之后，sql就选用了idx_age这个索引了。走了索引还是很慢有些sql，用explain命令看，明明是走索引的，但还是很慢。一般是两种情况：第一种是索引区分度太低，比如网页全路径的url链接，这拿来做索引，一眼看过去全都是同一个域名，如果前缀索引的长度建得不够长，那这走索引跟走全表扫描似的，正确姿势是尽量让索引的区分度更高，比如域名去掉，只拿后面URI部分去做索引。第二种是索引中匹配到的数据太大，这时候需要关注的是explain里的rows字段了。它是用于预估这个查询语句需要查的行数的，它不一定完全准确，但可以体现个大概量级。当它很大时，一般常见的是下面几种情况。如果这个字段具有唯一的属性，比如电话号码等，一般是不应该有大量重复的，那可能是你代码逻辑出现了大量重复插入的操作，你需要检查下代码逻辑，或者需要加个唯一索引限制下。如果这个字段下的数据就是会很大，是否需要全部拿？如果不需要，加个limit限制下。如果确实要拿全部，那也不能一次性全拿，今天你数据量小，可能一次取一两万都没啥压力，万一哪天涨到了十万级别，那一次性取就有点吃不消了。你可能需要分批次取，具体操作是先用order by id排序一下，拿到一批数据后取最大id作为下次取数据的起始位置。连接数过小索引相关的原因我们聊完了，我们来聊聊，除了索引之外，还有哪些因素会限制我们的查询速度的。我们可以看到，mysql的server层里有个连接管理，它的作用是管理客户端和mysql之间的长连接。正常情况下，客户端与server层如果只有一条连接，那么在执行sql查询之后，只能阻塞等待结果返回，如果有大量查询同时并发请求，那么后面的请求都需要等待前面的请求执行完成后，才能开始执行。因此很多时候我们的应用程序，比如go或java这些，会打印出sql执行了几分钟的日志，但实际上你把这条语句单独拎出来执行，却又是毫秒级别的。这都是因为这些sql语句在等待前面的sql执行完成。怎么解决呢？如果我们能多建几条连接，那么请求就可以并发执行，后面的连接就不用等那么久了。而连接数过小的问题，受数据库和客户端两侧同时限制。数据库连接数过小Mysql的最大连接数默认是100, 最大可以达到16384。可以通过设置mysql的max_connections参数，更改数据库的最大连接数。12345678910mysql&gt; set global max_connections= 500;Query OK, 0 rows affected (0.00 sec)mysql&gt; show variables like 'max_connections';+-----------------+-------+| Variable_name | Value |+-----------------+-------+| max_connections | 500 |+-----------------+-------+1 row in set (0.00 sec)上面的操作，就把最大连接数改成了500。应用侧连接数过小数据库连接大小是调整过了，但貌似问题还是没有变化？还是有很多sql执行达到了几分钟，甚至超时？那有可能是因为你应用侧（go，java写的应用，也就是mysql的客户端）的连接数也过小。应用侧与mysql底层的连接，是基于TCP协议的长链接，而TCP协议，需要经过三次握手和四次挥手来实现建连和释放。如果我每次执行sql都重新建立一个新的连接的话，那就要不断握手和挥手，这很耗时。所以一般会建立一个长连接池，连接用完之后，塞到连接池里，下次要执行sql的时候，再从里面捞一条连接出来用，非常环保。我们一般写代码的时候，都会通过第三方的orm库来对数据库进行操作，而成熟的orm库，百分之一千万都会有个连接池。而这个连接池，一般会有个大小。这个大小就控制了你的连接数最大值，如果说你的连接池太小，都还没有数据库的大，那调了数据库的最大连接数也没啥作用。一般情况下，可以翻下你使用的orm库的文档，看下怎么设置这个连接池的大小，就几行代码的事情，改改就好。比如go语言里的gorm里是这么设置的1234567func Init() &#123; db, err := gorm.Open(mysql.Open(conn), config) sqlDB, err := db.DB() // SetMaxIdleConns 设置空闲连接池中连接的最大数量 sqlDB.SetMaxIdleConns(200) // SetMaxOpenConns 设置打开数据库连接的最大数量 sqlDB.SetMaxOpenConns(1000)buffer pool太小连接数是上去了，速度也提升了。我们在前面的数据库查询流程里，提到了进了innodb之后，会有一层内存buffer pool，用于将磁盘数据页加载到内存页中，只要查询到buffer pool里有，就可以直接返回，否则就要走磁盘IO，那就慢了。也就是说，如果我的buffer pool 越大，那我们能放的数据页就越多，相应的，sql查询时就更可能命中buffer pool，那查询速度自然就更快了。可以通过下面的命令查询到buffer pool的大小，单位是Byte。1234567mysql&gt; show global variables like 'innodb_buffer_pool_size';+-------------------------+-----------+| Variable_name | Value |+-------------------------+-----------+| innodb_buffer_pool_size | 134217728 |+-------------------------+-----------+1 row in set (0.01 sec)也就是128Mb。如果想要调大一点。可以执行12345678910mysql&gt; set global innodb_buffer_pool_size = 536870912;Query OK, 0 rows affected (0.01 sec)mysql&gt; show global variables like 'innodb_buffer_pool_size';+-------------------------+-----------+| Variable_name | Value |+-------------------------+-----------+| innodb_buffer_pool_size | 536870912 |+-------------------------+-----------+1 row in set (0.01 sec)这样就把buffer pool增大到512Mb了。但是吧，如果buffer pool大小正常，只是别的原因导致的查询变慢，那改buffer pool毫无意义。怎么知道buffer pool是不是太小了？这个我们可以看buffer pool的缓存命中率。通过 show status like &#39;Innodb_buffer_pool_%&#39;;可以看到跟buffer pool有关的一些信息。Innodb_buffer_pool_read_requests表示读请求的次数。Innodb_buffer_pool_reads 表示从物理磁盘中读取数据的请求次数。所以buffer pool的命中率就可以这样得到：1buffer pool 命中率 = 1 - (Innodb_buffer_pool_reads/Innodb_buffer_pool_read_requests) * 100%比如我上面截图里的就是，1 - (405/2278354) = 99.98%。可以说命中率非常高了。一般情况下buffer pool命中率都在99%以上，如果低于这个值，才需要考虑加大innodb buffer pool的大小。当然，还可以把这个命中率做到监控里，这样半夜sql变慢了，早上上班还能定位到原因，就很舒服。还有哪些骚操作？前面提到的是在存储引擎层里加入了buffer pool用于缓存内存页，这样可以加速查询。那同样的道理，server层也可以加个缓存，直接将第一次查询的结果缓存下来，这样下次查询就能立刻返回，听着挺美的。按道理，如果命中缓存的话，确实是能为查询加速的。但这个功能限制很大，其中最大的问题是只要数据库表被更新过，表里面的所有缓存都会失效，数据表频繁的更新，就会带来频繁的缓存失效。所以这个功能只适合用于那些不怎么更新的数据表。另外，这个功能在8.0版本之后，就被干掉了。所以这功能用来聊聊天可以，没必要真的在生产中使用啊。总结数据查询过慢一般是索引问题，可能是因为选错索引，也可能是因为查询的行数太多。客户端和数据库连接数过小，会限制sql的查询并发数，增大连接数可以提升速度。innodb里会有一层内存buffer pool用于提升查询速度，命中率一般&gt;99%，如果低于这个值，可以考虑增大buffer pool的大小，这样也可以提升速度。查询缓存（query cache）确实能为查询提速，但一般不建议打开，因为限制比较大，并且8.0以后的Mysql里已经将这个功能干掉了。]]></content>
      <categories>
        <category>技术</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL删除所有表]]></title>
    <url>%2F2024%2F125a754aee.html</url>
    <content type="text"><![CDATA[当我们需要在MySQL数据库中删除所有已经存在的表时，我们可以通过执行多条drop语句的方式逐个删除，但是这样过于繁琐，没有效率，而且容易出错。以下介绍一次性删除数据库中的所有表的方法。方案一：使用mysqldump命令mysqldump是一个备份工具，也可以用来删除数据库中的所有表。在命令行输入如下命令即可：1mysqldump -u [用户名] -p --add-drop-table --no-data [数据库名] | grep ^DROP | mysql -u [用户名] -p [数据库名]其中[用户名]指的是你在MySQL中的用户名，[数据库名]指的是需要删除表格的数据库名称。它执行的步骤是先使用mysqldump备份数据库，然后从备份文件中查找DROP语句并执行。下面是一个示例：1$ mysqldump -u root -p --add-drop-table --no-data mydb | grep ^DROP | mysql -u root -p mydb执行时需要输入你的MySQL账户密码。此方法适用于需要备份数据库的情况。方案二：利用MySQL的元数据MySQL的元数据中记录着所有的表名称，我们可以通过查询并删除所有表的方式来实现一次性删除。在命令行输入如下语句即可：123SELECT concat(&apos;DROP TABLE IF EXISTS `&apos;, table_schema, &apos;`.`&apos;, table_name, &apos;`;&apos;)FROM information_schema.tablesWHERE table_schema = &apos;[数据库名]&apos;;其中[数据库名]指的是需要删除表格的数据库名称。它执行的步骤是使用SELECT语句查询information_schema表中的tables表，构建并输出每个表格的DROP TABLE语句，然后执行这些语句。下面是一个示例：1234567891011mysql&gt; SELECT concat(&apos;DROP TABLE IF EXISTS `&apos;, table_schema, &apos;`.`&apos;, table_name, &apos;`;&apos;) -&gt; FROM information_schema.tables -&gt; WHERE table_schema = &apos;mydb&apos;;+--------------------------------------------------------------+| concat(&apos;DROP TABLE IF EXISTS `&apos;, table_schema, &apos;`.`&apos;, table_name, &apos;`;&apos;) |+--------------------------------------------------------------+| DROP TABLE IF EXISTS `mydb`.`table1`; || DROP TABLE IF EXISTS `mydb`.`table2`; || DROP TABLE IF EXISTS `mydb`.`table3`; |+--------------------------------------------------------------+3 rows in set (0.00 sec)执行查询返回的结果即可删除所有表格。此方法适用于非备份的情况。123DROP TABLE IF EXISTS `mydb`.`table1`; DROP TABLE IF EXISTS `mydb`.`table2`; DROP TABLE IF EXISTS `mydb`.`table3`;总结本文介绍了两种可以一次性删除MySQL数据库中所有表格的方法。使用mysqldump命令可以备份并删除数据库，而利用MySQL的元数据可以查询并删除所有表格。题外话：查看数据库大小sql：1SELECT table_schema "Database Name", sum( data_length + index_length ) / 1024 / 1024 / 1024"Database Size in GB" FROM information_schema.TABLES GROUP BY table_schema;]]></content>
      <categories>
        <category>技术</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubesphere监控信息缺失]]></title>
    <url>%2F2024%2F1153ecdc40.html</url>
    <content type="text"><![CDATA[K8s 1.24 已从 cAdvisor 中删除了 Docker 插件。虽然可以使用 cri-dockerd（Docker by Mirantis）来调整容器运行时，但 kubelet 无法再通过 cAdvisor 检索 Docker 容器信息，例如镜像、pod、容器标签等，导致kubesphere的pod监控页面监控信息缺失。kubesphere版本：v3.3.2k8s版本：v1.27.4pod监控没有数据：应用资源，用量排行也没有数据：标签缺失：原因：高版本 Kubernetes （v1.24及以上）使用 docker 运行时存在指标缺少关键标签问题， 可以切换运行时，或者额外部署一个cadvisor 负载。cAdvisor standalone &amp; ServiceMonitor yaml：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213apiVersion: v1kind: ServiceAccountmetadata: labels: app: cadvisor name: cadvisor namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: labels: app: cadvisor name: cadvisorrules:- apiGroups: - policy resourceNames: - cadvisor resources: - podsecuritypolicies verbs: - use---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: labels: app: cadvisor name: cadvisorroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cadvisorsubjects:- kind: ServiceAccount name: cadvisor namespace: kube-system---apiVersion: apps/v1kind: DaemonSetmetadata: annotations: seccomp.security.alpha.kubernetes.io/pod: docker/default labels: app: cadvisor name: cadvisor namespace: kube-systemspec: selector: matchLabels: app: cadvisor name: cadvisor template: metadata: annotations: scheduler.alpha.kubernetes.io/critical-pod: "" labels: app: cadvisor name: cadvisor spec: automountServiceAccountToken: false containers: - args: - --housekeeping_interval=10s - --max_housekeeping_interval=15s - --event_storage_event_limit=default=0 - --event_storage_age_limit=default=0 - --enable_metrics=app,cpu,disk,diskIO,memory,network,process - --docker_only - --store_container_labels=false - --whitelisted_container_labels=io.kubernetes.container.name,io.kubernetes.pod.name,io.kubernetes.pod.namespace image: gcr.io/cadvisor/cadvisor:v0.45.0 name: cadvisor ports: - containerPort: 8080 name: http protocol: TCP resources: limits: cpu: 800m memory: 2000Mi requests: cpu: 400m memory: 400Mi volumeMounts: - mountPath: /rootfs name: rootfs readOnly: true - mountPath: /var/run name: var-run readOnly: true - mountPath: /sys name: sys readOnly: true - mountPath: /var/lib/docker name: docker readOnly: true - mountPath: /dev/disk name: disk readOnly: true priorityClassName: system-node-critical serviceAccountName: cadvisor terminationGracePeriodSeconds: 30 tolerations: - key: node-role.kubernetes.io/controlplane value: "true" effect: NoSchedule - key: node-role.kubernetes.io/etcd value: "true" effect: NoExecute volumes: - hostPath: path: / name: rootfs - hostPath: path: /var/run name: var-run - hostPath: path: /sys name: sys - hostPath: path: /var/lib/docker name: docker - hostPath: path: /dev/disk name: disk---apiVersion: policy/v1beta1kind: PodSecurityPolicymetadata: labels: app: cadvisor name: cadvisor namespace: kube-systemspec: allowedHostPaths: - pathPrefix: / - pathPrefix: /var/run - pathPrefix: /sys - pathPrefix: /var/lib/docker - pathPrefix: /dev/disk fsGroup: rule: RunAsAny runAsUser: rule: RunAsAny seLinux: rule: RunAsAny supplementalGroups: rule: RunAsAny volumes: - '*'---apiVersion: v1kind: Servicemetadata: name: cadvisor labels: app: cadvisor namespace: kube-systemspec: selector: app: cadvisor ports: - name: cadvisor port: 8080 protocol: TCP targetPort: 8080---apiVersion: monitoring.coreos.com/v1kind: ServiceMonitormetadata: labels: app: cadvisor name: cadvisor namespace: kube-systemspec: endpoints: - metricRelabelings: - sourceLabels: - container_label_io_kubernetes_pod_name targetLabel: pod - sourceLabels: - container_label_io_kubernetes_container_name targetLabel: container - sourceLabels: - container_label_io_kubernetes_pod_namespace targetLabel: namespace - action: labeldrop regex: container_label_io_kubernetes_pod_name - action: labeldrop regex: container_label_io_kubernetes_container_name - action: labeldrop regex: container_label_io_kubernetes_pod_namespace port: cadvisor relabelings: - sourceLabels: - __meta_kubernetes_pod_node_name targetLabel: node - sourceLabels: - __metrics_path__ targetLabel: metrics_path replacement: /metrics/cadvisor - sourceLabels: - job targetLabel: job replacement: kubelet namespaceSelector: matchNames: - kube-system selector: matchLabels: app: cadvisor部署完成后监控恢复正常：]]></content>
      <categories>
        <category>技术</category>
        <category>K8S</category>
      </categories>
      <tags>
        <tag>K8S</tag>
        <tag>kubesphere</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos机器磁盘扩容]]></title>
    <url>%2F2024%2F11adf12f0a.html</url>
    <content type="text"><![CDATA[内网机器磁盘可用空间不足，发现磁盘分区不合理，且有的分区磁盘未挂载使用 把分区初始化为物理卷，以便被 LVM 使用：1pvcreate /dev/nvme0n1p3查看卷组信息：扩展卷组：1vgextend centos /dev/nvme0n1p3再查看卷组信息，发现有空闲的空间了：扩容空间：把全部空间扩容给 /dev/mapper/centos-root1lvextend -l +``100``%FREE /dev/mapper/centos-root看一下你的分区文件系统格式，运行cat /etc/fstab：扩展/dev/mapper/centos-root文件系统：如果是ext文件系统：resize2fs /dev/mapper/centos-root如果是XFS文件系统：xfs_growfs /dev/mapper/centos-root查看文件系统是否扩容成功：扩容前：扩容后：]]></content>
      <categories>
        <category>技术</category>
        <category>分区</category>
      </categories>
      <tags>
        <tag>磁盘</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[skywalking部署]]></title>
    <url>%2F2024%2F102994c0ea.html</url>
    <content type="text"><![CDATA[SkyWalking是一个开源的可观察性平台，用于收集，分析，聚合和可视化来自本地或者云服务中的数据。即使在整个云环境中，SkyWalking也能提供一种简便的方法来维护您的分布式系统的清晰视图。它是一个现代的APM（Application Performance Monitor 应用性能监测软件），专门为基于云、容器的分布式系统而设计。为什么要选择skywalking？SkyWalking提供了用于在许多不同情况下观察和监视分布式系统的解决方案，并通过agent方式，做到高性能、低损耗、无侵入性，与类似的功能组件如：Zipkin、Pinpoint、CAT相比，skywalking无论是从性能还是社区活跃度方面考虑，都具有一定的优势。skywalking监控维度skywalking从三个维度提供可观察项功能，分别是：服务，服务实例，端点服务：表示一组/一组工作负载，这些工作负载为传入请求提供相同的行为。服务实例：服务组中的每个单独工作负载都称为实例。像pods在Kubernetes中一样，它不必是单个OS进程，但是，如果您使用agent代理，则实例实际上是一个真正的OS进程。端点：服务中用于传入请求的路径，例如HTTP URI路径或gRPC服务类+方法签名。skywalking架构从逻辑上看，skywalking分为四个部分：探针，平台后端，存储和UI。探针：收集数据并重新格式化以符合SkyWalking的要求（不同的探针支持不同的来源）。平台后端：支持数据聚合，分析和流处理，涵盖跟踪，指标和日志。存储：设备通过开放/可插入的界面存储SkyWalking数据。您可以选择现有的实现，例如ElasticSearch，H2，MySQL，TiDB，InfluxDB，或者实现自己的实现。UI：是一个高度可定制的基于Web的界面，允许SkyWalking最终用户可视化和管理SkyWalking数据。skywalking安装docker-compose部署版本名称版本skywalking-oap-server8.9.1skywalking-ui8.9.1elasticsearch7.14.2java-agent8.8.0skywalking单机部署docker-compose.yml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051version: '3.3'services: elasticsearch: image: docker.elastic.co/elasticsearch/elasticsearch:7.14.2 container_name: elasticsearch restart: always ports: - 9200:9200 environment: - "TAKE_FILE_OWNERSHIP=true" #volumes 挂载权限 如果不想要挂载es文件改配置可以删除 - "discovery.type=single-node" #单机模式启动 - "TZ=Asia/Shanghai" # 设置时区 - "ES_JAVA_OPTS=-Xms512m -Xmx512m" # 设置jvm内存大小 volumes: - ./elasticsearch/logs:/usr/share/elasticsearch/logs - ./elasticsearch/data:/usr/share/elasticsearch/data# - ./elasticsearch/conf/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml ulimits: memlock: soft: -1 hard: -1 skywalking-oap-server: image: apache/skywalking-oap-server:8.9.1 container_name: skywalking-oap-server depends_on: - elasticsearch links: - elasticsearch restart: always ports: - 11800:11800 - 12800:12800 environment: SW_STORAGE: elasticsearch # 指定ES版本 SW_STORAGE_ES_CLUSTER_NODES: elasticsearch:9200 TZ: Asia/Shanghai# volumes:# - ./oap/conf/alarm-settings.yml:/skywalking/config/alarm-settings.yml skywalking-ui: image: apache/skywalking-ui:8.9.1 container_name: skywalking-ui depends_on: - skywalking-oap-server links: - skywalking-oap-server restart: always ports: - 8080:8080 environment: SW_OAP_ADDRESS: http://skywalking-oap-server:12800 TZ: Asia/Shanghai执行命令1docker-compose up -d等待大约2~3min，在浏览器中输入一下地址进行访问：1http://10.168.2.126:8080/skywalking使用agent目录简介如果想要使用可选插件将其复制到plugins目录即可【需要重启应用程序生效】，不想用只需在plugins目录中将插件删除【需要重启应用后生效】，gateway服务需要使用单独的agent，把optional-plugins目录里的apm-spring-cloud-gateway-3.x-plugin-8.8.0.jar复制到plugins目录里。把服务引到k8s集群使用skywalking.yml：1234567891011121314151617181920212223242526apiVersion: v1kind: Endpointsmetadata: name: oap namespace: qifusubsets:- addresses: - ip: 10.168.2.126 ports: - port: 11800 name: grpc - port: 12800 name: rest ---apiVersion: v1kind: Servicemetadata: name: oap namespace: qifuspec: ports: - port: 11800 name: grpc - port: 12800 name: rest创建ep和svc1kubectl apply -f skywalking.ymlidea 配置在VM参数中增加如下配置1"-javaagent:/app/skywalking-agent/skywalking-agent.jar", "-Dskywalking.agent.service_name=qifu-saas-bc-test", "-Dskywalking.collector.backend_service=oap.qifu.svc:11800",java -jar配置1java "-javaagent:/app/skywalking-agent/skywalking-agent.jar", "-Dskywalking.agent.service_name=qifu-saas-bc-test", "-Dskywalking.collector.backend_service=oap.qifu.svc:11800", -jar SpringBoot-0.0.1-SNAPSHOT.jar注意：javaagent 需配置在 -jar前面详细使用请看官网介绍：java-agent使用教程一切准备就绪后，启动项目1）仪表盘可以APM 处看见当前服务注意：如果此处没有出现服务，可以尝试访问项目中的任意接口或者多刷新几次页面)如果还是没有可以查看agent目录下的日志看是否存在报错访问项目中任意接口2）拓扑图点击服务本身可以展示各种指标3）追踪可以在此处看见刚刚访问的接口以及耗时情况(如果此时搜索不出来，需注意时间范围是否正确)4）性能剖析如果在追踪中发现了访问异常慢的接口可以通过性能剖析对其进行分析操作步骤新建任务选择服务(如果此处未出现服务名称选择，需要刷新时间)配置端点名称（注意此处最好是从追踪里面获取端点名称，需要加上请求方式，此处还有一点需要注意的是，接口的请求路径是全路径，需要加上项目的contex-path才行）设置监控时间采样数点击新建任务请求配置的接口出现如下页面（如果Sampled Traces未出现注意刷新时间【这是skywalking唯一一个不友好的地方，不管访问任务页面都要注意时间的影响】）点击分析按钮出现堆栈信息可以看见耗时最下最长的信息。5）日志采集日志采集需要配合TraceId一起做，有一定的侵入性在项目中引入jar包注意此处由于我使用的日志框架是logback，如果使用其他日志框架需引入对应的jar包目前支持的日志框架有：log4j、log4j2和 logback。12345678910&lt;dependency&gt; &lt;groupId&gt;org.apache.skywalking&lt;/groupId&gt; &lt;artifactId&gt;apm-toolkit-logback-1.x&lt;/artifactId&gt; &lt;version&gt;$&#123;apm-toolkit.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.skywalking&lt;/groupId&gt; &lt;artifactId&gt;apm-toolkit-trace&lt;/artifactId&gt; &lt;version&gt;$&#123;apm-toolkit.version&#125;&lt;/version&gt;&lt;/dependency&gt;日志数据格式化123456789&lt;!--日志文件输出格式--&gt;&lt;encoder class="ch.qos.logback.core.encoder.LayoutWrappingEncoder"&gt; &lt;layout class="org.apache.skywalking.apm.toolkit.log.logback.v1.x.TraceIdPatternLogbackLayout"&gt; &lt;pattern&gt; %d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%ip] [ %thread ] - [ %-5level ] [ %logger&#123;50&#125; : %line ] - [%tid] - %msg%n &lt;/pattern&gt; &lt;/layout&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;!-- 设置字符集 --&gt;&lt;/encoder&gt;日志采集12345678&lt;!-- skywalking日志收集 --&gt; &lt;appender name="skywalking-log" class="org.apache.skywalking.apm.toolkit.log.logback.v1.x.log.GRPCLogClientAppender"&gt; &lt;encoder class="ch.qos.logback.core.encoder.LayoutWrappingEncoder"&gt; &lt;layout class="org.apache.skywalking.apm.toolkit.log.logback.v1.x.mdc.TraceIdMDCPatternLogbackLayout"&gt; &lt;pattern&gt;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%ip] %thread %logger %level - [%tid] %msg %n&lt;/pattern&gt; &lt;/layout&gt; &lt;/encoder&gt; &lt;/appender&gt;完整配置文件logback-spring.xml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;configuration scan="true" scanPeriod="10 seconds"&gt; &lt;!-- 定义日志的根目录 --&gt; &lt;property name="LOG_HOME" value="logs"/&gt; &lt;!-- 定义日志文件名称 --&gt; &lt;!-- &lt;property name="appName" value="$&#123;&#125;"/&gt;--&gt; &lt;springProperty scope="context" name="APP_NAME" source="spring.application.name"/&gt; &lt;!-- 彩色日志 --&gt; &lt;!-- 彩色日志依赖的渲染类 --&gt; &lt;conversionRule conversionWord="clr" converterClass="org.springframework.boot.logging.logback.ColorConverter"/&gt; &lt;conversionRule conversionWord="wex" converterClass="org.springframework.boot.logging.logback.WhitespaceThrowableProxyConverter"/&gt; &lt;conversionRule conversionWord="wEx" converterClass="org.springframework.boot.logging.logback.ExtendedWhitespaceThrowableProxyConverter"/&gt; &lt;!-- 彩色日志格式 --&gt; &lt;property name="CONSOLE_LOG_PATTERN" value="$&#123;CONSOLE_LOG_PATTERN:-%clr(%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125;)&#123;faint&#125; %clr($&#123;LOG_LEVEL_PATTERN:-%5p&#125;) %clr($&#123;PID:- &#125;)&#123;magenta&#125; %clr(---)&#123;faint&#125; %clr([%15.15t])&#123;faint&#125; %clr(%-40.40logger&#123;39&#125;)&#123;cyan&#125; - [%tid] - %clr(:)&#123;faint&#125; %m%n$&#123;LOG_EXCEPTION_CONVERSION_WORD:-%wEx&#125;&#125;"/&gt; &lt;!-- ch.qos.logback.core.ConsoleAppender 表示控制台输出 --&gt; &lt;appender name="stdout" class="ch.qos.logback.core.ConsoleAppender"&gt; &lt;encoder class="ch.qos.logback.core.encoder.LayoutWrappingEncoder"&gt; &lt;layout class="org.apache.skywalking.apm.toolkit.log.logback.v1.x.TraceIdPatternLogbackLayout"&gt; &lt;pattern&gt;$&#123;CONSOLE_LOG_PATTERN&#125;&lt;/pattern&gt; &lt;/layout&gt; &lt;!-- 设置字符集 --&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;!-- 滚动记录文件，先将日志记录到指定文件，当符合某个条件时，将日志记录到其他文件 --&gt; &lt;appender name="appLogAppender" class="ch.qos.logback.core.rolling.RollingFileAppender"&gt; &lt;!--日志文件输出格式--&gt; &lt;encoder class="ch.qos.logback.core.encoder.LayoutWrappingEncoder"&gt; &lt;layout class="org.apache.skywalking.apm.toolkit.log.logback.v1.x.TraceIdPatternLogbackLayout"&gt; &lt;pattern&gt; %d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [ %thread ] - [ %-5level ] [ %logger&#123;50&#125; : %line ] - [%tid] - %msg%n &lt;/pattern&gt; &lt;/layout&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;!-- 设置字符集 --&gt; &lt;/encoder&gt; &lt;rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy"&gt; &lt;!-- 滚动时产生的文件的存放位置及文件名称 %d&#123;yyyy-MM-dd&#125;：按天进行日志滚动 %i：当文件大小超过maxFileSize时，按照i进行文件滚动 --&gt; &lt;fileNamePattern&gt;$&#123;LOG_HOME&#125;/$&#123;APP_NAME&#125;-%d&#123;yyyy-MM-dd&#125;-%i.log&lt;/fileNamePattern&gt; &lt;!-- 可选节点，控制保留的归档文件的最大数量，超出数量就删除旧文件。假设设置每天滚动， 且maxHistory是365，则只保存最近365天的文件，删除之前的旧文件。注意，删除旧文件是， 那些为了归档而创建的目录也会被删除。 --&gt; &lt;MaxHistory&gt;31&lt;/MaxHistory&gt; &lt;timeBasedFileNamingAndTriggeringPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP"&gt; &lt;maxFileSize&gt;30MB&lt;/maxFileSize&gt; &lt;/timeBasedFileNamingAndTriggeringPolicy&gt; &lt;/rollingPolicy&gt; &lt;/appender&gt; &lt;!-- skywalking日志收集 --&gt; &lt;appender name="skywalking-log" class="org.apache.skywalking.apm.toolkit.log.logback.v1.x.log.GRPCLogClientAppender"&gt; &lt;encoder class="ch.qos.logback.core.encoder.LayoutWrappingEncoder"&gt; &lt;layout class="org.apache.skywalking.apm.toolkit.log.logback.v1.x.mdc.TraceIdMDCPatternLogbackLayout"&gt; &lt;pattern&gt;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; %thread %logger %level - [%tid] %msg %n&lt;/pattern&gt; &lt;/layout&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;!-- root与logger是父子关系，没有特别定义则默认为root，任何一个类只会和一个logger对应， 要么是定义的logger，要么是root，判断的关键在于找到这个logger，然后判断这个logger的appender和level。 --&gt; &lt;springProfile name="dev,local,test"&gt; &lt;root level="info"&gt; &lt;appender-ref ref="stdout"/&gt; &lt;appender-ref ref="appLogAppender"/&gt; &lt;appender-ref ref="skywalking-log"/&gt; &lt;/root&gt; &lt;/springProfile&gt; &lt;springProfile name="prod"&gt; &lt;root level="info"&gt; &lt;appender-ref ref="appLogAppender"/&gt; &lt;/root&gt; &lt;/springProfile&gt;&lt;/configuration&gt;待一切配置好后(重启项目)需要等待3~5min就可以看见日志信息了注意时间范围TraceId获取配置了一个全局异常处理器，如果发生异常则返回TraceId,然后用该TraceId去查找相应的日志与链路信息123456@ExceptionHandler(value = Exception.class)@Tracepublic ResponseModel handleException(Exception e) &#123; log.error("异常堆栈=&gt;", e); return ResponseModel.errorMsg(RepCodeEnum.BLANK_ERROR, TraceContext.traceId());&#125;定位链路点击相关日志进入日志详情注意：日志的上传需要时间，不能立马获取到实时日志k8s部署skywalkingElasticsearch 部署Skywalking OAP Server会将数据存储到 Elasticsearch 中，并通过 Elasticsearch 进行数据查询和分析，这里直接使用已经部署好的es集群。数据初始化Job（init-job.yaml）1234567891011121314151617181920212223242526272829303132apiVersion: batch/v1kind: Jobmetadata: name: "skywalking-es-init" namespace: skywalking labels: app: skywalking-jobspec: template: metadata: name: "skywalking-es-init" labels: app: skywalking-job spec: serviceAccountName: skywalking-oap restartPolicy: Never initContainers: - name: wait-for-elasticsearch image: busybox:1.30 imagePullPolicy: IfNotPresent command: ['sh', '-c', 'for i in $(seq 1 60); do nc -z -w3 elasticsearch.qifu-middleware.svc 9200 &amp;&amp; exit 0 || sleep 5; done; exit 1'] containers: - name: oap image: skywalking.docker.scarf.sh/apache/skywalking-oap-server:9.7.0 imagePullPolicy: IfNotPresent env: - name: JAVA_OPTS value: "-Dmode=init -Xmx2g -Xms2g" - name: SW_STORAGE value: elasticsearch - name: SW_STORAGE_ES_CLUSTER_NODES value: "elasticsearch.qifu-middleware.svc:9200" # 指定es地址，由于在k8s内部访问需要通过https协议，这里就指定外部地址，需要pods可以解析此地址。部署OAP Serverrbac.yaml1234567891011121314151617181920212223242526272829303132333435363738apiVersion: v1kind: ServiceAccountmetadata: labels: app: skywalking name: skywalking-oap namespace: skywalking---kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: skywalking namespace: skywalking labels: app: skywalkingrules:- apiGroups: [""] resources: ["pods", "endpoints", "services", "nodes"] verbs: ["get", "watch", "list"]- apiGroups: ["extensions"] resources: ["deployments", "replicasets"] verbs: ["get", "watch", "list"]---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: skywalking namespace: skywalking labels: app: skywalkingroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: skywalkingsubjects:- kind: ServiceAccount name: skywalking-oap namespace: skywalkingdp.yaml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869apiVersion: apps/v1kind: Deploymentmetadata: labels: app: oap name: oap namespace: skywalkingspec: replicas: 1 selector: matchLabels: app: oap template: metadata: labels: app: oap spec: serviceAccountName: skywalking-oap initContainers: - name: wait-for-elasticsearch image: busybox:1.30 imagePullPolicy: IfNotPresent command: ['sh', '-c', 'for i in $(seq 1 60); do nc -z -w3 elasticsearch.qifu-middleware.svc 9200 &amp;&amp; exit 0 || sleep 5; done; exit 1'] containers: - name: oap image: skywalking.docker.scarf.sh/apache/skywalking-oap-server:9.7.0 imagePullPolicy: IfNotPresent livenessProbe: tcpSocket: port: 12800 initialDelaySeconds: 15 periodSeconds: 20 readinessProbe: tcpSocket: port: 12800 initialDelaySeconds: 15 periodSeconds: 20 ports: - containerPort: 11800 name: grpc - containerPort: 1234 name: prometheus-port - containerPort: 12800 name: rest env: - name: JAVA_OPTS value: "-Dmode=no-init -Xmx2g -Xms2g" - name: TZ # 设置时区 value: Asia/Shanghai - name: SW_OTEL_RECEIVER value: "default" - name: SW_ENABLE_UPDATE_UI_TEMPLATE # 开启试图可编辑，默认为：false value: "true" - name: SW_CLUSTER value: kubernetes - name: SW_CLUSTER_K8S_NAMESPACE value: "skywalking" - name: SW_CLUSTER_K8S_LABEL value: "app=oap" - name: SKYWALKING_COLLECTOR_UID valueFrom: fieldRef: fieldPath: metadata.uid - name: SW_STORAGE # 使用es作为存储 value: elasticsearch - name: SW_STORAGE_ES_CLUSTER_NODES # es地址 value: "elasticsearch.qifu-middleware.svc:9200" - name: SW_TELEMETRY value: "prometheus"svc.yaml123456789101112131415161718apiVersion: v1kind: Servicemetadata: name: oap-svc namespace: skywalking labels: app: oapspec: type: ClusterIP ports: - port: 11800 name: grpc - port: 1234 name: prometheus-port - port: 12800 name: rest selector: app: oap按以下顺序执行：1234kubectl apply -f rbac.yamlkubectl apply -f init-job.yamlkubectl apply -f dp.yamlkubectl apply -f svc.yaml注意，需要等init-job.yaml执行成功之后再执行dp.yaml，否则启动oap服务的时候会报错：table: alarm_record does not exist. OAP is running in ‘no-init’ mode, waiting… retry 3s later.init-job.yaml执行成功如图所示： （k8s集群重启后可能还会出现报错table: alarm_record does not exist. OAP is running in ‘no-init’ mode, waiting… retry 3s later.，把skywalking-es-init的job删掉再重新kubectl apply -f init-job.yaml，初始化成功后再启动oap就行）部署UIui-dp.yaml1234567891011121314151617181920212223242526272829apiVersion: apps/v1kind: Deploymentmetadata: name: ui namespace: skywalking labels: app: uispec: replicas: 1 selector: matchLabels: app: ui template: metadata: labels: app: ui spec: affinity: containers: - name: ui image: skywalking.docker.scarf.sh/apache/skywalking-ui:9.7.0 imagePullPolicy: IfNotPresent ports: - containerPort: 8080 name: page env: - name: SW_OAP_ADDRESS value: http://oap-svc:12800 # 根据oap的svc一致ui-svc.yaml123456789101112131415apiVersion: v1kind: Servicemetadata: labels: app: ui name: ui-svc namespace: skywalkingspec: type: ClusterIP ports: - port: 80 targetPort: 8080 protocol: TCP selector: app: uiing.yaml123456789101112131415161718192021222324apiVersion: networking.k8s.io/v1kind: Ingressmetadata: annotations: kubernetes.io/ingress.class: nginx kubesphere.io/creator: admin nginx.ingress.kubernetes.io/auth-realm: '''Authentication Required - admin''' nginx.ingress.kubernetes.io/auth-secret: skywalking-auth nginx.ingress.kubernetes.io/auth-type: basic generation: 1 name: skywalking-web namespace: skywalkingspec: rules: - host: skywalking.keyfil.com http: paths: - backend: service: name: ui-svc port: number: 80 path: / pathType: ImplementationSpecificskywalking的ui界面默认没有访问控制，可以通过下面基于方案，实现自定义服务的外部验证：安装httpd-tools：1yum install -y httpd-tools输入密码：1htpasswd -c auth admin生成文件auth，利用此文件进行创建secret1kubectl -n skywalking create secret generic skywalking-auth --from-file=auth这里有个坑，经过测试，在创建secret之前通过htpasswd工具生成的记录用户名密码的文件的文件名，必须叫auth，不然经过后续的一顿操作，最终访问的结果会报503 Service Temporarily Unavailable，这与传统方式配置nginx的basic auth是不同的，可能在源码中将此参数硬编码了，具体原因没有深究。部署服务：123kubectl apply -f ui-dp.yamlkubectl apply -f ui-svc.yamlkubectl apply -f ing.yaml把skywalking.keyfil.com域名解析到对应的ingress-nginx后，访问skywalking的UI界面就行：]]></content>
      <categories>
        <category>技术</category>
        <category>skywalking</category>
      </categories>
      <tags>
        <tag>链路追踪</tag>
        <tag>skywalking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s之Ingress资源详解]]></title>
    <url>%2F2024%2F0856fdeaa.html</url>
    <content type="text"><![CDATA[Ingress 是一种 Kubernetes 资源对象，用于对外暴露服务。它定义了不同主机名（域名）及 URL 和对应后端 Service 的绑定，根据不同的路径路由 HTTP 和 HTTPS 流量。Ingress Controller 是一个 Pod 服务，封装了一个 Web 前端负载均衡器，并根据 Ingress 的定义动态生成配置文件。以下是我工作中使用到的一些配置，记录一下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364用于认证： nginx.ingress.kubernetes.io/auth-realm: '''Authentication Required - admin''' #设置认证提示信息 nginx.ingress.kubernetes.io/auth-secret: kibana-auth #指定存储认证信息的secret nginx.ingress.kubernetes.io/auth-type: basic #设置认证类型为基本认证（basic）用于访问控制： nginx.ingress.kubernetes.io/whitelist-source-range: 121.33.0.0/16 #用于指定允许访问Ingress资源的IP地址范围 用于反向代理超时设置： nginx.ingress.kubernetes.io/proxy-connect-timeout: "300" nginx.ingress.kubernetes.io/proxy-read-timeout: "300" nginx.ingress.kubernetes.io/proxy-send-timeout: "300"用于文件大小限制： nginx.ingress.kubernetes.io/proxy-body-size: 50m #设置Nginx代理能够接收的最大请求体大小用于请求限制： nginx.ingress.kubernetes.io/limit-connections: '20' #设置每个客户端IP允许的最大并发连接数 nginx.ingress.kubernetes.io/limit-rps: '4' #设置每秒允许的请求速率，'4'表示每秒最多允许4个请求 nginx.ingress.kubernetes.io/limit-whitelist: '10.0.0.0/8,172.16.0.0/20' #指定不受连接数和请求速率限制的白名单IP地址范围用于证书设置： ingress.kubernetes.io/ssl-redirect: 'true' #控制是否将HTTP流量重定向到HTTPS spec: ingressClassName: nginx tls: - hosts: - biz.chain.test.com secretName: biz-chain-tls-secret用于缓存设置： nginx.ingress.kubernetes.io/configuration-snippet: | #提供自定义的Nginx配置片段，这些配置片段将被直接插入到Nginx的配置中，从而允许对Ingress的行为进行细粒度的控制 if ($request_uri = "/index.html") &#123; more_set_headers "cache-Control: no-cache, no-store"; &#125; if ($request_uri = "/") &#123; more_set_headers "cache-Control: no-cache, no-store"; &#125; if ($request_uri = "/login") &#123; more_set_headers "cache-Control: no-cache, no-store"; &#125; if ($request_uri ~ "/login.*") &#123; more_set_headers "Cache-Control: no-cache, no-store"; &#125;注解包含了四个if语句，每个语句都检查请求的URI，并根据URI设置不同的Cache-Control头部。具体来说：如果请求的URI是/index.html，则设置Cache-Control头部为no-cache, no-store。如果请求的URI是根路径/，同样设置Cache-Control头部为no-cache, no-store。如果请求的URI是/login，也设置Cache-Control头部为no-cache, no-store。如果请求的URI以/login开头（使用正则表达式匹配），同样设置Cache-Control头部为no-cache, no-store。Cache-Control头部用于控制浏览器和其他缓存机制如何缓存响应内容。no-cache指令表示在每次请求时都需要向服务器验证缓存内容是否仍然有效，而no-store指令则完全禁止缓存响应内容。通过这些配置，您可以确保对于特定的请求URI，浏览器不会缓存响应内容，这对于需要频繁更新或敏感信息的页面特别有用。例如，登录页面通常不应该被缓存，以防止敏感信息（如登录令牌）被存储在用户的浏览器中。用于特定文件验证： nginx.ingress.kubernetes.io/configuration-snippet: | if ($request_uri = /WW_verify_hLGj07zeeSPDbJaa.txt) &#123; add_header Content-Type text/plain; return 200 'JOFG56743244ukfjuhf'; &#125;利用annotation来实现这个功能，如果匹配到/WW_verify_hLGj07zeeSPDbJaa.txt路径，就返回JOFG56743244ukfjuhf。其他： nginx.ingress.kubernetes.io/use-regex: 'true' 当设置为'true'时，表示Ingress规则中的路径将使用正则表达式进行匹配 kubernetes.io/ingress.class: nginx 注解指定Ingress控制器类型为nginx kubesphere.io/creator: admin 注解用于标识创建者信息]]></content>
      <categories>
        <category>技术</category>
        <category>K8S</category>
      </categories>
      <tags>
        <tag>Ingress</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s之节点kubelet资源配置]]></title>
    <url>%2F2024%2F07f89bb99b.html</url>
    <content type="text"><![CDATA[由于Pod没有对内存及CPU进行限制，导致Pod在运行过程中所需的内存超过了节点本身的内存（OOM），从而导致节点崩溃，使得运行在该节点上的所有Pod都失败了。为了解决这个问题以及提高节点的稳定性，综合k8s的一些特性，方案如下1.每个节点为系统守护进程预留计算资源(CPU、内存、磁盘空间)2.Pod驱逐：节点资源到达一定使用量，开始驱逐 pod3.每个Pod需指定所需资源预留资源Kubernetes 的节点可以按照 Capacity 调度。默认情况下 pod 能够使用节点全部可用容量。 这是个问题，因为节点自己通常运行了不少驱动 OS 和 Kubernetes 的系统守护进程。 除非为这些系统守护进程留出资源，否则它们将与 Pod 争夺资源并导致节点资源短缺问题。kubelet 公开了一个名为 ‘Node Allocatable’ 的特性，有助于为系统守护进程预留计算资源。 Kubernetes 推荐集群管理员按照每个节点上的工作负载密度配置 ‘Node Allocatable’。Kubernetes 节点上的 ‘Allocatable’ 被定义为 Pod 可用计算资源量。 调度器不会超额申请 ‘Allocatable’。 目前支持 ‘CPU’、‘memory’ 和 ‘ephemeral-storage’ 这几个参数。可分配的节点暴露为 API 中 v1.Node 对象的一部分，也是 CLI 中 kubectl describe node 的一部分。在 kubelet 中，可以为两类系统守护进程预留资源。Node Capacity：Node的所有硬件资源allocatable的值即对应 describe node 时看到的allocatable容量，pod 调度的上限计算公式：节点上可配置值 = 总量 - 预留值 - 驱逐阈值1Allocatable = Capacity - Reserved(kube+system) - Eviction ThresholdKube-reservedKubelet 标志 ：–kube-reserved=[cpu=100m][,][memory=100Mi][,][ephemeral-storage=1Gi][,][pid=1000]Kubelet 标志 ：–kube-reserved-cgroup=kube-reserved 用来给诸如 kubelet、容器运行时、节点问题监测器等 Kubernetes 系统守护进程记述其资源预留值。 该配置并非用来给以 Pod 形式运行的系统守护进程预留资源。kube-reserved 通常是节点上 Pod 密度 的函数。Kube-reservedKubelet 标志 ：–kube-reserved=[cpu=100m][,][memory=100Mi][,][ephemeral-storage=1Gi][,][pid=1000]Kubelet 标志 ：–kube-reserved-cgroup=kube-reserved 用来给诸如 kubelet、容器运行时、节点问题监测器等 Kubernetes 系统守护进程记述其资源预留值。 该配置并非用来给以 Pod 形式运行的系统守护进程预留资源。kube-reserved 通常是节点上 Pod 密度 的函数。除了 cpu、内存 和 ephemeral-storage 之外，pid 可用来指定为 Kubernetes 系统守护进程预留指定数量的进程 ID。设置方式systemctl status kubelet 查看 kubelet启动的配置文件路径：–kubeconfig=/etc/kubernetes/kubelet.conf –config=/var/lib/kubelet/config.yaml编辑/var/lib/kubelet/config.yaml，添加如下内容123456apiVersion: kubelet.config.k8s.io/v1beta1...kubeReserved: # 配置 kube 资源预留, 根据实际情况进行设置 cpu: 1000m memory: 5Gi ephemeral-storage: 5Gi重启kubelet服务生效System-reservedKubelet 标志 ：–system-reserved=[cpu=100m][,][memory=100Mi][,][ephemeral-storage=1Gi][,][pid=1000]Kubelet 标志 ：–system-reserved-cgroup=system-reserved 用于为诸如 sshd、udev 等系统守护进程记述其资源预留值。 system-reserved 也应该为 kernel 预留 内存，因为目前 kernel 使用的内存并不记在 Kubernetes 的 Pod 上。 同时还推荐为用户登录会话预留资源（systemd 体系中的 user.slice）。除了 cpu、内存 和 ephemeral-storage 之外，pid 可用来指定为 Kubernetes 系统守护进程预留指定数量的进程 ID。设置方式编辑/var/lib/kubelet/config.yaml，添加如下内容123456apiVersion: kubelet.config.k8s.io/v1beta1...systemReserved: # 配置系统资源预留 cpu: 1000m memory: 10Gi ephemeral-storage: 10Gi参数解释：systemReserved：为系统预留的资源，确保节点上的系统进程有足够的资源运行。cpu: 500m：预留500毫核的CPU资源。memory: 4Gi：预留4GiB的内存资源。重启kubelet服务生效Eviction ThresholdsKubelet 标志 ：–eviction-hard=[memory.available&lt;500Mi]节点级别的内存压力将导致系统内存不足，这将影响到整个节点及其上运行的所有 Pod。 节点可以暂时离线直到内存已经回收为止。为了防止系统内存不足（或减少系统内存不足的可能性）， kubelet 提供了资源不足管理。 驱逐操作只支持 memory 和 ephemeral-storage。 通过 –eviction-hard 标志预留一些内存后，当节点上的可用内存降至预留值以下时， kubelet 将尝试驱逐 Pod。 如果节点上不存在系统守护进程，Pod 将不能使用超过 capacity-eviction-hard 所指定的资源量。 因此，为驱逐而预留的资源对 Pod 是不可用的。设置方式编辑/var/lib/kubelet/config.yaml，添加如下内容12345678910111213141516evictionHard: memory.available: 10% #如果需要配置具体值的话需要用引号，如：memory.available: "1Gi" nodefs.available: 10% nodefs.inodesFree: 5% imagefs.available: 15%evictionSoft: imagefs.available: 15% memory.available: 15% nodefs.available: 15% nodefs.inodesFree: 10%evictionSoftGracePeriod: imagefs.available: "90s" memory.available: "90s" nodefs.available: "90s" nodefs.inodesFree: "90s"evictionMaxPodGracePeriod: 100参数解释：evictionHard：硬驱逐阈值，当节点资源低于这些阈值时，Kubernetes会立即驱逐Pod。memory.available: 10%：可用内存低于10%时触发驱逐。nodefs.available: 10%：节点文件系统可用空间低于10%时触发驱逐。nodefs.inodesFree: 5%：节点文件系统可用inode数低于5%时触发驱逐。imagefs.available: 15%：镜像文件系统可用空间低于15%时触发驱逐。evictionSoft：软驱逐阈值，当节点资源低于这些阈值时，Kubernetes会在一定的宽限期内尝试驱逐Pod。imagefs.available: 15%：镜像文件系统可用空间低于15%时触发驱逐。memory.available: 15%：可用内存低于15%时触发驱逐。nodefs.available: 15%：节点文件系统可用空间低于15%时触发驱逐。nodefs.inodesFree: 10%：节点文件系统可用inode数低于10%时触发驱逐。evictionSoftGracePeriod：软驱逐的宽限期，Kubernetes会在这段时间内尝试驱逐Pod，如果资源仍然不足，则会触发硬驱逐。imagefs.available: &quot;90s&quot;：镜像文件系统可用空间低于15%时，宽限期为90秒。memory.available: &quot;90s&quot;：可用内存低于15%时，宽限期为90秒。nodefs.available: &quot;90s&quot;：节点文件系统可用空间低于15%时，宽限期为90秒。nodefs.inodesFree: &quot;90s&quot;：节点文件系统可用inode数低于10%时，宽限期为90秒。evictionMaxPodGracePeriod：Pod的最大宽限期，在驱逐Pod时，Kubernetes会等待这段时间以确保Pod有足够的时间优雅地终止。重启kubelet服务生效当触发驱逐的时候，有一个bug，驱逐的pod不会自动删除，状态会变为Error或者ContainerStatusUnknown可以写个脚本定时去清理：12345#!/bin/bashfor ns in `kubectl get ns | awk 'NR&gt;1&#123;print $1&#125;'`do kubectl get pods -n $&#123;ns&#125; | egrep 'ContainerStatusUnknown|Error' | awk '&#123;print $1&#125;' | xargs kubectl delete pod -n $&#123;ns&#125;done]]></content>
      <categories>
        <category>技术</category>
        <category>K8S</category>
      </categories>
      <tags>
        <tag>资源预留</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DNSserver部署]]></title>
    <url>%2F2024%2F07dee378fc.html</url>
    <content type="text"><![CDATA[之前已经发过一篇DNS服务器的部署教程，最近发现一个通过docker-compose启动的DNS服务，相对简单，也能实现需求，记录一下。通过docker-compose部署先创建网络：1docker network create devopsnetwork创建docker-compose.yml文件：1234567891011121314151617181920212223242526version: '3.1'services: dns-server: container_name: dns_server_13_1 hostname: dns-server restart: always image: swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/technitium/dns-server:13.1 ports: - "5380:5380/tcp" #DNS web console (HTTP) - "53:53/udp" #DNS service - "53:53/tcp" #DNS service environment: - TZ=Asia/Shanghai - DNS_SERVER_LOG_USING_LOCAL_TIME=true - DNS_SERVER_DOMAIN=dns-server #The primary domain name used by this DNS Server to identify itself. - DNS_SERVER_ADMIN_PASSWORD=devops6666 #DNS web console admin user password. sysctls: - net.ipv4.ip_local_port_range=1024 65000 volumes: - ./data:/etc/dns networks: - devopsnetwork networks: devopsnetwork: external: true启动服务：1docker compose up -d添加解析：启动完成后通过IP+端口访问web界面：点击Zones-Add Zone填写域名：添加解析：填写相关信息：保存即可。注意：设置此DNS服务器为DNS1时还需要设置DNS2为202.96.128.86或114.114.114.114才可以访问外部解析]]></content>
      <categories>
        <category>技术</category>
        <category>DNS</category>
      </categories>
      <tags>
        <tag>DNS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s部署新版elasticsearch+kibana并配置快照备份]]></title>
    <url>%2F2024%2F076d875a26.html</url>
    <content type="text"><![CDATA[Elasticsearch和Kibana是一对强大的开源工具，通常一起使用以构建实时数据分析和可视化解决方案。介绍Elasticsearch: Elasticsearch是一个分布式、高性能的实时搜索和分析引擎。它构建在开源搜索引擎库Lucene之上，并提供了一个分布式、可扩展的架构，用于存储、检索和分析大规模的数据。Elasticsearch支持全文搜索、结构化搜索、地理空间搜索等多种查询方式，并提供了强大的聚合和分析功能。它还支持实时数据的索引和搜索，使得您可以快速地在大数据集中进行复杂的搜索和分析操作。Kibana: Kibana是一个开源的数据可视化工具，专门用于分析和展示Elasticsearch中的数据。它提供了直观的图表、图形和仪表盘，可用于实时监控和可视化大规模数据集。Kibana允许用户通过仪表盘配置和自定义可视化组件，然后通过交互式的界面进行数据的探索和导航。它还支持复杂的数据过滤、查询和聚合操作，使用户能够深入了解数据并发现隐藏的模式和见解。通过将Elasticsearch和Kibana结合在一起，可以构建强大的实时数据分析和可视化解决方案。Elasticsearch负责高性能的数据存储、检索和分析，而Kibana则提供了直观易用的界面和工具，使您能够将数据转化为有意义的见解和洞察。无论是监控日志数据、分析业务指标，还是构建实时仪表盘和报告，Elasticsearch和Kibana的组合可以帮助您实现强大的数据分析和可视化需求。版本:es 7.17.6kibana 7.17.6k8s:1.19.16部署es创建es namespace123456apiVersion: v1kind: Namespacemetadata: name: elk labels: name: elk创建es StorageClass和pvc123456789101112131415161718192021apiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: es-storageclassprovisioner: nfs-storage-01allowVolumeExpansion: truereclaimPolicy: RetainapiVersion: v1kind: PersistentVolumeClaimmetadata: name: es-storageclass namespace: elk annotations: volume.beta.kubernetes.io/storage-class: "es-storageclass"spec: resources: requests: storage: 200Gi volumeMode: Filesystem accessModes: - ReadWriteOnce通过sts的方式来部署es集群123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130apiVersion: apps/v1kind: StatefulSetmetadata: name: es-cluster namespace: elkspec: # 必须设置 serviceName: es-cluster-svc replicas: 3 selector: # 设置标签 matchLabels: app: es-cluster template: metadata: labels: app: es-cluster spec: # 初始化容器 initContainers: - name: increase-vm-max-map image: busybox command: ["sysctl", "-w", "vm.max_map_count=262144"] securityContext: privileged: true - name: increase-fd-ulimit image: busybox command: ["sh", "-c", "ulimit -n 65536"] securityContext: privileged: true - name: increase-es-cluster image: busybox command: ["/bin/sh", "-c", "mkdir -p /data/esbak","chown -R elasticsearch.elasticsearch /data/esbak" ,"chmod -R 777 /data/esbak"] securityContext: privileged: true containers: - name: es-container image: elasticsearch:7.17.6 ports: # 容器内端口 - name: rest containerPort: 9200 protocol: TCP # 设置挂载目录 volumeMounts: - name: es-data mountPath: /usr/share/elasticsearch/data - name: es-plugins mountPath: /usr/share/elasticsearch/plugins #快照持久化路径 - name: snapshot-volume mountPath: /data/esbak # 设置环境变量 env: # 自定义集群名 - name: cluster.name value: k8s-es # 定义节点名，使用metadata.name名称 - name: node.name valueFrom: fieldRef: fieldPath: metadata.name # 初始化集群时，ES从中选出master节点 - name: cluster.initial_master_nodes # 对应metadata.name名称加编号，编号从0开始 value: "es-cluster-0,es-cluster-1,es-cluster-2" - name: discovery.zen.minimum_master_nodes value: "2" #es快照备份路径 - name: path.repo value: /data/esbak # 发现节点的地址，discovery.seed_hosts的值应包括所有master候选节点 # 如果discovery.seed_hosts的值是一个域名，且该域名解析到多个IP地址，那么es将处理其所有解析的IP地址。 - name: discovery.seed_hosts value: "es-cluster-0.es-cluster-svc,es-cluster-1.es-cluster-svc,es-cluster-2.es-cluster-svc" # 配置内存 - name: ES_JAVA_OPTS value: "-Xms8192m -Xmx8192m" - name: network.host value: "0.0.0.0" #允许来自任意源的跨域请求 - name: "http.cors.allow-origin" value: "*" #启用跨域资源共享 - name: "http.cors.enabled" value: "true" #索引分片数量 - name: "number_of_shards" value: "5" #分片的副本数量 - name: "number_of_replicas" value: "2" volumeClaimTemplates: - metadata: # 对应容器中volumeMounts.name name: es-data labels: app: es-volume spec: accessModes: [ "ReadWriteOnce" ] storageClassName: es-storageclass resources: requests: storage: 50Gi - metadata: name: es-plugins spec: accessModes: [ "ReadWriteOnce" ] storageClassName: es-storageclass resources: requests: storage: 2Gi - metadata: name: es-plugins spec: accessModes: [ "ReadWriteOnce" ] storageClassName: es-storageclass resources: requests: storage: 2Gi - metadata: name: snapshot-volume spec: accessModes: [ "ReadWriteOnce" ] storageClassName: es-storageclass # 申请资源的大小 resources: requests: storage: 100Gi部署es svc12345678910111213141516apiVersion: v1kind: Servicemetadata: name: es-svc namespace: elkspec: selector: app: es-cluster type: NodePort ports: - name: cluster port: 9200 targetPort: 9200 nodePort: 19200 - name: transport port: 9300发布es1kubectl applf -f ./验证ip+端口验证一下，浏览器访问、curl命令都可以部署es-head插件这个插件方便我们查询es集群状态编写yaml文件123456789101112131415161718192021222324252627282930313233343536373839404142434445apiVersion: apps/v1kind: Deploymentmetadata: labels: app: elasticsearch-head name: elasticsearch-head namespace: elkspec: replicas: 1 selector: matchLabels: app: elasticsearch-head template: metadata: labels: app: elasticsearch-head spec: containers: - image: springgos/es-head:latest imagePullPolicy: Always name: elasticsearch-head ports: - containerPort: 9100 name: 9100tcp191001 protocol: TCP resources: &#123;&#125; ---apiVersion: v1kind: Servicemetadata: labels: app: elasticsearch-head name: elasticsearch-head namespace: elkspec: ports: - name: 9100tcp191001 nodePort: 19100 port: 9100 protocol: TCP targetPort: 9100 selector: app: elasticsearch-head type: NodePort访问验证 部署kibana编写kibana-deploy.yaml和svc注意：kibana必须跟es保持同一个版本12345678910111213141516171819202122232425262728293031323334353637383940apiVersion: apps/v1kind: Deploymentmetadata: name: kibana namespace: elkspec: selector: matchLabels: app: kibana replicas: 1 template: metadata: labels: app: kibana spec: restartPolicy: Always containers: - name: kibana image: kibana:7.17.6 imagePullPolicy: Always ports: - containerPort: 5601 env: - name: ELASTICSEARCH_HOSTS value: http://es-cluster-svc:9200---apiVersion: v1kind: Servicemetadata: name: kibana namespace: elkspec: type: NodePort ports: - name: kibana port: 5601 targetPort: 5601 nodePort: 25601 selector: app: kibana然后发布应用1kubectl applf -f ./浏览器验证如果es集群没有启动正常这里面是会报错的通过kibana来创建es快照什么是es快照在Elasticsearch中，快照备份是一种将索引和集群的状态数据保存到可恢复的快照中的机制。通过创建快照，你可以在需要时恢复数据，或在不同的集群之间迁移数据。存储库（Repository）：存储库是用来保存快照数据的位置，可以是本地文件系统、远程文件系统或云存储服务。你需要在Elasticsearch中配置一个存储库，指定存储快照的位置。快照（Snapshot）：快照是在存储库中保存的索引和集群状态的副本。它包含了所有或指定的索引的数据、设置和元数据信息。过程（Process）：创建快照的过程涉及协调节点（coordinating node）与数据节点（data node）之间的协作。协调节点负责指导数据节点将数据写入存储库，并协调快照的创建过程创建es快照点击Stack Management创建快照库 参数解释File system location这个是定义快照仓库路径，我这边直接在es容器里面创建了所以我直接写的/data/esbak，填写的目录一定要存在哟，要不保存会报错提示没有这个目录。Snapshot compression参数用于压缩快照的索引映射和设置文件，而数据文件本身不会被压缩。减少快照对磁盘空间占用。Chunk size“分块大小”是用于在快照过程中将大文件分成较小单元的概念。当创建快照时，通常会将数据分割成更小、可管理的块，以便进行高效的存储和备份。Max snapshot bytes per second“Max snapshot bytes per second”是每个节点创建快照的最大速率。在数据存储系统中，当创建快照时，系统会复制数据并保存为快照。但是，为了避免对系统的过度负载，可以设置每个节点可以每秒复制的最大字节数。Max restore bytes per secondMax restore bytes per second” 是每个节点进行快照还原的最大速率。在数据存储系统中，当需要从快照还原数据时，系统会将快照中的数据复制回原始存储位置或目标位置。为了避免对系统造成过载，可以设置每个节点每秒复制的最大字节数。通过设置 “Max restore bytes per second”，可以限制每个节点进行快照还原的速率。这有助于平衡系统资源的使用，确保还原操作不会过多地消耗节点的处理能力或网络带宽。Read-only表示只有一个集群可以对该存储库进行写入操作，而其他所有集群只能进行读取操作。创建定时备份 参数解释Data streams and indices要备份索引和数据流，可以手动选择它们或定义索引模式以动态匹配它们Ignore unavailable indices在进行快照时忽略不可用的索引。否则，整个快照将会失败。在进行数据快照时，如果其中一个索引不可用（比如索引已被删除或处于不可访问状态），默认情况下整个快照操作都将失败。为了避免这种情况，可以选择忽略不可用的索引。通过设置 “忽略不可用的索引”，即使某些索引在快照时不可用，快照操作也会继续进行，并且其他可用的索引将会被备份。这样可以确保整个快照操作的成功。Allow partial indices允许备份部分索引，即允许备份包含不可用主分片的索引。否则，整个快照将会失败。在进行数据快照时，如果某个索引的主分片不可用（比如主分片丢失或不可访问），默认情况下整个快照操作都将失败。为了避免这种情况，可以选择允许备份部分索引。通过设置 “允许部分索引”，即使某个索引的主分片不可用，快照操作仍将继续进行，并且其他可用的分片将被备份。这样可以确保整个快照操作的成功。Include global state是指在进行数据快照时，将全局集群状态和系统索引作为快照的一部分进行存储。全局集群状态包含了有关整个Elasticsearch集群的配置和元数据信息，例如索引模板、映射和设置等。系统索引包含了与Elasticsearch集群运行和管理相关的索引，例如集群状态索引（cluster state）、索引模板索引、别名索引等。通过包含全局状态，可以完整地备份整个集群配置和元数据，以便在需要时进行还原或迁移。在进行快照时忽略不可用的索引。否则，整个快照将会失败。在进行数据快照时，如果其中一个索引不可用（比如索引已被删除或处于不可访问状态），默认情况下整个快照操作都将失败。为了避免这种情况，可以选择忽略不可用的索引。通过设置 “忽略不可用的索引”，即使某些索引在快照时不可用，快照操作也会继续进行，并且其他可用的索引将会被备份。这样可以确保整个快照操作的成功。参数解释Snapshots to retain最小快照数: 这是策略中必须保留的最低快照数。无论其他条件如何，都将保留至少此数量的快照。这确保了在最坏的情况下至少有一定数量的备份可供恢复。最大快照数: 这是策略中允许保留的最大快照数。超过此数量的快照将被删除，以避免无限增加存储空间的需求。保留较少的快照可以节省存储空间，并提供更好的管理快照的能力。 注意一下，kibana这个时区是美国时区，跟中国时区相差12小时验证在kibana和服务器看已经有备份了，快照备份成功]]></content>
      <categories>
        <category>技术</category>
        <category>es</category>
      </categories>
      <tags>
        <tag>es</tag>
        <tag>kibaba</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[certbot申请泛域名证书及自动续期]]></title>
    <url>%2F2024%2F06540ef920.html</url>
    <content type="text"><![CDATA[阿里云申请的免费证书有效期变短了，还有数量限制，而且不能申请泛域名证书，可以通过cerbot申请免费的域名证书并且自动续期。原理当我们使用 certbot 申请通配符证书时，需要手动添加 TXT 记录。每个 certbot 申请的证书有效期为 3 个月，虽然 certbot 提供了自动续期命令，但是当我们把自动续期命令配置为定时任务时，我们无法手动添加新的 TXT 记录用于 certbot 验证。好在 certbot 提供了一个 hook，可以编写一个 Shell 脚本。在续期的时候让脚本调用 DNS 服务商的 API 接口动态添加 TXT 记录，验证完成后再删除此记录。安装(CommandLine)安装 aliyun cli 工具1234wget https://aliyuncli.alicdn.com/aliyun-cli-linux-latest-amd64.tgztar xzvf aliyun-cli-linux-latest-amd64.tgzsudo cp aliyun /usr/local/binrm aliyun安装完成后需要配置凭证信息安装 certbot-dns-aliyun 插件12345wget https://cdn.jsdelivr.net/gh/justjavac/certbot-dns-aliyun@main/alidns.shsudo cp alidns.sh /usr/local/binsudo chmod +x /usr/local/bin/alidns.shsudo ln -s /usr/local/bin/alidns.sh /usr/local/bin/alidnsrm alidns.sh申请证书测试是否能正确申请：1certbot certonly -d *.example.com --manual --preferred-challenges dns --manual-auth-hook "alidns" --manual-cleanup-hook "alidns clean" --dry-run正式申请时去掉 --dry-run 参数：1certbot certonly -d *.example.com --manual --preferred-challenges dns --manual-auth-hook "alidns" --manual-cleanup-hook "alidns clean"证书续期1certbot renew --manual --preferred-challenges dns --manual-auth-hook "alidns" --manual-cleanup-hook "alidns clean" --dry-run如果以上命令没有错误，把 --dry-run 参数去掉。自动续期添加定时任务 crontab。1crontab -e输入11 1 */1 * * root certbot renew --manual --preferred-challenges dns --manual-auth-hook "alidns" --manual-cleanup-hook "alidns clean" --deploy-hook "nginx -s reload"上面脚本中的 --deploy-hook &quot;nginx -s reload&quot; 表示在续期成功后自动重启 nginx。安装（Docker）编写Dockerfile以及entrypoint.sh,确保他们在同一文件夹下，目前Dockerfile中默认下载amd64版本，其他架构请修改对应的Aliyun CLI URL。DockerFile：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647FROM alpine:latest# Install dependenciesRUN apk --no-cache add wget tar sudo certbot bash python3 py3-pip &amp;&amp; \ apk --no-cache add --virtual build-dependencies gcc musl-dev python3-dev libffi-dev openssl-dev make# Install aliyun-cliRUN wget https://aliyuncli.alicdn.com/aliyun-cli-linux-latest-amd64.tgz &amp;&amp; \ tar xzvf aliyun-cli-linux-latest-amd64.tgz &amp;&amp; \ mv aliyun /usr/local/bin &amp;&amp; \ rm aliyun-cli-linux-latest-amd64.tgz# Copy and install certbot-dns-aliyun pluginRUN wget https://cdn.jsdelivr.net/gh/justjavac/certbot-dns-aliyun@main/alidns.sh &amp;&amp; \ mv alidns.sh /usr/local/bin/alidns &amp;&amp; \ chmod +x /usr/local/bin/alidns# Create virtual environment for Python packagesRUN python3 -m venv /opt/venvENV PATH="/opt/venv/bin:$PATH"# Install Python dependencies in virtual environmentRUN pip install --upgrade pip &amp;&amp; \ pip install aliyun-python-sdk-core aliyun-python-sdk-alidns# Copy entrypoint scriptCOPY entrypoint.sh /usr/local/bin/entrypoint.shRUN chmod +x /usr/local/bin/entrypoint.sh# Set environment variables (to be provided during runtime)ENV REGION=""ENV ACCESS_KEY_ID=""ENV ACCESS_KEY_SECRET=""ENV DOMAIN=""ENV EMAIL=""ENV CRON_SCHEDULE="0 0 * * *"# Setup cron job for certbot renewRUN echo "$CRON_SCHEDULE /opt/venv/bin/certbot renew --manual --preferred-challenges dns --manual-auth-hook '/usr/local/bin/alidns' --manual-cleanup-hook '/usr/local/bin/alidns clean' --agree-tos --email $EMAIL --deploy-hook 'cp -r /etc/letsencrypt/live/$DOMAIN/* /etc/letsencrypt/certs'" &gt; /etc/crontabs/root# Create directory for certificatesRUN mkdir -p /etc/letsencrypt/certs# Make sure cron is runningRUN touch /var/log/cron.logENTRYPOINT ["/usr/local/bin/entrypoint.sh"]entrypoint.sh：12345678910111213#!/bin/bash# Activate the virtual environmentsource /opt/venv/bin/activate# Configure Aliyun CLIaliyun configure set --profile akProfile --mode AK --region $REGION --access-key-id $ACCESS_KEY_ID --access-key-secret $ACCESS_KEY_SECRET# Obtain the certificatecertbot certonly -d "$DOMAIN" --manual --preferred-challenges dns --manual-auth-hook "/usr/local/bin/alidns" --manual-cleanup-hook "/usr/local/bin/alidns clean" --agree-tos --email $EMAIL --non-interactive# Start cron daemoncrond -f -l 2创建Image进入Dockerfile同目录:1docker build -t certbot-alicli .使用代理（可选）:1234docker build . \ --build-arg "HTTP_PROXY=http://127.0.0.1:7890" \ --build-arg "HTTPS_PROXY=http://127.0.0.1:7890" \ -t certbot-alicli运行容器123456789docker run \-e REGION=YOUR_REGEION \-e ACCESS_KEY_ID=YOUR_ACCESS_KEY \-e ACCESS_KEY_SECRET=YOUR_ACCESS_SECRET \-e DOMAIN=YOUR_DOMAIN \-e EMAIL=YOUR_NOTIFICATION_EMAIL \ // 证书刷新通知邮箱-e CRON_SCHEDULE="0 0 * * *" \ // 自定义证书刷新间隔-v /path/letsencrypt:/etc/letsencrypt \ // 将容器内的证书路径完整映射到宿主机-d certbot-alicli]]></content>
      <categories>
        <category>技术</category>
        <category>域名证书</category>
      </categories>
      <tags>
        <tag>免费证书</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s安装cert-manager及签发泛域名证书]]></title>
    <url>%2F2024%2F067bd705c5.html</url>
    <content type="text"><![CDATA[cert-manager 是一个云原生证书管理开源项目，用于在 Kubernetes 集群中自动管理和颁发来自各种颁发源的 TLS 证书，它可以从各种受支持的来源颁发证书，包括 Let’s Encrypt、HashiCorp Vault和Venafi以及私有 PKI，它将确保证书定期有效和更新，并在到期前的适当时间尝试更新证书。架构Issuers/ClusterIssuers：定义使用 什么证书颁发机构 (CA) 来去颁发证书，Issuers和ClusterIssuers区别是issuers是一个名称空间级别的资源， 只能用来签发自己所在 namespace 下的证书，ClusterIssuer是个集群级别的资源 可以签发任意 namespace 下的证书Certificate：定义所需的 X.509 证书，该证书将更新并保持最新。Certificate是一个命名空间资源，当Certificate被创建时，它会去创建相应的CertificateRequest资源来去申请证书。安装安装cert-manager相对比较简单，这里安装的cert-manager版本为v1.4，注意该版本要求kubectl版本&gt;= v1.19.0-rc.1所有资源（CustomResourceDefinitions和 cert-manager、cainjector 和 webhook 组件）都包含在一个 YAML 清单文件中：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748wget https://github.com/jetstack/cert-manager/releases/download/v1.4.0/cert-manager.yamlroot@i-tsfhx8p6:~/qke-k8s/cert-manager# vi cert-manager.yamlroot@i-tsfhx8p6:~/qke-k8s/cert-manager# kubectl apply -f cert-manager.yaml customresourcedefinition.apiextensions.k8s.io/certificaterequests.cert-manager.io createdcustomresourcedefinition.apiextensions.k8s.io/certificates.cert-manager.io createdcustomresourcedefinition.apiextensions.k8s.io/challenges.acme.cert-manager.io createdcustomresourcedefinition.apiextensions.k8s.io/clusterissuers.cert-manager.io createdcustomresourcedefinition.apiextensions.k8s.io/issuers.cert-manager.io createdcustomresourcedefinition.apiextensions.k8s.io/orders.acme.cert-manager.io creatednamespace/cert-manager unchangedserviceaccount/cert-manager-cainjector createdserviceaccount/cert-manager createdserviceaccount/cert-manager-webhook createdclusterrole.rbac.authorization.k8s.io/cert-manager-cainjector createdclusterrole.rbac.authorization.k8s.io/cert-manager-controller-issuers createdclusterrole.rbac.authorization.k8s.io/cert-manager-controller-clusterissuers createdclusterrole.rbac.authorization.k8s.io/cert-manager-controller-certificates createdclusterrole.rbac.authorization.k8s.io/cert-manager-controller-orders createdclusterrole.rbac.authorization.k8s.io/cert-manager-controller-challenges createdclusterrole.rbac.authorization.k8s.io/cert-manager-controller-ingress-shim createdclusterrole.rbac.authorization.k8s.io/cert-manager-view createdclusterrole.rbac.authorization.k8s.io/cert-manager-edit createdclusterrole.rbac.authorization.k8s.io/cert-manager-controller-approve:cert-manager-io createdclusterrole.rbac.authorization.k8s.io/cert-manager-controller-certificatesigningrequests createdclusterrole.rbac.authorization.k8s.io/cert-manager-webhook:subjectaccessreviews createdclusterrolebinding.rbac.authorization.k8s.io/cert-manager-cainjector createdclusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-issuers createdclusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-clusterissuers createdclusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-certificates createdclusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-orders createdclusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-challenges createdclusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-ingress-shim createdclusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-approve:cert-manager-io createdclusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-certificatesigningrequests createdclusterrolebinding.rbac.authorization.k8s.io/cert-manager-webhook:subjectaccessreviews createdrole.rbac.authorization.k8s.io/cert-manager-cainjector:leaderelection createdrole.rbac.authorization.k8s.io/cert-manager:leaderelection createdrole.rbac.authorization.k8s.io/cert-manager-webhook:dynamic-serving createdrolebinding.rbac.authorization.k8s.io/cert-manager-cainjector:leaderelection createdrolebinding.rbac.authorization.k8s.io/cert-manager:leaderelection createdrolebinding.rbac.authorization.k8s.io/cert-manager-webhook:dynamic-serving createdservice/cert-manager createdservice/cert-manager-webhook createddeployment.apps/cert-manager-cainjector createddeployment.apps/cert-manager createddeployment.apps/cert-manager-webhook createdmutatingwebhookconfiguration.admissionregistration.k8s.io/cert-manager-webhook createdvalidatingwebhookconfiguration.admissionregistration.k8s.io/cert-manager-webhook created默认情况下，cert-manager 将安装到cert-manager 命名空间中，我们可以使用如下命令验证安装12345root@i-tsfhx8p6:~/qke-k8s/cert-manager# kubectl get pod -n cert-manager NAME READY STATUS RESTARTS AGEcert-manager-7456795566-9xcth 1/1 Running 0 47scert-manager-cainjector-75d558f967-5tjvc 1/1 Running 0 47scert-manager-webhook-7bb859bd44-7zsx5 1/1 Running 0 47s配置Issuer/ClusterIssuercert-manager支持以下几种证书颁发者SelfSignedCAVaultVenafiExternalACME这里只介绍使用ACME作为证书颁发者的方式。HTTP-01 校验原理HTTP-01 的校验原理是给你域名指向的 HTTP 服务增加一个临时 location ，Let’s Encrypt 会发送 http 请求到 http:///.well-known/acme-challenge/，YOUR_DOMAIN 就是被校验的域名，TOKEN 是 ACME 协议的客户端负责放置的文件，在这里 ACME 客户端就是 cert-manager，它通过修改或创建 Ingress 规则来增加这个临时校验路径并指向提供 TOKEN 的服务。Let’s Encrypt 会对比 TOKEN 是否符合预期，校验成功后就会颁发证书。此方法仅适用于给使用 Ingress 暴露流量的服务颁发证书，并且不支持泛域名证书。DNS-01 校验原理DNS-01 的校验原理是利用 DNS 提供商的 API Key 拿到你的 DNS 控制权限， 在 Let’s Encrypt 为 ACME 客户端提供令牌后，ACME 客户端 (cert-manager) 将创建从该令牌和您的帐户密钥派生的 TXT 记录，并将该记录放在 _acme-challenge.。 然后 Let’s Encrypt 将向 DNS 系统查询该记录，如果找到匹配项，就可以颁发证书。此方法不需要你的服务使用 Ingress，并且支持泛域名证书。校验方式对比HTTP-01 的校验方式的优点是: 配置简单通用，不管使用哪个 DNS 提供商都可以使用相同的配置方法；缺点是：需要依赖 Ingress，如果你的服务不是用 Ingress 暴露流量的就不适用，而且不支持泛域名证书。DNS-01 的校验方式的优点是没有 HTTP-01 校验方式缺点，不依赖 Ingress，也支持泛域名；缺点就是不同 DNS 提供商的配置方式不一样，而且 DNS 提供商有很多，cert-manager 的 Issuer 不可能每个都去支持，支持如下的dns提供商：AkamaiAzureDNSCloudFlareGoogleRoute53DigitalOceanRFC2136Cert-manager 还支持使用外部 webhook 的接入 DNS 提供商，支持如下webhookAliDNS-Webhookcert-manager-webhook-civocert-manager-webhook-dnspodcert-manager-webhook-dnsimplecert-manager-webhook-gandicert-manager-webhook-infomaniakcert-manager-webhook-inwxcert-manager-webhook-oci （Oracle 云基础设施）cert-manager-webhook-scalewaycert-manager-webhook-selectelcert-manager-webhook-softlayercert-manager-webhook-ibmciscert-manager-webhook-loopiacert-manager-webhook-arvanbizflycloud-certmanager-dns-webhookHTTP01配置示例12345678910111213141516171819apiVersion: cert-manager.io/v1kind: ClusterIssuermetadata: name: tlsspec: acme: # You must replace this email address with your own. # Let's Encrypt will use this to contact you about expiring # certificates, and issues related to your account. email: xxxxxxx@qq.com server: https://acme-v02.api.letsencrypt.org/directory privateKeySecretRef: # Secret resource that will be used to store the account's private key. name: issuer-account-key # Add a single challenge solver, HTTP01 using nginx solvers: - http01: ingress: class: nginx说明：metadata.name: 是我们创建的签发机构的名称，后面我们创建证书的时候会引用它spec.acme.email: 是你自己的邮箱，证书快过期的时候会有邮件提醒，不过 cert-manager 会利用 acme 协议自动给我们重新颁发证书来续期spec.acme.server: 是 acme 协议的服务端，我们这里用 Let’s Encrypt，这个地址就写死成这样就行spec.acme.privateKeySecretRef: 指示此签发机构的私钥将要存储到哪个 Secret 对象中，名称不重要spec.acme.solvers: 这里指示签发机构校验方式，有http01和dns01两种，该字段下配置的class和name只能同时存在一个，class指定使用的ingress class 名称，name比较少用，通常用于云上自带的ingress我们部署上述示例，并部署一个ingress 代理集群内的grafana服务验证签发证书123456789101112131415161718192021222324252627282930root@i-tsfhx8p6:~/qke-k8s/cert-manager# kubectl apply -f clusterissuer.yaml clusterissuer.cert-manager.io/tls createdroot@i-tsfhx8p6:~/qke-k8s/cert-manager# cat ingress/ingree-nginx.yamlapiVersion: networking.k8s.io/v1kind: Ingressmetadata: annotations: cert-manager.io/cluster-issuer: tls name: test namespace: monitoringspec: rules: - host: ingress-nginx.lishuai.fun http: paths: - backend: service: name: grafana port: number: 3000 path: / pathType: Prefix tls: # &lt; placing a host in the TLS config will indicate a certificate should be created - hosts: - ingress-nginx.lishuai.fun secretName: ingress-nginx--tls root@i-tsfhx8p6:~/qke-k8s/cert-manager# kubectl apply -f ingress/ingree-nginx.yaml ingress.networking.k8s.io/test created我们可以查看生成的certificate和签发的证书(安全原因删除了部分证书中的字符)1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950root@i-tsfhx8p6:~/qke-k8s/cert-manager# kubectl -n monitoring get certificateNAME READY SECRET AGEingress-nginx--tls True ingress-nginx--tls 2m26sroot@i-tsfhx8p6:~/qke-k8s/cert-manager# kubectl -n monitoring get secret ingress-nginx--tls -o yamlapiVersion: v1data: tls.crt: LS0tLS1CRUdJT********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 tls.key: LS0tLS1CRUdJTiBSx********************************************VFY2J3SStsNGFCM2NWMjNDMlIyM0wvbFNNY3MzSUIKZHZLQVQ5RUNnWUVBeHVhNlNWV0pyamJ5bmZwRVRQajNWa3ZTRm5rM0lxS1BZS2hUSW9VaGNmOGVSUkkvZzhkWApsVzVJZTNHbjhWaWlTTzB3TFJuZ3lHUVFSZDFDVGk5NEZNMmNabHA5czl3RHhWZFhlRm5FbmhhQVh5ZEVhQThmCkVteG04ek9TNWV0cnBiY2hueEI3eCtOaWo3MktZcFo4cnhobG1XcFRaRVJETzRveExWLzZYa1VDZ1lFQTRWTnMKUlkzME5LWmk2S0IxYWI1RFFYSTJTekJSeVdjbmd6ZjZRT3JEaVpldWsraS9CMGUrM2ZUNkRoSGl3bVNBcWtVUAo3b1Z0L1JNbkNDR1kwb1B1d0NkaSt5b1A0ejNOVTJpNWJDRk45eU12WUNkVWxVRG9KRVQ3YUdBV3QybTJtZDRiClgzaXpId1JaZVkyNk1FRFdUdUltSVFBdlRVdkRRS0RxeTNlSzRBc0NnWUVBZzRzdW9yZDJrZ2gzRnJIZ29BTjgKR00rV1J4U3R1VE5IbmNaVkRSeDlEUmFMbjJTOUt0c1llcFJ3VFd2U2hWUjRKOER1UHJYQnF1WTZ1T25uSXl4VAp2M1pvUEcwV2UzQkQ4aXljaGRUZ3F5ajRoM1hCMFF4SElYa2Q4VFFuci9XdHdQQkh4Um95c3ZVWVJ6WTBvcFVpCkt1NzRxcWplTkE3TlpFQTEyK3VBK3YwQ2dZRUFrVzRobEtieGRrWHAwdEUxMXdFcE1ZV0F5M2l0WVB1R1FpZ1EKQ25RN3JvUEs5c1lpL1pUdCtSNFRncDlDcDByc3pIajB0bk5DTVRSNlhjSXBlNzRSaTg0Z0VaSHRYVExYWWoxVwphQmI2MWtiTVhoZ2tmSXkvQ0NISnptMHVYRVVMeVRYVW53TXRRUml5azBUSlpqbUMyTGtYK1BiQWtQZ1VWcE5GCjEvc1pGRThDZ1lCekhVZE9LWVBjMUtlKzlwcGVvVlR1UjZ4bTlrTGJ0WHpPcXpuRVhQZUNMVmQyOWFBaTRGYUsKMXVocXdwcFVpL2x6N2VIb2pySDFmSXNaMDliTTd0aVBDY0FJK2NRemlGVWJvc2ZCdmxVakFzRVRNREFYbU5GLwpQYUhHWVR3MHVkVVlWL3YzdysxcXNsZG03a1kyN000anI4RGFITENKdlZERlpYMzRCMW9mVVE9PQotLS0tLUVORCBSU0EgUFJJVkFURSBLRVktLS0tLQo=kind: Secretmetadata: annotations: cert-manager.io/alt-names: ingress-nginx.lishuai.fun cert-manager.io/certificate-name: ingress-nginx--tls cert-manager.io/common-name: ingress-nginx.lishuai.fun cert-manager.io/ip-sans: "" cert-manager.io/issuer-group: cert-manager.io cert-manager.io/issuer-kind: ClusterIssuer cert-manager.io/issuer-name: tls cert-manager.io/uri-sans: "" creationTimestamp: "2021-07-31T08:25:14Z" managedFields: - apiVersion: v1 fieldsType: FieldsV1 fieldsV1: f:data: .: &#123;&#125; f:tls.crt: &#123;&#125; f:tls.key: &#123;&#125; f:metadata: f:annotations: .: &#123;&#125; f:cert-manager.io/alt-names: &#123;&#125; f:cert-manager.io/certificate-name: &#123;&#125; f:cert-manager.io/common-name: &#123;&#125; f:cert-manager.io/ip-sans: &#123;&#125; f:cert-manager.io/issuer-group: &#123;&#125; f:cert-manager.io/issuer-kind: &#123;&#125; f:cert-manager.io/issuer-name: &#123;&#125; f:cert-manager.io/uri-sans: &#123;&#125; f:type: &#123;&#125; manager: controller operation: Update time: "2021-07-31T08:25:14Z" name: ingress-nginx--tls namespace: monitoring resourceVersion: "274536" selfLink: /api/v1/namespaces/monitoring/secrets/ingress-nginx--tls uid: 929f9252-1905-4afc-b622-892151f1bbf3type: kubernetes.io/tls此时我们访问我们ingress文件里定义的域名可以看到此时证书已经签发DNS01配置示例这里使用的dns服务商为阿里云，需要使用AliDNS-Webhook安装alidns-webhook1wget https://raw.githubusercontent.com/pragkent/alidns-webhook/master/deploy/bundle.yaml建议修改文件中的acme.yourcompany.com1sed -i s/'acme.yourcompany.com'/'acme.lishuai.fun'/g bundle.yaml安装alidns-webhook1kubectl apply -f bundle.yaml创建一个包含阿里dns凭据的secert12345678apiVersion : v1 kind : Secret metadata : name : alidns-secret namespace : cert-manager data : access-key : YOUR_ACCESS_KEY(base64) secret-key : YOUR_SECRET_KEY(base64)或者使用如下命令行方式创建1kubectl -n cert-manager create secret generic alidns-secret --from-literal=access-key='YOUR_ACCESS_KEY' --from-literal=secret-key='YOUR_SECRET_KEY'编写ClusterIssuer yaml文件123456789101112131415161718192021222324apiVersion: cert-manager.io/v1kind: ClusterIssuermetadata: name: letsencrypt-stagingspec: acme: # Change to your letsencrypt email email: 912988434@qq.com server: https://acme-v02.api.letsencrypt.org/directory privateKeySecretRef: name: letsencrypt-staging-account-key solvers: - dns01: webhook: groupName: acme.lishuai.fun #须和bundle.yaml文件中定义的groupname 一致 solverName: alidns config: region: "" accessKeySecretRef: name: alidns-secret key: access-key secretKeySecretRef: name: alidns-secret key: secret-key创建ClusterIssuer1kubectl apply -f ClusterIssuer.yaml我们创建一个 手动去申请证书12345678910111213141516171819202122232425root@i-tsfhx8p6:~/qke-k8s/cert-manager/alidns-webhook# cat certificate.yaml apiVersion: cert-manager.io/v1kind: Certificatemetadata: name: lishuai-fun-tlsspec: secretName: lishuai-fun-tls dnsNames: - lishuai.fun - "*.lishuai.fun" issuerRef: name: letsencrypt-staging kind: ClusterIssuerroot@i-tsfhx8p6:~/qke-k8s/cert-manager/alidns-webhook# kubectl apply -f certificate.yaml certificate.cert-manager.io/lishuai-fun-tls createdroot@i-tsfhx8p6:~/qke-k8s/cert-manager/alidns-webhook# kubectl get secrets NAME TYPE DATA AGEdefault-token-v24ww kubernetes.io/service-account-token 3 25hlishuai-fun-tls-4wcmc Opaque 1 1sqingcloud kubernetes.io/dockerconfigjson 1 25hroot@i-tsfhx8p6:~/qke-k8s/cert-manager/alidns-webhook# kubectl get certificateNAME READY SECRET AGElishuai-fun-tls False lishuai-fun-tls 10s刚创建后我们查看certificate会发现ready状态为false,此时稍等会自动在我们的dns解析里创建txt 记录，然后去签发证书，该字段就会变为true此时查看发现secert和certificate都发生变化12345678root@i-tsfhx8p6:~/qke-k8s/cert-manager/alidns-webhook# kubectl get certificateNAME READY SECRET AGElishuai-fun-tls True lishuai-fun-tls 5m27sroot@i-tsfhx8p6:~/qke-k8s/cert-manager/alidns-webhook# kubectl get secrets NAME TYPE DATA AGEdefault-token-v24ww kubernetes.io/service-account-token 3 25hlishuai-fun-tls kubernetes.io/tls 2 2m14s #此时DATA 变为2，说明里面存着真正的证书文件qingcloud kubernetes.io/dockerconfigjson 1 25h查看证书详情123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117root@i-tsfhx8p6:~/qke-k8s/cert-manager/ingress# kubectl get secrets lishuai-fun-tls -o json |jq --raw-output '.data["tls.crt"]'|base64 -d #从secert中解析证书-----BEGIN CERTIFICATE-----MIIFKzCCBBOgAwIBAgISBO2cqIdYIq3fZjib8wahs0QfMA0GCSqGSIb3DQEBCwUAMDIxCzAJBgNVBAYTAlVTMRYwFAYDVQQKEw1MZXQncyBFbmNyeXB0MQswCQYDVQQDEwJSMzAeFw0yMTA3MzExNjM0MzBaFw0yMTEwMjkxNjM0MjhaMBYxFDASBgNVBAMTC2xpc2h1YWkuZnVuMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA2KTUfVYd7JIVfqcouQWNU1ly6hYrSxYLk9swEK5G2klaEX/5RaqybqVHvttZiAahvuZswhTyzHcCIzDCzrKucs1ZFE9RT9OKAvmY1e6IxeEUbz2Hi2ZBpP81sLO9G9t3x0U9RY75hf/iTgOGI2ZCm9quxROApMpiWNHHNO6GHk7sDZSKGjbgRttnbowh6HhQ2l22q3W0L71cAxMEYM/WkKVSUhKKuMewChylieHckCouIpALF5R162x+0fYYFo22e2OulmpfRfgGTRsmAZfSKMgj93H4yZfUwfT+tAM5Tvk4emk2NWVwmlGfzcfa+20nmhcswUtChOBG1jGFtw8QeQIDAQABo4ICVTCCAlEwDgYDVR0PAQH/BAQDAgWgMB0GA1UdJQQWMBQGCCsGAQUFBwMBBggrBgEFBQcDAjAMBgNVHRMBAf8EAjAAMB0GA1UdDgQWBBRJ7mAMSacV9+dtpnyLJowYhXAlWDAfBgNVHSMEGDAWgBQULrMXt1hWy65QCUDm.....root@i-tsfhx8p6:~/qke-k8s/cert-manager/alidns-webhook# kubectl get secrets lishuai-fun-tls -o json |jq --raw-output '.data["tls.crt"]'|base64 -d &gt;tls.crt root@i-tsfhx8p6:~/qke-k8s/cert-manager/alidns-webhook# openssl x509 -in tls.crt -noout -textCertificate: Data: Version: 3 (0x2) Serial Number: 04:ed:9c:a8:87:58:22:ad:df:66:38:9b:f3:06:a1:b3:44:1f Signature Algorithm: sha256WithRSAEncryption Issuer: C = US, O = Let's Encrypt, CN = R3 Validity Not Before: Jul 31 16:34:30 2021 GMT Not After : Oct 29 16:34:28 2021 GMT Subject: CN = lishuai.fun Subject Public Key Info: Public Key Algorithm: rsaEncryption RSA Public-Key: (2048 bit) Modulus: 00:d8:a4:d4:7d:56:1d:ec:92:15:7e:a7:28:b9:05: 8d:53:59:72:ea:16:2b:4b:16:0b:93:db:30:10:ae: 46:da:49:5a:11:7f:f9:45:aa:b2:6e:a5:47:be:db: 59:88:06:a1:be:e6:6c:c2:14:f2:cc:77:02:23:30: c2:ce:b2:ae:72:cd:59:14:4f:51:4f:d3:8a:02:f9: 98:d5:ee:88:c5:e1:14:6f:3d:87:8b:66:41:a4:ff: 35:b0:b3:bd:1b:db:77:c7:45:3d:45:8e:f9:85:ff: e2:4e:03:86:23:66:42:9b:da:ae:c5:13:80:a4:ca: 62:58:d1:c7:34:ee:86:1e:4e:ec:0d:94:8a:1a:36: e0:46:db:67:6e:8c:21:e8:78:50:da:5d:b6:ab:75: b4:2f:bd:5c:03:13:04:60:cf:d6:90:a5:52:52:12: 8a:b8:c7:b0:0a:1c:a5:89:e1:dc:90:2a:2e:22:90: 0b:17:94:75:eb:6c:7e:d1:f6:18:16:8d:b6:7b:63: ae:96:6a:5f:45:f8:06:4d:1b:26:01:97:d2:28:c8: 23:f7:71:f8:c9:97:d4:c1:f4:fe:b4:03:39:4e:f9: 38:7a:69:36:35:65:70:9a:51:9f:cd:c7:da:fb:6d: 27:9a:17:2c:c1:4b:42:84:e0:46:d6:31:85:b7:0f: 10:79 Exponent: 65537 (0x10001) X509v3 extensions: X509v3 Key Usage: critical Digital Signature, Key Encipherment X509v3 Extended Key Usage: TLS Web Server Authentication, TLS Web Client Authentication X509v3 Basic Constraints: critical CA:FALSE X509v3 Subject Key Identifier: 49:EE:60:0C:49:A7:15:F7:E7:6D:A6:7C:8B:26:8C:18:85:70:25:58 X509v3 Authority Key Identifier: keyid:14:2E:B3:17:B7:58:56:CB:AE:50:09:40:E6:1F:AF:9D:8B:14:C2:C6 Authority Information Access: OCSP - URI:http://r3.o.lencr.org CA Issuers - URI:http://r3.i.lencr.org/ X509v3 Subject Alternative Name: DNS:*.lishuai.fun, DNS:lishuai.fun X509v3 Certificate Policies: Policy: 2.23.140.1.2.1 Policy: 1.3.6.1.4.1.44947.1.1.1 CPS: http://cps.letsencrypt.org CT Precertificate SCTs: Signed Certificate Timestamp: Version : v1 (0x0) Log ID : F6:5C:94:2F:D1:77:30:22:14:54:18:08:30:94:56:8E: E3:4D:13:19:33:BF:DF:0C:2F:20:0B:CC:4E:F1:64:E3 Timestamp : Jul 31 17:34:30.686 2021 GMT Extensions: none Signature : ecdsa-with-SHA256 30:45:02:21:00:B3:44:02:18:42:96:53:DA:94:34:99: 1B:DE:3A:37:85:72:E4:2B:43:D6:66:99:D9:4D:DF:02: A7:5A:A4:54:80:02:20:34:C4:45:7D:B4:F8:19:8D:A4: 09:41:3A:7E:23:B3:4D:4B:04:76:9E:E1:F2:94:EC:50: 2C:BF:A7:3B:71:75:E4 Signed Certificate Timestamp: Version : v1 (0x0) Log ID : 6F:53:76:AC:31:F0:31:19:D8:99:00:A4:51:15:FF:77: 15:1C:11:D9:02:C1:00:29:06:8D:B2:08:9A:37:D9:13 Timestamp : Jul 31 17:34:30.744 2021 GMT Extensions: none Signature : ecdsa-with-SHA256 30:45:02:20:26:99:A1:07:A5:B1:48:10:B4:8D:30:20: E4:AC:A2:44:1A:8B:FD:17:52:FF:CC:9E:FC:A2:02:66: C2:B7:A9:9C:02:21:00:BB:F2:DD:2E:1A:1C:20:56:BE: 96:7C:94:56:EC:40:9F:9C:0F:70:F2:DC:CF:65:DC:7B: 37:26:1E:02:5E:E9:9E Signature Algorithm: sha256WithRSAEncryption 23:b9:f4:b1:c7:95:3e:ef:5d:41:42:1d:17:a9:1b:d2:f1:b6: 42:61:ca:d4:63:9e:c4:f1:11:0d:8a:03:29:66:9b:98:58:98: 42:2d:eb:ee:d8:16:f5:7e:4f:e6:28:c1:82:bd:5e:b0:28:82: 56:a0:0a:34:13:d6:15:98:8c:97:c6:c2:7f:64:50:d7:19:4c: 25:45:4e:c0:ff:ee:90:63:29:bb:ba:01:1f:62:b5:d4:5d:50: 49:0d:af:a0:f6:94:af:68:29:1a:46:34:13:c0:e4:26:68:22: ba:fe:58:c7:6e:79:a4:3b:9f:80:dc:cd:ef:44:21:57:e1:36: 2e:36:b2:98:a4:ea:38:f9:ed:f9:3e:31:df:47:03:32:63:8f: 8a:9a:a2:2a:7d:4a:2b:23:c0:e6:9f:35:b4:b6:3a:ac:c1:e6: 40:91:5c:f0:5c:17:48:9b:ac:49:5f:f6:91:e8:03:0a:9c:37: 48:46:40:29:b1:85:72:40:3c:05:a8:3d:13:67:20:4a:36:6f: 4c:c0:a9:b8:40:dc:96:3b:99:da:82:3a:71:6f:41:93:aa:e2: 22:c4:19:14:7b:8c:64:0b:b5:9b:b1:d0:56:b1:17:93:72:d7: 94:7e:a9:08:f6:f4:0c:1d:4e:36:be:1d:14:ff:a2:9a:02:af: 82:9d:bb:8f我们也可以使用一些网站提供的在线证书查看我们可以看到这个证书是个泛域名证书，此时我们ingress 可以直接使用该secert，不需要再去添加注解执行使用的issuer/cluseteissuer了，例如12345678910111213141516171819202122root@i-tsfhx8p6:~/qke-k8s/cert-manager/ingress# cat alertmanager-ingress.yaml apiVersion: networking.k8s.io/v1kind: Ingressmetadata: name: alertmanager namespace: monitoringspec: rules: - host: alertmanager-qke.lishuai.fun http: paths: - backend: service: name: alertmanager-main port: number: 9093 path: / pathType: Prefix tls: # &lt; placing a host in the TLS config will indicate a certificate should be created - hosts: - alertmanager-qke.lishuai.fun secretName: lishuai-fun-tls创建Certificate这里拿我们上面创建的那个Certificate来讲解123456789101112apiVersion: cert-manager.io/v1kind: Certificatemetadata: name: lishuai-fun-tlsspec: secretName: lishuai-fun-tls dnsNames: - lishuai.fun - "*.lishuai.fun" issuerRef: name: letsencrypt-staging kind: ClusterIssuerspec.secretName 指示证书最终存到哪个 Secret 中spec.issuerRef.kind 值为 ClusterIssuer 说明签发机构不在本 namespace 下，而是在全局spec.issuerRef.name 我们创建的签发机构的名称 (ClusterIssuer.metadata.name)spec.dnsNames 指示该证书的可以用于哪些域名下面是一个官方的包含完整字段的Certificate1234567891011121314151617181920212223242526272829303132333435363738394041apiVersion: cert-manager.io/v1kind: Certificatemetadata: name: example-com namespace: sandboxspec: # Secret names are always required. secretName: example-com-tls duration: 2160h # 90d renewBefore: 360h # 15d subject: organizations: - jetstack # The use of the common name field has been deprecated since 2000 and is # discouraged from being used. commonName: example.com isCA: false privateKey: algorithm: RSA encoding: PKCS1 size: 2048 usages: - server auth - client auth # At least one of a DNS Name, URI, or IP address is required. dnsNames: - example.com - www.example.com uris: - spiffe://cluster.local/ns/sandbox/sa/example ipAddresses: - 192.168.0.5 # Issuer references are always required. issuerRef: name: ca-issuer # We can reference ClusterIssuers by changing the kind here. # The default value is Issuer (i.e. a locally namespaced Issuer) kind: Issuer # This is optional since cert-manager will default to this value however # if you are using an external issuer, change this to that issuer group. group: cert-manager.io转载自：k8s安装cert-manager及签发泛域名证书 · kubernetes]]></content>
      <categories>
        <category>技术</category>
        <category>域名证书</category>
      </categories>
      <tags>
        <tag>免费证书</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s使用NFS建立持久卷]]></title>
    <url>%2F2024%2F062698ba09.html</url>
    <content type="text"><![CDATA[由于服务在k8s集群是以容器方式启动，容器重启的话数据会丢失，所以我们一般会把重要的数据挂载到机器上，这里使用NFS做k8s的持久卷。部署NFS 服务端关闭防火墙12$ systemctl stop firewalld.service$ systemctl disable firewalld.service安装配置 nfs1$ yum -y install nfs-utils rpcbind共享目录设置权限：以 /data/k8s目录存放数据1chmod 755 /data/k8s/配置 nfs，nfs 的默认配置文件在 /etc/exports 文件下，在该文件中添加下面的配置信息：12$ vi /etc/exports/data/k8s *(rw,sync,no_root_squash)配置说明：/data/k8s：是共享的数据目录*：表示任何人都有权限连接，当然也可以是一个网段，一个 IP，也可以是域名rw：读写的权限sync：表示文件同时写入硬盘和内存no_root_squash：当登录 NFS 主机使用共享目录的使用者是 root 时，其权限将被转换成为匿名使用者，通常它的 UID 与 GID，都会变成 nobody 身份启动服务 nfs 需要向 rpc 注册，rpc 一旦重启了，注册的文件都会丢失，向他注册的服务都需要重启123456789101112$ systemctl start rpcbind.service$ systemctl enable rpcbind$ systemctl status rpcbind● rpcbind.service - RPC bind service Loaded: loaded (/usr/lib/systemd/system/rpcbind.service; disabled; vendor preset: enabled) Active: active (running) since Tue 2018-07-10 20:57:29 CST; 1min 54s ago Process: 17696 ExecStart=/sbin/rpcbind -w $RPCBIND_ARGS (code=exited, status=0/SUCCESS) Main PID: 17697 (rpcbind) Tasks: 1 Memory: 1.1M CGroup: /system.slice/rpcbind.service └─17697 /sbin/rpcbind -wrunning 证明启动成功了。然后启动 nfs 服务1234567891011$ systemctl start nfs.service$ systemctl enable nfs$ systemctl status nfs● nfs-server.service - NFS server and services Loaded: loaded (/usr/lib/systemd/system/nfs-server.service; enabled; vendor preset: disabled) Drop-In: /run/systemd/generator/nfs-server.service.d └─order-with-mounts.conf Active: active (exited) since Fri 2020-12-11 16:59:51 CST; 2 weeks 4 days ago Main PID: 83896 (code=exited, status=0/SUCCESS) CGroup: /system.slice/nfs-server.serviceacticve代表启动成功另外我们还可以通过下面的命令确认下：1234567$ rpcinfo -p|grep nfs 100003 3 tcp 2049 nfs 100003 4 tcp 2049 nfs 100227 3 tcp 2049 nfs_acl 100003 3 udp 2049 nfs 100003 4 udp 2049 nfs 100227 3 udp 2049 nfs_acl查看具体目录挂载权限：12$ cat /var/lib/nfs/etab/data/k8s *(rw,sync,wdelay,hide,nocrossmnt,secure,no_root_squash,no_all_squash,no_subtree_check,secure_locks,acl,no_pnfs,anonuid=65534,anongid=65534,sec=sys,secure,no_root_squash,no_all_squash)到这里我们就把 nfs server 给安装成功了。在其他机器查看共享目录1sudo showmount -e 192.168.108.100返回值如下，表示创建成功123Export list for 192.168.108.100:/data/k8s 192.168.108.0/24设置存储分配器的权限创建nfs-client-provisioner-authority.yaml文件12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485apiVersion: v1kind: ServiceAccountmetadata: name: nfs-client-provisionerreplace with namespace where provisioner is deployednamespace: defaultkind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: nfs-client-provisioner-runnerrules: - apiGroups: [""] resources: ["persistentvolumes"] verbs: ["get", "list", "watch", "create", "delete"] - apiGroups: [""] resources: ["persistentvolumeclaims"] verbs: ["get", "list", "watch", "update"] - apiGroups: ["storage.k8s.io"] resources: ["storageclasses"] verbs: ["get", "list", "watch"] - apiGroups: [""] resources: ["events"] verbs: ["create", "update", "patch"]---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: run-nfs-client-provisionersubjects: - kind: ServiceAccount name: nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: default roleRef: kind: ClusterRole name: nfs-client-provisioner-runner apiGroup: rbac.authorization.k8s.io---kind: RoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: leader-locking-nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: defaultrules: - apiGroups: [""] resources: ["endpoints"] verbs: ["get", "list", "watch", "create", "update", "patch"]---kind: RoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: leader-locking-nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: defaultsubjects: - kind: ServiceAccount name: nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: default roleRef: kind: Role name: leader-locking-nfs-client-provisioner apiGroup: rbac.authorization.k8s.io创建NFS存储分配器创建nfs-client-provisioner.yaml文件12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152apiVersion: apps/v1kind: Deploymentmetadata: name: nfs-client-provisionerspec: replicas: 1 strategy: type: Recreate selector: matchLabels: app: nfs-client-provisioner template: metadata: labels: app: nfs-client-provisioner spec: serviceAccountName: nfs-client-provisioner containers: - name: nfs-client-provisioner image: quay.io/external_storage/nfs-client-provisioner:latest volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: # 存储分配器名称 - name: PROVISIONER_NAME value: nfs-provisioner # NFS服务器地址，设置为自己的IP - name: NFS_SERVER value: 192.168.108.100 # NFS共享目录地址 - name: NFS_PATH value: /data/k8s volumes: - name: nfs-client-root nfs: # 设置为自己的IP server: 192.168.108.100 # 对应NFS上的共享目录 path: /data/k8s创建StorageClass创建nfs-storage-class.yaml文件1234567891011121314151617181920apiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: nfs-data# 存储分配器的名称# 对应“nfs-client-provisioner.yaml”文件中env.PROVISIONER_NAME.valueprovisioner: nfs-provisioner# 允许pvc创建后扩容allowVolumeExpansion: Trueparameters: # 资源删除策略，“true”表示删除PVC时，同时删除绑定的PV archiveOnDelete: "true"查看StorageClass1kubectl get storageclass创建PVC创建nfs-pvc.yaml文件12345678910111213141516171819202122232425apiVersion: v1kind: PersistentVolumeClaimmetadata: # 注意，后面Deployment申请资源需要用到此处的名称 name: nfs-pvcspec: # 设置资源的访问策略，ReadWriteMany表示该卷可以被多个节点以读写模式挂载； accessModes: - ReadWriteMany # 设置资源的class名称 # 注意，此处的名称必须与“nfs-storage-class.yaml”中的storageClassName相同 storageClassName: nfs-data # 设置申请的资源大小 resources: requests: storage: 100Mi查看PVC1kubectl get pvc创建Deployment控制器创建nfs-deployment-python.yaml文件12345678910111213141516171819202122232425262728293031323334353637383940414243apiVersion: apps/v1kind: Deploymentmetadata: name: nfs-deployment-pythonspec: replicas: 2 selector: matchLabels: app: python-nfs template: metadata: labels: app: python-nfs spec: containers: - name: python-nfs image: python:3.8.2 imagePullPolicy: IfNotPresent command: ['/bin/bash', '-c', '--'] # 启动"python -m http.server 80"服务，“&gt;&gt;”表示向文件中追加数据 args: ['echo "&lt;p&gt; The host is $(hostname) &lt;/p&gt;" &gt;&gt; /containerdata/podinfor; python -m http.server 80'] # 设置80端口 ports: - name: http containerPort: 80 # 设置挂载点 volumeMounts: # 此处的名称与volumes有对应关系 - name: python-nfs-data mountPath: /containerdata # 配置nfs存储卷 volumes: # 此处的名称需与spec.containers.volumeMounts.name相同 - name: python-nfs-data # 向PVC申请资源，此处的名称对应# nfs-pvc.yaml文件中的metadata.name persistentVolumeClaim: claimName: nfs-pvc]]></content>
      <categories>
        <category>技术</category>
        <category>NFS</category>
      </categories>
      <tags>
        <tag>K8S</tag>
        <tag>NFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Prometheus实现运维告警]]></title>
    <url>%2F2024%2F06bbd18b10.html</url>
    <content type="text"><![CDATA[搭建好监控环境并且监控了服务器、数据库、应用，运维人员可以实时了解当前被监控对象的运行情况，但是他们不可能时时坐在电脑边上盯着DashBoard，这就需要一个告警功能，当服务器或应用指标异常时发送告警，通过邮件或者短信的形式告诉运维人员及时处理。接下来就来介绍非常重要的功能——告警。 告警的实现方式Prometheus将数据采集和告警分成了两个模块。报警规则配置在Prometheus Servers上，然后发送报警信息到AlertManger等告警系统，然后在告警系统管理这些报警信息、聚合报警信息、然后通过email、短信等方式发送消息告警。目前，实现告警功能主要有以下几种方式：使用prometheus提供的Alertmanager告警组件（功能全面，告警规则配置比较复杂）。OneAlert等其他第三方组件（配置简单，可以实现短信、电话、微信等多种告警方式，但是依赖第三方平台，而且是收费的）Grafana 等自带的告警功能（配置简单）相比于Grafana的图形化界面，Alertmanager需要依靠配置文件实现，虽然配置比较繁琐，但是胜在功能强大灵活。接下来我们就使用Alertmanager一步一步实现告警通知。Grafana告警新版本的Grafana提供了告警配置，直接在dashboard监控panel中设置告警即可。Grafana 支持多种告警方式，这里以邮件为例，演示Grafana如何设置邮件告警功能。配置邮件服务step1：要启用 email 报警需要在启动配置文件中 /conf/default.ini开启 SMTP 服务，具体配置如下：1234567891011121314151617[smtp]enabled = truehost = smtp.163.com:25user = xxx@163.com # 邮件地址# If the password contains # or ; you have to wrap it with triple quotes. Ex """#password;"""password = xxx # 授权码cert_file =key_file =skip_verify = truefrom_address = xxx@163.com # 发件人地址from_name = Grafanaehlo_identity = dashboard.example.com[emails]welcome_email_on_sign_up = falsetemplates_pattern = emails/*.htmlcontent_types = text/html这里的邮箱服务使用的是163的邮箱服务，需要提前打开邮箱的SMTP服务并申请授权码，示例中的邮箱地址和密码请换成自己的邮箱和授权码。step2：重启Grafana，验证邮件是否配置成功，点击页面上的 Alerting | Contact points 添加Contact Points。点击页面上的 New contact point 按钮，添加一个邮件通知渠道。选择邮件方式，并输入收件人的邮箱后保存即可，验证邮箱是否配置成功，点击 Test 按钮，Grafana 会发送一封测试邮件到收件人邮箱。如果能收到邮件，说明配置成功。配置告警规则配置好邮件发送和接收的Contact Points 之后，接下来我们配置Grafana的告警规则。step1：创建告警告警规则，首先在某个Panel 上的下拉箭头中，选择 Edit | Alert。step2：接下来点击 Create alert from this panel 按钮，给此panel 创建告警规则。如上图所示，我们以node_memory_MemFree_bytes （服务器可用内存）指标为例，设置告警规则：当服务器可用内存低于4.65G 时告警。step3：告警名称，间隔时间等设置。step4：设置完其他相关的参数之后，点击Save 保存，即可查看告警的情况。如上图所示，Grafana已经产生了一条Pending状态的告警的记录，当此记录变为Firing状态就说明已经告警成功，并发送了邮件通知。以上，我们把Grafana的告警功能介绍完了，Grafana虽然比较直观，但是相比Alertmanager而言不够灵活，不支持变量，如果系统不复杂的话，可以考虑使用Grafana。Alertmanager告警类型Alertmanager提供了以下两种告警方式：邮件接收器 email_config，发送邮件通知；Webhook接收器 webhook_config，使用post方式向配置的url地址发送数据请求。安装alertmanagerstep1：安装Alertmanager首先在prometheus官网，下载Alertmanager组件，并上传到服务器解压即可。12345# 解压到/usr/local/prometheus目录下：tar -zxvf alertmanager-0.24.0.linux-amd64.tar.gz -C /usr/local/prometheus# 修改目录名：cd /usr/local/prometheusmv alertmanager-0.24.0.linux-amd64 alertmanager-0.24.0step 2：配置Alertmanager修改alertmanager.yml 文件，增加Email等相关配置，具体如下：12345678910111213141516171819202122232425262728global: resolve_timeout: 5m # alertmanager在持续多久没有收到新告警后标记为resolved smtp_from: 'xxx@163.com' # 发件人邮箱地址 smtp_smarthost: 'smtp.163.com:25' # 邮箱smtp地址 smtp_auth_username: 'xxx@163.com' # 发件人的登陆用户名，默认和发件人地址一致 smtp_auth_password: 'xxx' # 发件人的登陆密码，有时候是授权码 smtp_hello: '163.com' smtp_require_tls: # 是否需要tls协议。默认是true route: group_by: [alertname] # 通过alertname的值对告警进行分类 group_wait: 10s # 一组告警第一次发送之前等待的时延，即产生告警10s将组内新产生的消息合并发送，通常是0s~几分钟（默认是30s） group_interval: 2m # 一组已发送过初始告警通知的告警，接收到新告警后，下次发送通知前等待时延，通常是5m或更久（默认是5m） repeat_interval: 5m # 一组已经发送过通知的告警，重复发送告警的间隔，通常设置为3h或者更久（默认是4h） receiver: 'default-receiver' # 设置告警接收人receivers:- name: 'default-receiver' email_configs: - to: 'xxx@163.com' send_resolved: true # 发送恢复告警通知 inhibit_rules: # 抑制规则 - source_match: # 源匹配级别，当匹配成功发出通知，其他级别产生告警将被抑制 severity: 'critical' # 告警时间级别（告警级别根据规则自定义） target_match: severity: 'warning' # 匹配目标成功后，新产生的目标告警为'warning'将被抑制 equal: ['alertname','dev','instance'] # 基于这些标签抑制匹配告警的级别这里的邮箱服务使用的是163的邮箱服务，需要提前打开邮箱的SMTP服务并申请授权码，示例中的邮箱地址和密码请换成自己的邮箱和授权码。配置文件比较复杂，可以使用./amtool check-config alertmanager.yml 命令校验文件是否正确。step 3：启动运行配置文件修改完成后，接下来我们将Alertmanager运行起来，具体命令如下：1234#alertmanagercd /usr/local/prometheus/alertmanager-0.24.0#执行启动命令，指定数据访问的urlalertmanager --config.file=/usr/local/prometheus/alertmanager-0.24.0/alertmanager.y命令执行成功后，在浏览器中访问：http://10.2.1.231:9093/ 默认端口9093如上图所示，我们成功打开Alertmanager 管理页面，说明Alertmanager配置、启动成功。将Alertmanager添加到Prometheus前面我们说了，告警规则是配置在Prometheus Servers上，然后发送报警信息到AlertManger中的，那么接下来我们把Alertmanager添加到Prometheus中。step1：Prometheus告警配置修改prometheus.yml 配置文件，增加告警地址和告警规则，具体配置如下：1234567891011# Alertmanager configurationalerting: alertmanagers: - static_configs: - targets: [10.2.1.231:9093]# Load rules once and periodically evaluate them according to the global 'evaluation_interval'.# 加载指定的规则文件rule_files: - "first.rules" - "rules/*.yml"上面的配置，主要是配置Alertmanager的地址和规则文件加载路径。告警规则读取prometheus目录的rule下的所有以yml结尾的文件。step2：配置告警规则前面在prometheus.yml 中配置了规则的路径，所以，接下来在prometheus的根目录下创建rules目录。这里以服务器资源状态状态为例，制定cpu、内存、磁盘的告警。创建pods_rule.yaml文件。具体配置如下：12345678910111213141516171819202122232425262728293031323334353637groups: - name: alertmanager_pod.rules rules: - alert: Pod_all_cpu_usage expr: (sum by(name)(rate(container_cpu_usage_seconds_total&#123;image!=""&#125;[5m]))*100) &gt; 1 for: 2m labels: serverity: critical service: pods annotations: description: 容器 &#123;&#123; $labels.name &#125;&#125; CPU 资源利用率大于 10% , (current value is &#123;&#123; $value &#125;&#125;) summary: Dev CPU 负载告警 - alert: Pod_all_memory_usage expr: sort_desc(avg by(name)(irate(node_memory_MemFree_bytes &#123;name!=""&#125;[5m]))) &gt; 2147483648 # 内存大于2G for: 2m labels: severity: critical annotations: description: 容器 &#123;&#123; $labels.name &#125;&#125; Memory 资源利用大于 2G , (current value is &#123;&#123; $value &#125;&#125;) summary: Dev Memory 负载告警 - alert: Pod_all_network_receive_usage expr: sum by(name)(irate(container_network_reveive_bytes_total&#123;container_name="POD"&#125;[1m])) &gt; 52428800 # 大于50M for: 2m labels: severity: critical annotations: description: 容器 &#123;&#123; $labels.name &#125;&#125; network_receive 资源利用大于 50M , (current value is &#123;&#123; $value &#125;&#125;) - alert: node内存可用大小 expr: node_memory_MemFree_bytes &lt; 5368709120 # 内存小于5G for: 2m labels: severity: critical annotations: description: node可用内存小于5Gstep3：重启Prometheus配置完成后重启Prometheus，访问Prometheus查看告警配置。在浏览器中输入：http://10.2.1.231:9090/alerts 查看告警配置是否成功。如上图所示，我们在Prometheus的Alerts下可以看到对应的告警配置。FIRING说明告警已成功。此时Alertmanager应该相关的告警数据。打开http://10.2.1.231:9093/#/alerts 查看报警情况。如上图所示，Alertmanager收到了Prometheus发过来的告警消息，前面我们在Alertmanager中配置了邮件地址，可以去邮箱中查看是否收到邮件。Alertmanager的告警内容支持使用模板配置，可以使用邮件模板进行渲染。]]></content>
      <categories>
        <category>技术</category>
        <category>Prometheus</category>
      </categories>
      <tags>
        <tag>Prometheus</tag>
        <tag>AlertManger</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Prometheus监控K8S集群]]></title>
    <url>%2F2024%2F06dba56ffd.html</url>
    <content type="text"><![CDATA[Prometheus是一个开源系统监控和警报工具包，最初由 SoundCloud构建。自 2012 年启动以来，许多公司和组织都采用了 Prometheus，该项目拥有非常活跃的开发者和用户社区。它现在是一个独立的开源项目，独立于任何公司进行维护。为了强调这一点，并明确项目的治理结构，Prometheus 于 2016 年作为继Kubernetes之后的第二个托管项目加入了云原生计算基金会。监控方案Cadvisor + node-exporter + prometheus + grafanaCadvisor：数据采集node-exporter：汇总prometheus：处理、存储grafana：展示监控流程容器监控：Prometheus使用cadvisor采集容器监控指标，而cadvisor集成在K8S的kubelet中所以无需部署，通过Prometheus进程存储，使用grafana进行展示。node节点监控：node端的监控通过node_exporter采集当前主机的资源，通过Prometheus进程存储，最后使用grafana进行展示。master节点监控：master的监控通过kube-state-metrics插件从K8S获取到apiserver的相关数据并通过网页页面暴露出来，然后通过Prometheus进程存储，最后使用grafana进行展示Kubernetes监控指标K8S自身监控指标node资源利用率：监控node节点上的cpu、内存、硬盘等硬件资源node数量：监控node数量与资源利用率、业务负载的比例情况，对成本、资源扩展进行评估pod数量：监控当负载到一定程度时，node与pod的数量。评估负载到哪个阶段，大约需要多少服务器，以及每个pod的资源占用率，然后进行整体评估资源对象状态：在K8S运行过程中，会创建很多pod、控制器、任务等，这些内容都是由K8S中的资源对象进行维护，所以可以对资源对象进行监控，获取资源对象的状态Pod的监控每个项目中pod的数量：分别监控正常、有问题的pod数量容器资源利用率：统计当前pod的资源利用率，统计pod中的容器资源利用率，结合cpu、网络、内存进行评估应用程序：监控项目中程序的自身情况，例如并发、请求响应等监控指标具体实现案例Pod性能cadvisor容器的CPU、内存利用率Node性能node-exporternode节点的CPU、内存利用率K8S资源对象kube-state-metricspod、deployment、service服务发现从k8s的api中发现抓取的目标，并且始终与k8s集群状态保持一致。动态获取被抓取的目标，实时从api中获取当前状态是否存在，此过程为服务发现。自动发现支持的组件：node：自动发现集群中的node节点pod：自动发现运行的容器和端口service：自动发现创建的serviceIP、端口endpoints：自动发现pod中的容器ingress：自动发现创建的访问入口和规则Prometheus监控Kubernetes部署实践部署准备案例仅在master节点pull image123456789101112[root@master ~]# git clone https://github.com/redhatxl/k8s-prometheus-grafana.git #这个仓库的yaml有几个错误，在本文章末尾已经改过来了，可以直接使用末尾的yaml文件Cloning into 'k8s-prometheus-grafana'...remote: Enumerating objects: 21, done.remote: Total 21 (delta 0), reused 0 (delta 0), pack-reused 21Unpacking objects: 100% (21/21), done.[root@master ~]# lltotal 24drwxr-xr-x 5 root root 94 Jul 12 16:07 k8s-prometheus-grafana #克隆的目录#在所有节点提前下载镜像[root@master ~]# docker pull prom/node-exporter [root@master ~]# docker pull prom/prometheus:v2.0.0[root@master ~]# docker pull grafana/grafana:6.1.4采用daemonset方式部署node-exporter1234567[root@master ~]# cd k8s-prometheus-grafana/[root@master k8s-prometheus-grafana]# lltotal 8drwxr-xr-x 2 root root 81 Jul 12 16:07 grafana-rw-r--r-- 1 root root 668 Jul 12 16:07 node-exporter.yamldrwxr-xr-x 2 root root 106 Jul 12 16:07 prometheus-rw-r--r-- 1 root root 117 Jul 12 16:07 README.md123456789101112131415161718192021222324252627282930313233343536373839404142[root@master k8s-prometheus-grafana]# cat node-exporter.yaml---apiVersion: apps/v1kind: DaemonSetmetadata: name: node-exporter namespace: kube-system labels: k8s-app: node-exporterspec: selector: matchLabels: k8s-app: node-exporter template: metadata: labels: k8s-app: node-exporter spec: containers: - image: prom/node-exporter name: node-exporter ports: - containerPort: 9100 protocol: TCP name: http---apiVersion: v1kind: Servicemetadata: labels: k8s-app: node-exporter name: node-exporter namespace: kube-systemspec: ports: - name: http port: 9100 nodePort: 31672 protocol: TCP type: NodePort selector: k8s-app: node-exporter1234567891011121314151617181920212223242526272829303132[root@master k8s-prometheus-grafana]# kubectl apply -f node-exporter.yamldaemonset.apps/node-exporter createdservice/node-exporter created [root@master k8s-prometheus-grafana]# kubectl get pods -ANAMESPACE NAME READY STATUS RESTARTS AGEdefault recycler-for-prometheus-data 0/1 ContainerCreating 0 5m42skube-system coredns-7ff77c879f-m986g 1/1 Running 0 29hkube-system coredns-7ff77c879f-xhknw 1/1 Running 0 29hkube-system etcd-master 1/1 Running 0 29hkube-system kube-apiserver-master 1/1 Running 0 29hkube-system kube-controller-manager-master 1/1 Running 2 29hkube-system kube-flannel-ds-ln5f6 1/1 Running 0 26hkube-system kube-flannel-ds-zhq42 1/1 Running 0 29hkube-system kube-proxy-9bssw 1/1 Running 0 26hkube-system kube-proxy-wcdzk 1/1 Running 0 29hkube-system kube-scheduler-master 1/1 Running 3 29hkube-system node-exporter-bnkm8 1/1 Running 0 2m19s #这就是新创建的[root@master k8s-prometheus-grafana]# kubectl get daemonset -ANAMESPACE NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGEkube-system kube-flannel-ds 2 2 2 2 2 &lt;none&gt; 29hkube-system kube-proxy 2 2 2 2 2 kubernetes.io/os=linux 29hkube-system node-exporter 1 1 1 1 1 &lt;none&gt; 2m23s #这个是新的daemonset[root@master k8s-prometheus-grafana]# kubectl get service -ANAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEdefault kubernetes ClusterIP 172.16.0.1 &lt;none&gt; 443/TCP 29hkube-system kube-dns ClusterIP 172.16.0.10 &lt;none&gt; 53/UDP,53/TCP,9153/TCP 29hkube-system node-exporter NodePort 172.16.201.35 &lt;none&gt; 9100:31672/TCP 2m29s #这是新的service部署Prometheus123456789101112131415161718192021[root@master k8s-prometheus-grafana]# cd prometheus/[root@master prometheus]# lltotal 20-rw-r--r-- 1 root root 5631 Jul 12 16:07 configmap.yaml-rw-r--r-- 1 root root 1119 Jul 12 16:07 prometheus.deploy.yml-rw-r--r-- 1 root root 237 Jul 12 16:07 prometheus.svc.yml-rw-r--r-- 1 root root 716 Jul 12 16:07 rbac-setup.yaml [root@master prometheus]# kubectl apply -f rbac-setup.yamlclusterrole.rbac.authorization.k8s.io/prometheus configuredserviceaccount/prometheus configuredclusterrolebinding.rbac.authorization.k8s.io/prometheus configured[root@master prometheus]# kubectl apply -f configmap.yamlconfigmap/prometheus-config configured [root@master prometheus]# kubectl apply -f prometheus.deploy.ymldeployment.apps/prometheus created[root@master prometheus]# kubectl apply -f prometheus.svc.ymlservice/prometheus created部署grafana123456789101112131415[root@master prometheus]# cd ../grafana/[root@master grafana]# lltotal 12-rw-r--r-- 1 root root 1449 Jul 12 16:07 grafana-deploy.yaml-rw-r--r-- 1 root root 256 Jul 12 16:07 grafana-ing.yaml-rw-r--r-- 1 root root 225 Jul 12 16:07 grafana-svc.yaml[root@master grafana]# kubectl apply -f grafana-deploy.yamldeployment.apps/grafana-core created[root@master grafana]# kubectl apply -f grafana-svc.yamlservice/grafana created[root@master grafana]# kubectl apply -f grafana-ing.yamlingress.extensions/grafana created校验测试1）查看pod/svc信息12345678910111213141516171819202122232425[root@master grafana]# kubectl get pods -ANAMESPACE NAME READY STATUS RESTARTS AGEdefault recycler-for-prometheus-data 0/1 ContainerCreating 0 2m7skube-system coredns-7ff77c879f-m986g 1/1 Running 0 30hkube-system coredns-7ff77c879f-xhknw 1/1 Running 0 30hkube-system etcd-master 1/1 Running 0 30hkube-system grafana-core-768b6bf79c-wcmk9 1/1 Running 0 2m48skube-system kube-apiserver-master 1/1 Running 0 30hkube-system kube-controller-manager-master 1/1 Running 2 30hkube-system kube-flannel-ds-ln5f6 1/1 Running 0 26hkube-system kube-flannel-ds-zhq42 1/1 Running 0 29hkube-system kube-proxy-9bssw 1/1 Running 0 26hkube-system kube-proxy-wcdzk 1/1 Running 0 30hkube-system kube-scheduler-master 1/1 Running 3 30hkube-system node-exporter-bnkm8 1/1 Running 0 18mkube-system prometheus-7486bf7f4b-f8k6x 1/1 Running 0 7m12s [root@master grafana]# kubectl get svc -ANAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEdefault kubernetes ClusterIP 172.16.0.1 &lt;none&gt; 443/TCP 30hkube-system grafana NodePort 172.16.198.56 &lt;none&gt; 3000:30931/TCP 5m15s #grafana端口30931kube-system kube-dns ClusterIP 172.16.0.10 &lt;none&gt; 53/UDP,53/TCP,9153/TCP 30hkube-system node-exporter NodePort 172.16.201.35 &lt;none&gt; 9100:31672/TCP 22m #node-exporter端口31672kube-system prometheus NodePort 172.16.176.125 &lt;none&gt; 9090:30003/TCP 9m12s #prometheus端口300032）查看页面访问http://10.10.11.202:31672/metrics，这是node-exporter采集的数据。访问http://10.10.11.202:30003，这是Prometheus的页面，依次点击Status&gt;Targets可以看到已经成功连接到k8s的apiserver。访问http://10.10.11.202:30931，这是grafana的页面，账户、密码都是admin。grafana模版配置1）添加数据源，点击add，点击Prometheus 2)依次进行设置，这里的URL需要注意：URL需要写成，service.namespace:port 的格式，例如：123456789[root@master grafana]# kubectl get svc -ANAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEdefault kubernetes ClusterIP 172.16.0.1 &lt;none&gt; 443/TCP 46hkube-system grafana NodePort 172.16.195.186 &lt;none&gt; 3000:30931/TCP 4m16skube-system kube-dns ClusterIP 172.16.0.10 &lt;none&gt; 53/UDP,53/TCP,9153/TCP 46hkube-system node-exporter NodePort 172.16.201.35 &lt;none&gt; 9100:31672/TCP 17hkube-system prometheus NodePort 172.16.176.125 &lt;none&gt; 9090:30003/TCP 16h#以这里为例，namespace是kube-system,service是prometheus，pod端口是9090，那么最后的URL就是；http://prometheus.kube-system:9090 3)导入K8S Dashboard模板 4)name自定义，uid可以为空，会自己生成，然后选择数据源，选择刚才创建的Prometheus，最后点击import5）效果图展示yaml配置文件node-exporter.yaml1234567891011121314151617181920212223242526272829303132333435363738394041---apiVersion: apps/v1kind: DaemonSetmetadata: name: node-exporter namespace: kube-system labels: k8s-app: node-exporterspec: selector: matchLabels: k8s-app: node-exporter template: metadata: labels: k8s-app: node-exporter spec: containers: - image: prom/node-exporter name: node-exporter ports: - containerPort: 9100 protocol: TCP name: http---apiVersion: v1kind: Servicemetadata: labels: k8s-app: node-exporter name: node-exporter namespace: kube-systemspec: ports: - name: http port: 9100 nodePort: 31672 protocol: TCP type: NodePort selector: k8s-app: node-exporterrbac-setup.yaml123456789101112131415161718192021222324252627282930313233343536373839apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: name: prometheusrules:- apiGroups: [""] resources: - nodes - nodes/proxy - services - endpoints - pods verbs: ["get", "list", "watch"]- apiGroups: - extensions resources: - ingresses verbs: ["get", "list", "watch"]- nonResourceURLs: ["/metrics"] verbs: ["get"]---apiVersion: v1kind: ServiceAccountmetadata: name: prometheus namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: prometheusroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: prometheussubjects:- kind: ServiceAccount name: prometheus namespace: kube-systemconfigmap.yaml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156apiVersion: v1kind: ConfigMapmetadata: name: prometheus-config namespace: kube-systemdata: prometheus.yml: | global: scrape_interval: 15s evaluation_interval: 15s scrape_configs: - job_name: 'kubernetes-apiservers' kubernetes_sd_configs: - role: endpoints scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name] action: keep regex: default;kubernetes;https - job_name: 'kubernetes-nodes' kubernetes_sd_configs: - role: node scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address__ replacement: kubernetes.default.svc:443 - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/$&#123;1&#125;/proxy/metrics - job_name: 'kubernetes-cadvisor' kubernetes_sd_configs: - role: node scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address__ replacement: kubernetes.default.svc:443 - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/$&#123;1&#125;/proxy/metrics/cadvisor - job_name: 'kubernetes-service-endpoints' kubernetes_sd_configs: - role: endpoints relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape] action: keep regex: true - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme] action: replace target_label: __scheme__ regex: (https?) - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port] action: replace target_label: __address__ regex: ([^:]+)(?::\d+)?;(\d+) replacement: $1:$2 - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] action: replace target_label: kubernetes_name - job_name: 'kubernetes-services' kubernetes_sd_configs: - role: service metrics_path: /probe params: module: [http_2xx] relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe] action: keep regex: true - source_labels: [__address__] target_label: __param_target - target_label: __address__ replacement: blackbox-exporter.example.com:9115 - source_labels: [__param_target] target_label: instance - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] target_label: kubernetes_name - job_name: 'kubernetes-ingresses' kubernetes_sd_configs: - role: ingress relabel_configs: - source_labels: [__meta_kubernetes_ingress_annotation_prometheus_io_probe] action: keep regex: true - source_labels: [__meta_kubernetes_ingress_scheme,__address__,__meta_kubernetes_ingress_path] regex: (.+);(.+);(.+) replacement: $&#123;1&#125;://$&#123;2&#125;$&#123;3&#125; target_label: __param_target - target_label: __address__ replacement: blackbox-exporter.example.com:9115 - source_labels: [__param_target] target_label: instance - action: labelmap regex: __meta_kubernetes_ingress_label_(.+) - source_labels: [__meta_kubernetes_namespace] target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_ingress_name] target_label: kubernetes_name - job_name: 'kubernetes-pods' kubernetes_sd_configs: - role: pod relabel_configs: - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape] action: keep regex: true - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port] action: replace regex: ([^:]+)(?::\d+)?;(\d+) replacement: $1:$2 target_label: __address__ - action: labelmap regex: __meta_kubernetes_pod_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_pod_name] action: replace target_label: kubernetes_pod_nameprometheus.deploy.yml12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849---apiVersion: apps/v1kind: Deploymentmetadata: labels: name: prometheus-deployment name: prometheus namespace: kube-systemspec: replicas: 1 selector: matchLabels: app: prometheus template: metadata: labels: app: prometheus spec: containers: - image: prom/prometheus:v2.0.0 name: prometheus command: - "/bin/prometheus" args: - "--config.file=/etc/prometheus/prometheus.yml" - "--storage.tsdb.path=/prometheus" - "--storage.tsdb.retention=24h" ports: - containerPort: 9090 protocol: TCP volumeMounts: - mountPath: "/prometheus" name: data - mountPath: "/etc/prometheus" name: config-volume resources: requests: cpu: 100m memory: 100Mi limits: cpu: 500m memory: 2500Mi serviceAccountName: prometheus volumes: - name: data emptyDir: &#123;&#125; - name: config-volume configMap: name: prometheus-configprometheus.svc.yml12345678910111213141516---kind: ServiceapiVersion: v1metadata: labels: app: prometheus name: prometheus namespace: kube-systemspec: type: NodePort ports: - port: 9090 targetPort: 9090 nodePort: 30003 selector: app: prometheusgrafana-deploy.yaml12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455apiVersion: apps/v1kind: Deploymentmetadata: name: grafana-core namespace: kube-system labels: app: grafana component: corespec: replicas: 1 selector: matchLabels: app: grafana template: metadata: labels: app: grafana component: core spec: containers: - image: grafana/grafana:6.1.4 name: grafana-core imagePullPolicy: IfNotPresent # env: resources: # keep request = limit to keep this container in guaranteed class limits: cpu: 100m memory: 100Mi requests: cpu: 100m memory: 100Mi env: # The following env variables set up basic auth twith the default admin user and admin password. - name: GF_AUTH_BASIC_ENABLED value: "true" - name: GF_AUTH_ANONYMOUS_ENABLED value: "false" # - name: GF_AUTH_ANONYMOUS_ORG_ROLE # value: Admin # does not really work, because of template variables in exported dashboards: # - name: GF_DASHBOARDS_JSON_ENABLED # value: "true" readinessProbe: httpGet: path: /login port: 3000 # initialDelaySeconds: 30 # timeoutSeconds: 1 #volumeMounts: #先不进行挂载 #- name: grafana-persistent-storage # mountPath: /var #volumes: #- name: grafana-persistent-storage #emptyDir: &#123;&#125;grafana-svc.yaml123456789101112131415apiVersion: v1kind: Servicemetadata: name: grafana namespace: kube-system labels: app: grafana component: corespec: type: NodePort ports: - port: 3000 selector: app: grafana component: coregrafana-svc.yaml1234567891011121314apiVersion: extensions/v1beta1kind: Ingressmetadata: name: grafana namespace: kube-systemspec: rules: - host: k8s.grafana http: paths: - path: / backend: serviceName: grafana servicePort: 3000]]></content>
      <categories>
        <category>技术</category>
        <category>Prometheus</category>
      </categories>
      <tags>
        <tag>K8S</tag>
        <tag>Prometheus</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[prometheus.yaml.gz修改配置方法]]></title>
    <url>%2F2024%2F065377e507.html</url>
    <content type="text"><![CDATA[通过kubesphere安装的Prometheus的配置文件是加密的，想要修改的话需要先把文件解压，解密，修改完之后再压缩，加密再替换回去。加密文件：解压：123456789101112131415161718192021kubectl get secret -n prometheus prometheus-prometheus-kube-prometheus-prometheus -oyamlapiVersion: v1data: prometheus.yaml.gz: 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kind: Secretmetadata: creationTimestamp: "2024-04-24T06:16:07Z" labels: managed-by: prometheus-operator name: prometheus-prometheus-kube-prometheus-prometheus namespace: prometheus ownerReferences: - apiVersion: monitoring.coreos.com/v1 blockOwnerDeletion: true controller: true kind: Prometheus name: prometheus-kube-prometheus-prometheus uid: 0a2d7794-16ef-4884-a3c0-4c70227f05bd resourceVersion: "10317315" uid: c2bd7b0c-b1c3-4b33-b2c6-c4f3d384601atype: Opaque1echo -n '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' | base64 -d | gzip -d &gt; prometheus.yaml修改参数，例如：scrape_timeout: 30s改为60s修改完之后再压缩：1$ cat prometheus.yaml | gzip | base64 -w 0 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#把生成的加密信息替换回去就行。]]></content>
      <categories>
        <category>技术</category>
        <category>Prometheus</category>
      </categories>
      <tags>
        <tag>K8S</tag>
        <tag>Prometheus</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Prometheus原理详解]]></title>
    <url>%2F2024%2F067829fd23.html</url>
    <content type="text"><![CDATA[Prometheus 最开始是由 SoundCloud 开发的开源监控告警系统，是 Google BorgMon 监控系统的开源版本。在 2016 年，Prometheus 加入 CNCF，成为继 Kubernetes 之后第二个被 CNCF 托管的项目。随着 Kubernetes 在容器编排领头羊地位的确立，Prometheus 也成为 Kubernetes 容器监控的标配。本文接下来将会对 Prometheus 做一个介绍。简介特性通过指标名称和标签(key/value对）区分的多维度、时间序列数据模型灵活的查询语法 PromQL不需要依赖额外的存储，一个服务节点就可以工作利用http协议，通过pull模式来收集时间序列数据需要push模式的应用可以通过中间件gateway来实现监控目标支持服务发现和静态配置支持各种各样的图表和监控面板组件核心组件整个Prometheus生态包含多个组件，除了Prometheus server组件其余都是可选的Prometheus Server：主要的核心组件，用来收集和存储时间序列数据。Client Library:：客户端库，为需要监控的服务生成相应的 metrics 并暴露给 Prometheus server。当 Prometheus server 来 pull 时，直接返回实时状态的 metrics。push gateway：主要用于短期的 jobs。由于这类 jobs 存在时间较短，可能在 Prometheus 来 pull 之前就消失了。为此，这次 jobs 可以直接向 Prometheus server 端推送它们的 metrics。这种方式主要用于服务层面的 metrics，对于机器层面的 metrices，需要使用 node exporter。Exporters: 用于暴露已有的第三方服务的 metrics 给 Prometheus。Alertmanager: 从 Prometheus server 端接收到 alerts 后，会进行去除重复数据，分组，并路由到对收的接受方式，发出报警。常见的接收方式有：电子邮件，pagerduty，OpsGenie, webhook 等。各种支持工具。Prometheus 框架图我们从上面的架构图可以看出 Prometheus 的主要模块包含:Server, Exporters, Pushgateway, PromQL, Alertmanager, WebUI 等。我们逐一认识一下各个模块的功能作用。模块介绍Retrieval是负责定时去暴露的目标页面上去抓取采样指标数据。Storage 是负责将采样数据写入指定的时序数据库存储。PromQL 是Prometheus提供的查询语言模块。可以和一些webui比如grfana集成。Jobs / Exporters:Prometheus 可以从 Jobs 或 Exporters 中拉取监控数据。Exporter 以 Web API 的形式对外暴露数据采集接口。Prometheus Server:Prometheus 还可以从其他的 Prometheus Server 中拉取数据。Pushgateway:对于一些以临时性 Job 运行的组件，Prometheus 可能还没有来得及从中 pull 监控数据的情况下，这些 Job 已经结束了，Job 运行时可以在运行时将监控数据推送到 Pushgateway 中，Prometheus 从 Pushgateway 中拉取数据，防止监控数据丢失。Service discovery:是指 Prometheus 可以动态的发现一些服务，拉取数据进行监控，如从DNS，Kubernetes，Consul 中发现, file_sd 是静态配置的文件。AlertManager:是一个独立于 Prometheus 的外部组件，用于监控系统的告警，通过配置文件可以配置一些告警规则，Prometheus 会把告警推送到 AlertManager。工作流程大概的工作流程如下：Prometheus server 定期从配置好的 jobs 或者 exporters 中拉 metrics，或者接收来自 Pushgateway 发过来的 metrics，或者从其他的 Prometheus server 中拉 metrics。Prometheus server 在本地存储收集到的 metrics，并运行已定义好的 alert.rules，记录新的时间序列或者向 Alertmanager 推送警报。Alertmanager 根据配置文件，对接收到的警报进行处理，发出告警。在图形界面中，可视化采集数据。Prometheus相关概念内部存储机制Prometheus有着非常高效的时间序列数据存储方法，每个采样数据仅仅占用3.5byte左右空间，上百万条时间序列，30秒间隔，保留60天，大概花了200多G（引用官方PPT）。Prometheus内部主要分为三大块：Retrieval是负责定时去暴露的目标页面上去抓取采样指标数据Storage是负责将采样数据写磁盘PromQL是Prometheus提供的查询语言模块。数据模型Prometheus 存储的所有数据都是时间序列数据（Time Serie Data，简称时序数据）。时序数据是具有时间戳的数据流，该数据流属于某个度量指标（Metric）和该度量指标下的多个标签（Label）。每个Metric name代表了一类的指标，他们可以携带不同的Labels，每个Metric name + Label组合成代表了一条时间序列的数据。在Prometheus的世界里面，所有的数值都是64bit的。每条时间序列里面记录的其实就是64bit timestamp(时间戳) + 64bit value(采样值)。Metric name（指标名称）：该名字应该具有语义，一般用于表示 metric 的功能，例如：http_requests_total, 表示 http 请求的总数。其中，metric 名字由 ASCII 字符，数字，下划线，以及冒号组成，且必须满足正则表达式 a-zA-Z_:*。Lables（标签）：使同一个时间序列有了不同维度的识别。例如 http_requests_total{method=“Get”} 表示所有 http 请求中的 Get 请求。当 method=“post” 时，则为新的一个 metric。标签中的键由 ASCII 字符，数字，以及下划线组成，且必须满足正则表达式 a-zA-Z_:*。timestamp(时间戳)：数据点的时间，表示数据记录的时间。Sample Value（采样值）：实际的时间序列，每个序列包括一个 float64 的值和一个毫秒级的时间戳。例如图上的数据：12http_requests_total&#123;status="200",method="GET"&#125;http_requests_total&#123;status="404",method="GET"&#125;根据上面的分析，时间序列的存储似乎可以设计成key-value存储的方式（基于BigTable）。进一步拆分，可以像下面这样子：上图的第二条样式就是现在Prometheus内部的表现形式了，name是特定的label标签，代表了metric name。再回顾一下Prometheus的整体流程：Metric类型Prometheus定义了4种不同的指标类型(metric type)：Counter（计数器）、Gauge（仪表盘）、Histogram（直方图）、Summary（摘要）1、Counter（计数器）一种累加的 metric，典型的应用如：请求的个数，结束的任务数， 出现的错误数等等。【例如】查询 http_requests_total{method=“get”, job=“Prometheus”, handler=“query”} 返回 8，10 秒后，再次查询，则返回 14。2、Gauge（仪表盘）数据是一个瞬时值，如果当前内存用量，它随着时间变化忽高忽低。【例如】go_goroutines{instance=“172.17.0.2”, job=“Prometheus”} 返回值 147，10 秒后返回 124。3、Histogram（直方图）Histogram 取样观测的结果（一般是请求持续时间或响应大小）并在一个可配置的分布区间（bucket）内计算这些结果。其也提供所有观测结果的总和。Histogram 有一个基本 metric名称，在一次抓取中展现多个时间序列：累加的 counter，代表观测区间：_bucket{le=””}所有观测值的总数：_sum观测到的事件数量：_count例如 Prometheus server 中prometheus_local_storage_series_chunks_persisted, 表示 Prometheus 中每个时序需要存储的 chunks 数量，我们可以用它计算待持久化的数据的分位数。4、Summary（摘要）和 histogram 相似，summary 取样观测的结果（一般是请求持续时间或响应大小）。但是它还提供观测的次数和所有值的总和，它通过一个滑动的时间窗口计算可配置的分位数。Summary 有一个基本的 metric名称，在一次抓取中展现多个时间序列：观测事件的流式φ-分位数（0 ≤ φ ≤ 1）：{quantile=”φ”}所有观测值的总和：_sum观测的事件数量：_count例如 Prometheus server 中 prometheus_target_interval_length_seconds。Histogram 和Summary的对比序号histogramSummary配置区间配置分位数和滑动窗口客户端性能只需增加counters代价小需要流式计算代价高服务端性能计算分位数消耗大，可能会耗时无需计算，代价小时序数量_sum、_count、bucket_sum、_count、quantile分位数误差bucket的大小有关φ的配置有关φ和滑动窗口Prometheus 表达式设置客户端设置聚合根据表达式聚合一般不可聚合以下是类型为histogram和summary的样本输出示例：1234567891011121314151617# A histogram, which has a pretty complex representation in the text format:# HELP http_request_duration_seconds A histogram of the request duration.# TYPE http_request_duration_seconds histogramhttp_request_duration_seconds_bucket&#123;le="0.05"&#125; 24054http_request_duration_seconds_bucket&#123;le="0.1"&#125; 33444http_request_duration_seconds_bucket&#123;le="0.2"&#125; 100392http_request_duration_seconds_bucket&#123;le="+Inf"&#125; 144320http_request_duration_seconds_sum 53423http_request_duration_seconds_count 144320# Finally a summary, which has a complex representation, too:# HELP rpc_duration_seconds A summary of the RPC duration in seconds.# TYPE rpc_duration_seconds summaryrpc_duration_seconds&#123;quantile="0.01"&#125; 3102rpc_duration_seconds&#123;quantile="0.05"&#125; 3272rpc_duration_seconds&#123;quantile="0.5"&#125; 4773rpc_duration_seconds_sum 1.7560473e+07rpc_duration_seconds_count 2693任务（JOBS）与实例（INSTANCES）用Prometheus术语来说，可以抓取的端点称为instance，通常对应于单个进程。具有相同目的的instances 的集合（例如，出于可伸缩性或可靠性而复制的过程）称为job。例如，一个具有四个复制实例的API服务器作业:job: api-serverinstance 1: 1.2.3.4:5670instance 2: 1.2.3.4:5671instance 3: 5.6.7.8:5670instance 4: 5.6.7.8:5671instance: 一个单独 scrape 的目标， 一般对应于一个进程。:jobs: 一组同种类型的 instances（主要用于保证可扩展性和可靠性）Node exporterNode exporter 主要用于暴露 metrics 给 Prometheus，其中 metrics 包括：cpu 的负载，内存的使用情况，网络等。PushgatewayPushgateway 是 Prometheus 生态中一个重要工具，使用它的原因主要是：Prometheus 采用 pull 模式，可能由于不在一个子网或者防火墙原因，导致Prometheus 无法直接拉取各个 target数据。在监控业务数据的时候，需要将不同数据汇总, 由 Prometheus 统一收集。由于以上原因，不得不使用 pushgateway，但在使用之前，有必要了解一下它的一些弊端：将多个节点数据汇总到 pushgateway, 如果 pushgateway 挂了，受影响比多个 target 大。Prometheus 拉取状态 up 只针对 pushgateway, 无法做到对每个节点有效。Pushgateway 可以持久化推送给它的所有监控数据。因此，即使你的监控已经下线，prometheus 还会拉取到旧的监控数据，需要手动清理 pushgateway 不要的数据。TSDB简介TSDB(Time Series Database)时序列数据库，我们可以简单的理解为一个优化后用来处理时间序列数据的软件，并且数据中的数组是由时间进行索引的。时间序列数据库的特点大部分时间都是写入操作。写入操作几乎是顺序添加，大多数时候数据到达后都以时间排序。写操作很少写入很久之前的数据，也很少更新数据。大多数情况在数据被采集到数秒或者数分钟后就会被写入数据库。删除操作一般为区块删除，选定开始的历史时间并指定后续的区块。很少单独删除某个时间或者分开的随机时间的数据。基本数据大，一般超过内存大小。一般选取的只是其一小部分且没有规律，缓存几乎不起任何作用。读操作是十分典型的升序或者降序的顺序读。高并发的读操作十分常见。常见的时间序列数据库TSDB项目官网influxDBhttps://influxdata.com/RRDtoolhttp://oss.oetiker.ch/rrdtool/Graphitehttp://graphiteapp.org/OpenTSDBhttp://opentsdb.net/Kdb+http://kx.com/Druidhttp://druid.io/KairosDBhttp://kairosdb.github.io/Prometheushttps://prometheus.io/PromQL查询表达式PromQL的四种数据类型：即时向量（Instant vector） ：包含每个时间序列单个样品的一组时间序列，共享相同的时间戳范围向量（Range vector） ：包含一个范围内数据点的一组时间序列标量（Scalar） ： 一个简单的数字浮点值字符串（String） ：一个简单的字符串值；当前未使用即时矢量选择器即时向量选择器允许选择一组时间序列，或者某个给定的时间戳的样本数据。下面这个例子选择了具有http_requests_total的时间序列：http_requests_total你可以通过附加一组标签，并用{}括起来，来进一步筛选这些时间序列。下面这个例子只选择有http_requests_total名称的、有prometheus工作标签的、有canary组标签的时间序列：http_requests_total另外，也可以也可以将标签值反向匹配，或者对正则表达式匹配标签值。下面列举匹配操作符：=：选择正好相等的字符串标签!=：选择不相等的字符串标签=~：选择匹配正则表达式的标签（或子标签）!~：选择不匹配正则表达式的标签（或子标签）例如，选择staging、testing、development环境下的，GET之外的HTTP方法的http_requests_total的时间序列：http_requests_total范围矢量选择器范围向量表达式正如即时向量表达式一样运行，但是前者返回从当前时刻开始的一定时间范围的时间序列集合回来。语法是，在一个向量表达式之后添加[]来表示时间范围，持续时间用数字表示，后接下面单元之一：s：secondsm：minutesh：hoursd：daysw：weeksy：years在下面这个例子中，我们选择最后5分钟的记录，metric名称为http_requests_total、作业标签为prometheus的时间序列的所有值：http_requests_total{job=”prometheus”}[5m]偏移量修改器所述offset可以改变时间为查询中的个别时刻和范围矢量偏移。例如，以下表达式返回http_requests_total相对于当前查询评估时间的过去5分钟值 ：http_requests_total offset 5m同样适用于范围向量。这将返回http_requests_total一周前的5分钟费率 ：rate(http_requests_total[5m] offset 1w)更多操作符，请参考官方文档使用聚合操作PromQL提供的聚合操作可以用来对这些时间序列进行处理，形成一条新的时间序列12345678# 查询系统所有http请求的总量sum(http_request_total)# 按照mode计算主机CPU的平均使用时间avg(node_cpu) by (mode)# 按照主机查询各个主机的CPU使用率sum(sum(irate(node_cpu&#123;mode!='idle'&#125;[5m])) / sum(irate(node_cpu[5m]))) by (instance)常见的聚合函数sum (求和)min (最小值)max (最大值)avg (平均值)stddev (标准差)stdvar (标准方差)count (计数)count_values (对value进行计数)bottomk (后n条时序)topk (前n条时序)quantile (分位数)更多函数，请参考官方文档Exporter介绍Exporter是prometheus监控中重要的组成部分，负责数据指标的采集。广义上讲所有可以向Prometheus提供监控样本数据的程序都可以被称为一个Exporter。而Exporter的一个实例称为target。官方给出的插件有blackbox_exporter、node_exporter、mysqld_exporter、snmp_exporter等，第三方的插件有redis_exporter，cadvisor等。官方和一些社区提供好多exproter, 我们可以直接拿过来采集我们的数据。 官方的exporter地址： https://prometheus.io/docs/instrumenting/exporters/常见的Exporter简介blackbox_exporterGitHub地址：https://github.com/prometheus/blackbox_exporterbloackbox exporter是prometheus社区提供的黑盒监控解决方案，运行用户通过HTTP、HTTPS、DNS、TCP以及ICMP的方式对网络进行探测。这里通过blackbox对我们的站点信息进行采集。node_exporterGitHub地址：https://github.com/prometheus/node_exporternode_exporter主要用来采集机器的性能指标数据，包括cpu，内存，磁盘，io等基本信息。mysqld_exportermysql_exporter是用来收集MysQL或者Mariadb数据库相关指标的，mysql_exporter需要连接到数据库并有相关权限。GitHub地址：https://github.com/prometheus/mysqld_exportersnmp_exporterSNMP Exporter 从 SNMP 服务中采集信息提供给 Promethers 监控系统使用。GitHub地址：https://github.com/prometheus/snmp_exporterExporter的来源从Exporter的来源上来讲，主要分为两类：社区提供Prometheus社区提供了丰富的Exporter实现，涵盖了从基础设施，中间件以及网络等各个方面的监控功能。这些Exporter可以实现大部分通用的监控需求。下表列举一些社区中常用的Exporter：范围常用Exporter数据库MySQL Exporter, Redis Exporter, MongoDB Exporter, MSSQL Exporter等硬件Apcupsd Exporter，IoT Edison Exporter， IPMI Exporter, Node Exporter等消息队列Beanstalkd Exporter, Kafka Exporter, NSQ Exporter, RabbitMQ Exporter等存储Ceph Exporter, Gluster Exporter, HDFS Exporter, ScaleIO Exporter等HTTP服务Apache Exporter, HAProxy Exporter, Nginx Exporter等API服务AWS ECS Exporter， Docker Cloud Exporter, Docker Hub Exporter, GitHub Exporter等日志Fluentd Exporter, Grok Exporter等监控系统Collectd Exporter, Graphite Exporter, InfluxDB Exporter, Nagios Exporter, SNMP Exporter等其它Blockbox Exporter, JIRA Exporter, Jenkins Exporter， Confluence Exporter等用户自定义除了直接使用社区提供的Exporter程序以外，用户还可以基于Prometheus提供的Client Library创建自己的Exporter程序，目前Promthues社区官方提供了对以下编程语言的支持：Go、Java/Scala、Python、Ruby。同时还有第三方实现的如：Bash、C++、Common Lisp、Erlang,、Haskeel、Lua、Node.js、PHP、Rust等。Exporter的运行方式从Exporter的运行方式上来讲，又可以分为：独立运行由于操作系统本身并不直接支持Prometheus，同时用户也无法通过直接从操作系统层面上提供对Prometheus的支持。因此，用户只能通过独立运行一个程序的方式，通过操作系统提供的相关接口，将系统的运行状态数据转换为可供Prometheus读取的监控数据。 除了Node Exporter以外，比如MySQL Exporter、Redis Exporter等都是通过这种方式实现的。 这些Exporter程序扮演了一个中间代理人的角色（数据转换）。集成到应用中（推荐）为了能够更好的监控系统的内部运行状态，有些开源项目如Kubernetes，ETCD等直接在代码中使用了Prometheus的Client Library，提供了对Prometheus的直接支持。这种方式打破的监控的界限，让应用程序可以直接将内部的运行状态暴露给Prometheus，适合于一些需要更多自定义监控指标需求的项目。Exporter规范所有的Exporter程序都需要按照Prometheus的规范，返回监控的样本数据。以Node Exporter为例，当访问/metrics地址时会返回以下内容：直接curl拿不到数据，就得授权12# 取前面10行$ curl -s -k --header "Authorization: Bearer $TOKEN" https://192.168.0.113:6443/metrics|head -1012345678910# HELP aggregator_openapi_v2_regeneration_count [ALPHA] Counter of OpenAPI v2 spec regeneration count broken down by causing APIService name and reason.# TYPE aggregator_openapi_v2_regeneration_count counteraggregator_openapi_v2_regeneration_count&#123;apiservice="*",reason="startup"&#125; 0aggregator_openapi_v2_regeneration_count&#123;apiservice="k8s_internal_local_delegation_chain_0000000002",reason="update"&#125; 0aggregator_openapi_v2_regeneration_count&#123;apiservice="v1beta1.metrics.k8s.io",reason="add"&#125; 0aggregator_openapi_v2_regeneration_count&#123;apiservice="v1beta1.metrics.k8s.io",reason="update"&#125; 0# HELP aggregator_openapi_v2_regeneration_duration [ALPHA] Gauge of OpenAPI v2 spec regeneration duration in seconds.# TYPE aggregator_openapi_v2_regeneration_duration gaugeaggregator_openapi_v2_regeneration_duration&#123;reason="add"&#125; 0.929158077aggregator_openapi_v2_regeneration_duration&#123;reason="startup"&#125; 0.509336209Exporter返回的样本数据，主要由三个部分组成：样本的一般注释信息（HELP），样本的类型注释信息（TYPE）和样本。Prometheus会对Exporter响应的内容逐行解析：如果当前行以# HELP开始，Prometheus将会按照以下规则对内容进行解析，得到当前的指标名称以及相应的说明信息：# HELP &lt;metrics_name&gt; &lt;doc_string&gt;如果当前行以# TYPE开始，Prometheus会按照以下规则对内容进行解析，得到当前的指标名称以及指标类型:# TYPE &lt;metrics_name&gt; &lt;metrics_type&gt;TYPE注释行必须出现在指标的第一个样本之前。如果没有明确的指标类型需要返回为untyped。 除了# 开头的所有行都会被视为是监控样本数据。 每一行样本需要满足以下格式规范:metric_name [“{“ label_name “=” &quot; label_value &quot; { “,” label_name “=” &quot; label_value &quot; } [ “,” ] “}”] value [ timestamp ]node-exporter简介Exporter是Prometheus的指标数据收集组件。它负责从目标Jobs收集数据，并把收集到的数据转换为Prometheus支持的时序数据格式。 和传统的指标数据收集组件不同的是，他只负责收集，并不向Server端发送数据，而是等待Prometheus Server 主动抓取。node-exporter用于采集node的运行指标，包括node的cpu、load、filesystem、meminfo、network等基础监控指标，类似于zabbix监控系统的的zabbix-agent原理图如下：检查node-exporter服务12345678910111213$ kubectl get pods -n monitoring -o wide|grep node-exporter# 查看pod内的node_exporter进程$ kubectl exec -it node-exporter-dc65j -n monitoring -- ps -ef|grep node_exporter# 获取容器ID$ docker ps |grep node_exporter# 查看docker 容器的pid$ docker inspect -f &#123;&#123;.State.Pid&#125;&#125; 8b3f0c3ea055# 再通过pid进入命名空间$ nsenter -n -t8303# 再查看进程$ ps -ef|grep node_exporter# 退出当前命名空间$ exit设计到yaml文件12345678910node-exporter-clusterRoleBinding.yaml # 角色绑定node-exporter-clusterRole.yaml # 角色node-exporter-daemonset.yaml # daemonset，容器配置，node-exporter配置node-exporter-prometheusRule.yaml # 采集规则node-exporter-serviceAccount.yaml # 服务账号# K8s集群内的Prometheus抓取监测数据是通过servicemonitor这个crd来完成的。# 每个servicemonitor对应Prometheus中的一个target。# 每个servicemonitor对应一个或多个service，负责获取这些service上指定端口暴露的监测数据，并向Prometheus上报。node-exporter-serviceMonitor.yaml node-exporter-service.yaml # 服务服务自动发现任何被监控的目标都需要事先纳入到监控系统中才能进行时序数据采集、存储、告警和展示，监控目标可以通过配置信息以静态形式指定，也可以让Prometheus通过服务发现的机制进行动态管理。讲服务发现之前，先来讲一下传统配置方式首先需要安装node-exporter，获取node metrics，并且暴露一个端口；然后去Prometheus Server的prometheus.yaml文件中在scarpe_config中添加node-exporter的job，配置node-exporter的地址和端口等信息；再然后，需要重启Prometheus服务；最后等待prometheus服务来拉取监控信息，就完成添加一个node-exporter监控的任务。示例配置如下（prometheus.yml）：123- job_name: 'node-exporter' static_configs: - targets: ['192.168.0.113:9090'] #这里我修改了端口为9090重启服务1$ systemctl restart prometheuskube-prometheus服务自动发现首先第一步和传统方式一样，部署一个node-exporter来获取监控项；然后编写一个ServiceMonitor通过labelSelector选择刚才部署的node-exporter，由于Operator在部署Prometheus的时候默认指定了Prometheus选择label为：prometheus: kube-prometheus的ServiceMonitor，所以只需要在ServiceMonitor上打上prometheus: kube-prometheus标签就可以被Prometheus选择了；完成以上两步就完成了对主机资源的监控，不需要改Prometheus配置文件，也不需要重启Prometheus服务，是不是很方便，Operator观察到ServiceMonitor发生变化，会动态生成Prometheus配置文件，并保证配置文件实时生效。添加k8s外部监控配置过程一个项目开始可能很难实现全部容器化，比如数据库、CDH集群。但是我们依然需要监控他们，如果分成两套prometheus不利于管理，所以我们统一添加这些监控到kube-prometheus中。关于 additionalScrapeConfigs 属性的具体介绍，我们可以使用 kubectl explain 命令来了解详细信息：1$ kubectl explain prometheus.spec.additionalScrapeConfigs那么接下来我们新建 prometheus-additional.yaml 文件，添加额外监控组件配置scrape_configs。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364$ cat &lt;&lt; EOF &gt; prometheus-additional.yaml- job_name: 'node-exporter-others' static_configs: - targets: - *.*.*.113:31190 - *.*.*.114:31190 - *.*.*.115:31190- job_name: 'mysql-exporter' static_configs: - targets: - *.*.*.104:9592 - *.*.*.125:9592 - *.*.*.128:9592- job_name: 'nacos-exporter' metrics_path: '/nacos/actuator/prometheus' static_configs: - targets: - *.*.*.113:8848 - *.*.*.114:8848 - *.*.*.115:8848- job_name: 'elasticsearch-exporter' static_configs: - targets: - *.*.*.113:9597 - *.*.*.114:9597 - *.*.*.115:9597- job_name: 'zookeeper-exporter' static_configs: - targets: - *.*.*.113:9595 - *.*.*.114:9595 - *.*.*.115:9595- job_name: 'nginx-exporter' static_configs: - targets: - *.*.*.113:9593 - *.*.*.114:9593 - *.*.*.115:9593- job_name: 'redis-exporter' static_configs: - targets: - *.*.*.113:9594- job_name: 'redis-exporter-targets' static_configs: - targets: - redis://*.*.*.113:7090 - redis://*.*.*.114:7090 - redis://*.*.*.115:7091 metrics_path: /scrape relabel_configs: - source_labels: [__address__] target_label: __param_target - source_labels: [__param_target] target_label: instance - target_label: __address__ replacement: *.*.*.113:9594EOF然后我们需要将这些监控配置以secret资源类型存储到k8s集群中。1$ kubectl create secret generic additional-scrape-configs --from-file=prometheus-additional.yaml -n monitoring修改 prometheus 文件additionalScrapeConfigs：增加额外监控项配置，具体配置查看“添加k8s外部监控”。1$ vi prometheus-prometheus.yaml添加如下内容123additionalScrapeConfigs: name: additional-scrape-configs key: prometheus-additional.yaml检查1$ grep -n -C5 'additionalScrapeConfigs' prometheus-prometheus.yaml是配置生效1$ kubectl apply -f prometheus-prometheus.yamlPushgateway 简介Pushgateway 是 Prometheus 生态中一个重要工具，使用它的原因主要是：Prometheus 采用 pull 模式，可能由于不在一个子网或者防火墙原因，导致Prometheus 无法直接拉取各个 target 数据。在监控业务数据的时候，需要将不同数据汇总, 由 Prometheus 统一收集。由于以上原因，不得不使用 pushgateway，但在使用之前，有必要了解一下它的一些弊端：将多个节点数据汇总到 pushgateway, 如果 pushgateway 挂了，受影响比多个 target 大。Prometheus 拉取状态 up 只针对 pushgateway, 无法做到对每个节点有效。Pushgateway 可以持久化推送给它的所有监控数据。即使你的监控已经下线，prometheus 还会拉取到旧的监控数据，需要手动清理 pushgateway 不要的数据。]]></content>
      <categories>
        <category>技术</category>
        <category>Prometheus</category>
      </categories>
      <tags>
        <tag>K8S</tag>
        <tag>Prometheus</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s亲和性，反亲和性，污点和容忍度详解]]></title>
    <url>%2F2024%2F0511e88003.html</url>
    <content type="text"><![CDATA[在k8s中，你可以约束一个 Pod 以便限制其只能在特定的节点上运行， 或优先在特定的节点上运行。有几种方法可以实现这点，推荐的方法都是用 标签选择算符来进行选择。 通常这样的约束不是必须的，因为调度器将自动进行合理的放置（比如，将 Pod 分散到节点上， 而不是将 Pod 放置在可用资源不足的节点上等等）。但在某些情况下，你可能需要进一步控制 Pod 被部署到哪个节点。例如，确保 Pod 最终落在连接了 SSD 的机器上， 或者将来自两个不同的服务且有大量通信的 Pod 被放置在同一个可用区。你可以使用下列方法中的任何一种来选择 Kubernetes 对特定 Pod 的调度：与节点标签匹配的 nodeSelector亲和性与反亲和性nodeName 字段Pod 拓扑分布约束环境配置123456[root@k8s-master ~]# kubectl get nodes NAME STATUS ROLES AGE VERSIONk8s-master Ready control-plane 11d v1.24.4k8s-slave1 Ready &lt;none&gt; 11d v1.24.4k8s-slave2 Ready &lt;none&gt; 11d v1.24.4[root@k8s-master ~]#nodeNamenodeName：在 Kubernetes 中，NodeName 是每个 Node 节点的唯一标识符，它是一个字符串，通常是节 点的主机名（hostname）。在创建 Pod 时，可以通过指定 nodeName 字段来将 Pod 调度到特定的 Node 节点上。指定pod运行在哪个节点上123456789101112131415[root@k8s-master day9]# cat nodename_pod.yaml apiVersion: v1kind: Podmetadata: name: nodename-pod namespace: default labels: app: nginxspec: nodeName: k8s-slave2 containers: - name: mynginx ports: - containerPort: 80 image: nginxnodeSelector在 Kubernetes 中，Node 节点选择器（NodeSelector）是用来将 Pod 调度到特定节点的一种机 制。它基于 Node 节点的标签（Node Labels）来进行筛选和匹配，从而使 Pod 能够被分配到满足其要 求的节点上。具体来说，Node 节点选择器允许用户为 Node 节点打上一组键值对标签，例如： nodeType=compute、diskType=ssd 等等。当用户在创建 Pod 时指定一个或多个 Node Selector 时，Kubernetes 调度器会根据这些选择器的键值对与 Node 节点的标签进行匹配，最终将 Pod 分配到 匹配的节点上。例如，如果用户定义了一个 Pod 需要一个节点具有标签“nodeType=compute”，那么 Kubernetes 调度器将只会将该 Pod 调度到具有此标签的节点上。如果没有节点具有该标签，则 Pod 将 保持处于 Pending 状态直到有符合条件的节点出现。例子展示：创建一个pod， nodeSelector选择要带ssd标签的。1234567891011121314151617root@k8s-master day9]# cat nodeSelector_pod.yaml apiVersion: v1kind: Podmetadata: name: nodeselector-pod namespace: default labels: app: nginxspec: nodeSelector: disktype: ssd containers: - name: mynginx2 ports: - containerPort: 80 image: nginx[root@k8s-master day9]# kubectl apply -f nodeSelector_pod.yaml打标签常用命令12345678#添加标签kubectl label nodes k8s-slave1 disktype=ssd#查看标签kubectl get nodes --show-labels#删除标签kubectl label nodes k8s-slave1 disktype-#帮助命令kubectl label --help亲和性 affinity在 Kubernetes 中，亲和性（Affinity）是一种机制，用于控制 Pod 调度器将 Pod 调度到与其最匹 配的节点上。亲和性机制允许用户定义一组规则，以便将 Pod 调度到具有指定特征的节点上，这些特征 可以是节点的标签（Labels）、污点（Taints）或其他属性。亲和性有两种类型：节点亲和性（Node Affinity）和 Pod 亲和性（Pod Affinity）。节点亲和性指 定了节点和 Pod 之间的关系，而 Pod 亲和性指定了 Pod 和其他 Pod 之间的关系。node节点亲和性（Node Affinity）1kubectl explain pod.spec1kubectl explain pod.spec.affinity123[root@k8s-master day9]# kubectl explain pod.spec.affinity.nodeAffinity preferredDuringSchedulingIgnoredDuringExecution #软亲和性 requiredDuringSchedulingIgnoredDuringExecution #硬亲和性 硬亲和性，创建一个案例1234567891011121314151617181920212223[root@k8s-master ~]# cat nodeaffinity.yaml apiVersion: v1kind: Podmetadata: name: nodeaffinity-1 namespace: default labels: app: myapp item: prospec: containers: - name: myapp image: tomcat imagePullPolicy: IfNotPresent affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: a operator: In values: - b 打上一个a=b的标签软亲和性案例，单个条件1234567891011121314151617181920212223[root@k8s-master day9]# cat nodeaffinity_ruan.yaml apiVersion: v1kind: Podmetadata: name: nodeaffinity-ruan namespace: default labels: app: myapp-ruanspec: containers: - name: myapp image: tomcat imagePullPolicy: IfNotPresent affinity: nodeAffinity: preferredDuringSchedulingIgnoredDuringExecution: - preference: matchExpressions: - key: a1 operator: In values: - b1 weight: 80上面说明软亲和性是可以运行这个 pod 的，尽管没有任何一个节点具有 a1=b1 这个标签.软亲和第二个案例,多个条件备注：yaml 文件解释：使用 weight 字段来指定权重。weight 字段是一个整数值，表示优先级，取值范围为 1 到 100。如果多个节点满足软亲和性的要求，那么 Kubernetes 会根据节点的权重来决定 pod 最终调度的 节点。在这个例子中，weight=80 表示这个软亲和性的优先级为 80。具体案例分享： 假设我们有一个 Kubernetes 集群，其中有多个节点，每个节点都有不同的标签。现在有一个 Pod，需要运行在 CPU 性能较好的节点上，同时如果有多个节点都满足要求，优先选择具有更多内存的 节点。此时，我们可以使用节点亲和性和软亲和性来指定 Pod 的调度。首先，我们可以在节点上添加标签，例如，我们可以给性能较好的节点打上 cpu=high 的标签，给 内存较大的节点打上 memory=large 的标签。其次，我们可以在 Pod 的配置文件中，使用 requiredDuringSchedulingIgnoredDuringExecution 字段来指定硬亲和性，让 Pod 只能调度到具 有 cpu=high 标签的节点上。最后，我们可以使用 preferredDuringSchedulingIgnoredDuringExecution 字段来指定软亲和 性，同时设置权重值，让 Kubernetes 在有多个节点满足软亲和性时，优先选择具有更多内存的节点。12345678910111213141516171819202122232425262728293031323334353637383940414243[root@k8s-master day9]# kubectl get nodes NAME STATUS ROLES AGE VERSIONk8s-master Ready control-plane 12d v1.24.4k8s-slave1 Ready &lt;none&gt; 12d v1.24.4 # 4c 8gk8s-slave2 Ready &lt;none&gt; 12d v1.24.4 #4c 4gk8s-slave3 Ready &lt;none&gt; 40h v1.24.4 #4c 4gk8s-slave4 Ready &lt;none&gt; 40h v1.24.4 #4c 4gapiVersion: v1kind: Podmetadata: name: nodeaffinity-ruan2 namespace: default labels: app: myapp-ruan2spec: containers: - name: myapp2 image: tomcat imagePullPolicy: IfNotPresent affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: cpu operator: In values: - high preferredDuringSchedulingIgnoredDuringExecution: - preference: matchExpressions: - key: memory operator: In values: - high weight: 80 - preference: matchExpressions: - key: memory operator: In values: - low weight: 60123456789101112131415161718#主机打标签[root@k8s-master day9]# kubectl label nodes k8s-slave1 cpu=highnode/k8s-slave1 labeled[root@k8s-master day9]# kubectl label nodes k8s-slave2 cpu=highnode/k8s-slave2 labeled[root@k8s-master day9]# kubectl label nodes k8s-slave3 cpu=highnode/k8s-slave3 labeled[root@k8s-master day9]# kubectl label nodes k8s-slave4 cpu=highnode/k8s-slave4 labeled[root@k8s-master day9]# kubectl label nodes k8s-slave4 memory=lownode/k8s-slave4 labeled[root@k8s-master day9]# kubectl label nodes k8s-slave3 memory=lownode/k8s-slave3 labeled[root@k8s-master day9]# kubectl label nodes k8s-slave2 memory=lownode/k8s-slave2 labeled[root@k8s-master day9]# kubectl label nodes k8s-slave1 memory=highnode/k8s-slave1 labeled[root@k8s-master day9]#pod 亲和性Kubernetes 中的 Pod 亲和性可以用来控制将一个 Pod 调度到与其它 Pod 具有特定关系的节点 上。这可以实现多种不同的需求，例如：• 通过 pod 亲和性提高服务的性能和可靠性：通过将相关服务部署在同一个节点上，可以降低网 络延迟和提高服务响应速度。此外，如果某个节点出现故障，只会影响到部署在该节点上的服 务，不会影响到其它服务。• 通过 pod 反亲和性分离敏感数据：通过将处理敏感数据的 Pod 与其它 Pod 隔离开来，不调 度到同一个节点上，可以提高系统的安全性。podAffinity：表示 Pod 与其它 Pod 的亲和性。可以使用 requiredDuringSchedulingIgnoredDuringExecution、 preferredDuringSchedulingIgnoredDuringExecution等字段来指定 Pod 与其它 Pod 的亲和性规则。这些字段中可以使用 topologyKey 和 labelSelector 字段来指定 Pod 之间的关系。pod 亲和性分两种：podaffinity（pod 亲和性）：Pod 亲和性是指一组 Pod 可以被调度到同一节点上，即它们互相吸 引，倾向于被调度在同一台节点上。例如，假设我们有一组具有相同标签的 Pod，通过使用 Pod 亲和性规则，我们可以让它们在同一节点上运行，以获得更高的性能和更好的可靠性。podunaffinity（pod 反亲和性）：Pod 反亲和性是指一组 Pod 不应该被调度到同一节点上，即它 们互相排斥，避免被调度在同一台节点上。例如，如果我们有一组应用程序 Pod，我们可以使用 Pod 反亲和性规则来避免它们被调度到同一节点上，以减少单点故障的风险和提高可靠性。 定义两个pod，调度到同一位置1234567891011[root@k8s-master day9]# cat podaffinity-require.yaml apiVersion: v1kind: Podmetadata: name: first labels: app: firstspec: containers: - name: myapp image: tomcat123456789101112131415161718[root@k8s-master day9]# cat podaffinity-require2.yaml apiVersion: v1kind: Podmetadata: name: second labels: app: secondspec: containers: - name: myapp2 image: nginx affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - &#123;key: app, operator: In, values: ["first"]&#125; topologyKey: kubernetes.io/hostname 这个案例中，第一个pod调度到哪个节点，第二个pod也会 s调度到同样的节点。pod反亲和性定义两个pod，第一个pod做为标准，第二个pod跟它调度到不同的节点上1234567891011121314151617181920212223242526272829[root@k8s-master day9]# cat podunaffinity-require.yaml apiVersion: v1kind: Podmetadata: name: first-1 labels: app: first-1spec: containers: - name: myapp image: tomcat[root@k8s-master day9]# cat podunaffinity-require2.yaml apiVersion: v1kind: Podmetadata: name: second-2 labels: app: second-2spec: containers: - name: myapp2-2 image: nginx affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - &#123;key: app, operator: In, values: ["first-1"]&#125; topologyKey: kubernetes.io/hostname 上面的案例就是pod的反亲和性，两个pod不会在一个节点上。pod高级用法：污点和容忍度在 k8s 集群中，节点可能会发生各种问题，如资源瓶颈、故障、维护等，这些问题可能会影响节点 上运行的 pod。为了解决这些问题，k8s 提供了污点和容忍度机制。污点（Taint）是一种节点上的属性，它表示该节点存在某种问题，如磁盘空间不足、内存不足等。 如果一个节点被标记了污点，那么默认情况下，k8s 不会在该节点上调度新的 Pod。这是因为 k8s 希 望确保 Pod 在运行时能够获得足够的资源，并且不会因为节点问题而意外终止。容忍度（Toleration）是一种 Pod 的属性，容忍度只是告诉 Kubernetes，这个 Pod 可以容忍 特定的污点，但是并不意味着 Pod 必须被调度到拥有这个污点的节点上。实际上，Pod 仍然可以被调度到没有对应污点的节点上。使用容忍度的主要目的是允许某些 Pod 能够运行在一些不符合正常调度要求的节点上，例如由于节点故障或维护等原因导致节点暂时不可用时，一些 Pod 定义容忍度之后可以被调度到其他节点上，以确保应用程序的高可用性。假设我们有一个 k8s 集群，其中有两个节点：k8s-slave1 和k8s-slave2。我们想要在 k8s-slave1上运行一个特定的应用程序 Pod，但是 k8s-slave1 节点有一个特定的污点，需要通过容忍度来容忍。首先，我们给k8s-slave1打一个污点，可以使用一下命令：12[root@k8s-master day9]# kubectl taint node k8s-slave1 a=b:NoSchedulenode/k8s-slave1 tainted这会在 k8s-slave1 节点上打一个名为 a=b 的污点，它的效果是 NoSchedule，意味着新的 Pod 无法被调度到该节点上，除非该 Pod 设置了容忍度。1234567891011121314151617[root@k8s-master day9]# cat podtaint.yaml apiVersion: v1kind: Podmetadata: name: example-pod labels: app: taintspec: containers: - name: example-container image: ikubernetes/myapp:v1 imagePullPolicy: IfNotPresent tolerations: - key: "a" operator: "Equal" value: "b" effect: "NoSchedule"在刚才的yaml文件定义了一个容忍度规则，它告诉k8s，如果有一个a=b、效果为NoSchedule的污点，则允许被调度到有这个污点的节点上。1注意！ 定义了容忍度，也不代表pod一定会创建在有污点的主机上，它也会调度到没有污点的节点上。定义一个pod，但不定义容忍度，那它肯定不会调度到k8s-slave1节点上。123456789101112[root@k8s-master day9]# cat podtaint-1.yaml apiVersion: v1kind: Podmetadata: name: taint-1 labels: app: taint-1spec: containers: - name: taint image: ikubernetes/myapp:v1 imagePullPolicy: IfNotPresent设置污点常用命令12345678#打污点kubectl taint node k8s-slave1 a=b:NoSchedule#查看污点kubectl describe nodes k8s-slave1 |grep Taint#取消污点 kubectl taint node k8s-slave1 a-查看污点如何定义的12345678910111213141516171819202122232425262728293031323334353637[root@k8s-master day9]# kubectl explain node.spec.taintsKIND: NodeVERSION: v1RESOURCE: taints &lt;[]Object&gt;DESCRIPTION: If specified, the node's taints. The node this Taint is attached to has the "effect" on any pod that does not tolerate the Taint.FIELDS: effect &lt;string&gt; -required- Required. The effect of the taint on pods that do not tolerate the taint. Valid effects are NoSchedule, PreferNoSchedule and NoExecute. Possible enum values: - `"NoExecute"` Evict any already-running pods that do not tolerate the taint. Currently enforced by NodeController. - `"NoSchedule"` Do not allow new pods to schedule onto the node unless they tolerate the taint, but allow all pods submitted to Kubelet without going through the scheduler to start, and allow all already-running pods to continue running. Enforced by the scheduler. - `"PreferNoSchedule"` Like TaintEffectNoSchedule, but the scheduler tries not to schedule new pods onto the node, rather than prohibiting new pods from scheduling onto the node entirely. Enforced by the scheduler. key &lt;string&gt; -required- Required. The taint key to be applied to a node. timeAdded &lt;string&gt; TimeAdded represents the time at which the taint was added. It is only written for NoExecute taints. value &lt;string&gt; The taint value corresponding to the taint key.node.spec.taints 字段是定义节点上的污点（taints）。每个污点由三个属性组成：key、value 和 effect。key：污点的名称，可以是任意字符串。在同一节点上，多个污点的 key 不能相同。value：污点的值，可以是任意字符串，用于进一步标识污点。可以为空字符串。effect：污点的效果，用于告诉 Kubernetes，哪些 Pod 可以被调度到该节点上。有三种效果可供选择：NoSchedule：表示新的 Pod 无法被调度到该节点上，但不会影响已有的Pod。PreferNoSchedule：表示新的 Pod 会尽量不被调度到该节点上，除非没有其它节点可调度。NoExecute：表示在该节点上运行的现有 Pod，如果不符合污点要求，则会被逐渐终止并移除。查看pod容忍度如何定义12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455[root@k8s-master day9]# kubectl explain pods.spec.tolerationsKIND: PodVERSION: v1RESOURCE: tolerations &lt;[]Object&gt;DESCRIPTION: If specified, the pod's tolerations. The pod this Toleration is attached to tolerates any taint that matches the triple &lt;key,value,effect&gt; using the matching operator &lt;operator&gt;.FIELDS: effect &lt;string&gt; Effect indicates the taint effect to match. Empty means match all taint effects. When specified, allowed values are NoSchedule, PreferNoSchedule and NoExecute. Possible enum values: - `"NoExecute"` Evict any already-running pods that do not tolerate the taint. Currently enforced by NodeController. - `"NoSchedule"` Do not allow new pods to schedule onto the node unless they tolerate the taint, but allow all pods submitted to Kubelet without going through the scheduler to start, and allow all already-running pods to continue running. Enforced by the scheduler. - `"PreferNoSchedule"` Like TaintEffectNoSchedule, but the scheduler tries not to schedule new pods onto the node, rather than prohibiting new pods from scheduling onto the node entirely. Enforced by the scheduler. key &lt;string&gt; Key is the taint key that the toleration applies to. Empty means match all taint keys. If the key is empty, operator must be Exists; this combination means to match all values and all keys. operator &lt;string&gt; Operator represents a key's relationship to the value. Valid operators are Exists and Equal. Defaults to Equal. Exists is equivalent to wildcard for value, so that a pod can tolerate all taints of a particular category. Possible enum values: - `"Equal"` - `"Exists"` tolerationSeconds &lt;integer&gt; TolerationSeconds represents the period of time the toleration (which must be of effect NoExecute, otherwise this field is ignored) tolerates the taint. By default, it is not set, which means tolerate the taint forever (do not evict). Zero and negative values will be treated as 0 (evict immediately) by the system. value &lt;string&gt; Value is the taint value the toleration matches to. If the operator is Exists, the value should be empty, otherwise just a regular string.[root@k8s-master day9]#key：表示污点的键，必须与节点上的污点键相匹配才能容忍此污点。value：表示污点的值，必须与节点上的污点值相匹配才能容忍此污点。如果未指定，则默认为空字 符串。operator：表示匹配操作符。可以是 Equal、Exists 。在 Equal 操作符下，如果 key、value 和 effect 都与节点上的污点匹配，则该容忍度规则会被视为匹配。因此，key、value 和 effect 都需要匹配才能算作是匹配的容忍度规则。”Exists” 表示只要节点上存在 key 字段指定的污点，无论其对应 的 value 是什么，都将被视为匹配的容忍度规则。effect：表示容忍度的作用效果。跟 node 节点上的 effect 匹配即可。tolerationSeconds：要想在 pod 容忍度中指定 tolerationSeconds 参数，effect 字段必须设置 为 NoExecute 才可以。如果一个节点上出现了被标记为 NoExecute 的污点，所有已经运行在该节点 上的 Pod 不能容忍这个污点，将会被终止并且删除。如果当 pod 定义容忍度时候容忍 node 节点排斥 等级是 Noexecute 污点时，Pod 会等待 tolerationSeconds 指定的时间，如果在这段时间内污点被 删除，则 Pod 将可以继续在该节点上运行。 否则过了这个时间 pod 就被删除了。案例NoSchedule再看一个案例，假如 k8s 集群有三个节点，k8s-master 、k8s-slave1、k8s-slave2，k8s-master 默 认是控制节点，把 k8s-slave1和 k8s-slave2 都打污点，测试让 pod 调度到 k8s-slave1 上。12345[root@k8s-master day9]# kubectl taint node k8s-slave1 key=value:NoSchedulenode/k8s-slave1 tainted[root@k8s-master day9]# kubectl taint node k8s-slave2 key=value:NoSchedulenode/k8s-slave2 tainted[root@k8s-master day9]#定义pod没有容忍度123456789101112[root@k8s-master day9]# cat pod-toleration-1.yaml apiVersion: v1kind: Podmetadata: name: my-pod labels: app: my-podspec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent让pod调度到k8s-slave2上123456789101112131415161718[root@k8s-master day9]# cat pod-toleration-2.yaml apiVersion: v1kind: Podmetadata: name: my-pod-1 labels: app: my-pod-1spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent tolerations: - key: "key" operator: "Exists" effect: "NoSchedule" nodeSelector: kubernetes.io/hostname: k8s-slave2把之前创建的污点都删除1234[root@k8s-master day9]# kubectl taint node k8s-slave1 key-node/k8s-slave1 untainted[root@k8s-master day9]# kubectl taint node k8s-slave2 key-node/k8s-slave2 untainted案例，演示tolerationSeconds场景创建一个pod，定义容忍度1234567891011121314151617[root@k8s-master day9]# cat pod-toleration-3.yaml apiVersion: v1kind: Podmetadata: name: my-pod-1 labels: app: my-pod-1spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent tolerations: - key: "key" operator: "Exists" effect: "NoExecute" tolerationSeconds: 300总结综上，我们可以总结出来 node 节点亲和性、node 节点选择器、node 节点污点分别适用什么场 景：节点亲和性（Node Affinity）适合用于将 Pod 部署在特定的节点上的场景，比如需要特定硬件 或软件配置的节点，或者需要将某些 Pod 部署在与其他相关服务相同的节点上。通过使用节点亲和性， 可以确保 Pod 在运行时可以获得所需的资源，并提高应用程序的性能和可靠性。节点选择器（Node Selector）是一种简单的调度策略，它允许在 Pod 中指定一个节点标签，使 得 pod 只能调度到拥有该标签的节点上。如果没有节点满足条件，则该 Pod 无法调度。污点（Taints）是一种用于标记节点的机制，可以在节点上设置污点来阻止不应该在该节点上运行 的 Pod 调度到该节点上。只有带有相应容忍污点（Tolerations）的 Pod 才能被调度到该节点上。例 如，可以在节点上设置污点来阻止一些敏感应用程序运行在该节点上。在生产环境中，这些调度策略可以用于以下场景：节点亲和性：在多节点集群中，通过将某些 Pod 调度到拥有特定硬件或软件配置的节点上，以最大化应用程序性能。例如，可以将需要 GPU 计算能力的机器学习任务调度到拥有 GPU 的节点上，以 提高计算速度。 节点选择器：在多租户环境中，可以使用节点选择器将不同的 Pod 部署在不同的节点上，以隔离 应用程序并避免资源竞争。例如，可以将测试环境的 Pod 调度到专用的测试节点上，而将生产环境的 Pod 调度到专用的生产节点上。 污点和容忍：在生产环境中，可以使用污点和容忍来确保敏感应用程序不会运行在错误的节点上。 例如，可以在拥有机密数据的节点上设置污点，并将只允许在经过身份验证的容器内运行的应用程序的容 忍添加到 Pod 中，以确保敏感数据得到保护。]]></content>
      <categories>
        <category>技术</category>
        <category>K8S</category>
      </categories>
      <tags>
        <tag>K8S</tag>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB集群搭建]]></title>
    <url>%2F2024%2F0513fc1146.html</url>
    <content type="text"><![CDATA[MongoDB是一个基于分布式文件存储的数据库。由C++语言编写。旨在为WEB应用提供可扩展的高性能数据存储解决方案。MongoDB是一个介于关系数据库和非关系数据库之间的产品，是非关系数据库当中功能最丰富，最像关系数据库的。它支持的数据结构非常松散，是类似json的bson格式，因此可以存储比较复杂的数据类型。Mongo最大的特点是它支持的查询语言非常强大，其语法有点类似于面向对象的查询语言，几乎可以实现类似关系数据库单表查询的绝大部分功能，而且还支持对数据建立索引。MongoDB集群简介mongodb 集群搭建的方式有三种：主从备份（Master - Slave）模式，或者叫主从复制模式。副本集（Replica Set）模式分片（Sharding）模式其中，第一种方式基本没什么意义，官方也不推荐这种方式搭建。另外两种分别就是副本集和分片的方式。Mongo分片高可用集群搭建概述为解决mongodb在replica set每个从节点上面的数据库均是对数据库的全量拷贝，从节点压力在高并发大数据量的场景下存在很大挑战，同时考虑到后期mongodb集群的在数据压力巨大时的扩展性，应对海量数据引出了分片机制。什么是分片分片是将数据库进行拆分，将其分散在不同的机器上的过程，无需功能强大的服务器就可以存储更多的数据，处理更大的负载，在总数据中，将集合切成小块，将这些块分散到若干片中，每个片只负载总数据的一部分，通过一个知道数据与分片对应关系的组件mongos的路由进程进行操作。基础组件其利用到了四个组件：mongos，config server，shard，replica setmongos数据库集群请求的入口，所有请求需要经过mongos进行协调，无需在应用层面利用程序来进行路由选择，mongos其自身是一个请求分发中心，负责将外部的请求分发到对应的shard服务器上，mongos作为统一的请求入口，为防止mongos单节点故障，一般需要对其做HA（高可用，Highly Available缩写）。config server配置服务器，存储所有数据库元数据（分片，路由）的配置。mongos本身没有物理存储分片服务器和数据路由信息，只是缓存在内存中来读取数据，mongos在第一次启动或后期重启时候，就会从config server中加载配置信息，如果配置服务器信息发生更新会通知所有的mongos来更新自己的状态，从而保证准确的请求路由，生产环境中通常也需要多个config server，防止配置文件存在单节点丢失问题。shard在传统意义上来讲，如果存在海量数据，单台服务器存储1T压力非常大，考虑到数据库的硬盘，网络IO，还有CPU，内存的瓶颈，如果多台进行分摊1T的数据，到每台上就是可估量的较小数据，在mongodb集群只要设置好分片规则，通过mongos操作数据库，就可以自动把对应的操作请求转发到对应的后端分片服务器上。replica set在总体mongodb集群架构中，对应的分片节点，如果单台机器下线，对应整个集群的数据就会出现部分缺失，这是不能发生的，因此对于shard节点需要replica set来保证数据的可靠性，生产环境通常为2个副本+1个仲裁。整体架构整体架构涉及到15个节点，我们这里使用Docker容器进行部署那么我们先来总结一下我们搭建一个高可用集群需要多少个Mongomongos： 3台configserver ： 3台shard ： 3片； 每个分片由三个节点构成容器部署情况角色端口暴漏端口描述角色config-server127017–配置节点1–config-server227017–配置节点2–config-server327017–配置节点3–mongos-server12701730001路由节点1–mongos-server22701730002路由节点2–mongos-server32701730003路由节点3–shard1-server127017–分片1节点1Primaryshard1-server227017–分片1节点2Secondryshard1-server327017–分片1节点3Arbitershard2-server127017–分片2节点1Primaryshard2-server227017–分片2节点2Secondryshard2-server327017–分片2节点3Arbitershard3-server127017–分片3节点1Primaryshard3-server227017–分片3节点2Secondryshard3-server327017–分片3节点3Arbiter整体架构预览基础环境准备安装Docker本次使用Docker环境进行搭建，需要提前准备好Docker环境创建Docker网络因为需要使用Docker搭建MongoDB集群，所以先创建Docker网络12docker network create mongo-clusterdocker network ls搭建ConfigServer副本集我们先来搭建ConfigServer的副本集，这里面涉及到三个节点，我们需要创建配置文件以及启动容器创建挂载目录我们需要创建对应的挂载目录来存储配置文件以及日志文件123456# 创建配置文件目录mkdir -p /tmp/mongo-cluster/config-server/conf# 创建数据文件目录mkdir -p /tmp/mongo-cluster/config-server/data/&#123;1..3&#125;# 创建日志文件目录mkdir -p /tmp/mongo-cluster/config-server/logs/&#123;1..3&#125;创建密钥文件因为我们知道搭建的话一定要高可用，而且一定要权限，这里mongo之间通信采用秘钥文件，所以我们先进行生成密钥文件1234# 创建密钥文件openssl rand -base64 756 &gt; /tmp/mongo-cluster/config-server/conf/mongo.key# 设置chmod 600 /tmp/mongo-cluster/config-server/conf/mongo.key创建配置文件因为由多个容器，配置文件是一样的，我们只需要创建一个配置文件，其他的容器统一读取该配置文件即可12345678910111213141516171819202122echo "# 日志文件storage: # mongod 进程存储数据目录，此配置仅对 mongod 进程有效 dbPath: /data/dbsystemLog: destination: file logAppend: true path: /data/logs/mongo.log# 网络设置net: port: 27017 #端口号# bindIp: 127.0.0.1 #绑定ipreplication: replSetName: configsvr #副本集名称sharding: clusterRole: configsvr # 集群角色，这里配置的角色是配置节点security: authorization: enabled #是否开启认证 keyFile: /data/configdb/conf/mongo.key #keyFile路径" &gt; /tmp/mongo-cluster/config-server/conf/mongo.conf启动容器启动config-server11234567docker run --name config-server1 -d \--net=mongo-cluster \--privileged=true \-v /tmp/mongo-cluster/config-server:/data/configdb \-v /tmp/mongo-cluster/config-server/data/1:/data/db \-v /tmp/mongo-cluster/config-server/logs/1:/data/logs \mongo --config /data/configdb/conf/mongo.conf启动config-server21234567docker run --name config-server2 -d \--net=mongo-cluster \--privileged=true \-v /tmp/mongo-cluster/config-server:/data/configdb \-v /tmp/mongo-cluster/config-server/data/2:/data/db \-v /tmp/mongo-cluster/config-server/logs/2:/data/logs \mongo --config /data/configdb/conf/mongo.conf启动config-server31234567docker run --name config-server3 -d \--net=mongo-cluster \--privileged=true \-v /tmp/mongo-cluster/config-server:/data/configdb \-v /tmp/mongo-cluster/config-server/data/3:/data/db \-v /tmp/mongo-cluster/config-server/logs/3:/data/logs \mongo --config /data/configdb/conf/mongo.conf初始化config-server登录容器进入第一台容器12docker exec -it config-server1 bashmongo -port 27017执行命令执行以下命令进行MongoDB容器的初始化12345678910rs.initiate( &#123; _id: "configsvr", members: [ &#123; _id : 1, host : "config-server1:27017" &#125;, &#123; _id : 2, host : "config-server2:27017" &#125;, &#123; _id : 3, host : "config-server3:27017" &#125; ] &#125;)如果出现OK表示MongoDB配置服务器已经初始化成功创建用户因为我们需要对用户进行权限管理，我们需要创建用户，这里为了演示，我们创建超级用户 权限是root12use admindb.createUser(&#123;user:"root",pwd:"root",roles:[&#123;role:'root',db:'admin'&#125;]&#125;)这样就在MongoDB的admin数据库添加了一个用户名为root 密码是root的用户搭建Shard分片组由于mongos是客户端，所以我们先搭建好config以及shard之后再搭建mongos。创建挂载目录我们先创建挂载目录123456# 创建配置文件目录mkdir -p /tmp/mongo-cluster/shard&#123;1..3&#125;-server/conf# 创建数据文件目录mkdir -p /tmp/mongo-cluster/shard&#123;1..3&#125;-server/data/&#123;1..3&#125;# 创建日志文件目录mkdir -p /tmp/mongo-cluster/shard&#123;1..3&#125;-server/logs/&#123;1..3&#125;搭建shard1分片组在同一台服务器上初始化一组分片创建密钥文件因为集群只需要一个密钥文件，我们可以将config-server中的密钥文件复制过来1cp /tmp/mongo-cluster/config-server/conf/mongo.key /tmp/mongo-cluster/shard1-server/conf/配置配置文件因为有多个容器，配置文件是一样的，我们只需要创建一个配置文件，其他的容器统一读取该配置文件即可12345678910111213141516171819202122echo "# 日志文件storage: # mongod 进程存储数据目录，此配置仅对 mongod 进程有效 dbPath: /data/dbsystemLog: destination: file logAppend: true path: /data/logs/mongo.log# 网络设置net: port: 27017 #端口号# bindIp: 127.0.0.1 #绑定ipreplication: replSetName: shard1 #复制集名称是 shardsvrsharding: clusterRole: shardsvr # 集群角色，这里配置的角色是分片节点security: authorization: enabled #是否开启认证 keyFile: /data/configdb/conf/mongo.key #keyFile路径" &gt; /tmp/mongo-cluster/shard1-server/conf/mongo.conf启动shard1-server11234567docker run --name shard1-server1 -d \--net=mongo-cluster \--privileged=true \-v /tmp/mongo-cluster/shard1-server:/data/configdb \-v /tmp/mongo-cluster/shard1-server/data/1:/data/db \-v /tmp/mongo-cluster/shard1-server/logs/1:/data/logs \mongo --config /data/configdb/conf/mongo.conf启动shard1-server21234567docker run --name shard1-server2 -d \--net=mongo-cluster \--privileged=true \-v /tmp/mongo-cluster/shard1-server:/data/configdb \-v /tmp/mongo-cluster/shard1-server/data/2:/data/db \-v /tmp/mongo-cluster/shard1-server/logs/2:/data/logs \mongo --config /data/configdb/conf/mongo.conf启动shard1-server31234567docker run --name shard1-server3 -d \--net=mongo-cluster \--privileged=true \-v /tmp/mongo-cluster/shard1-server:/data/configdb \-v /tmp/mongo-cluster/shard1-server/data/3:/data/db \-v /tmp/mongo-cluster/shard1-server/logs/3:/data/logs \mongo --config /data/configdb/conf/mongo.conf初始化shard1分片组并且制定第三个副本集为仲裁节点12docker exec -it shard1-server1 bin/bashmongo -port 27017登录后进行初始化节点，这里面arbiterOnly:true是设置为仲裁节点1234567891011#进行副本集配置rs.initiate( &#123; _id : "shard1", members: [ &#123; _id : 0, host : "shard1-server1:27017" &#125;, &#123; _id : 1, host : "shard1-server2:27017" &#125;, &#123; _id : 2, host : "shard1-server3:27017",arbiterOnly:true &#125; ] &#125;);显示OK即副本集创建成功创建用户因为我们需要对用户进行权限管理，我们需要创建用户，这里为了演示，我们创建超级用户 权限是root12use admindb.createUser(&#123;user:"root",pwd:"root",roles:[&#123;role:'root',db:'admin'&#125;]&#125;)查看节点信息1rs.isMaster()搭建shard2分片组创建密钥文件因为集群只需要一个密钥文件，我们可以将config-server中的密钥文件复制过来1cp /tmp/mongo-cluster/config-server/conf/mongo.key /tmp/mongo-cluster/shard2-server/conf/配置配置文件因为有多个容器，配置文件是一样的，我们只需要创建一个配置文件，其他的容器统一读取该配置文件即可12345678910111213141516171819202122echo "# 日志文件storage: # mongod 进程存储数据目录，此配置仅对 mongod 进程有效 dbPath: /data/dbsystemLog: destination: file logAppend: true path: /data/logs/mongo.log# 网络设置net: port: 27017 #端口号# bindIp: 127.0.0.1 #绑定ipreplication: replSetName: shard2 #复制集名称是 shard2sharding: clusterRole: shardsvr # 集群角色，这里配置的角色是分片节点security: authorization: enabled #是否开启认证 keyFile: /data/configdb/conf/mongo.key #keyFile路径" &gt; /tmp/mongo-cluster/shard2-server/conf/mongo.conf启动shard2-server11234567docker run --name shard2-server1 -d \--net=mongo-cluster \--privileged=true \-v /tmp/mongo-cluster/shard2-server:/data/configdb \-v /tmp/mongo-cluster/shard2-server/data/1:/data/db \-v /tmp/mongo-cluster/shard2-server/logs/1:/data/logs \mongo --config /data/configdb/conf/mongo.conf启动shard2-server21234567docker run --name shard2-server2 -d \--net=mongo-cluster \--privileged=true \-v /tmp/mongo-cluster/shard2-server:/data/configdb \-v /tmp/mongo-cluster/shard2-server/data/2:/data/db \-v /tmp/mongo-cluster/shard2-server/logs/2:/data/logs \mongo --config /data/configdb/conf/mongo.conf启动shard2-server31234567docker run --name shard2-server3 -d \--net=mongo-cluster \--privileged=true \-v /tmp/mongo-cluster/shard2-server:/data/configdb \-v /tmp/mongo-cluster/shard2-server/data/3:/data/db \-v /tmp/mongo-cluster/shard2-server/logs/3:/data/logs \mongo --config /data/configdb/conf/mongo.conf初始化shard2分片组登录节点后进行初始化分片212docker exec -it shard2-server1 bin/bashmongo -port 27017执行下面的命令进行初始化分片2，arbiterOnly:true参数是设置为仲裁节点1234567891011#进行副本集配置rs.initiate( &#123; _id : "shard2", members: [ &#123; _id : 0, host : "shard2-server1:27017" &#125;, &#123; _id : 1, host : "shard2-server2:27017" &#125;, &#123; _id : 2, host : "shard2-server3:27017",arbiterOnly:true &#125; ] &#125;);返回ok就表示创建用户因为我们需要对用户进行权限管理，我们需要创建用户，这里为了演示，我们创建超级用户 权限是root12use admindb.createUser(&#123;user:"root",pwd:"root",roles:[&#123;role:'root',db:'admin'&#125;]&#125;)搭建shard3分片组创建密钥文件因为集群只需要一个密钥文件，我们可以将config-server中的密钥文件复制过来1cp /tmp/mongo-cluster/config-server/conf/mongo.key /tmp/mongo-cluster/shard3-server/conf/配置配置文件因为有多个容器，配置文件是一样的，我们只需要创建一个配置文件，其他的容器统一读取该配置文件即可12345678910111213141516171819202122echo "# 日志文件storage: # mongod 进程存储数据目录，此配置仅对 mongod 进程有效 dbPath: /data/dbsystemLog: destination: file logAppend: true path: /data/logs/mongo.log# 网络设置net: port: 27017 #端口号# bindIp: 127.0.0.1 #绑定ipreplication: replSetName: shard3 #复制集名称是 shard3sharding: clusterRole: shardsvr # 集群角色，这里配置的角色是分片节点security: authorization: enabled #是否开启认证 keyFile: /data/configdb/conf/mongo.key #keyFile路径" &gt; /tmp/mongo-cluster/shard3-server/conf/mongo.conf启动shard3-server11234567docker run --name shard3-server1 -d \--net=mongo-cluster \--privileged=true \-v /tmp/mongo-cluster/shard3-server:/data/configdb \-v /tmp/mongo-cluster/shard3-server/data/1:/data/db \-v /tmp/mongo-cluster/shard3-server/logs/1:/data/logs \mongo --config /data/configdb/conf/mongo.conf启动shard3-server21234567docker run --name shard3-server2 -d \--net=mongo-cluster \--privileged=true \-v /tmp/mongo-cluster/shard3-server:/data/configdb \-v /tmp/mongo-cluster/shard3-server/data/2:/data/db \-v /tmp/mongo-cluster/shard3-server/logs/2:/data/logs \mongo --config /data/configdb/conf/mongo.conf启动shard3-server31234567docker run --name shard3-server3 -d \--net=mongo-cluster \--privileged=true \-v /tmp/mongo-cluster/shard3-server:/data/configdb \-v /tmp/mongo-cluster/shard3-server/data/3:/data/db \-v /tmp/mongo-cluster/shard3-server/logs/3:/data/logs \mongo --config /data/configdb/conf/mongo.conf初始化shard3分片组登录节点后进行初始化分片212docker exec -it shard3-server1 bin/bashmongo -port 27017执行下面的命令进行初始化分片3，arbiterOnly:true参数是设置为仲裁节点1234567891011#进行副本集配置rs.initiate( &#123; _id : "shard3", members: [ &#123; _id : 0, host : "shard3-server1:27017" &#125;, &#123; _id : 1, host : "shard3-server2:27017" &#125;, &#123; _id : 2, host : "shard3-server3:27017",arbiterOnly:true &#125; ] &#125;);创建用户因为我们需要对用户进行权限管理，我们需要创建用户，这里为了演示，我们创建超级用户 权限是root12use admindb.createUser(&#123;user:"root",pwd:"root",roles:[&#123;role:'root',db:'admin'&#125;]&#125;)搭建Mongosmongos负责查询与数据写入的路由，是实例访问的统一入口，是一个无状态的节点，每一个节点都可以从config-server节点获取到配置信息创建挂载目录我们需要创建对应的挂载目录来存储配置文件以及日志文件123456# 创建配置文件目录mkdir -p /tmp/mongo-cluster/mongos-server/conf# 创建数据文件目录mkdir -p /tmp/mongo-cluster/mongos-server/data/&#123;1..3&#125;# 创建日志文件目录mkdir -p /tmp/mongo-cluster/mongos-server/logs/&#123;1..3&#125;创建密钥文件因为集群只需要一个密钥文件，我们可以将config-server中的密钥文件复制过来1cp /tmp/mongo-cluster/config-server/conf/mongo.key /tmp/mongo-cluster/mongos-server/conf/创建配置文件因为有多个容器，配置文件是一样的，我们只需要创建一个配置文件，其他的容器统一读取该配置文件即可,因为Mongos只负责路由，就不需要数据文件了，并且mongos服务是不负责认证的，需要将authorization配置项删除1234567891011121314151617echo "# 日志文件systemLog: destination: file logAppend: true path: /data/logs/mongo.log# 网络设置net: port: 27017 #端口号# bindIp: 127.0.0.1 #绑定ip# 配置分片，这里面配置的是需要读取的配置节点的信息sharding: configDB: configsvr/config-server1:27017,config-server2:27017,config-server3:27017security: keyFile: /data/configdb/conf/mongo.key #keyFile路径" &gt; /tmp/mongo-cluster/mongos-server/conf/mongo.conf启动mongos集群启动mongos112345678docker run --name mongos-server1 -d \-p 30001:27017 \--net=mongo-cluster \--privileged=true \--entrypoint "mongos" \-v /tmp/mongo-cluster/mongos-server:/data/configdb \-v /tmp/mongo-cluster/mongos-server/logs/1:/data/logs \mongo --config /data/configdb/conf/mongo.conf启动mongos212345678docker run --name mongos-server2 -d \-p 30002:27017 \--net=mongo-cluster \--privileged=true \--entrypoint "mongos" \-v /tmp/mongo-cluster/mongos-server:/data/configdb \-v /tmp/mongo-cluster/mongos-server/logs/2:/data/logs \mongo --config /data/configdb/conf/mongo.conf启动mongos312345678docker run --name mongos-server3 -d \-p 30003:27017 \--net=mongo-cluster \--privileged=true \--entrypoint "mongos" \-v /tmp/mongo-cluster/mongos-server:/data/configdb \-v /tmp/mongo-cluster/mongos-server/logs/3:/data/logs \mongo --config /data/configdb/conf/mongo.conf配置mongos-server1因为mongos是无中心的配置，所以每一台都需要进行分片配置进入容器12docker exec -it mongos-server1 /bin/bashmongo -port 27017登录Mongos使用前面设置的root用户密码12use admin;db.auth("root","root");配置分片进行配置分片信息123sh.addShard("shard1/shard1-server1:27017,shard1-server2:27017,shard1-server3:27017")sh.addShard("shard2/shard2-server1:27017,shard2-server2:27017,shard2-server3:27017")sh.addShard("shard3/shard3-server1:27017,shard3-server2:27017,shard3-server3:27017")配置mongos-server2因为mongos是无中心的配置，所以每一台都需要进行分片配置进入容器12docker exec -it mongos-server2 /bin/bashmongo -port 27017登录Mongos使用前面设置的root用户密码12use admin;db.auth("root","root");配置分片进行配置分片信息123sh.addShard("shard1/shard1-server1:27017,shard1-server2:27017,shard1-server3:27017")sh.addShard("shard2/shard2-server1:27017,shard2-server2:27017,shard2-server3:27017")sh.addShard("shard3/shard3-server1:27017,shard3-server2:27017,shard3-server3:27017")配置mongos-server3因为mongos是无中心的配置，所以每一台都需要进行分片配置进入容器12docker exec -it mongos-server3 /bin/bashmongo -port 27017登录Mongos使用前面设置的root用户密码12use admin;db.auth("root","root");配置分片进行配置分片信息123sh.addShard("shard1/shard1-server1:27017,shard1-server2:27017,shard1-server3:27017")sh.addShard("shard2/shard2-server1:27017,shard2-server2:27017,shard2-server3:27017")sh.addShard("shard3/shard3-server1:27017,shard3-server2:27017,shard3-server3:27017")Docker-compose方式搭建环境准备初始化目录脚本1234567891011121314151617181920212223# 创建config-server 目录# 创建配置文件目录mkdir -p /tmp/mongo-cluster/config-server/conf# 创建数据文件目录mkdir -p /tmp/mongo-cluster/config-server/data/&#123;1..3&#125;# 创建日志文件目录mkdir -p /tmp/mongo-cluster/config-server/logs/&#123;1..3&#125;# 创建shard-server 目录# 创建配置文件目录mkdir -p /tmp/mongo-cluster/shard&#123;1..3&#125;-server/conf# 创建数据文件目录mkdir -p /tmp/mongo-cluster/shard&#123;1..3&#125;-server/data/&#123;1..3&#125;# 创建日志文件目录mkdir -p /tmp/mongo-cluster/shard&#123;1..3&#125;-server/logs/&#123;1..3&#125;# 创建mongos-server 目录# 创建配置文件目录mkdir -p /tmp/mongo-cluster/mongos-server/conf# 创建数据文件目录mkdir -p /tmp/mongo-cluster/mongos-server/data/&#123;1..3&#125;# 创建日志文件目录mkdir -p /tmp/mongo-cluster/mongos-server/logs/&#123;1..3&#125;生成密钥文件123456789101112# 创建密钥文件openssl rand -base64 756 &gt; /tmp/mongo-cluster/config-server/conf/mongo.key# 设置chmod 600 /tmp/mongo-cluster/config-server/conf/mongo.keycp /tmp/mongo-cluster/config-server/conf/mongo.key /tmp/mongo-cluster/shard1-server/conf/cp /tmp/mongo-cluster/config-server/conf/mongo.key /tmp/mongo-cluster/shard2-server/conf/cp /tmp/mongo-cluster/config-server/conf/mongo.key /tmp/mongo-cluster/shard3-server/conf/cp /tmp/mongo-cluster/config-server/conf/mongo.key /tmp/mongo-cluster/mongos-server/conf/创建配置文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114echo "# 日志文件storage: # mongod 进程存储数据目录，此配置仅对 mongod 进程有效 dbPath: /data/dbsystemLog: destination: file logAppend: true path: /data/logs/mongo.log# 网络设置net: port: 27017 #端口号# bindIp: 127.0.0.1 #绑定ipreplication: replSetName: configsvr #副本集名称sharding: clusterRole: configsvr # 集群角色，这里配置的角色是配置节点security: authorization: enabled #是否开启认证 keyFile: /data/configdb/conf/mongo.key #keyFile路径" &gt; /tmp/mongo-cluster/config-server/conf/mongo.confecho "# 日志文件storage: # mongod 进程存储数据目录，此配置仅对 mongod 进程有效 dbPath: /data/dbsystemLog: destination: file logAppend: true path: /data/logs/mongo.log# 网络设置net: port: 27017 #端口号# bindIp: 127.0.0.1 #绑定ipreplication: replSetName: shard1 #复制集名称是 shardsvrsharding: clusterRole: shardsvr # 集群角色，这里配置的角色是分片节点security: authorization: enabled #是否开启认证 keyFile: /data/configdb/conf/mongo.key #keyFile路径" &gt; /tmp/mongo-cluster/shard1-server/conf/mongo.confecho "# 日志文件storage: # mongod 进程存储数据目录，此配置仅对 mongod 进程有效 dbPath: /data/dbsystemLog: destination: file logAppend: true path: /data/logs/mongo.log# 网络设置net: port: 27017 #端口号# bindIp: 127.0.0.1 #绑定ipreplication: replSetName: shard2 #复制集名称是 shard2sharding: clusterRole: shardsvr # 集群角色，这里配置的角色是分片节点security: authorization: enabled #是否开启认证 keyFile: /data/configdb/conf/mongo.key #keyFile路径" &gt; /tmp/mongo-cluster/shard2-server/conf/mongo.confecho "# 日志文件storage: # mongod 进程存储数据目录，此配置仅对 mongod 进程有效 dbPath: /data/dbsystemLog: destination: file logAppend: true path: /data/logs/mongo.log# 网络设置net: port: 27017 #端口号# bindIp: 127.0.0.1 #绑定ipreplication: replSetName: shard3 #复制集名称是 shard3sharding: clusterRole: shardsvr # 集群角色，这里配置的角色是分片节点security: authorization: enabled #是否开启认证 keyFile: /data/configdb/conf/mongo.key #keyFile路径" &gt; /tmp/mongo-cluster/shard3-server/conf/mongo.confecho "# 日志文件systemLog: destination: file logAppend: true path: /data/logs/mongo.log# 网络设置net: port: 27017 #端口号 bindIp: 0.0.0.0 #绑定ip# 配置分片，这里面配置的是需要读取的配置节点的信息sharding: configDB: configsvr/config-server1:27017,config-server2:27017,config-server3:27017security: keyFile: /data/configdb/conf/mongo.key #keyFile路径" &gt; /tmp/mongo-cluster/mongos-server/conf/mongo.conf启动服务docker-compos配置文件使用docker-compos方式启动Docker容器123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207version: '2'services: config-server1: image: mongo container_name: config-server1 privileged: true networks: - mongo-cluster-network command: --config /data/configdb/conf/mongo.conf volumes: - /etc/localtime:/etc/localtime - /tmp/mongo-cluster/config-server:/data/configdb - /tmp/mongo-cluster/config-server/data/1:/data/db - /tmp/mongo-cluster/config-server/logs/1:/data/logs config-server2: image: mongo container_name: config-server2 privileged: true networks: - mongo-cluster-network command: --config /data/configdb/conf/mongo.conf volumes: - /etc/localtime:/etc/localtime - /tmp/mongo-cluster/config-server:/data/configdb - /tmp/mongo-cluster/config-server/data/2:/data/db - /tmp/mongo-cluster/config-server/logs/2:/data/logs config-server3: image: mongo container_name: config-server3 privileged: true networks: - mongo-cluster-network command: --config /data/configdb/conf/mongo.conf volumes: - /etc/localtime:/etc/localtime - /tmp/mongo-cluster/config-server:/data/configdb - /tmp/mongo-cluster/config-server/data/3:/data/db - /tmp/mongo-cluster/config-server/logs/3:/data/logs shard1-server1: image: mongo container_name: shard1-server1 privileged: true networks: - mongo-cluster-network command: --config /data/configdb/conf/mongo.conf volumes: - /etc/localtime:/etc/localtime - /tmp/mongo-cluster/shard1-server:/data/configdb - /tmp/mongo-cluster/shard1-server/data/1:/data/db - /tmp/mongo-cluster/shard1-server/logs/1:/data/logs shard1-server2: image: mongo container_name: shard1-server2 privileged: true networks: - mongo-cluster-network command: --config /data/configdb/conf/mongo.conf volumes: - /etc/localtime:/etc/localtime - /tmp/mongo-cluster/shard1-server:/data/configdb - /tmp/mongo-cluster/shard1-server/data/2:/data/db - /tmp/mongo-cluster/shard1-server/logs/2:/data/logs shard1-server3: image: mongo container_name: shard1-server3 privileged: true networks: - mongo-cluster-network command: --config /data/configdb/conf/mongo.conf volumes: - /etc/localtime:/etc/localtime - /tmp/mongo-cluster/shard1-server:/data/configdb - /tmp/mongo-cluster/shard1-server/data/3:/data/db - /tmp/mongo-cluster/shard1-server/logs/3:/data/logs shard2-server1: image: mongo container_name: shard2-server1 privileged: true networks: - mongo-cluster-network command: --config /data/configdb/conf/mongo.conf volumes: - /etc/localtime:/etc/localtime - /tmp/mongo-cluster/shard2-server:/data/configdb - /tmp/mongo-cluster/shard2-server/data/1:/data/db - /tmp/mongo-cluster/shard2-server/logs/1:/data/logs shard2-server2: image: mongo container_name: shard2-server2 privileged: true networks: - mongo-cluster-network command: --config /data/configdb/conf/mongo.conf volumes: - /etc/localtime:/etc/localtime - /tmp/mongo-cluster/shard2-server:/data/configdb - /tmp/mongo-cluster/shard2-server/data/2:/data/db - /tmp/mongo-cluster/shard2-server/logs/2:/data/logs shard2-server3: image: mongo container_name: shard2-server3 privileged: true networks: - mongo-cluster-network command: --config /data/configdb/conf/mongo.conf volumes: - /etc/localtime:/etc/localtime - /tmp/mongo-cluster/shard2-server:/data/configdb - /tmp/mongo-cluster/shard2-server/data/3:/data/db - /tmp/mongo-cluster/shard2-server/logs/3:/data/logs shard3-server1: image: mongo container_name: shard3-server1 privileged: true networks: - mongo-cluster-network command: --config /data/configdb/conf/mongo.conf volumes: - /etc/localtime:/etc/localtime - /tmp/mongo-cluster/shard3-server:/data/configdb - /tmp/mongo-cluster/shard3-server/data/1:/data/db - /tmp/mongo-cluster/shard3-server/logs/1:/data/logs shard3-server2: image: mongo container_name: shard3-server2 privileged: true networks: - mongo-cluster-network command: --config /data/configdb/conf/mongo.conf volumes: - /etc/localtime:/etc/localtime - /tmp/mongo-cluster/shard3-server:/data/configdb - /tmp/mongo-cluster/shard3-server/data/2:/data/db - /tmp/mongo-cluster/shard3-server/logs/2:/data/logs shard3-server3: image: mongo container_name: shard3-server3 privileged: true networks: - mongo-cluster-network command: --config /data/configdb/conf/mongo.conf volumes: - /etc/localtime:/etc/localtime - /tmp/mongo-cluster/shard3-server:/data/configdb - /tmp/mongo-cluster/shard3-server/data/3:/data/db - /tmp/mongo-cluster/shard3-server/logs/3:/data/logs mongos-server1: image: mongo container_name: mongos-server1 privileged: true entrypoint: "mongos" networks: - mongo-cluster-network command: --config /data/configdb/conf/mongo.conf ports: - "30001:27017" volumes: - /etc/localtime:/etc/localtime - /tmp/mongo-cluster/mongos-server:/data/configdb - /tmp/mongo-cluster/mongos-server/logs/1:/data/logs command: --config /data/configdb/conf/mongo.conf mongos-server2: image: mongo container_name: mongos-server2 privileged: true entrypoint: "mongos" networks: - mongo-cluster-network ports: - "30002:27017" volumes: - /etc/localtime:/etc/localtime - /tmp/mongo-cluster/mongos-server:/data/configdb - /tmp/mongo-cluster/mongos-server/logs/2:/data/logs command: --config /data/configdb/conf/mongo.conf mongos-server3: image: mongo container_name: mongos-server3 privileged: true entrypoint: "mongos" networks: - mongo-cluster-network ports: - "30003:27017" volumes: - /etc/localtime:/etc/localtime - /tmp/mongo-cluster/mongos-server:/data/configdb - /tmp/mongo-cluster/mongos-server/logs/3:/data/logs command: --config /data/configdb/conf/mongo.confnetworks: mongo-cluster-network: driver: bridge启动服务1docker-compose up -d初始化文件执行下面脚本进行容器初始化1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071docker exec -it config-server1 bashmongo -port 27017rs.initiate( &#123; _id: "configsvr", members: [ &#123; _id : 1, host : "config-server1:27017" &#125;, &#123; _id : 2, host : "config-server2:27017" &#125;, &#123; _id : 3, host : "config-server3:27017" &#125; ] &#125;)use admindb.createUser(&#123;user:"root",pwd:"root",roles:[&#123;role:'root',db:'admin'&#125;]&#125;)db.auth("root","root")db.createUser(&#123;user:"test",pwd:"test",roles:[&#123;role:'readWrite',db:'test'&#125;]&#125;)docker exec -it shard1-server1 bin/bashmongo -port 27017#进行副本集配置rs.initiate( &#123; _id : "shard1", members: [ &#123; _id : 0, host : "shard1-server1:27017" &#125;, &#123; _id : 1, host : "shard1-server2:27017" &#125;, &#123; _id : 2, host : "shard1-server3:27017",arbiterOnly:true &#125; ] &#125;);use admindb.createUser(&#123;user:"root",pwd:"root",roles:[&#123;role:'root',db:'admin'&#125;]&#125;)db.auth("root","root")db.createUser(&#123;user:"test",pwd:"test",roles:[&#123;role:'readWrite',db:'test'&#125;]&#125;)docker exec -it shard2-server1 bin/bashmongo -port 27017#进行副本集配置rs.initiate( &#123; _id : "shard2", members: [ &#123; _id : 0, host : "shard2-server1:27017" &#125;, &#123; _id : 1, host : "shard2-server2:27017" &#125;, &#123; _id : 2, host : "shard2-server3:27017",arbiterOnly:true &#125; ] &#125;);use admindb.createUser(&#123;user:"root",pwd:"root",roles:[&#123;role:'root',db:'admin'&#125;]&#125;)db.auth("root","root")db.createUser(&#123;user:"test",pwd:"test",roles:[&#123;role:'readWrite',db:'test'&#125;]&#125;)docker exec -it shard3-server1 bin/bashmongo -port 27017#进行副本集配置rs.initiate( &#123; _id : "shard3", members: [ &#123; _id : 0, host : "shard3-server1:27017" &#125;, &#123; _id : 1, host : "shard3-server2:27017" &#125;, &#123; _id : 2, host : "shard3-server3:27017",arbiterOnly:true &#125; ] &#125;);use admindb.createUser(&#123;user:"root",pwd:"root",roles:[&#123;role:'root',db:'admin'&#125;]&#125;)db.auth("root","root")db.createUser(&#123;user:"test",pwd:"test",roles:[&#123;role:'readWrite',db:'test'&#125;]&#125;)初始化分片123456789101112131415161718192021222324docker exec -it mongos-server1 /bin/bashmongo -port 27017use admin;db.auth("root","root");sh.addShard("shard1/shard1-server1:27017,shard1-server2:27017,shard1-server3:27017")sh.addShard("shard2/shard2-server1:27017,shard2-server2:27017,shard2-server3:27017")sh.addShard("shard3/shard3-server1:27017,shard3-server2:27017,shard3-server3:27017")docker exec -it mongos-server2 /bin/bashmongo -port 27017use admin;db.auth("root","root");sh.addShard("shard1/shard1-server1:27017,shard1-server2:27017,shard1-server3:27017")sh.addShard("shard2/shard2-server1:27017,shard2-server2:27017,shard2-server3:27017")sh.addShard("shard3/shard3-server1:27017,shard3-server2:27017,shard3-server3:27017")docker exec -it mongos-server3 /bin/bashmongo -port 27017use admin;db.auth("root","root");sh.addShard("shard1/shard1-server1:27017,shard1-server2:27017,shard1-server3:27017")sh.addShard("shard2/shard2-server1:27017,shard2-server2:27017,shard2-server3:27017")sh.addShard("shard3/shard3-server1:27017,shard3-server2:27017,shard3-server3:27017")]]></content>
      <categories>
        <category>技术</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx之error.log记录报错信息分析]]></title>
    <url>%2F2024%2F02701bd567.html</url>
    <content type="text"><![CDATA[我们经常遇到各种各样的nginx错误日志，平时根据一些nginx错误日志就可以分析出原因了，不过不是很系统，这里记录一下关于nginx的error.log的详细说明，方便以后查看了解。错误信息描述“upstream prematurely（过早的） closed connection”请求uri的时候出现的异常，是由于upstream还未返回应答给用户时用户断掉连接造成的，对系统没有影响，可以忽略“recv() failed (104: Connection reset by peer)”（1）服务器的并发连接数超过了其承载量，服务器会将其中一些连接Down掉； （2）客户关掉了浏览器，而服务器还在给客户端发送数据； （3）浏览器端按了Stop“(111: Connection refused) while connecting to upstream”用户在连接时，若遇到后端upstream挂掉或者不通，会收到该错误“(111: Connection refused) while reading response header from upstream”用户在连接成功后读取数据时，若遇到后端upstream挂掉或者不通，会收到该错误“(111: Connection refused) while sending request to upstream”Nginx和upstream连接成功后发送数据时，若遇到后端upstream挂掉或者不通，会收到该错误“(110: Connection timed out) while connecting to upstream”nginx连接后面的upstream时超时“(110: Connection timed out) while reading upstream”nginx读取来自upstream的响应时超时“(110: Connection timed out) while reading response header from upstream”nginx读取来自upstream的响应头时超时“(110: Connection timed out) while reading upstream”nginx读取来自upstream的响应时超时“(104: Connection reset by peer) while connecting to upstream”upstream发送了RST，将连接重置“upstream sent invalid header while reading response header from upstream”upstream发送的响应头无效“upstream sent no valid HTTP/1.0 header while reading response header from upstream”upstream发送的响应头无效“client intended to send too large body”用于设置允许接受的客户端请求内容的最大值，默认值是1M，client发送的body超过了设置值“reopening logs”用户发送kill -USR1命令“gracefully shutting down”,用户发送kill -WINCH命令“no servers are inside upstream”upstream下未配置server“no live upstreams while connecting to upstream”upstream下的server全都挂了“SSL_do_handshake() failed”SSL握手失败“SSL_write() failed (SSL:) while sending to client”“(13: Permission denied) while reading upstream”“(98: Address already in use) while connecting to upstream”“(99: Cannot assign requested address) while connecting to upstream”“ngx_slab_alloc() failed: no memory in SSL session shared cache”ssl_session_cache大小不够等原因造成“could not add new SSL session to the session cache while SSL handshaking”ssl_session_cache大小不够等原因造成“send() failed (111: Connection refused)”]]></content>
      <categories>
        <category>技术</category>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用VMware安装wim或esd格式的Win10镜像]]></title>
    <url>%2F2024%2F02bbbec077.html</url>
    <content type="text"><![CDATA[有时我们需要在虚拟机中安装个 Win10 系统做测试使用，如果使用原版镜像不仅体积大、安装慢，而且占用资源也很多。因此我们常常会选择第三方封装的系统了，可以一键部署，开箱即用。然而，第三方封装的镜像一般是 wim 或者 esd 格式的，用 WMware 是无法直接安装的。本文介绍如何借助 PE 系统来进行安装。制作系统镜像（1）这里我们准备的 Win10 镜像是 esd 格式的，首先要将其转换成 iso 格式的。我们下载一个 UltraISO 软件，启动后选择“文件”-&gt;“新建”-&gt;“UDF DVD映像”：（2）找到我们的 esd 格式镜像，将其添加进来：（3）点击保存按钮，选择保存路径，点击生成，即可生成 iso 镜像。制作 PE 镜像（1）这里我使用的是“微 PE 工具箱”，首先到其官网（点击访问）将程序下载到本地：（2）打开微 PE 工具箱，选择右下角光盘图标：（3）选择 PE 镜像保存路径，点击“立即生成 ISO”，得到 PE 镜像 iso 文件：创建虚拟机（1）因为我这次安装的镜像是 64 位的 win10，那么就创建一个 64 位的 win10 虚拟机，虚拟机 CD/DVD 选项选择我们准备好的 PE 镜像：（2）开启虚拟机，第一个是高分辨率，第二个是低分辨率 1024*768，这里我们选第二个：创建磁盘分区（1）进入 PE 系统后我们会发现目前还看不到分配给虚拟机的磁盘空间，我们运行桌面上的“分区助手(无损)”程序。（2）启动后选择硬盘，然后点击“转化成GPT硬盘”按钮开始转换：（3）转化后再次选中硬盘，点击左侧的“快速分区”按钮：（4）在弹出的对话框中将分区数改成 1，硬盘类型设置为 GPT，最后点击“执行”按钮开始分区。安装系统（1）将虚拟机的“CD/DVD 驱动器”设置成我们第一步制作的系统镜像：（2）然后打开 PE 系统桌面的“Windows安装器”，第一个选择栏选择我们通过 CD 驱动加载进来的系统镜像，第二个引导驱动器位置不动，第三个安装磁盘位置选刚才制作的分区。然后点击开始安装。（3）安装完毕后，关闭虚拟机，然后再开启虚拟机，就会进入到系统安装界面。（4）安装完毕后，就可以使用了。]]></content>
      <categories>
        <category>桌面</category>
        <category>系统</category>
      </categories>
      <tags>
        <tag>系统安装</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[K8S集群部署]]></title>
    <url>%2F2023%2F1192b6432e.html</url>
    <content type="text"><![CDATA[Kubeadm 是一个K8s 部署工具，提供kubeadm init 和kubeadm join，用于快速部署Kubernetes 集群。完整的卸载k8s（有需要的话）1234567891011121314151617181920212223242526272829303132333435363738394041424344# 首先清理运行到k8s群集中的pod，使用kubectl delete node --all# 使用脚本停止所有k8s服务for service in kube-apiserver kube-controller-manager kubectl kubelet etcd kube-proxy kube-scheduler; do systemctl stop $servicedone# 使用命令卸载k8skubeadm reset -f# 卸载k8s相关程序yum -y remove kube*# 删除相关的配置文件modprobe -r ipiplsmod# 然后手动删除配置文件和flannel网络配置和flannel网口：rm -rf /etc/cnirm -rf /root/.kube# 删除cni网络ifconfig cni0 downip link delete cni0ifconfig flannel.1 downip link delete flannel.1# 删除残留的配置文件rm -rf ~/.kube/rm -rf /etc/kubernetes/rm -rf /etc/systemd/system/kubelet.service.drm -rf /etc/systemd/system/kubelet.servicerm -rf /etc/systemd/system/multi-user.target.wants/kubelet.servicerm -rf /var/lib/kubeletrm -rf /usr/libexec/kubernetes/kubelet-pluginsrm -rf /usr/bin/kube*rm -rf /opt/cnirm -rf /var/lib/etcdrm -rf /var/etcd# 更新镜像yum clean allyum makecache安装kube集群（4节点）k8s重置命令（如果初始化的过程出现了错误就使用重置命令）：kubeadm reset准备工作（所有的节点都执行）编辑4台服务器的 /etc/hosts 文件 ,添加下面内容（每个节点都执行一遍）：1234192.168.2.1 node1192.168.2.2 node2192.168.2.3 node3192.168.2.4 node4设置hostname（以node1为例）：1hostnamectl set-hostname node1 # node1 是自定义名字或者修改 /etc/hostname 文件，写入node1（其他的子节点都一样）：1vim /etc/hostname修改之后/etc/hostname的内容为：1node1所有节点执行时间同步：1234# 启动chronyd服务systemctl start chronydsystemctl enable chronyddate所有节点禁用SELinux和Firewalld服务：1234systemctl stop firewalldsystemctl disable firewalldsed -i 's/enforcing/disabled/' /etc/selinux/config # 重启后生效所有节点禁用swap分区：12345678# 临时禁用swap分区swapoff -a# 永久禁用swap分区vi /etc/fstab # 注释掉下面的设置# /dev/mapper/centos-swap swap# 之后需要重启服务器生效所有节点添加网桥过滤和地址转发功能：12345678cat &gt; /etc/sysctl.d/kubernetes.conf &lt;&lt; EOFnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1net.ipv4.ip_forward = 1EOF# 然后执行,生效sysctl --system然后所有节点安装docker-ce（略）需要注意的是要配置docker的cgroupdriver：打开docker配置文件进行修改，该文件默认情况下不存在，可以新建一个，不同操作系统的文件位置不一样。1vim /etc/docker/daemon.json1234&#123; // 添加这行 "exec-opts": ["native.cgroupdriver=systemd"],&#125;重新加载配置并重启服务：12systemctl daemon-reloadsystemctl restart docker最终配置文件内容如下：12345&#123; "exec-opts": [ "native.cgroupdriver=systemd" ]&#125;所有节点的kubernetes镜像切换成国内源：123456789cat &gt; /etc/yum.repos.d/kubernetes.repo &lt;&lt; EOF[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64enabled=1gpgcheck=0repo_gpgcheck=0gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOF所有节点安装指定版本 kubeadm，kubelet 和 kubectl（我这里选择1.23.0版本的）：1234yum install -y kubelet-1.23.0 kubeadm-1.23.0 kubectl-1.23.0# 设置kubelet开机启动（看你自己）systemctl enable kubelet更改kubelet的容器路径（如果需要的话，不需要可以跳过）1vim /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf修改完之后配置文件如下：12[Service]Environment="KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --root-dir=/mnt/sdb_new/kubelet/ --kubeconfig=/etc/kubernetes/kubelet.conf"使配置生效：123systemctl daemon-reloadsystemctl restart dockersystemctl restart kubelet部署Kubernetes集群覆盖kubernetes的镜像地址（只需要在master节点上操作初始化命令）首先要覆盖kubeadm的镜像地址，因为这个是外网的无法访问，需要替换成国内的镜像地址，使用此命令列出集群在配置过程中需要哪些镜像：123456789[root@node1 home]# kubeadm config images listI0418 18:26:04.047449 19242 version.go:255] remote version is much newer: v1.27.1; falling back to: stable-1.23k8s.gcr.io/kube-apiserver:v1.23.17k8s.gcr.io/kube-controller-manager:v1.23.17k8s.gcr.io/kube-scheduler:v1.23.17k8s.gcr.io/kube-proxy:v1.23.17k8s.gcr.io/pause:3.6k8s.gcr.io/etcd:3.5.1-0k8s.gcr.io/coredns/coredns:v1.8.6更改为阿里云的镜像地址：123456789[root@node1 home]# kubeadm config images list --image-repository registry.aliyuncs.com/google_containersI0418 18:28:18.740057 20021 version.go:255] remote version is much newer: v1.27.1; falling back to: stable-1.23registry.aliyuncs.com/google_containers/kube-apiserver:v1.23.17registry.aliyuncs.com/google_containers/kube-controller-manager:v1.23.17registry.aliyuncs.com/google_containers/kube-scheduler:v1.23.17registry.aliyuncs.com/google_containers/kube-proxy:v1.23.17registry.aliyuncs.com/google_containers/pause:3.6registry.aliyuncs.com/google_containers/etcd:3.5.1-0registry.aliyuncs.com/google_containers/coredns:v1.8.6然后将镜像手动拉取下来，这样在初始化的时候回更快一些（还有一个办法就是直接在docker上把镜像pull下来，docker只要配置一下国内源即可快速的将镜像pull下来）：123456789[root@node1 home]# kubeadm config images pull --image-repository registry.aliyuncs.com/google_containersI0418 18:28:31.795554 20088 version.go:255] remote version is much newer: v1.27.1; falling back to: stable-1.23[config/images] Pulled registry.aliyuncs.com/google_containers/kube-apiserver:v1.23.17[config/images] Pulled registry.aliyuncs.com/google_containers/kube-controller-manager:v1.23.17[config/images] Pulled registry.aliyuncs.com/google_containers/kube-scheduler:v1.23.17[config/images] Pulled registry.aliyuncs.com/google_containers/kube-proxy:v1.23.17[config/images] Pulled registry.aliyuncs.com/google_containers/pause:3.6[config/images] Pulled registry.aliyuncs.com/google_containers/etcd:3.5.1-0[config/images] Pulled registry.aliyuncs.com/google_containers/coredns:v1.8.6初始化kubernetes（只需要在master节点上操作初始化命令）初始化 Kubernetes，指定网络地址段 和 镜像地址（后续的子节点可以使用join命令进行动态的追加）：12345678910111213[root@node1 home]# kubeadm init \ --apiserver-advertise-address=192.168.2.1 \ --image-repository registry.aliyuncs.com/google_containers \ --kubernetes-version v1.23.0 \ --service-cidr=10.96.0.0/12 \ --pod-network-cidr=10.244.0.0/16 \ --ignore-preflight-errors=all# –apiserver-advertise-address # 集群通告地址(master 机器IP，这里用的万兆网)# –image-repository # 由于默认拉取镜像地址k8s.gcr.io国内无法访问，这里指定阿里云镜像仓库地址# –kubernetes-version #K8s版本，与上面安装的一致# –service-cidr #集群内部虚拟网络，Pod统一访问入口，可以不用更改，直接用上面的参数# –pod-network-cidr #Pod网络，与下面部署的CNI网络组件yaml中保持一致，可以不用更改，直接用上面的参数执行完之后要手动执行一些参数（尤其是 加入集群的join命令 需要复制记录下载）：12345678910111213141516171819202122[addons] Applied essential addon: kube-proxyYour Kubernetes control-plane has initialized successfully!To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/configAlternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.confYou should now deploy a pod network to the cluster.Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/Then you can join any number of worker nodes by running the following on each as root:kubeadm join 192.168.2.1:6443 --token ochspx.15in9qkiu5z8tx2y \ --discovery-token-ca-cert-hash sha256:1f31202107af96a07df9fd78c3aa9bb44fd40076ac123e8ff28d6ab691a02a31执行参数：12345[root@node1 home]# mkdir -p $HOME/.kube[root@node1 home]# sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config[root@node1 home]# sudo chown $(id -u):$(id -g) $HOME/.kube/config[root@node1 home]# [root@node1 home]# vim /root/.bash_profile加入以下这段：123456# 超级用户变量export KUBECONFIG=/etc/kubernetes/admin.conf# 设置别名alias k=kubectl# 设置kubectl命令补齐功能source &lt;(kubectl completion bash)激活 .bash_profile：1[root@node1 home]# source /root/.bash_profile这段要复制记录下来（来自k8s初始化成功之后出现的join命令，需要先配置完Flannel才能加入子节点），后续子节点加入master节点需要执行这段命令：12kubeadm join 192.168.2.1:6443 --token ochspx.15in9qkiu5z8tx2y \ --discovery-token-ca-cert-hash sha256:1f31202107af96a07df9fd78c3aa9bb44fd40076ac123e8ff28d6ab691a02a31设定kubeletl网络（主节点部署）部署容器网络，CNI网络插件(在Master上执行，著名的有flannel、calico、canal和kube-router等，简单易用的实现是为CoreOS提供的flannel项目)，这里使用Flannel实现。下载kube-flannel.yml：1[root@node1 home]# wget https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml然后修改配置文件，找到如下位置，修改 Newwork 与执行 kubeadm init 输入的网段一致：1234567net-conf.json: | &#123; "Network": "10.244.0.0/16", "Backend"": &#123; "Type": "vxlan" &#125;&#125;修改配置之后安装组件（如果安装的时候卡在pull镜像的时候，试一试手动用docker将镜像拉取下来）：1[root@node1 home]# kubectl apply -f kube-flannel.yml查看flannel pod状态（必须要为Running状态，如果kube-flannel起不来，那么就用kubectl describe pod kube-flannel-ds-f5jn6 -n kube-flannel命令查看pod起不来的原因，然后去搜度娘获取解决方案）：1234567891011[root@node1 home]# # 必须所有的容器都是Running[root@node1 home]# kubectl get pod --all-namespacesNAMESPACE NAME READY STATUS RESTARTS AGEkube-flannel kube-flannel-ds-f5jn6 1/1 Running 0 8m21skube-system coredns-6d8c4cb4d-ctqw5 1/1 Running 0 42mkube-system coredns-6d8c4cb4d-n52fq 1/1 Running 0 42mkube-system etcd-k8s-master 1/1 Running 0 42mkube-system kube-apiserver-k8s-master 1/1 Running 0 42mkube-system kube-controller-manager-k8s-master 1/1 Running 0 42mkube-system kube-proxy-swpkz 1/1 Running 0 42mkube-system kube-scheduler-k8s-master 1/1 Running 0 42m查看通信状态：1234567891011121314151617181920[root@node1 home]# kubectl get pod -n kube-systemNAME READY STATUS RESTARTS AGEcoredns-6d8c4cb4d-ctqw5 1/1 Running 0 52mcoredns-6d8c4cb4d-n52fq 1/1 Running 0 52metcd-k8s-master 1/1 Running 0 53mkube-apiserver-k8s-master 1/1 Running 0 53mkube-controller-manager-k8s-master 1/1 Running 0 53mkube-proxy-swpkz 1/1 Running 0 52mkube-scheduler-k8s-master 1/1 Running 0 53m[root@node1 home]# [root@node1 home]# 获取主节点的状态[root@node1 home]# kubectl get csWarning: v1 ComponentStatus is deprecated in v1.19+NAME STATUS MESSAGE ERRORcontroller-manager Healthy okscheduler Healthy oketcd-0 Healthy &#123;"health":"true","reason":""&#125;[root@node1 home]# kubectl get nodeNAME STATUS ROLES AGE VERSIONnode1 Ready control-plane,master 52m v1.23.0查看节点状态（此时还只有主节点，还没添加子节点）：123[root@node1 home]# kubectl get nodeNAME STATUS ROLES AGE VERSIONnode1 Ready control-plane,master 53m v1.23.0至此 K8s master主服务器 已经部署完成！子节点加入集群（在子节点上操作）初始化会生成join命令，需要在子节点执行即可，以下token作为举例，以实际为主，例如：1234567891011121314[root@node2 home]# kubeadm join 192.168.2.1:6443 --token ochspx.15in9qkiu5z8tx2y --discovery-token-ca-cert-hash sha256:1f31202107af96a07df9fd78c3aa9bb44fd40076ac123e8ff28d6ab691a02a31[preflight] Running pre-flight checks[preflight] Reading configuration from the cluster...[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"[kubelet-start] Starting the kubelet[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...This node has joined the cluster:* Certificate signing request was sent to apiserver and a response was received.* The Kubelet was informed of the new secure connection details.Run 'kubectl get nodes' on the control-plane to see this node join the cluster.默认的 join token 有效期限为24小时，当过期后该 token 就不能用了，这时需要重新创建 token，创建新的join token需要在主节点上创建，创建命令如下：1[root@node1 home]# kubeadm token create --print-join-command加入之后再在主节点查看集群中节点的状态（必须要都为Ready状态）：12345[root@node1 home]# kubectl get nodesNAME STATUS ROLES AGE VERSIONnode1 Ready control-plane,master 63m v1.23.0node2 Ready &lt;none&gt; 3m57s v1.23.0node3 Ready &lt;none&gt; 29s v1.23.0如果所有的节点STATUS都为Ready的话，那么到此，所有的子节点加入完成！删除子节点（在master主节点上操作）12345# kubectl drain &lt;node name&gt; --delete-local-data --force --ignore-daemonsets# 其中 &lt;node name&gt; 是在k8s集群中使用 &lt;kubectl get nodes&gt; 查询到的节点名称# 假设这里删除 node3 子节点[root@node1 home]# kubectl drain node3 --delete-local-data --force --ignore-daemonsets[root@node1 home]# kubectl delete node node3然后在删除的子节点上操作重置k8s（重置k8s会删除一些配置文件），这里在node3子节点上操作：123456789101112131415161718192021222324[root@node3 home]# # 子节点重置k8s[root@node3 home]# kubeadm reset[reset] WARNING: Changes made to this host by 'kubeadm init' or 'kubeadm join' will be reverted.[reset] Are you sure you want to proceed? [y/N]: y[preflight] Running pre-flight checksW0425 01:59:40.412616 15604 removeetcdmember.go:80] [reset] No kubeadm config, using etcd pod spec to get data directory[reset] No etcd config found. Assuming external etcd[reset] Please, manually reset etcd to prevent further issues[reset] Stopping the kubelet service[reset] Unmounting mounted directories in "/var/lib/kubelet"[reset] Deleting contents of config directories: [/etc/kubernetes/manifests /etc/kubernetes/pki][reset] Deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf][reset] Deleting contents of stateful directories: [/var/lib/kubelet /var/lib/dockershim /var/run/kubernetes /var/lib/cni]The reset process does not clean CNI configuration. To do so, you must remove /etc/cni/net.dThe reset process does not reset or clean up iptables rules or IPVS tables.If you wish to reset iptables, you must do so manually by using the "iptables" command.If your cluster was setup to utilize IPVS, run ipvsadm --clear (or similar)to reset your system's IPVS tables.The reset process does not clean your kubeconfig files and you must remove them manually.Please, check the contents of the $HOME/.kube/config file.然后在被删除的子节点上手动删除k8s配置文件、flannel网络配置文件 和 flannel网口：1234567[root@node3 home]# rm -rf /etc/cni/net.d/[root@node3 home]# rm -rf /root/.kube/config[root@node3 home]# # 删除cni网络[root@node3 home]# ifconfig cni0 down[root@node3 home]# ip link delete cni0[root@node3 home]# ifconfig flannel.1 down[root@node3 home]# ip link delete flannel.1部署k8s dashboard（这里使用Kubepi）Kubepi是一个简单高效的k8s集群图形化管理工具，方便日常管理K8S集群，高效快速的查询日志定位问题的工具部署KubePI（随便在哪个节点部署，我这里在主节点部署）：123[root@node1 home]# docker pull kubeoperator/kubepi-server[root@node1 home]# # 运行容器[root@node1 home]# docker run --privileged -itd --restart=unless-stopped --name kube_dashboard -v /home/docker-mount/kubepi/:/var/lib/kubepi/ -p 8000:80 kubeoperator/kubepi-server登录：123# 地址: http://192.168.2.1:8000# 默认用户名：admin# 默认密码：kubepi填写集群名称，默认认证模式，填写apisever地址及token：kubepi导入集群:获取登录需要用到的ip地址和登录token：12345678910111213[root@node1 home]# # 在 k8s 主节点上创建用户，并获取token[root@node1 home]# kubectl create sa kubepi-user --namespace kube-systemserviceaccount/kubepi-user created[root@node1 home]# kubectl create clusterrolebinding kubepi-user --clusterrole=cluster-admin --serviceaccount=kube-system:kubepi-userclusterrolebinding.rbac.authorization.k8s.io/kubepi-user created[root@node1 home]# [root@node1 home]# # 在主节点上获取新建的用户 kubeapi-user 的 token[root@node1 home]# kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep kubepi-user | awk '&#123;print $1&#125;') | grep token: | awk '&#123;print $2&#125;'eyJhbGciOiJSUzI1NiIsImtpZCI6IkhVeUtyc1BpU1JvRnVacXVqVk1PTFRkaUlIZm1KQTV6Wk9WSExSRllmd0kifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJrdWJlcGktdXNlci10b2tlbi10cjVsMiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJrdWJlcGktdXNlciIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjJiYzlhZDRjLWVjZTItNDE2Mi04MDc1LTA2NTI0NDg0MzExZiIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTprdWJlcGktdXNlciJ9.QxkR1jBboqTYiVUUVO4yGhfWmlLDA5wHLo_ZnjAuSLZQDyVevCgBluL6l7y7UryRdId6FmBZ-L0QitvOuTsurcjGL2QHxPE_yZsNW7s9K7eikxJ8q-Q_yOvnADtAueH_tcMGRGW9Zyec2TlmcGTZCNaNUme84TfMlWqX7oP3GGJGMbMGN7H4fPXh-Qqrdp-0MJ3tP-dk3koZUEu3amrq8ExSmjIAjso_otrgFWbdSOMkCXKsqb9yuZzaw7u5Cy18bH_HW6RbNCRT5jGs5aOwzuMAd0HQ5iNm-5OISI4Da6jGdjipLXejcC1H-xWgLlJBx0RQWu41yoPNF57cG1NubQ[root@node1 home]# [root@node1 home]# # 在主节点上获取 apiserver 地址[root@node1 home]# cat ~/.kube/config | grep server: | awk '&#123;print $2&#125;'https://192.168.2.1:6443将上面获取的api地址和token填入页面即可，name可以自己随意取。到此，KubePI安装完成！安装metrics k8s集群监控插件k8s metrics插件提供了 top 命令可用于统计 k8s集群资源 的使用情况，它包含有 node 和 pod 两个⼦命令，分别显⽰ node 节点和 Pod 对象的资源使⽤信息。kubectl top 命令依赖于 metrics 接口。k8s 系统默认未安装该接口，需要单独部署：12[root@k8s-master k8s-install]# kubectl top poderror: Metrics API not available下载部署文件下载 metrics 接口的部署文件 metrics-server-components.yaml1234567891011121314151617181920[root@k8s-master k8s-install]# wget https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml -O metrics-server-components.yaml--2022-10-11 00:13:01-- https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml正在解析主机 github.com (github.com)... 20.205.243.166正在连接 github.com (github.com)|20.205.243.166|:443... 已连接。已发出 HTTP 请求，正在等待回应... 302 Found位置：https://github.com/kubernetes-sigs/metrics-server/releases/download/metrics-server-helm-chart-3.8.2/components.yaml [跟随至新的 URL]--2022-10-11 00:13:01-- https://github.com/kubernetes-sigs/metrics-server/releases/download/metrics-server-helm-chart-3.8.2/components.yaml再次使用存在的到 github.com:443 的连接。已发出 HTTP 请求，正在等待回应... 302 Found位置：https://objects.githubusercontent.com/github-production-release-asset-2e65be/92132038/d85e100a-2404-4c5e-b6a9-f3814ad4e6e5?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20221010%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20221010T161303Z&amp;X-Amz-Expires=300&amp;X-Amz-Signature=efa1ff5dd16b6cd86b6186adb3b4c72afed8197bdf08e2bffcd71b9118137831&amp;X-Amz-SignedHeaders=host&amp;actor_id=0&amp;key_id=0&amp;repo_id=92132038&amp;response-content-disposition=attachment%3B%20filename%3Dcomponents.yaml&amp;response-content-type=application%2Foctet-stream [跟随至新的 URL]--2022-10-11 00:13:02-- https://objects.githubusercontent.com/github-production-release-asset-2e65be/92132038/d85e100a-2404-4c5e-b6a9-f3814ad4e6e5?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20221010%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20221010T161303Z&amp;X-Amz-Expires=300&amp;X-Amz-Signature=efa1ff5dd16b6cd86b6186adb3b4c72afed8197bdf08e2bffcd71b9118137831&amp;X-Amz-SignedHeaders=host&amp;actor_id=0&amp;key_id=0&amp;repo_id=92132038&amp;response-content-disposition=attachment%3B%20filename%3Dcomponents.yaml&amp;response-content-type=application%2Foctet-stream正在解析主机 objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...正在连接 objects.githubusercontent.com (objects.githubusercontent.com)|185.199.109.133|:443... 已连接。已发出 HTTP 请求，正在等待回应... 200 OK长度：4181 (4.1K) [application/octet-stream]正在保存至: “metrics-server-components.yaml”100%[============================================================================================================================&gt;] 4,181 --.-K/s 用时 0.01s 2022-10-11 00:13:10 (385 KB/s) - 已保存 “metrics-server-components.yaml” [4181/4181])修改镜像地址将部署文件中镜像地址修改为国内的地址。大概在部署文件的第 140 行。原配置是：1image: k8s.gcr.io/metrics-server/metrics-server:v0.6.1修改后的配置是：1image: registry.cn-hangzhou.aliyuncs.com/google_containers/metrics-server:v0.6.1可使用如下命令实现修改：1sed -i &apos;s/k8s.gcr.io\/metrics-server/registry.cn-hangzhou.aliyuncs.com\/google_containers/g&apos; metrics-server-components.yaml部署 metrics 接口12345678910[root@k8s-master k8s-install]# kubectl create -f metrics-server-components.yaml serviceaccount/metrics-server createdclusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader createdclusterrole.rbac.authorization.k8s.io/system:metrics-server createdrolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader createdclusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator createdclusterrolebinding.rbac.authorization.k8s.io/system:metrics-server createdservice/metrics-server createddeployment.apps/metrics-server createdapiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created查看该 metric pod 的运行情况：12[root@k8s-master k8s-install]# kubectl get pods --all-namespaces | grep metricskube-system metrics-server-6ffc8966f5-84hbb 0/1 Running 0 2m23s查看该 pod 的情况，发现是探针问题：Readiness probe failed: HTTP probe failed with statuscode: 5001234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495[root@k8s-master k8s-install]# kubectl describe pod metrics-server-6ffc8966f5-84hbb -n kube-systemName: metrics-server-6ffc8966f5-84hbbNamespace: kube-systemPriority: 2000000000Priority Class Name: system-cluster-criticalNode: k8s-slave2/192.168.100.22Start Time: Tue, 11 Oct 2022 00:27:33 +0800Labels: k8s-app=metrics-server pod-template-hash=6ffc8966f5Annotations: &lt;none&gt;Status: RunningIP: 10.244.2.9IPs: IP: 10.244.2.9Controlled By: ReplicaSet/metrics-server-6ffc8966f5Containers: metrics-server: Container ID: docker://e913a075e0381b98eabfb6e298f308ef69dfbd7c672bdcfb75bb2ff3e4b5a0a4 Image: registry.cn-hangzhou.aliyuncs.com/google_containers/metrics-server:v0.6.1 Image ID: docker-pullable://registry.cn-hangzhou.aliyuncs.com/google_containers/metrics-server@sha256:5ddc6458eb95f5c70bd13fdab90cbd7d6ad1066e5b528ad1dcb28b76c5fb2f00 Port: 4443/TCP Host Port: 0/TCP Args: --cert-dir=/tmp --secure-port=4443 --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --kubelet-use-node-status-port --metric-resolution=15s State: Running Started: Tue, 11 Oct 2022 00:27:45 +0800 Ready: False Restart Count: 0 Requests: cpu: 100m memory: 200Mi Liveness: http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3 Readiness: http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3 Environment: &lt;none&gt; Mounts: /tmp from tmp-dir (rw) /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-x2spb (ro)Conditions: Type Status Initialized True Ready False ContainersReady False PodScheduled True Volumes: tmp-dir: Type: EmptyDir (a temporary directory that shares a pod&apos;s lifetime) Medium: SizeLimit: &lt;unset&gt; kube-api-access-x2spb: Type: Projected (a volume that contains injected data from multiple sources) TokenExpirationSeconds: 3607 ConfigMapName: kube-root-ca.crt ConfigMapOptional: &lt;nil&gt; DownwardAPI: trueQoS Class: BurstableNode-Selectors: kubernetes.io/os=linuxTolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300sEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 7m27s default-scheduler Successfully assigned kube-system/metrics-server-6ffc8966f5-84hbb to k8s-slave2 Normal Pulling 7m26s kubelet Pulling image &quot;registry.cn-hangzhou.aliyuncs.com/google_containers/metrics-server:v0.6.1&quot; Normal Pulled 7m15s kubelet Successfully pulled image &quot;registry.cn-hangzhou.aliyuncs.com/google_containers/metrics-server:v0.6.1&quot; in 10.976606194s Normal Created 7m15s kubelet Created container metrics-server Normal Started 7m15s kubelet Started container metrics-server Warning Unhealthy 2m17s (x31 over 6m47s) kubelet Readiness probe failed: HTTP probe failed with statuscode: 500进而查看 pod 的日志：[root@k8s-master k8s-install]# kubectl logs metrics-server-6ffc8966f5-84hbb -n kube-system I1010 16:27:46.228594 1 serving.go:342] Generated self-signed cert (/tmp/apiserver.crt, /tmp/apiserver.key)I1010 16:27:46.633494 1 secure_serving.go:266] Serving securely on [::]:4443I1010 16:27:46.633585 1 requestheader_controller.go:169] Starting RequestHeaderAuthRequestControllerI1010 16:27:46.633616 1 shared_informer.go:240] Waiting for caches to sync for RequestHeaderAuthRequestControllerI1010 16:27:46.633653 1 dynamic_serving_content.go:131] &quot;Starting controller&quot; name=&quot;serving-cert::/tmp/apiserver.crt::/tmp/apiserver.key&quot;I1010 16:27:46.634221 1 tlsconfig.go:240] &quot;Starting DynamicServingCertificateController&quot;W1010 16:27:46.634296 1 shared_informer.go:372] The sharedIndexInformer has started, run more than once is not allowedI1010 16:27:46.634365 1 configmap_cafile_content.go:201] &quot;Starting controller&quot; name=&quot;client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file&quot;I1010 16:27:46.634370 1 shared_informer.go:240] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-fileI1010 16:27:46.634409 1 configmap_cafile_content.go:201] &quot;Starting controller&quot; name=&quot;client-ca::kube-system::extension-apiserver-authentication::client-ca-file&quot;I1010 16:27:46.634415 1 shared_informer.go:240] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-fileE1010 16:27:46.641663 1 scraper.go:140] &quot;Failed to scrape node&quot; err=&quot;Get \&quot;https://192.168.100.22:10250/metrics/resource\&quot;: x509: cannot validate certificate for 192.168.100.22 because it doesn&apos;t contain any IP SANs&quot; node=&quot;k8s-slave2&quot;E1010 16:27:46.645389 1 scraper.go:140] &quot;Failed to scrape node&quot; err=&quot;Get \&quot;https://192.168.100.20:10250/metrics/resource\&quot;: x509: cannot validate certificate for 192.168.100.20 because it doesn&apos;t contain any IP SANs&quot; node=&quot;k8s-master&quot;E1010 16:27:46.652261 1 scraper.go:140] &quot;Failed to scrape node&quot; err=&quot;Get \&quot;https://192.168.100.21:10250/metrics/resource\&quot;: x509: cannot validate certificate for 192.168.100.21 because it doesn&apos;t contain any IP SANs&quot; node=&quot;k8s-slave1&quot;I1010 16:27:46.733747 1 shared_informer.go:247] Caches are synced for RequestHeaderAuthRequestController I1010 16:27:46.735167 1 shared_informer.go:247] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file I1010 16:27:46.735194 1 shared_informer.go:247] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file E1010 16:28:01.643646 1 scraper.go:140] &quot;Failed to scrape node&quot; err=&quot;Get \&quot;https://192.168.100.22:10250/metrics/resource\&quot;: x509: cannot validate certificate for 192.168.100.22 because it doesn&apos;t contain any IP SANs&quot; node=&quot;k8s-slave2&quot;E1010 16:28:01.643805 1 scraper.go:140] &quot;Failed to scrape node&quot; err=&quot;Get \&quot;https://192.168.100.21:10250/metrics/resource\&quot;: x509: cannot validate certificate for 192.168.100.21 because it doesn&apos;t contain any IP SANs&quot; node=&quot;k8s-slave1&quot;E1010 16:28:01.646721 1 scraper.go:140] &quot;Failed to scrape node&quot; err=&quot;Get \&quot;https://192.168.100.20:10250/metrics/resource\&quot;: x509: cannot validate certificate for 192.168.100.20 because it doesn&apos;t contain any IP SANs&quot; node=&quot;k8s-master&quot;I1010 16:28:13.397373 1 server.go:187] &quot;Failed probe&quot; probe=&quot;metric-storage-ready&quot; err=&quot;no metrics to serve&quot;基本可以确定 pod 异常是因为：Readiness Probe 探针检测到 Metris 容器启动后对 http Get 探针存活没反应，具体原因是：cannot validate certificate for 192.168.100.22 because it doesn’t contain any IP SANs” node=”k8s-slave2”查看 metrics-server 的文档（https://github.com/kubernetes…），有如下一段说明：12Kubelet certificate needs to be signed by cluster Certificate Authority (or disable certificate validation by passing --kubelet-insecure-tls to Metrics Server)意思是：kubelet 证书需要由集群证书颁发机构签名(或者通过向 Metrics Server 传递参数 –kubelet-insecure-tls 来禁用证书验证)。由于是测试环境，我们选择使用参数禁用证书验证，生产环境不推荐这样做！！！在大概 139 行的位置追加参数：123456789spec: containers: - args: - --cert-dir=/tmp - --secure-port=4443 - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname - --kubelet-use-node-status-port - --metric-resolution=15s - --kubelet-insecure-tlsapply 部署文件：12345678910111213141516171819[root@k8s-master k8s-install]# kubectl apply -f metrics-server-components.yamlWarning: resource serviceaccounts/metrics-server is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.serviceaccount/metrics-server configuredWarning: resource clusterroles/system:aggregated-metrics-reader is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader configuredWarning: resource clusterroles/system:metrics-server is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.clusterrole.rbac.authorization.k8s.io/system:metrics-server configuredWarning: resource rolebindings/metrics-server-auth-reader is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader configuredWarning: resource clusterrolebindings/metrics-server:system:auth-delegator is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator configuredWarning: resource clusterrolebindings/system:metrics-server is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server configuredWarning: resource services/metrics-server is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.service/metrics-server configuredWarning: resource deployments/metrics-server is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.deployment.apps/metrics-server configuredWarning: resource apiservices/v1beta1.metrics.k8s.io is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io configuredmetrics pod 已经正常运行：12[root@k8s-master k8s-install]# kubectl get pod -A | grep metricskube-system metrics-server-fd9598766-8zphn 1/1 Running 0 89s再次执行 kubectl top 命令成功：12345678910111213141516[root@k8s-master k8s-install]# kubectl top podNAME CPU(cores) MEMORY(bytes) front-end-59bc6df748-699vb 0m 3Mi front-end-59bc6df748-r7pkr 0m 3Mi kucc4 1m 2Mi legacy-app 1m 1Mi my-demo-nginx-998bbf8f5-9t9pw 0m 0Mi my-demo-nginx-998bbf8f5-lfgvw 0m 0Mi my-demo-nginx-998bbf8f5-nfn7r 1m 0Mi nginx-kusc00401 0m 3Mi[root@k8s-master k8s-install]# [root@k8s-master k8s-install]# kubectl top nodeNAME CPU(cores) CPU% MEMORY(bytes) MEMORY% k8s-master 232m 5% 1708Mi 46% k8s-slave1 29m 1% 594Mi 34% k8s-slave2 25m 1% 556Mi 32%k8s常用命令集合k8s常用命令集合：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172# 查看当前集群的所有的节点kubectl get node# 显示 Node 的详细信息（一般用不着）kubectl describe node node1# 查看所有的podkubectl get pod --all-namespaces# 查看pod的详细信息kubectl get pods -o wide --all-namespaces# 查看所有创建的服务kubectl get service# 查看所有的deploykubectl get deploy# 重启 pod（这个方式会删除原来的pod，然后再重新生成一个pod达到重启的目的）# 有yaml文件的重启kubectl replace --force -f xxx.yaml# 无yaml文件的重启kubectl get pod &lt;POD_NAME&gt; -n &lt;NAMESPACE&gt; -o yaml | kubectl replace --force -f -# 查看pod的详细信息kubectl describe pod nfs-client-provisioner-65c77c7bf9-54rdp -n default# 根据 yaml 文件创建Pod资源kubectl apply -f pod.yaml# 删除基于 pod.yaml 文件定义的Pod kubectl delete -f pod.yaml#获取 Kubernetes 集群中的 Pod 列表，并将详细结果显示为宽表格kubectl get pod -o wide#基于现有的pod导出yaml文件kubectl get pod &lt;pod_name&gt; -o yaml &gt; pod.yaml#根据CPU使用率自动扩展 Pod 的数量kubectl autoscale deployment &lt;deployment_name&gt; --min=&lt;min_replicas&gt; --max=&lt;max_replicas&gt; --cpu-percent=&lt;cpu_percent&gt;# 查看容器的日志kubectl logs &lt;pod-name&gt;# 实时查看日志kubectl logs -f &lt;pod-name&gt;# 若 pod 只有一个容器，可以不加 -ckubectl log &lt;pod-name&gt; -c &lt;container_name&gt;# 返回所有标记为 app=frontend 的 pod 的合并日志kubectl logs -l app=frontend# 通过bash获得 pod 中某个容器的TTY，相当于登录容器# kubectl exec -it &lt;pod-name&gt; -c &lt;container-name&gt; -- basheg:kubectl exec -it redis-master-cln81 -- bash# 查看 endpoint 列表kubectl get endpoints# 查看已有的tokenkubeadm token list#滚动升级（在yaml文件里修改新的images）kubectl apply -f svc-zipkin.yaml --record或者直接用命令，更新名为my-deployment的部署中my-container容器的镜像kubectl set image deployment/my-deployment my-container=my-image:my-tag#暂停升级kubectl rollout pause deployment zipkin-server#继续升级kubectl rollout resume deployment zipkin-server#查看升级历史kubectl rollout history deployment zipkin-server#回滚到上一级kubectl rollout undo deployment zipkin-server#回滚制定版本（根据rollout history的查看结果）kubectl rollout undo deployment zipkin-server --to-revision=13]]></content>
      <categories>
        <category>技术</category>
        <category>K8S</category>
      </categories>
      <tags>
        <tag>K8S</tag>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[K8S入门]]></title>
    <url>%2F2023%2F11fbdfb25a.html</url>
    <content type="text"><![CDATA[Kubernetes（k8s）作为云原生的核心平台，吸引了越来越多的运维、开发、测试以及其他技术员去了解学习。随着行业越来越内卷，k8s已经被广泛使用。什么是K8s它前生是谷歌的Borg系统，后经过Go语言重写，在2014 年开源了 Kubernetes 项目,并捐献给CNCF 基金会开源,即Kubernetes，之所以简称k8s，因为 Kubernetes 中间有 8个字母。K8s是一个可移植的、用于自动化部署、扩展和管理容器化应用的开源容器编排技术。K8s使部署和管理微服务架构应用程序变得很简单。它通过在集群之上形成一个抽象层来实现这一点，允许开发团队平滑地部署应用程序，而 K8s主要处理以下任务：控制和管理应用程序对资源的使用自动负载均衡应用程序的多个实例之间请求监控资源使用和资源限制，为了可以自动阻止应用消耗过多的资源并且可以再次恢复它们如果主机资源耗尽或主机死机，将应用程序实例从一台主机迁移到另一台主机是一个可行的选项当有新的主机加入集群时，新增加的额外资源可以被自动使用K8s为何被广泛应用为什么现在有很多企业依赖K8s来满足他们的容器编排需求? 因为k8s在Docker技术的基础上，为容器化的应用提供部署运行、资源调度、服务发现和动态伸缩等一系列完整功能，提高了大规模容器集群管理的便捷性。同时Kubernetes是一个完备的分布式系统支撑平台，具有完备的集群管理能力，多扩多层次的安全防护和准入机制、多租户应用支撑能力、透明的服务注册和发现机制、內建智能负载均衡器、强大的故障发现和自我修复能力、服务滚动升级和在线扩容能力、可扩展的资源自动调度机制以及多粒度的资源配额管理能力。同时Kubernetes提供完善的管理工具，涵盖了包括开发、部署测试、运维监控在内的各个环节。概括为：可移植性和灵活性： K8s有很强的兼容性，因为它可以在各种基础设施和环境设置下运行。大多数其他编排器没有这种灵活性。开源： CNCF负责管理K8s，这是一个完全开源、由社区驱动的项目。它有许多重要的企业赞助商，但没有一家公司能“控制”这个平台或者控制它的发展方向。多云兼容性： K8s不但可以将工作负载托管在单个云上，而且可以将工作负载分布在多个云上。 K8s也能轻松地将其环境从一个云扩展到另一个云。虽然其它编排器也能支持多云架构，但K8s在多云兼容性性方面可以完全超越它们。市场领导者： 大部分公司都在使用K8s，根据红帽公司的一项调查，K8s被客户广泛使用 (88%)，尤其在生产环境中(74%)。K8s架构K8s是一个架构良好的分布式系统的例子，它将集群中的所有机器都视为单个资源池的一部分。K8s与其他成熟的分布式系统一样，主要是由控制节点(master)、工作节点(node)构成，每个节点上都会安装不同的组件。控制节点MasterK8S中的Master是集群控制节点，负责整个集群的管理和控制ApiServer : 资源操作的唯一入口，接收用户输入的命令，提供认证、授权、API注册和发现等机制，其他模块通过API Server查询或修改数据，只有API Server才直接和etcd进行交互；Scheduler : 负责集群资源调度，通过API Server的Watch接口监听新建Pod副本信息，按照预定的调度策略将Pod调度到相应的node节点上；ControllerManager : K8S里所有资源对象的自动化控制中心，通过 api-server 提供的 restful 接口实时监控集群内每个资源对象的状态，发生故障时，导致资源对象的工作状态发生变化，就进行干预，尝试将资源对象从当前状态恢复为预期的工作状态，常见的 controller 有 Namespace Controller、Node Controller、Service Controller、ServiceAccount Controller、Token Controller、ResourceQuote Controller、Replication Controller等；Etcd ： 是Kubernetes的存储状态的数据库（所有master的持续状态都存在etcd的一个实例中）工作节点NodeNode是K8S集群中的工作负载节点，每个Node都会被Master分配一些工作负载，当某个Node宕机时，其上的工作负载会被Master自动转移到其他节点上Kubelet : 负责维护容器的生命周期，即通过控制docker，控制Pod 的创建、启动、监控、重启、销毁等工作，处理Master节点下发到本节点的任务；KubeProxy : 负责制定数据包的转发策略，并以守护进程的模式对各个节点的pod信息实时监控并更新转发规则，service收到请求后会根据kube-proxy制定好的策略来进行请求的转发，从而实现负载均衡，总的来说，负责为Service提供cluster内部的服务发现和负载均衡；Docker : 负责节点上容器的各种操作；其他组件学习kubernetes的核心，就是学习如何对集群上的Pod、Pod控制器、Service、存储等各种资源进行操作Pod： kubernetes的最小控制单元，容器都是运行在pod中的，一个pod中可以有1个或者多个容器Controller： 控制器，通过它来实现对pod的管理，比如启动pod、停止pod、伸缩pod的数量等等Service： pod对外服务的统一入口，下面可以维护者同一类的多个podLabel： 标签，用于对pod进行分类，同一类pod会拥有相同的标签NameSpace： 命名空间，用来隔离pod的运行环境kubernetes在集群启动之后，会默认创建几个namespace:default、kube-node-lease、kube-public、kube-system。默认情况下，kubernetes集群中的所有的Pod都是可以相互访问的，但是在实际中，可能不想让两个Pod之间进行互相的访问，那此时就可以将两个Pod划分到不同的namespace下。kubernetes通过将集群内部的资源分配到不同的Namespace中，可以形成逻辑上的”组”，以方便不同的组的资源进行隔离使用和管理。部署实例nginx服务部署为了方便理解，我们部署一个nginx服务来说明kubernetes系统各个组件调用关系：首先要了解，一旦kubernetes环境启动之后，master和node都会将自身的信息存储到etcd数据库中。一个nginx服务的安装请求会首先被发送到master节点的apiServer组件，apiServer组件会调用scheduler组件来决定到底应该把这个服务安装到哪个node节点上。在此时，它会从etcd中读取各个node节点的信息，然后按照一定的算法进行选择，并将结果告知apiServer，apiServer调用controller-manager去调度Node节点安装nginx服务，kubelet接收到指令后，会通知docker，然后由docker来启动一个nginx的pod。pod是kubernetes的最小操作单元，容器必须跑在pod中。至此，一个nginx服务就运行了，如果需要访问nginx，就需要通过kube-proxy来对pod产生访问的代理，这样，外界用户就可以访问集群中的nginx服务了。Pod创建流程1、客户端提交创建请求，可以通过API Server的Restful API，也可以使用kubectl命令行工具。支持的数据类型包括JSON和YAML。2、API Server处理用户请求，存储Pod数据到etcd。3、调度器通过API Server查看未绑定的Pod。尝试为Pod分配主机。4、过滤主机 (调度预选)：调度器用一组规则过滤掉不符合要求的主机。比如Pod指定了所需要的资源量，那么可用资源比Pod需要的资源量少的主机会被过滤掉。5、主机打分(调度优选)：对第一步筛选出的符合要求的主机进行打分，在主机打分阶段，调度器会考虑一些整体优化策略，比如把容一个Replication Controller的副本分布到不同的主机上，使用最低负载的主机等。6、选择主机：选择打分最高的主机，进行binding操作，结果存储到etcd中。7、kubelet根据调度结果执行Pod创建操作： 绑定成功后，scheduler会调用APIServer的API在etcd中创建一个boundpod对象，描述在一个工作节点上绑定运行的所有pod信息。运行在每个工作节点上的kubelet也会定期与etcd同步boundpod信息，一旦发现应该在该工作节点上运行的boundpod对象没有更新，则调用Docker API创建并启动pod内的容器。在整个生命周期中，Pod会出现5种状态（相位），分别如下：挂起(Pending): apiserver已经创建了pod资源对象，但它尚未被调度完成或者仍处于下载镜像的过程中运行中(Running): pod已经被调度至某节点，并且所有容器都已经被kubelet创建完成成功(Succeeded): pod中的所有容器都已经成功终止并且不会被重启失败(Failed): 所有容器都已经终止，但至少有一个容器终止失败，即容器返回了非0值的退出状态未知(Unknown): apiserver无法正常获取到pod对象的状态信息，通常由网络通信失败所导致K8s常用命令kubectl 命令的语法如下：1kubectl [command] [TYPE] [NAME] [flags]kubectl命令中，指定执行什么操作（command，如create,delete,describe ,get ,apply 等）指定什么类型资源对象（type）指定此类型的资源对象名称（name）指定可选参数（flags）查看pod，nodee，service，endpoints，secret等信息1kubectl get 组件名 #例如kubectl get pod查看资源状态，比如有一组deployment内的pod没起来，一般用于pod调度过程出现的问题排查1kubectl describe pod pod名 #先用kubectl get pod查看查看node节点或者是pod资源（cpu，内存资源）使用情况1kubectl top 组件名 #例如kubectl top node kubectl top pod进入pod内部1kubectl exec -ti pod名 /bin/bash #先用kubectl get pod查看删除pod1kubectl delete pod -n查看集群健康状态1kubectl get cs基于 pod.yaml 定义的名称删除指定资源1kubectl delete -f pod.yaml查看容器的日志1kubectl logs -f &lt;pod-name&gt; # 实时查看日志创建资源1kubectl apply -f ./my-manifest.yaml]]></content>
      <categories>
        <category>技术</category>
        <category>K8S</category>
      </categories>
      <tags>
        <tag>K8S</tag>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis哨兵模式部署]]></title>
    <url>%2F2023%2F10b68cbed0.html</url>
    <content type="text"><![CDATA[上一篇文章讲解了Redis哨兵模式的原理，今天来实战一下，部署Redis哨兵高可用。下载1wget http://download.redis.io/releases/redis-5.0.14.tar.gz解压文件1tar -zxvf redis-5.0.14.tar.gz -C /opt/安装1234cd redis-5.0.14/makemkdir -p /opt/redismake PREFIX=/opt/redis install拷贝配置文件123456cd /opt/redismkdir /opt/redis/datamkdir /opt/redis/logsmkdir /opt/redis/confcp ../redis-5.0.14/*.conf ./conf/三台机器配置redis.conf修改以下项目123456789101112131415161718192021222324252627282930313233343536373839# 第一台机器（master）bind 192.168.233.50port 6379pidfile /opt/redis/data/redis.pidlogfile /opt/redis/logs/redis.logdir /opt/redis/data/daemonize yes #后台启动protected-mode no# replicaof 192.168.233.50 6379 # master不要配这个replica-read-only yes # slave只读，不能写操作requirepass redis # 给redis设置一个密码，连接时需要指定masterauth redis # 和requirepass密码一样# 第二台机器（slave）bind 192.168.233.51port 6379pidfile /opt/redis/data/redis.pidlogfile /opt/redis/logs/redis.logdir /opt/redis/data/daemonize yes #后台启动protected-mode noreplicaof 192.168.233.50 6379 # 指定masterreplica-read-only yes # slave只读，不能写操作requirepass redis # 给redis设置一个密码，连接时需要指定masterauth redis # 和requirepass密码一样# 第三台机器（slave）bind 192.168.233.52port 6379pidfile /opt/redis/data/redis.pidlogfile /opt/redis/logs/redis.logdir /opt/redis/data/daemonize yes #后台启动protected-mode noreplicaof 192.168.233.50 6379 # 指定masterreplica-read-only yes # slave只读，不能写操作requirepass redis # 给redis设置一个密码，连接时需要指定masterauth redis # 和requirepass密码一样三台机器配置sentinel.conf12345678910111213141516171819202122232425262728293031bind 192.168.233.50port 26379 #Sentinel使用端口daemonize yes #守护线程启动(即后台启动)# 保护模式关闭，这样其他服务起就可以访问此台redisprotected-mode nopidfile /opt/redis/data/redis-sentinel.pidlogfile /opt/redis/logs/redis-sentinel.logdir /opt/redis/data # 工作目录#重要的来了#sentinel monitor &lt;master-name&gt; &lt;ip&gt; &lt;redis-port&gt; &lt;quorum&gt;#告诉sentinel去监听地址为ip:port的一个master,这里的master-name可以自定义,quorum是一个数字,指明当有多少个sentinel认为一个master失效时,master才算真正失效.需要注意的是master-ip sentinel monitor master1 192.168.233.50 6379 2sentinel auth-pass master1 redis # master中redis的密码#sentinel down-after-milliseconds &lt;master-name&gt; &lt;milliseconds&gt; #这个配置项指定需要多少时间无响应,一个master才会被这个sentinel主观地认为是不可用的.单位是毫秒,默认为30秒sentinel down-after-milliseconds master1 30000#sentinel parallel-syncs &lt;master-name&gt; &lt;numslaves&gt; #这个配置项指定了在发生failover主备切换时最多可以有多少个slave同时对新的master进行同步,这个数字越小,完成failover所需的时间就越长,但是如果这个数字越大,就意味着越 多的slave因为replication而不可用.可以通过将这个值设为1(默认就是1)来保证每次只有一个slave处于不能处理命令请求的状态sentinel parallel-syncs master1 1#sentinel failover-timeout &lt;master-name&gt; &lt;milliseconds&gt;# failover过期时间，当failover开始后，在此时间内仍然没有触发任何failover操作，当前sentinel 将会认为此次failover失败,默认为3分钟,单位为毫秒sentinel failover-timeout master1 180000#是否拒绝从新配置通知脚本,默认拒绝(yes).# sentinel deny-scripts-reconfig yes启动reids和redis-sentinel123456789101112131415161718192021cd /opt/redisbin/redis-server ../conf/redis.confbin/redis-sentinel ../conf/sentinel.confps -ef|grep redisroot 5155 1 0 4月18 ? 00:02:20 bin/redis-server 192.168.233.52:6379root 19703 1 0 10:52 ? 00:00:00 bin/redis-sentinel 192.168.233.50:26379 [sentinel]root 19708 18564 0 10:52 pts/0 00:00:00 grep --color=auto redisredis]# cat logs/redis-sentinel.log 20202:X 19 Apr 2023 11:32:17.990 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo20202:X 19 Apr 2023 11:32:17.990 # Redis version=5.0.14, bits=64, commit=00000000, modified=0, pid=20202, just started20202:X 19 Apr 2023 11:32:17.990 # Configuration loaded20203:X 19 Apr 2023 11:32:17.994 * Increased maximum number of open files to 10032 (it was originally set to 1024).20203:X 19 Apr 2023 11:32:17.996 * Running mode=sentinel, port=26379.20203:X 19 Apr 2023 11:32:17.996 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.20203:X 19 Apr 2023 11:32:17.997 # Sentinel ID is b380ad31e39697f72ffaac428a82d3d6ac21062d20203:X 19 Apr 2023 11:32:17.997 # +monitor master master1 192.168.233.50 6379 quorum 220203:X 19 Apr 2023 11:32:48.058 # +sdown master master1 192.168.233.50 637920203:X 19 Apr 2023 11:32:48.058 # +sdown slave 192.168.233.52:6379 192.168.233.52 6379 @ master1 192.168.233.50 637920203:X 19 Apr 2023 11:32:48.058 # +sdown slave 192.168.233.51:6379 192.168.233.51 6379 @ master1 192.168.233.50 6379登录redis客户端查看主从关系123456789101112131415161718/opt/redis/bin/redis-cli -a redis -h hadoop01 -p 6379Warning: Using a password with '-a' or '-u' option on the command line interface may not be safe.hadoop01:6379&gt; hadoop01:6379&gt; hadoop01:6379&gt; info replication# Replicationrole:masterconnected_slaves:2slave0:ip=192.168.233.52,port=6379,state=online,offset=14,lag=0slave1:ip=192.168.233.51,port=6379,state=online,offset=14,lag=0master_replid:19cd44d6abe9c6a872863e713c25869e4a590998master_replid2:0000000000000000000000000000000000000000master_repl_offset:14second_repl_offset:-1repl_backlog_active:1repl_backlog_size:1048576repl_backlog_first_byte_offset:1repl_backlog_histlen:14测试master切换123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# 1.正常启动后，hadoop01为master/opt/redis/bin/redis-cli -a redis -h hadoop01 -p 6379hadoop01:6379&gt; info replication# Replicationrole:masterconnected_slaves:2slave0:ip=192.168.233.51,port=6379,state=online,offset=6654076,lag=0slave1:ip=192.168.233.52,port=6379,state=online,offset=6653769,lag=0master_replid:94a4846bcad96026bb82b9eca0f32d58be74a1cfmaster_replid2:ea15cbfd6f165275295a66b24ecf6aacc49423acmaster_repl_offset:6654218second_repl_offset:6620735repl_backlog_active:1repl_backlog_size:1048576repl_backlog_first_byte_offset:5605643repl_backlog_histlen:1048576# 2.master掉线后，hadoop03变为master/opt/redis/bin/redis-cli -a redis -h hadoop03 -p 6379hadoop03:6379&gt; info replication# Replicationrole:masterconnected_slaves:1slave0:ip=192.168.233.51,port=6379,state=online,offset=6732533,lag=0master_replid:81d768dfcdca5c707f3938b544705bd7d18e867fmaster_replid2:94a4846bcad96026bb82b9eca0f32d58be74a1cfmaster_repl_offset:6732675second_repl_offset:6727797repl_backlog_active:1repl_backlog_size:1048576repl_backlog_first_byte_offset:6653770repl_backlog_histlen:78906# 2.查看sentinel日志可以看到master切换过程tail -f logs/redis-sentinel.log 20944:X 20 Apr 2023 17:09:08.644 # +sdown master master1 192.168.233.50 637920944:X 20 Apr 2023 17:09:08.672 # +new-epoch 420944:X 20 Apr 2023 17:09:08.673 # +vote-for-leader 5896ea07aa70daf08c9d84c040bcaafecf10b041 420944:X 20 Apr 2023 17:09:08.698 # +odown master master1 192.168.233.50 6379 #quorum 3/220944:X 20 Apr 2023 17:09:08.698 # Next failover delay: I will not start a failover before Thu Apr 20 17:15:09 202320944:X 20 Apr 2023 17:09:09.478 # +config-update-from sentinel 5896ea07aa70daf08c9d84c040bcaafecf10b041 192.168.233.50 26379 @ master1 192.168.233.50 637920944:X 20 Apr 2023 17:09:09.478 # +switch-master master1 192.168.233.50 6379 192.168.233.52 637920944:X 20 Apr 2023 17:09:09.478 * +slave slave 192.168.233.51:6379 192.168.233.51 6379 @ master1 192.168.233.52 637920944:X 20 Apr 2023 17:09:09.478 * +slave slave 192.168.233.50:6379 192.168.233.50 6379 @ master1 192.168.233.52 637920944:X 20 Apr 2023 17:09:39.544 # +sdown slave 192.168.233.50:6379 192.168.233.50 6379 @ master1 192.168.233.52 6379# 3.再次启动hadoop01/opt/redis/bin/redis-cli -a redis -h hadoop03 -p 6379hadoop03:6379&gt; info replication# Replicationrole:masterconnected_slaves:2slave0:ip=192.168.233.51,port=6379,state=online,offset=6773718,lag=0slave1:ip=192.168.233.50,port=6379,state=online,offset=6773718,lag=0master_replid:81d768dfcdca5c707f3938b544705bd7d18e867fmaster_replid2:94a4846bcad96026bb82b9eca0f32d58be74a1cfmaster_repl_offset:6773860second_repl_offset:6727797repl_backlog_active:1repl_backlog_size:1048576repl_backlog_first_byte_offset:6653770repl_backlog_histlen:120091]]></content>
      <categories>
        <category>技术</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
        <tag>哨兵模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis哨兵模式详解]]></title>
    <url>%2F2023%2F104a1a49cf.html</url>
    <content type="text"><![CDATA[Redis 的主从复制模式下，一旦主节点由于故障不能提供服务，需要手动将从节点晋升为 主节点，同时还要通知客户端更新主节点地址，这种故障处理方式从一定程度上是无法接受的。Redis 2.8 以后提供了 Redis Sentinel 哨兵机制 来解决这个问题。哨兵简介哨兵的组成Redis 哨兵（Sentinel）是由一个或多个 Sentinel 实例组成的 Sentinel 系统，可以监视任意多个主服务器，以及这些主服务器的所有从服务器。Sentinel 本质上是一个运行在特殊状模式下的 Redis 服务器。Sentinel 模式下 Redis 服务器只支持 PING、SENTINEL、INFO、SUBSCRIBE、UNSUBSCRIBE、PSUBSCRIBE、PUNSUBSCRIBE 七个命令。哨兵的作用​ Redis官方文档的描述如下：监控（Monitoring）：哨兵会不断地检查主节点和从节点是否运作正常。自动故障转移（Automatic failover）：当主节点不能正常工作时，哨兵会开始自动故障转移操作，它会将失效主节点的其中一个从节点升级为新的主节点，并让其他从节点改为复制新的主节点。配置提供者（Configuration provider）：客户端在初始化时，通过连接哨兵来获得当前Redis服务的主节点地址。通知（Notification）：哨兵可以将故障转移的结果发送给客户端。​ 其中，监控和自动故障转移功能，使得哨兵可以及时发现主节点故障并完成转移；而配置提供者和通知功能，则需要在与客户端的交互中才能体现。哨兵的运行机制哨兵启动​ 当一个 Sentinel 启动时，会创建连向被监视的主服务器的网络连接。它可以向主服务器发送命令，并从命令回复中获取相关的信息。对于每个被 Sentinel 监视的主服务器，Sentinel 会创建两个连向主服务器的异步网络：​ 命令连接：用于向主服务器发送命令（包括ping、info），并接受命令回复，用来获取从节点的信息与Sentinel 、Redis节点的状态信息。​ 订阅连接：用于订阅主服务器的 sentinel:hello 频道，用来和其他Sentinel 实例建立连接。订阅频道​ Sentinel实例会向与其连接的节点（包括主从节点）的 sentinel:hello 频道发送消息， 并订阅该频道（SUBSCRIBE sentinel:hello）。这样一来，当后续的Sentinel实例与主库建立连接，发送消息时，之前订阅该频道的消息就能够收到新实例加入的消息，然后就可以从这个频道直接获取新实例的信息并建立网络连接。​ 以下图为例，哨兵 1 把自己的 IP（172.16.19.3）和端口（26579）发布到sentinel:hello频道上，哨兵 2 和 3 订阅了该频道。那么此时，哨兵 2 和 3 就可以从这个频道直接获取哨兵 1 的 IP 地址和端口号。然后，哨兵 2、3 可以和哨兵 1 建立网络连接。​ Sentinel 对 sentinel:hello 频道的订阅会一直持续到 Sentinel 与服务器断开连接为止。在在此之前Sentinel 会以每两秒一次的频率，通过命令向所有被监视的主服务器和从服务器发送自己的信息及主节点相关的配置。获取服务器信息​ Sentinel 向服务器（主从节点都有）发送 INFO 命令，获取主服务器及它的从服务器信息。获取主服务器信息​ Sentinel 默认会以每十秒一次的频率，通过命令连接向被监视的服务器发送 INFO 命令，并通过分析 INFO 命令的回复来获取主服务器的当前信息。主服务自身信息：包括 run_id 域记录的服务器运行 ID，以及 role 域记录的服务器角色主服务的从服务器信息：包括 IP 地址和端口号获取从服务器信息​ 当 Sentinel 发现主服务器有新的从服务器出现时，Sentinel 除了会为这个新的从服务器创建相应的实例结构之外，Sentinel 还会创建连接到从服务器的命令连接和订阅连接。检测服务器状态​ Sentinel 向 Redis 服务器发送PING命令，检查其状态。​ 每个 Sentinel 节点会以每秒一次的频率对 Redis 节点和 其它 的 Sentinel 节点发送 PING 命令，并通过节点的回复来判断节点是否在线。哨兵正是通过这个机制得以判断主节点是否下线，这将涉及到2个新的改变，主观下线与客观下线。​ 注意：一个有效的 PING 回复可以是：+PONG、-LOADING 或者 -MASTERDOWN。如果服务器返回除以上三种回复之外的其他回复，又或者在指定时间内没有回复 PING 命令， 那么 Sentinel 认为服务器返回的回复 无效（non-valid）。故障处理​ 在上文中，我们留了2个概念（主观下线与客观下线）没有解释，这一节我们结合哨兵对故障的处理进行讲解。主库下线判定主观下线​ 主观下线适用于所有主节点和从节点。如果最后一次有效回复PING命令的时间超过 down-after-milliseconds 毫秒，则会判定该节点为主观下线（SDOWN）。​ 注意：主从节点都可能被标记为主观下线，只不过只会对主机节点的客观下线进行下一步判断。在一般情况下，每个 Sentinel 会以每 10 秒一次的频率，向它已知的所有主服务器和从服务器发送 INFO 命令。当一个主服务器被 Sentinel 标记为客观下线时，Sentinel 向下线主服务器的所有从服务器发送INFO命令的频率，会从10秒一次改为每秒一次。客观下线​ 客观下线只适用于主节点。当 Sentinel 将一个主服务器判断为主管下线后，为了确认这个主服务器是否真的下线，会向同样监视这一主服务器的其他 Sentinel 询问（ 通过sentinel is-master-down-by-addr指令判断），看它们是否也认为主服务器已经下线。​ 其他哨兵会根据自己和主库的连接情况，做出 Y 或 N 的响应，Y 相当于赞成票，N相当于反对票。如果赞成票数（这里是2）是大于等于哨兵配置文件中的 quorum 配置项（比如quorum=2）, 则可以判定主库客观下线了。当足够数量的 Sentinel 认为主服务器已下线，就判定其为客观下线（ODOWN），并对其执行故障转移操作。哨兵集群的选举​ 当一个主服务器被判断为客观下线时，监视这个下线主服务器的各个 Sentinel 会进行选举，选举出一个领头的 Sentinel，并由领头 Sentinel 对下线主服务器执行故障转移操作。​ 哨兵的选举机制使用Raft（Raft算法详解）选举算法：​ 每个发现服务客观下线的sentinel，都会要求其他sentinel将自己设置成领头。所有的sentinel都有且只有一次将某个sentinel选举成领头的机会（在一轮选举中），一旦选举某个sentinel为领头，不能更改。领头sentinel是先到先得，一旦当前sentinel设置了领头sentinel，以后要求设置sentinel为领头请求都会被拒绝。​ 成为领头sentinel的条件如下：拿到半数以上的赞成票；拿到的票数同时还需要大于等于哨兵配置文件中的 quorum 值。​ 当有3个哨兵时，quorum 设置为 2，那么，任何一个想成为 Leader 的哨兵只要拿到 2 张赞成票，就可以了。新主库的选出​ 主库被判定为客观下线后，就需要从剩余的从库中选择一个新的主库。过滤列表中所有处于下线或断线状态的从服务器。过滤列表中所有最近五秒没有回复过 Sentinel Leader 的 INFO 命令的从服务器。过滤所有与已下线主服务器连接断开超过 down-after-milliseconds *10 毫秒的从服务器（down-after-milliseconds 指定了判断主服务器下线所需的时间）。​ 然后， Sentinel Leader 先选出优先级最高的从服务器；如果优先级一样高，再选择复制偏移量最大的从服务器；如果结果还不唯一，则选出运行 ID 最小的从服务器。故障的转移​ 在上文中，Sentinel Leader 已经挑选出了新的主节点，接下来就要进行故障转移了。​ 首先向这个从节点发送 SLAVEOF no one 命令，将其转换为主节点（5.0 中应该是replicaof no one）。然后Sentinel Leader 会向所有从节点发送 SLAVEOF 命令，让所有从节点指向新的主节点并复制新的主节点上的数据，然后通知客户端主节点已更换，最后将旧的主服务器标记为从服务器。当旧的主服务器重新上线，Sentinel Leader 会向它发送 SLAVEOF 命令，让其成为从服务器。​​ 最后，需要注意的是，主从切换并不一定就能完成，下面举个例子，Redis 1主4从，5个哨兵，哨兵配置quorum为2，如果3个哨兵故障，此时主节点宕机。​ 由于quorum=2，所以当一个哨兵判断主库“主观下线”后，询问另外一个哨兵后也会得到同样的结果，2个哨兵都判定“主观下线”，达到了quorum的值，因此，哨兵集群可以判定主库为“客观下线”。​ 但哨兵不能完成主从切换。哨兵标记主库“客观下线后”，在选举“哨兵领导者”时，一个哨兵必须拿到超过多数的选票(5/2+1=3票)。但目前只有2个哨兵活着，无论怎么投票，一个哨兵最多只能拿到2票，永远无法达到N/2+1选票的结果。​ 当没有足够数量的 Sentinel 同意 主服务器 下线时， 主服务器 的 客观下线状态 就会被移除。当 主服务器 重新向 Sentinel 的 PING 命令返回 有效回复 时，主服务器 的 主观下线状态 就会被移除。同样，从服务器的主观下线状态在PING命令得到有效回复后移除。补充数据丢失问题的产生​ 即使使用了哨兵机制，也并不代表能彻底的解决数据丢失的问题，下面我们看下以下2个问题异步复制导致的数据丢失​ 因为 master-&gt;slave 的复制是异步的，所以可能有部分数据还没复制到 slave，master 就宕机了，此时这部分数据就丢失了。脑裂导致的数据丢失​ 脑裂，即一个主从关系中出现了2个主节点，例如某个 master 所在机器突然脱离了正常的网络，跟其他 slave 机器不能连接，但是实际上 master 还运行着。此时哨兵可能就会认为 master 宕机了，然后开启选举，将其他 slave 切换成了 master。这个时候，集群里就会有两个 master ，也就是所谓的脑裂。​ 此时虽然某个 slave 被切换成了 master，但是可能 client 还没来得及切换到新的 master，还继续向旧 master 写数据。因此旧 master 再次恢复的时候，会被作为一个 slave 挂到新的 master 上去，自己的数据会清空，重新从新的 master 复制数据。而新的 master 并没有后来 client 写入的数据，因此，这部分数据也就丢失了。数据丢失问题的解决方案​ 以上无论是第一种还是第二种情况，都是因为主从节点间网络通信异常导致，于是我们可以通过对主节点写入数据的条件进行一定的限制：12min-slaves-to-write 3min-slaves-max-lag 10​ 当从服务器小于 3 个，或三个从服务器的延迟（lag）都大于等于 10 秒时，主服务器将拒绝执行写命令，直至故障恢复。通过对主节点进行限制，能够保证当主节点挂机后丢失的数据尽量少。]]></content>
      <categories>
        <category>技术</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
        <tag>哨兵模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LVS+KeepAlived高可用部署实战应用]]></title>
    <url>%2F2023%2F10d48c1061.html</url>
    <content type="text"><![CDATA[高可用集群（High Availability Cluster，简称HA Cluster），是指以减少服务中断时间为目的得服务器集群技术。它通过保护用户得业务程序对外部间断提供的服务，把因为软件，硬件，认为造成的故障对业务得影响降低到最小程度。之前部署的LVS有说到一个缺点就是LVS只是前端调度的功能，没有健康检查，导致如果后端服务器出现故障的话，请求还是会发到故障的服务器，今天我们通过Keepalive健康检查来解决这个问题，同时，Keepalive还对director做冗余，如果主director故障了，备director会及时顶上工作，实现高可用。KeepAlivedkeepAlived简介Keepalived的作用是检测服务器状态，如果有一台web服务器宕机，或工作出现故障，Keepalived将检测到，并将有故障的服务器从系统中剔除，同时使用其他服务器代替该服务器的工作，当服务器工作正常后Keepalived自动将服务器加入到服务器群中。VRRP协议在现实的网络环境中。主机之间的通信都是通过配置静态路由或者(默认网关)来完成的，而主机之间的路由器一旦发生故障，通信就会失效，因此这种通信模式当中，路由器就成了一个单点瓶颈，为了解决这个问题，就引入了VRRP协议。VRRP协议是一种容错的主备模式的协议，保证当主机的下一跳路由出现故障时，由另一台路由器来代替出现故障的路由器进行工作，通过VRRP可以在网络发生故障时透明的进行设备切换而不影响主机之间的数据通信。故障迁移原理在 Keepalived 服务正常工作时，主 Master 节点会不断地向备节点发送（多播的方式）心跳消息，用以告诉备 Backup 节点自己还活着，当主 Master 节点发生故障时，就无法发送心跳消息，备节点也就因此无法继续检测到来自主 Master 节点的心跳了，于是调用自身的接管程序，接管主 Master 节点的 IP 资源及服务。而当主 Master 节点恢复时，备 Backup 节点又会释放主节点故障时自身接管的 IP 资源及服务，恢复到原来的备用角色。keepAlived原理Keepalived工作在TCP/IP参考模型的三层、四层、五层分布式选主策略在一个一主多备的Keepalived集群中，priority值最大的将成为集群中的MASTER节点，而其他都是BACKUP节点。在MASTER节点发生故障后，BACKUP节点之间将进行“民主选举”，通过对节点优先级值priority和weight的计算，选出新的MASTER节点接管集群服务。设置priority和weightweight值为正数时在vrrp_script中指定的脚本如果检测成功，那么MASTER节点的权值将是weight值与priority值之和；如果脚本检测失效，那么MASTER节点的权值保持为priority值MASTER 节点vrrp_script脚本检测失败时，如果MASTER节点priority值小于BACKUP节点weight值与priority值之和，将发生主、备切换。MASTER节点vrrp_script脚本检测成功时，如果MASTER节点weight值与priority值之和大于BACKUP节点weight值与priority值之和，主节点依然为主节点，不发生切换。weight值为负数时在vrrp_script中指定的脚本如果检测成功，那么MASTER节点的权值仍为priority值，当脚本检测失败时，MASTER节点的权值将是priority值与weight值之差MASTER节点vrrp_script脚本检测失败时，如果MASTER节点priority值与weight值之差小于BACKUP节点priority值，将发生主、备切换。MASTER节点vrrp_scrip脚本检测成功时，如果MASTER节点priority值大于BACKUP节点priority值时，主节点依然为主节点，不发生切换。weight设置标准对于weight值的设置，有一个简单的标准，即weight值的绝对值要大于MASTER和BACKUP节点priority值之差。由此可见，对于weight值的设置要非常谨慎，如果设置不好，主节点发生故障时将导致集群角色选举失败，使集群陷于瘫痪状态。LVS+keepAlived实战实战拓扑为了测试lvs的高可用，这里需要增加一台lvs服务器，需在此服务器上安装ipvsadm。keepAlived安装和配置在两台lvs服务器上都需要安装keepAlived，安装命令如下:1yum install -y keepalivedkeepAlived安装完成后，在/etc/keepalived目录下有一个keepalived.conf配置文件，内容如下:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160! Configuration File for keepalivedglobal_defs &#123; notification_email &#123; acassen@firewall.loc failover@firewall.loc sysadmin@firewall.loc &#125; notification_email_from Alexandre.Cassen@firewall.loc smtp_server 192.168.200.1 smtp_connect_timeout 30 router_id LVS_DEVEL vrrp_skip_check_adv_addr vrrp_strict vrrp_garp_interval 0 vrrp_gna_interval 0&#125;#上面的配置无需关注，重点关注和修改下面的配置vrrp_instance VI_1 &#123; state MASTER #标识当前lvs是主，根据实际lvs服务器规划确定，可选值MASTER和BACKUP interface eth0 #lvs服务器提供服务器的网卡，根据实际服务器网卡进行修改 virtual_router_id 51 #lvs提供的服务所属ID，目前无需修改 priority 100 #lvs服务器的优先级，主服务器最高，备份服务器要低于主服务器 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; #virtual_ipaddress用于配置VIP和LVS服务器的网卡绑定关系，一般需要修改 #示例: 192.168.116.134/24 dev ens33 label ens33:9 virtual_ipaddress &#123; 192.168.200.16 192.168.200.17 192.168.200.18 &#125;&#125;#配置lvs服务策略，相当于ipvsadm -A -t 192.168.116.134:80 -s rr，一般需要修改virtual_server 192.168.200.100 443 &#123; delay_loop 6 lb_algo rr #配置lvs调度算法，默认轮询 lb_kind NAT #配置lvs工作模式，可以改为DR persistence_timeout 50 #用于指定同一个client在多久内，只去请求第一次提供服务的RS，为查看轮询效果，这里需要改为0 protocol TCP#TCP协议 #配置RS信息，相当于ipvsadm -a -t 192.168.116.134:80 -r 192.168.116.131 -g real_server 192.168.201.100 443 &#123; weight 1 #当前RS的权重 SSL_GET &#123; #SSL_GET健康检查，一般改为HTTP_GET #两个url可以删除一个，url内的内容改为path /和status_code 200，digest删除 url &#123; path / digest ff20ad2481f97b1754ef3e12ecd3a9cc &#125; url &#123; path /mrtg/ digest 9b3a0c85a887a256d6939da88aabd8cd &#125; connect_timeout 3 nb_get_retry 3 delay_before_retry 3 &#125; &#125;&#125;#下面的配置实际是两组lvs服务的配置，含义和上面的lvs服务配置一致。如果用不到，下面的配置可以全部删除virtual_server 10.10.10.2 1358 &#123; delay_loop 6 lb_algo rr lb_kind NAT persistence_timeout 50 protocol TCP sorry_server 192.168.200.200 1358 real_server 192.168.200.2 1358 &#123; weight 1 HTTP_GET &#123; url &#123; path /testurl/test.jsp digest 640205b7b0fc66c1ea91c463fac6334d &#125; url &#123; path /testurl2/test.jsp digest 640205b7b0fc66c1ea91c463fac6334d &#125; url &#123; path /testurl3/test.jsp digest 640205b7b0fc66c1ea91c463fac6334d &#125; connect_timeout 3 nb_get_retry 3 delay_before_retry 3 &#125; &#125; real_server 192.168.200.3 1358 &#123; weight 1 HTTP_GET &#123; url &#123; path /testurl/test.jsp digest 640205b7b0fc66c1ea91c463fac6334c &#125; url &#123; path /testurl2/test.jsp digest 640205b7b0fc66c1ea91c463fac6334c &#125; connect_timeout 3 nb_get_retry 3 delay_before_retry 3 &#125; &#125;&#125;virtual_server 10.10.10.3 1358 &#123; delay_loop 3 lb_algo rr lb_kind NAT persistence_timeout 50 protocol TCP real_server 192.168.200.4 1358 &#123; weight 1 HTTP_GET &#123; url &#123; path /testurl/test.jsp digest 640205b7b0fc66c1ea91c463fac6334d &#125; url &#123; path /testurl2/test.jsp digest 640205b7b0fc66c1ea91c463fac6334d &#125; url &#123; path /testurl3/test.jsp digest 640205b7b0fc66c1ea91c463fac6334d &#125; connect_timeout 3 nb_get_retry 3 delay_before_retry 3 &#125; &#125; real_server 192.168.200.5 1358 &#123; weight 1 HTTP_GET &#123; url &#123; path /testurl/test.jsp digest 640205b7b0fc66c1ea91c463fac6334d &#125; url &#123; path /testurl2/test.jsp digest 640205b7b0fc66c1ea91c463fac6334d &#125; url &#123; path /testurl3/test.jsp digest 640205b7b0fc66c1ea91c463fac6334d &#125; connect_timeout 3 nb_get_retry 3 delay_before_retry 3 &#125; &#125;&#125;配置keepAlived基于上述配置文件和实战拓扑图及服务器规划，对两台lvs服务器分别修改keepalived.conf配置如下:lvs主服务器1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465! Configuration File for keepalivedglobal_defs &#123; notification_email &#123; acassen@firewall.loc failover@firewall.loc sysadmin@firewall.loc &#125; notification_email_from Alexandre.Cassen@firewall.loc smtp_server 192.168.200.1 smtp_connect_timeout 30 router_id LVS_DEVEL vrrp_skip_check_adv_addr #vrrp_strict vrrp_garp_interval 0 vrrp_gna_interval 0&#125;vrrp_instance VI_1 &#123; state MASTER interface ens33 virtual_router_id 51 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 192.168.116.134/24 dev ens33 label ens33:9 &#125;&#125;virtual_server 192.168.116.134 80 &#123; delay_loop 6 lb_algo rr lb_kind DR persistence_timeout 0 protocol TCP real_server 192.168.116.131 80 &#123; weight 1 HTTP_GET &#123; url &#123; path / status 200 &#125; connect_timeout 3 nb_get_retry 3 delay_before_retry 3 &#125; &#125; real_server 192.168.116.132 80 &#123; weight 1 HTTP_GET &#123; url &#123; path / status 200 &#125; connect_timeout 3 nb_get_retry 3 delay_before_retry 3 &#125; &#125;&#125;lvs备份服务器1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465! Configuration File for keepalivedglobal_defs &#123; notification_email &#123; acassen@firewall.loc failover@firewall.loc sysadmin@firewall.loc &#125; notification_email_from Alexandre.Cassen@firewall.loc smtp_server 192.168.200.1 smtp_connect_timeout 30 router_id LVS_DEVEL vrrp_skip_check_adv_addr #vrrp_strict vrrp_garp_interval 0 vrrp_gna_interval 0&#125;vrrp_instance VI_1 &#123; state BACKUP interface ens33 virtual_router_id 51 priority 80 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 192.168.116.134/24 dev ens33 label ens33:9 &#125;&#125;virtual_server 192.168.116.134 80 &#123; delay_loop 6 lb_algo rr lb_kind DR persistence_timeout 0 protocol TCP real_server 192.168.116.131 80 &#123; weight 1 HTTP_GET &#123; url &#123; path / status 200 &#125; connect_timeout 3 nb_get_retry 3 delay_before_retry 3 &#125; &#125; real_server 192.168.116.132 80 &#123; weight 1 HTTP_GET &#123; url &#123; path / status 200 &#125; connect_timeout 3 nb_get_retry 3 delay_before_retry 3 &#125; &#125;&#125;注意：配置文件中的key和大括号之间一定要有空格启动keepAlived在两台lvs服务器上分别启动keepAlived，命令如下:1service keepalived start高可用测试 4.3.1 测试环境检查上述步骤执行完毕后，可以在lvs主服务器和备份服务器分别执行ifconfig命令，可以查看到VIP被绑定到了主服务器，如下:123456789101112131415161718192021222324[root@lvs01 ~]# ifconfigens33: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 192.168.116.130 netmask 255.255.255.0 broadcast 192.168.116.255 inet6 fe80::3264:bc00:653f:77b2 prefixlen 64 scopeid 0x20&lt;link&gt; inet6 fe80::cc13:177d:ec0:60d2 prefixlen 64 scopeid 0x20&lt;link&gt; inet6 fe80::b105:ad45:a07e:f946 prefixlen 64 scopeid 0x20&lt;link&gt; ether 00:50:56:38:1d:eb txqueuelen 1000 (Ethernet) RX packets 1574 bytes 135506 (132.3 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 840 bytes 96383 (94.1 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0ens33:9: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 192.168.116.134 netmask 255.255.255.0 broadcast 0.0.0.0 ether 00:50:56:38:1d:eb txqueuelen 1000 (Ethernet)lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10&lt;host&gt; loop txqueuelen 1000 (Local Loopback) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0这样，就可以在客户端请求VIP192.168.116.134来进行测试。测试负载均衡在客户端发起请求，测试负载均衡，如下:12345678[root@client ~]# curl 192.168.116.134this is RS02[root@client ~]# curl 192.168.116.134this is RS01[root@client ~]# curl 192.168.116.134this is RS02[root@client ~]# curl 192.168.116.134this is RS01测试RS高可用关闭一台RS后(这里可以使用ifconfig 网卡名 down命令暂时关闭网卡)，客户端继续发起请求，查看是否可以正常访问，如下:1234567[root@client ~]# curl 192.168.116.134this is RS02[root@client ~]# curl 192.168.116.134this is RS02[root@client ~]# curl 192.168.116.134this is RS02[root@client ~]# curl 192.168.116.134会发现，此时客户端可以正常访问，但只有RS2在提供服务。这说明，keepAlived检测到了RS1服务器异常，将其剔除了。此时再启动RS1服务器，客户端继续访问，会发现响应结果如下，keepAlived检测到RS1服务器恢复正常，又将其加入服务列表了。12345678[root@client ~]# curl 192.168.116.134this is RS01[root@client ~]# curl 192.168.116.134this is RS02[root@client ~]# curl 192.168.116.134this is RS01[root@client ~]# curl 192.168.116.134this is RS02测试LVS高可用这里主要进行两个测试：测试lvs主服务宕机使用ifconfig 网卡名 down命令，关闭主服务器网卡，此时主服务器不能提供服务。观察备份服务器是否将VIP绑定到自己，以及客户端是否可以继续正常访问。如下：关闭主服务器网卡1[root@lvs01 keepalived]# ifconfig ens33 down观察备份服务器，会发现VIP已经绑定过来了。这里实际是keepAlived检测到了主服务器的异常，而做出的故障转移和自动切换。123456789101112131415161718192021222324[root@lvs02 ~]# ifconfigens33: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 192.168.116.135 netmask 255.255.255.0 broadcast 192.168.116.255 inet6 fe80::3264:bc00:653f:77b2 prefixlen 64 scopeid 0x20&lt;link&gt; inet6 fe80::cc13:177d:ec0:60d2 prefixlen 64 scopeid 0x20&lt;link&gt; inet6 fe80::b105:ad45:a07e:f946 prefixlen 64 scopeid 0x20&lt;link&gt; ether 00:50:56:3a:95:05 txqueuelen 1000 (Ethernet) RX packets 1891 bytes 167840 (163.9 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 830 bytes 77459 (75.6 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0ens33:9: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 192.168.116.134 netmask 255.255.255.0 broadcast 0.0.0.0 ether 00:50:56:3a:95:05 txqueuelen 1000 (Ethernet)lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10&lt;host&gt; loop txqueuelen 1000 (Local Loopback) RX packets 6 bytes 528 (528.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 6 bytes 528 (528.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0观察客户端是否可以继续正常访问12345678[root@client ~]# curl 192.168.116.134this is RS01[root@client ~]# curl 192.168.116.134this is RS02[root@client ~]# curl 192.168.116.134this is RS01[root@client ~]# curl 192.168.116.134this is RS02测试lvs主服务器恢复上述测试通过后，可以开启主服务器网卡，让其能够提供服务，然后观察VIP是否会回到主服务器。开启主服务器网卡1ifconfig ens33 up查看主服务器和备份服务器主服务器123456789101112131415161718192021222324[root@lvs01 ~]# ifconfigens33: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 192.168.116.130 netmask 255.255.255.0 broadcast 192.168.116.255 inet6 fe80::3264:bc00:653f:77b2 prefixlen 64 scopeid 0x20&lt;link&gt; inet6 fe80::cc13:177d:ec0:60d2 prefixlen 64 scopeid 0x20&lt;link&gt; inet6 fe80::b105:ad45:a07e:f946 prefixlen 64 scopeid 0x20&lt;link&gt; ether 00:50:56:38:1d:eb txqueuelen 1000 (Ethernet) RX packets 2323 bytes 219033 (213.8 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 1953 bytes 189317 (184.8 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0ens33:9: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 192.168.116.134 netmask 255.255.255.0 broadcast 0.0.0.0 ether 00:50:56:38:1d:eb txqueuelen 1000 (Ethernet)lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10&lt;host&gt; loop txqueuelen 1000 (Local Loopback) RX packets 7 bytes 616 (616.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 7 bytes 616 (616.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0备份服务器1234567891011121314151617181920[root@lvs02 ~]# ifconfigens33: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 192.168.116.135 netmask 255.255.255.0 broadcast 192.168.116.255 inet6 fe80::3264:bc00:653f:77b2 prefixlen 64 scopeid 0x20&lt;link&gt; inet6 fe80::cc13:177d:ec0:60d2 prefixlen 64 scopeid 0x20&lt;link&gt; inet6 fe80::b105:ad45:a07e:f946 prefixlen 64 scopeid 0x20&lt;link&gt; ether 00:50:56:3a:95:05 txqueuelen 1000 (Ethernet) RX packets 2182 bytes 197998 (193.3 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 1117 bytes 100885 (98.5 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10&lt;host&gt; loop txqueuelen 1000 (Local Loopback) RX packets 6 bytes 528 (528.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 6 bytes 528 (528.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0会发现，VIP重新绑定到了主服务器。]]></content>
      <categories>
        <category>技术</category>
        <category>LVS</category>
      </categories>
      <tags>
        <tag>LVS</tag>
        <tag>Keepalive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LVS的安装部署]]></title>
    <url>%2F2023%2F108c14b9c0.html</url>
    <content type="text"><![CDATA[LVS是Linux Virtual Server的简写，在1998年5月由章文嵩博士成立，工作在OSI模型的四层，基于IP进行负载均衡， 在linux2.2内核时，IPVS就已经以内核补丁的形式出现， 从2.4版本以后，IPVS已经成为linux官方标准内核的一部分。系统必须有ip_vs模块12345678#查看ip_vs模块[root@node1 ~]# lsmod |grep -i ip_vsip_vs_rr 12600 1 ip_vs 141432 3 ip_vs_rrnf_conntrack 133053 7 ip_vs,nf_nat,nf_nat_ipv4,nf_nat_ipv6,xt_conntrack,nf_conntrack_ipv4,nf_conntrack_ipv6libcrc32c 12644 4 xfs,ip_vs,nf_nat,nf_conntrack#没有，加载ip_vs[root@node1 ~]# modprobe ip_vsLVS的安装yum安装123#1、安装清华的epel源#2、安装ipvadmyum install -y ipvsadm源码包安装1234567891011#https://mirrors.edge.kernel.org/pub/linux/utils/kernel/ipvsadm/选择与内核相同版本的软件mkdir -p /server/toolscd /server/toolswget -c https://mirrors.edge.kernel.org/pub/linux/utils/kernel/ipvsadm/ipvsadm-1.30.tar.xzln -s /usr/src/kernels/3.10.0-1062.18.1.el7.x86_64/ /usr/src/linuxtar xzvf ipvsadm-1.30.tar.gz cd ipvsadm-1.30make make install#如果编译报错，请安装依赖包#yum install -y popt-static kernel-devel make gcc openssl-devel lftplibnl* popt* openssl-devel lftplibnl* popt* libnl* libpopt* gcc*ipvsadm命令详解ipvsadm是ipvs的管理器，需要yum安装。LVS 相关软件程序包：ipvsadm Unit File: ipvsadm.service主程序：/usr/sbin/ipvsadm规则保存工具：/usr/sbin/ipvsadm-save规则重载工具：/usr/sbin/ipvsadm-restore配置文件：/etc/sysconfig/ipvsadm-configipvs调度规则文件：/etc/sysconfig/ipvsadmipvsadm 命令ipvsadm核心功能：1231、集群服务管理：增、删、改2、集群服务的RS管理：增、删、改3、查看ipvsadm 工具用法：12345678910111213#管理集群服务ipvsadm -A|E -t|u|f service-address [-s scheduler] [-p [timeout]] [-M netmask][--pe persistence_engine] [-b sched-flags]ipvsadm -D -t|u|f service-address #删除ipvsadm –C #清空ipvsadm –R #重载,相当于ipvsadm-restoreipvsadm -S [-n] #保存,相当于ipvsadm-save#管理集群中的RSipvsadm -a|e -t|u|f service-address -r server-address [-g|i|m] [-w weight]ipvsadm -d -t|u|f service-address -r server-addressipvsadm -L|l [options]ipvsadm -Z [-t|u|f service-address]防火墙标记FWM：FireWall Mark MARK target 可用于给特定的报文打标记 –set-mark value 其中：value 可为0xffff格式，表示十六进制数字 借助于防火墙标记来分类报文，而后基于标记定义集群服务；可将多个不同的应用使用同一个集群服务 进行调度 实现方法： 在Director主机打标记：1iptables -t mangle -A PREROUTING -d $vip -p $proto -m multiport --dports $port1,$port2,… -j MARK --set-mark NUMBER范例:1234567891011121314151617181920[root@lvs ~]#iptables -t mangle -A PREROUTING -d 172.16.0.100 -p tcp -mmultiport --dports 80,443 -j MARK --set-mark 10[root@lvs ~]#ipvsadm -C[root@lvs ~]#ipvsadm -A -f 10 -s rr[root@lvs ~]#ipvsadm -a -f 10 -r 10.0.0.7 -g[root@lvs ~]#ipvsadm -a -f 10 -r 10.0.0.17 -g[root@lvs ~]#ipvsadm -LnIP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags-&gt; RemoteAddress:Port Forward Weight ActiveConn InActConnFWM 10 rr-&gt; 10.0.0.7:0 Route 1 0 0-&gt; 10.0.0.17:0 Route 1 0 0[root@lvs ~]#cat /proc/net/ip_vsIP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags-&gt; RemoteAddress:Port Forward Weight ActiveConn InActConnFWM 0000000A rr-&gt; 0A000011:0000 Route 1 0 9-&gt; 0A000007:0000 Route 1 0 9在Director主机基于标记定义集群服务：1ipvsadm -A -f NUMBER [options]范例：12345678910111213141516171819[root@lvs ~]#ipvsadm -A -f 10[root@lvs ~]#ipvsadm -a -f 10 -r 10.0.0.7 -g[root@lvs ~]#ipvsadm -a -f 10 -r 10.0.0.17 -g[root@lvs ~]#ipvsadm -LnIP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags-&gt; RemoteAddress:Port Forward Weight ActiveConn InActConnFWM 10 wlc-&gt; 10.0.0.7:0 Route 1 0 0-&gt; 10.0.0.17:0 Route 1 0 0[root@LVS ~]#cat /proc/net/ip_vsIP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags-&gt; RemoteAddress:Port Forward Weight ActiveConn InActConnTCP AC14C8C8:0050 rr-&gt; 0A000011:0050 Masq 1 0 0-&gt; 0A000007:0050 Masq 1 0 0123456789101112131415161718LVS 持久连接session 绑定：对共享同一组RS的多个集群服务，需要统一进行绑定，lvs sh算法无法实现 持久连接（ lvs persistence ）模板：实现无论使用任何调度算法，在一段时间内（默认360s ），能够实现将来自同一个地址的请求始终发往同一个RS1ipvsadm -A|E -t|u|f service-address [-s scheduler] [-p [timeout]]持久连接实现方式：1231、每端口持久（PPC）：每个端口定义为一个集群服务，每集群服务单独调度2、每防火墙标记持久（PFWMC）：基于防火墙标记定义集群服务；可实现将多个端口上的应用统一调度，即所谓的port Affinity3、每客户端持久（PCC）：基于0端口（表示所有服务）定义集群服务，即将客户端对所有应用的请求都调度至后端主机，必须定义为持久模式范例：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354[root@lvs ~]#ipvsadm -E -f 10 -p[root@lvs ~]#ipvsadm -LnIP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags-&gt; RemoteAddress:Port Forward Weight ActiveConn InActConnFWM 10 wlc persistent 360-&gt; 10.0.0.7:0 Route 1 0 15-&gt; 10.0.0.17:0 Route 1 0 7[root@lvs ~]#ipvsadm -E -f 10 -p 3600[root@lvs ~]#ipvsadm -LnIP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags-&gt; RemoteAddress:Port Forward Weight ActiveConn InActConnFWM 10 wlc persistent 3600-&gt; 10.0.0.7:0 Route 1 0 79-&gt; 10.0.0.17:0 Route 1 0 7[root@lvs ~]#cat /proc/net/ip_vs_connPro FromIP FPrt ToIP TPrt DestIP DPrt State Expires PEName PEDataTCP C0A80006 C816 AC100064 01BB 0A000011 01BB FIN_WAIT 67TCP C0A80006 C812 AC100064 01BB 0A000011 01BB FIN_WAIT 67TCP C0A80006 9A36 AC100064 0050 0A000011 0050 FIN_WAIT 65TCP C0A80006 C806 AC100064 01BB 0A000011 01BB FIN_WAIT 65TCP C0A80006 9A3E AC100064 0050 0A000011 0050 FIN_WAIT 66TCP C0A80006 C81A AC100064 01BB 0A000011 01BB FIN_WAIT 67TCP C0A80006 C80A AC100064 01BB 0A000011 01BB FIN_WAIT 66TCP C0A80006 9A3A AC100064 0050 0A000011 0050 FIN_WAIT 66TCP C0A80006 9A4E AC100064 0050 0A000011 0050 FIN_WAIT 68TCP C0A80006 9A42 AC100064 0050 0A000011 0050 FIN_WAIT 67TCP C0A80006 9A46 AC100064 0050 0A000011 0050 FIN_WAIT 67TCP C0A80006 C81E AC100064 01BB 0A000011 01BB FIN_WAIT 68IP C0A80006 0000 0000000A 0000 0A000011 0000 NONE 948TCP C0A80006 C80E AC100064 01BB 0A000011 01BB FIN_WAIT 66TCP C0A80006 9A4A AC100064 0050 0A000011 0050 FIN_WAIT 67[root@lvs ~]#ipvsadm -LncIPVS connection entriespro expire state source virtual destinationTCP 00:46 FIN_WAIT 192.168.0.6:51222 172.16.0.100:443 10.0.0.17:443TCP 00:46 FIN_WAIT 192.168.0.6:51218 172.16.0.100:443 10.0.0.17:443TCP 00:45 FIN_WAIT 192.168.0.6:39478 172.16.0.100:80 10.0.0.17:80TCP 00:45 FIN_WAIT 192.168.0.6:51206 172.16.0.100:443 10.0.0.17:443TCP 00:46 FIN_WAIT 192.168.0.6:39486 172.16.0.100:80 10.0.0.17:80TCP 00:47 FIN_WAIT 192.168.0.6:51226 172.16.0.100:443 10.0.0.17:443TCP 00:45 FIN_WAIT 192.168.0.6:51210 172.16.0.100:443 10.0.0.17:443TCP 00:45 FIN_WAIT 192.168.0.6:39482 172.16.0.100:80 10.0.0.17:80TCP 00:47 FIN_WAIT 192.168.0.6:39502 172.16.0.100:80 10.0.0.17:80TCP 00:46 FIN_WAIT 192.168.0.6:39490 172.16.0.100:80 10.0.0.17:80TCP 00:46 FIN_WAIT 192.168.0.6:39494 172.16.0.100:80 10.0.0.17:80TCP 00:47 FIN_WAIT 192.168.0.6:51230 172.16.0.100:443 10.0.0.17:443IP 15:27 NONE 192.168.0.6:0 0.0.0.10:0 10.0.0.17:0TCP 00:46 FIN_WAIT 192.168.0.6:51214 172.16.0.100:443 10.0.0.17:443TCP 00:47 FIN_WAIT 192.168.0.6:39498 172.16.0.100:80 10.0.0.17:801234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253部署LVS NATLVS NAT模式注意事项LVS NAT模式工作原理用户请求LVS VIP到达director（LVS服务器：LB）（公网VIP：211.1.1.1），director 将请求的报文的目标IP地址改成后端的real server IP地址，同时将报文的目标端口也改成后端选定的real server相应端口，最后将报文发送到real server，real server将数据返给director，director再把数据发送给用户。（两次请求都经过director， 所以访问大的话，director会成为瓶颈），123451）、LVS服务器至少2块物理网卡，一块连接公网（VIP），一块连接内网；2）、后端Realserver机器的默认网关设置为LVS的内网IP地址；3）、保证LVS内网网卡通常跟Realserver在同一网段；4）、LVS NAT模式后端Realserver机器数量不超过30台；5）、用户的请求进入和返回均会经过LVS，LVS会成为瓶颈。实验环境12345678DR: inode1:外网ip:10.0.0.101 内网ip:172.16.1.101RS1: inode2:172.16.1.102-----&gt;web页面 www.ywx1.comRS2: inode3:172.16.1.103-----&gt;web页面 www.ywx2.com[root@inode1 ~]# uname -r3.10.0-862.el7.x86_64[root@inode1 ~]# cat /etc/redhat-release CentOS Linux release 7.5.1804 (Core)部署RS1和RS2的nginx这里已经部署好了：12345[root@inode2 ~]# curl 172.16.1.102www.ywx1.com[root@inode3 ~]# curl 172.16.1.103www.ywx2.com将RS1和RS2的网关配置为DR的内网ip地址：172.16.1.101123456789101112131415161718192021222324252627[root@inode2 ~]# sed -i '$aGATEWAY=172.16.1.101' /etc/sysconfig/network-scripts/ifcfg-eth1[root@inode2 ~]# cat /etc/sysconfig/network-scripts/ifcfg-eth1TYPE=EthernetBOOTPROTO=staticDEFROUTE=yesNAME=eth1DEVICE=eth1ONBOOT=yesIPADDR=172.16.1.102PREFIX=24GATEWAY=172.16.1.101[root@inode2 ~]#systemctl restart network[root@inode3 ~]# sed -i '$aGATEWAY=172.16.1.101' /etc/sysconfig/network-scripts/ifcfg-eth1[root@inode3 ~]# cat /etc/sysconfig/network-scripts/ifcfg-eth1TYPE=EthernetBOOTPROTO=staticDEFROUTE=yesNAME=eth1DEVICE=eth1ONBOOT=yesIPADDR=172.16.1.103PREFIX=24GATEWAY=172.16.1.101[root@inode3 ~]#systemctl restart network部署NAT的LVS第一步：安装LVS1yum install -y ipvsadm第二步：把DR的外网ip：10.0.0.101作为VIP，加入lvs集群1234ipvsadm -A -t 10.0.0.101:80 -s rr#-A 添加虚拟服务器的VIP #-t TCP协议，ip:port #-s 指定算法为RR轮询模式第三步：在虚拟集群10.10.10.101中，加入后端Realserver服务器12345678ipvsadm -a -t 10.0.0.101:80 -r 172.16.1.102 -m -w 50ipvsadm -a -t 10.0.0.101:80 -r 172.16.1.103 -m -w 50# -a，往虚拟服务器集群中添加真实服务器；# -t，TCP协议；# -r，指定后端realserver服务器的IP和端口；# -m，指定NAT转发模式；# -w，weight权重设置；查看LVS信息1234567[root@inode1 ~]# ipvsadm -L -n IP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConnTCP 10.0.0.101:80 rr -&gt; 172.16.1.102:80 Masq 50 0 0 -&gt; 172.16.1.103:80 Masq 50 0 0第四步：LVS NAT模式能够实现数据转发，还要依靠Linux内核开启转发功能1234567891011121314151617#临时生效echo 1 &gt; /proc/sys/net/ipv4/ip_forward#关闭icmp的重定向echo 0 &gt; /proc/sys/net/ipv4/conf/all/send_redirectsecho 0 &gt; /proc/sys/net/ipv4/conf/default/send_redirectsecho 0 &gt; /proc/sys/net/ipv4/conf/eth0/send_redirectsecho 0 &gt; /proc/sys/net/ipv4/conf/eth1/send_redirects#永久生效vim /etc/sysctl.confnet.ipv4.ip_forward = 1net.ipv4.conf.all.send_redirects = 0net.ipv4.conf.default.send_redirects = 0net.ipv4.conf.eth0.send_redirects = 0net.ipv4.conf.eth1.send_redirects = 0sysctl -p第五步：测试在inode4上访问10.0.0.1011234[root@inode4 ~]# curl 10.0.0.101www.ywx2.com[root@inode4 ~]# curl 10.0.0.101www.ywx1.com在DR inode1上观察InAction发现是负载均衡1234567[root@inode1 ~]# ipvsadm -LnIP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConnTCP 10.0.0.101:80 rr -&gt; 172.16.1.102:80 Masq 50 0 3 -&gt; 172.16.1.103:80 Masq 50 0 2注意：在实验模式NAT时，要关闭RS服务器上的外网网卡，否则会因为RS上有外网路由的问题，造成VIP地址无法访问后端页面删除和添加RS删除RS1（inode2）12345678[root@inode1 ~]# ipvsadm -d -t 10.0.0.101:80 -r 172.16.1.102[root@inode1 ~]# ipvsadm -LnIP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConnTCP 10.0.0.101:80 rr -&gt; 172.16.1.103:80 Masq 50 0 7继续在inode4上访问VIP12345678910[root@inode4 ~]# curl 10.0.0.101www.ywx2.com[root@inode4 ~]# curl 10.0.0.101www.ywx2.com[root@inode4 ~]# curl 10.0.0.101www.ywx2.com[root@inode4 ~]# curl 10.0.0.101www.ywx2.com[root@inode4 ~]# curl 10.0.0.101www.ywx2.comRS1被删除后，客户端访问没有影响。重新添加会RS1：12345678[root@inode1 ~]# ipvsadm -a -t 10.0.0.101:80 -r 172.16.1.102 -m -w 50[root@inode1 ~]# ipvsadm -LnIP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConnTCP 10.0.0.101:80 rr -&gt; 172.16.1.102:80 Masq 50 0 0 -&gt; 172.16.1.103:80 Masq 50 0 0inode4上访问VIP：1234567891011121314151617181920[root@inode4 ~]# curl 10.0.0.101www.ywx1.com[root@inode4 ~]# curl 10.0.0.101www.ywx2.com[root@inode4 ~]# curl 10.0.0.101www.ywx1.com[root@inode4 ~]# curl 10.0.0.101www.ywx2.com在lvs 10.0.0.101上查看lvs连接[root@node1 ~]# cat /proc/net/ip_vs_connPro FromIP FPrt ToIP TPrt DestIP DPrt State Expires PEName PEDataTCP 0A000068 B522 0A000065 0050 ACA80167 0050 TIME_WAIT 116TCP 0A000068 B51C 0A000065 0050 ACA80166 0050 TIME_WAIT 113TCP 0A000068 B516 0A000065 0050 ACA80167 0050 TIME_WAIT 110TCP 0A000068 B51E 0A000065 0050 ACA80167 0050 TIME_WAIT 114TCP 0A000068 B520 0A000065 0050 ACA80166 0050 TIME_WAIT 115TCP 0A000068 B51A 0A000065 0050 ACA80167 0050 TIME_WAIT 112TCP 0A000068 B514 0A000065 0050 ACA80166 0050 TIME_WAIT 109TCP 0A000068 B518 0A000065 0050 ACA80166 0050 TIME_WAIT 111lvs自动均衡到2台服务器上。我们关闭RS1(inode2)上的nginx服务：1[root@inode2 ~]# nginx -s stop继续使用inode4上访问：12345678910111213141516[root@inode4 ~]# curl 10.0.0.101curl: (7) Failed connect to 10.0.0.101:80; Connection refused[root@inode4 ~]# curl 10.0.0.101www.ywx2.com[root@inode4 ~]# curl 10.0.0.101curl: (7) Failed connect to 10.0.0.101:80; Connection refused[root@inode4 ~]# curl 10.0.0.101www.ywx2.com[root@inode4 ~]# curl 10.0.0.101curl: (7) Failed connect to 10.0.0.101:80; Connection refused[root@inode4 ~]# curl 10.0.0.101www.ywx2.com[root@inode4 ~]# curl 10.0.0.101curl: (7) Failed connect to 10.0.0.101:80; Connection refused[root@inode4 ~]# curl 10.0.0.101www.ywx2.com一个访问正常，一个访问报错，因为LVS只是前端调度的功能，没有健康检查（下一篇文章会加入Keepalive来解决这个问题）。保存lvs规则12345[root@node1 ~]# ipvsadm -Sn &gt; /tmp/ipvsadm[root@node1 ~]# cat /tmp/ipvsadm-A -t 10.0.0.101:80 -s rr-a -t 10.0.0.101:80 -r 172.168.1.102:80 -m -w 50-a -t 10.0.0.101:80 -r 172.168.1.103:80 -m -w 50清空lvs规则并重新导入1234567891011121314151617181920212223#清空ipvsadm规程[root@node1 ~]# ipvsadm -LnIP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConnTCP 10.0.0.101:80 rr -&gt; 172.168.1.102:80 Masq 50 0 0 -&gt; 172.168.1.103:80 Masq 50 0 0 [root@node1 ~]# ipvsadm -C[root@node1 ~]# ipvsadm -LnIP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConn#重新导入ipvsadm规则[root@node1 ~]# ipvsadm -R &lt; /tmp/ipvsadm [root@node1 ~]# ipvsadm -LnIP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConnTCP 10.0.0.101:80 rr -&gt; 172.168.1.102:80 Masq 50 0 0 -&gt; 172.168.1.103:80 Masq 50 0 0部署单网段LVS DRLVS DR模式注意事项LVS DR模式工作原理 用户请求LVS VIP到达director（LB均衡器），director将请求的报文的目标MAC地址改成后端的real server MAC地址，目标IP为VIP（不变），源IP为用户IP地址（保持不变），然后Director将报文发送到real server，real server检测到目标IP为自己本地VIP，如果在同一个网段，然后将请求直接返给用户。如果用户跟 real server不在一个网段，则通过网关返回用户。DR模式注意事项12345678910111213141516171819202122232425262728291)LVS DR模式要求LVS和RS服务器同一个物理网段（二层网络）；2）LVS修改数据报文的目标MAC地址，目标VIP保持不变；3）LVS和RS服务器的网卡块数没有要求，单个网卡即可；4）RS服务器配置VIP地址，只能配置在LO回环网卡上并且抑制VIP的ARP广播（防止跟其它主机配置的VIP冲突）；5）LVS服务器需要配置VIP地址，配置在真实网卡设备上，保证真实网卡不能抑制VIP的ARP广播；6）arp_ignore参数（1）含义：只响应目标IP是本地真实网卡上配置的IP（对RS而言），只响应真实网卡（eth0、ens33等），不响应lo网卡上的VIP地址；7）arp_announce参数（2）含义：忽略报文的源IP地址，使用主机上能够跟用户通信的真实网卡发送数据（对RS而言），源地址为lo上的VIP地址则忽略，数据直接从真实网卡上发送。DR模型中各主机上均需要配置VIP，解决地址冲突的方式有三种：(1) 在前端网关做静态绑定(2) 在各RS使用arptables(3) 在各RS修改内核参数，来限制arp响应和通告的级别限制响应级别：arp_ignore0：默认值，表示可使用本地任意接口上配置的任意地址进行响应1：仅在请求的目标IP配置在本地主机的接收到请求报文的接口上时，才给予响应限制通告级别：arp_announce0：默认值，把本机所有接口的所有信息向每个接口的网络进行通告1：尽量避免将接口信息向非直接连接网络进行通告2：必须避免将接口信息向非本网络进行通告配置要点1. Director 服务器采用双IP桥接网络，一个是VIP，一个DIP2. Web服务器采用和DIP相同的网段和Director连接3. 每个Web服务器配置VIP4. 每个web服务器可以出外网实验环境12345678910111213[root@inode1 ~]# uname -r3.10.0-862.el7.x86_64[root@inode1 ~]# cat /etc/redhat-release CentOS Linux release 7.5.1804 (Core) LVS inode1:10.0.0.101RS1 inode2:10.0.0.102 -----&gt; 页面 www.ywx1.comRS2 inode3:10.0.0.103 -----&gt; 页面 www.ywx2.comVIP 10.0.0.111client:172.168.1.104 GW:172.168.1.105rouer: 172.168.1.105 10.0.0.105router和client部署及LVS RS1 RS2的ipRouter部署123456789101112131415161718192021222324252627282930echo 'net.ipv4.ip_forward=1' &gt;&gt; /etc/sysctl.confsysctl -pcd /etc/sysconfig/network-scripts/[root@node5 network-scripts]# cat ifcfg-eth0TYPE="Ethernet"PROXY_METHOD="none"BROWSER_ONLY="no"BOOTPROTO="none"DEFROUTE="yes"IPV4_FAILURE_FATAL="no"NAME="eth0"DEVICE="eth0"ONBOOT="yes"IPADDR="10.0.0.105"PREFIX="24"GATEWAY="10.0.0.254"DNS1="223.5.5.5"[root@node5 network-scripts]# cat ifcfg-eth1TYPE=EthernetPROXY_METHOD=noneBROWSER_ONLY=noBOOTPROTO=noneDEFROUTE=yesIPV4_FAILURE_FATAL=noNAME=eth1DEVICE=eth1ONBOOT=yesIPADDR=172.168.1.105PREFIX=24Client部署12345678910111213[root@node4 ~]# vim /etc/sysconfig/network-scripts/ifcfg-eth1TYPE=EthernetPROXY_METHOD=noneBROWSER_ONLY=noBOOTPROTO=noneDEFROUTE=yesIPV4_FAILURE_FATAL=noNAME=eth1DEVICE=eth1ONBOOT=yesIPADDR=172.168.1.104PREFIX=24GAREWAY=172.168.1.105LVS （node1） ip1234567891011121314[root@node1 ~]# cat /etc/sysconfig/network-scripts/ifcfg-eth0TYPE="Ethernet"PROXY_METHOD="none"BROWSER_ONLY="no"BOOTPROTO"none"DEFROUTE="yes"IPV4_FAILURE_FATAL="no"NAME="eth0"DEVICE="eth0"ONBOOT="yes"IPADDR="10.0.0.101"PREFIX="24"GATEWAY="10.0.0.105"DNS1="223.5.5.5"RS1 （node2） ip1234567891011121314[root@node2 network-scripts]# cat /etc/sysconfig/network-scripts/ifcfg-eth0TYPE="Ethernet"PROXY_METHOD="none"BROWSER_ONLY="no"BOOTPROTO="none"DEFROUTE="yes"IPV4_FAILURE_FATAL="no"NAME="eth0"DEVICE="eth0"ONBOOT="yes"IPADDR="10.0.0.102"PREFIX="24"GATEWAY="10.0.0.105"DNS1="223.5.5.5"RS2 （node3 ）ip1234567891011121314[root@node3 network-scripts]# cat /etc/sysconfig/network-scripts/ifcfg-eth0TYPE="Ethernet"PROXY_METHOD="none"BROWSER_ONLY="no"BOOTPROTO="none"DEFROUTE="yes"IPV4_FAILURE_FATAL="no"NAME="eth0"DEVICE="eth0"ONBOOT="yes"IPADDR="10.0.0.103"PREFIX="24"GATEWAY="10.0.0.105"DNS1="223.5.5.5"部署RS1和RS2的nginx这里已经部署好了：12345[root@inode2 ~]# curl 10.0.0.102www.ywx1.com[root@inode3 ~]# curl 10.0.0.103www.ywx2.com部署LVS第一步：安装LVS1yum install -y ipvsadm第二步：把10.0.0.111作为VIP，加入lvs集群1ipvsadm -A -t 10.0.0.111 -s rr第三步：把RS1 inode2和RS2 inode3加入lvs集群12ipvsadm -a -t 10.0.0.111:80 -r 10.0.0.102:80 -g -w 50ipvsadm -a -t 10.0.0.111:80 -r 10.0.0.103:80 -g -w 50查看ipvsadm1234567[root@node1 ~]# ipvsadm -LnIP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConnTCP 10.0.0.111:80 rr -&gt; 10.0.0.102:80 Route 50 0 0 -&gt; 10.0.0.103:80 Route 50 0 0第四步：DR （inode1） 上绑定VIP地址方法一：12[root@node1 ~]# ifconfig eth0:0 10.0.0.111 netmask 255.255.255.255 broadcast 10.0.0.111[root@node1 ~]# /sbin/route add -host 10.0.0.111 dev eth0:0方法二：123456789[root@node1 ~]# cd /etc/sysconfig/network-scripts/[root@node1 ~]# cp ifcfg-eth0 ifcfg-eth0:0[root@node1 ~]# vim ifcfg-eth0:0TYPE=EthernetBOOTPROTO=staticDEVICE=eth0:0ONBOOT=yesIPADDR=10.0.0.111NETMASK=255.255.255.255第五步：RS1 （inode2）和RS2 （inode3）绑定VIP地址方法一：12[root@node2 ~]# ifconfig lo:0 10.0.0.111 netmask 255.255.255.255 broadcast 10.0.0.111 [root@node2 ~]# /sbin/route add -host 10.0.0.111 dev lo:0方法二：123456789[root@node2 ~]# cd /etc/sysconfig/network-scripts/[root@node2 ~]# cp ifcfg-lo ifcfg-lo:0[root@node2 ~]# vim ifcfg-lo:0DEVICE=lo:0IPADDR=10.0.0.111NETMASK=255.255.255.255ONBOOT=yesNAME=loopback第六步：在RS1 （inode2）和RS2 （inode3）上配置arp抑制123456789[root@node2 ~]#echo "1" &gt;/proc/sys/net/ipv4/conf/lo/arp_ignore[root@node2 ~]#echo "2" &gt;/proc/sys/net/ipv4/conf/lo/arp_announce[root@node2 ~]#echo "1" &gt;/proc/sys/net/ipv4/conf/all/arp_ignore[root@node2 ~]#echo "2" &gt;/proc/sys/net/ipv4/conf/all/arp_announce[root@node2 ~]#sysctl -p [root@node2 ~]#cat /proc/sys/net/ipv4/conf/lo/arp_ignore[root@node2 ~]#cat /proc/sys/net/ipv4/conf/lo/arp_announce[root@node2 ~]#cat /proc/sys/net/ipv4/conf/all/arp_ignore[root@node2 ~]#cat /proc/sys/net/ipv4/conf/all/arp_announce第七步：测试在inode4上访问VIP12345678[root@inode4 ~]# curl 10.0.0.111www.ywx1.com[root@inode4 ~]# curl 10.0.0.111www.ywx2.com[root@inode4 ~]# curl 10.0.0.111www.ywx1.com[root@inode4 ~]# curl 10.0.0.111www.ywx2.com部署多网段LVS DR多网段LVS DR是vip与RIP不在同一个网段123456789101112[root@inode1 ~]# uname -r3.10.0-862.el7.x86_64[root@inode1 ~]# cat /etc/redhat-release CentOS Linux release 7.5.1804 (Core) LVS inode1:10.0.0.101RS1 inode2:10.0.0.102 -----&gt; 页面 www.ywx1.comRS2 inode3:10.0.0.103 -----&gt; 页面 www.ywx2.comVIP 192.168.1.100client:172.168.1.104 GW:172.168.1.105rouer: 172.168.1.105 10.0.0.105 192.168.1.200(与vip通信使用)router和client部署及LVS RS1 RS2的ipRouter部署1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556[root@node5 network-scripts]#echo 'net.ipv4.ip_forward=1' &gt;&gt; /etc/sysctl.conf[root@node5 network-scripts]#sysctl -p[root@node5 network-scripts]#cd /etc/sysconfig/network-scripts/[root@node5 network-scripts]# cat ifcfg-eth0TYPE="Ethernet"PROXY_METHOD="none"BROWSER_ONLY="no"BOOTPROTO="none"DEFROUTE="yes"IPV4_FAILURE_FATAL="no"NAME="eth0"DEVICE="eth0"ONBOOT="yes"IPADDR="10.0.0.105"PREFIX="24"GATEWAY="10.0.0.254"DNS1="223.5.5.5"[root@node5 network-scripts]# cat ifcfg-eth1TYPE=EthernetPROXY_METHOD=noneBROWSER_ONLY=noBOOTPROTO=noneDEFROUTE=yesIPV4_FAILURE_FATAL=noNAME=eth1DEVICE=eth1ONBOOT=yesIPADDR=172.168.1.105PREFIX=24#在eth0:0上配置192.168.1.200[root@node5 network-scripts]#ifconfig eth0:0 192.168.1.200 netmask 255.255.255.0 broadcast 192.168.1.200[root@node5 network-scripts]#/sbin/route add -host 192.168.1.200 dev eth0:0 [root@node5 ~]# ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:0c:29:f5:01:8d brd ff:ff:ff:ff:ff:ff inet 10.0.0.105/24 brd 10.0.0.255 scope global noprefixroute eth0 valid_lft forever preferred_lft forever inet 192.168.1.200/32 brd 192.168.1.200 scope global eth0:0 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:fef5:18d/64 scope link valid_lft forever preferred_lft forever3: eth1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:0c:29:f5:01:97 brd ff:ff:ff:ff:ff:ff inet 172.168.1.105/24 brd 172.168.1.255 scope global noprefixroute eth1 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:fef5:197/64 scope link valid_lft forever preferred_lft foreverClient部署12345678910111213[root@node4 ~]# vim /etc/sysconfig/network-scripts/ifcfg-eth1TYPE=EthernetPROXY_METHOD=noneBROWSER_ONLY=noBOOTPROTO=noneDEFROUTE=yesIPV4_FAILURE_FATAL=noNAME=eth1DEVICE=eth1ONBOOT=yesIPADDR=172.168.1.104PREFIX=24GAREWAY=172.168.1.105LVS （node1） ip1234567891011121314[root@node1 ~]# cat /etc/sysconfig/network-scripts/ifcfg-eth0TYPE="Ethernet"PROXY_METHOD="none"BROWSER_ONLY="no"BOOTPROTO="none"DEFROUTE="yes"IPV4_FAILURE_FATAL="no"NAME="eth0"DEVICE="eth0"ONBOOT="yes"IPADDR="10.0.0.101"PREFIX="24"GATEWAY="10.0.0.105"DNS1="223.5.5.5"RS1 （node2） ip1234567891011121314[root@node2 network-scripts]# cat /etc/sysconfig/network-scripts/ifcfg-eth0TYPE="Ethernet"PROXY_METHOD="none"BROWSER_ONLY="no"BOOTPROTO="none"DEFROUTE="yes"IPV4_FAILURE_FATAL="no"NAME="eth0"DEVICE="eth0"ONBOOT="yes"IPADDR="10.0.0.102"PREFIX="24"GATEWAY="10.0.0.105"DNS1="223.5.5.5"RS2（ node3） ip1234567891011121314[root@node3 network-scripts]# cat /etc/sysconfig/network-scripts/ifcfg-eth0TYPE="Ethernet"PROXY_METHOD="none"BROWSER_ONLY="no"BOOTPROTO="none"DEFROUTE="yes"IPV4_FAILURE_FATAL="no"NAME="eth0"DEVICE="eth0"ONBOOT="yes"IPADDR="10.0.0.103"PREFIX="24"GATEWAY="10.0.0.105"DNS1="223.5.5.5"RS1和RS2部署nginx这里已经部署好了：12345[root@inode2 ~]# curl 10.0.0.102www.ywx1.com[root@inode3 ~]# curl 10.0.0.103www.ywx2.comDR （inode1 ）上绑定VIP地址方法一：12[root@inode1 ~]# ifconfig lo:1 192.168.1.100 netmask 255.255.255.255 broadcast 192.168.1.100[root@inode1 ~]# /sbin/route add -host 192.168.1.100 dev lo:1方法二：12345678[root@inode1 ~]# cd /etc/sysconfig/network-scripts/[root@inode1 ~]# cp ifcfg-lo ifcfg-lo:1[root@inode1 ~]# vim ifcfg-lo:1DEVICE=lo:1IPADDR=192.168.1.100NETMASK=255.255.255.255ONBOOT=yesNAME=loopbackRS1 （inode2）和RS2 （inode3）上绑定VIP地址方法一：12[root@inode2 ~]# ifconfig lo:1 192.168.1.100 netmask 255.255.255.255 broadcast 192.168.1.100[root@inode2 ~]# /sbin/route add -host 192.168.1.100 dev lo:1方法二：123456789[root@inode2 ~]# cd /etc/sysconfig/network-scripts/[root@inode2 ~]# cp ifcfg-lo ifcfg-lo:1[root@inode2 ~]# vim ifcfg-lo:1DEVICE=lo:1IPADDR=192.168.1.100NETMASK=255.255.255.255ONBOOT=yesNAME=loopback2RS1 （inode2）和RS2 （inode3）上配置arp抑制123456789[root@inode2 ~]# echo "1" &gt;/proc/sys/net/ipv4/conf/lo/arp_ignore[root@inode2 ~]# echo "2" &gt;/proc/sys/net/ipv4/conf/lo/arp_announce[root@inode2 ~]# echo "1" &gt;/proc/sys/net/ipv4/conf/all/arp_ignore[root@inode2 ~]# echo "2" &gt;/proc/sys/net/ipv4/conf/all/arp_announce[root@inode2 ~]# sysctl -p [root@inode2 ~]# cat /proc/sys/net/ipv4/conf/lo/arp_ignore[root@inode2 ~]# cat /proc/sys/net/ipv4/conf/lo/arp_announce[root@inode2 ~]# cat /proc/sys/net/ipv4/conf/all/arp_ignore[root@inode2 ~]# cat /proc/sys/net/ipv4/conf/all/arp_announce在DR（inode1）上部署LVS第一步：安装LVS1[root@inode1 ~]# yum install -y ipvsadm第二步：把192.168.1.100作为VIP，加入lvs集群1[root@inode1 ~]# ipvsadm -A -t 192.168.1.100 -s rr第三步：把RS1 inode2和RS2 inode3加入lvs集群12[root@inode1 ~]# ipvsadm -a -t 192.168.1.100:80 -r 10.0.0.102:80 -g -w 50[root@inode1 ~]# ipvsadm -a -t 192.168.1.100:80 -r 10.0.0.103:80 -g -w 50查看ipvsadm12345678[root@node1 ~]# ipvsadm -LnIP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConnTCP 192.168.1.100:80 rr -&gt; 10.0.0.102:80 Route 50 0 0 -&gt; 10.0.0.103:80 Route 50 0 0 1234567测试12345678[root@node4 ~]# curl 192.168.1.100www.ywx2.com[root@node4 ~]# curl 192.168.1.100www.ywx1.com[root@node4 ~]# curl 192.168.1.100www.ywx2.com[root@node4 ~]# curl 192.168.1.100www.ywx1.comlvs_dr脚本lvs_dr_vs.sh：12345678910111213141516171819202122232425262728293031#!/bin/bash#Author:wangxiaochun#Date:2017-08-13vip='192.168.1.100'iface='lo:1'mask='255.255.255.255'port='80'rs1='10.0.0.102'rs2='10.0.0.103'scheduler='wrr'type='-g'rpm -q ipvsadm &amp;&gt; /dev/null || yum -y install ipvsadm &amp;&gt; /dev/nullcase $1 instart)ifconfig $iface $vip netmask $mask #broadcast $vip upiptables -Fipvsadm -A -t $&#123;vip&#125;:$&#123;port&#125; -s $scheduleripvsadm -a -t $&#123;vip&#125;:$&#123;port&#125; -r $&#123;rs1&#125; $type -w 1ipvsadm -a -t $&#123;vip&#125;:$&#123;port&#125; -r $&#123;rs2&#125; $type -w 1echo "The VS Server is Ready!";;stop)ipvsadm -Cifconfig $iface downecho "The VS Server is Canceled!";;*)echo "Usage: $(basename $0) start|stop"exit 1;;esaclvs_dr_rs：12345678910111213141516171819202122232425262728293031#!/bin/bash#Author:wangxiaochun#Date:2017-08-13vip=192.168.1.100mask='255.255.255.255'dev=lo:1#rpm -q httpd &amp;&gt; /dev/null || yum -y install httpd &amp;&gt;/dev/null#service httpd start &amp;&gt; /dev/null &amp;&amp; echo "The httpd Server is Ready!"#echo "`hostname -I`" &gt; /var/www/html/index.htmlcase $1 instart)echo 1 &gt; /proc/sys/net/ipv4/conf/all/arp_ignoreecho 1 &gt; /proc/sys/net/ipv4/conf/lo/arp_ignoreecho 2 &gt; /proc/sys/net/ipv4/conf/all/arp_announceecho 2 &gt; /proc/sys/net/ipv4/conf/lo/arp_announceifconfig $dev $vip netmask $mask #broadcast $vip upecho "The RS Server is Ready!";;stop)ifconfig $dev downecho 0 &gt; /proc/sys/net/ipv4/conf/all/arp_ignoreecho 0 &gt; /proc/sys/net/ipv4/conf/lo/arp_ignoreecho 0 &gt; /proc/sys/net/ipv4/conf/all/arp_announceecho 0 &gt; /proc/sys/net/ipv4/conf/lo/arp_announceecho "The RS Server is Canceled!";;*)echo "Usage: $(basename $0) start|stop"exit 1;;esac]]></content>
      <categories>
        <category>技术</category>
        <category>LVS</category>
      </categories>
      <tags>
        <tag>LVS</tag>
        <tag>负责均衡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LVS详解]]></title>
    <url>%2F2023%2F1036d363c3.html</url>
    <content type="text"><![CDATA[LVS（Linux Virtual Server），是一个极好的负载均衡解决方案，它将一个真实服务器集群虚拟成一台服务器来对外提供服务，同时在真实服务器集群中实现了负载均衡，从linux2.4开始已经被收录到linux核心中。LVS有什么用随着互联网在人们生活中的普及，企业级应用迎来了海量数据的冲击，如微信、美团外卖、微信支付等应用每天的使用人数都在千万以上，仅靠单台机器提供服务已经行不通了。我们可以使用多台服务器分摊这些压力，当一定数量的服务器作为一个整体对外提供服务，并且分摊压力时，我们可以称这些服务器为“负载均衡集群”。LVS就是一个优秀的负载均衡集群方案，它理论上能够无限水平扩展，使得服务能够应对海量数据的冲击。LVS的原理是什么首先描述一下LVS中的一些术语：VS：Virtual Server 虚拟服务器，通常是分发器RS：Real Server 实际提供服务器的真实服务器CIP：Client IP 客户的客户端IPVIP：Virtual Server IP 虚拟服务器的IPRIP：Real Server IP 真实服务器的IPDIP：Director IP 分发器的IPCIP &lt;–&gt; VIP == DIP &lt;–&gt; RIP 客户端访问VIP，DIP将请求转发到RIPLVS的原理如下图所示分发服务器以VIP对外提供服务器，当接收到客户端服务请求时，便根据预定的分发策略（例如轮询）将请求分发到Real Server中，由Real Server做实际的业务处理。当Real Server处理完成后，根据不同的模式，会使用不同的方式返回请求结果。LVS的3种工作模式根据服务返回方式和集群分布的不同，LVS有3中不同的工作模式，他们分别是：NAT（地址转换）模式、DR（直接路由）模式和TUN（隧道）模式。NAT地址转换模式NAT地址转换模式是最为简单的一种模式，它的原理如下图所示（为了简洁，用户只画了一个）：NAT其实就是通过网络地址转换来实现负载均衡的，下面是它的请求流程：1.分发服务器Director Server（后面简称DS）接受到请求后，通过分发策略得出要将此请求分发到Real Server1。于是将请求报文的目的地址改为RIP1，发送出去。2.Real Server1收到一个目标地址为自己的数据包，于是接受并进行处理。3.处理完成后，Real Server1将RIP1——&gt;CIP的数据包发送出去。4.DS接收到RIP1——&gt;CIP的数据包后，将源地址改为VIP，然后发送出去：VIP——&gt;CIP。 这个过程中，DS仅仅起到一个地址转换和分发的作用。在NAT模式中，请求和响应报文都要通过DS，当真实服务器的数量越来越多时，分发器DS将会成为整个集群系统的性能瓶颈。下面的DR模式解决这个问题。DR直接路由模式互联网应用中存在一个规律：请求报文较短而响应报文往往包含大量的数据。如果能将请求和响应分开处理，即在负载调度器(Director)中只负责调度请求而响应直接由RealServer返回给客户，将极大地提高整个集群系统的吞吐量。这就是DR的实现原理，原理图如下所示：在DR模型中，只有在请求的时候会经过DR，响应的数据包由Real Server直接返回给用户，该模式是3中模式中最常用的。它的请求过程如下所示：1.DR接受到请求后，通过分发策略得出要将此请求分发到Real Server2。DS就将数据帧中的目标MAC地址修改为Real Server2的MAC地址,然后再将数据帧发送出去。（为什么要用MAC地址？因为此时Real Server也有配置有VIP）2.当Real Server2 收到一个源地址为CIP目标地址为VIP的数据包时,Real Server2发现目标地址为VIP,而VIP是自己,于是接受数据包并给予处理。3.Real Server2处理完成后，会将一个源地址为VIP而目标地址为CIP的数据包发送出去，此时的响应请求就不会再经过DS，而是直接响应给用户了。4.在这个过程中存在一个问题，由于RealServer也配置了VIP，那么当CIP——&gt;VIP的数据包到达服务局域网，进行广播时，所有的服务器都会进行应答，此时先应答的服务器就会收到数据包，这样就失去了负载均衡的能力。因此在使用DR模式时，通常会采用一些方式来确保请求数据包只会由DS接收，例如抑制Real Server对广播的应答，或者直接在路由器中对DS进行绑定等。TNU隧道模式TNU模式与DR模式非常相似，它同样是只有请求信息会经过DS，应答信息由Real Server直接返回给用户。不过DR模式中，要求DS和所有的Real Server必须在一个局域网中，而TNU模式去掉了这个限制。TNU模式原理图如下所示：在TNU模式中，DS与Real Server不必在一个网络中。DS在接到请求报文之后，在报文的上面再加一层源地址为DIP，目的地址为RIP2的IP首部，然后通过广域网发送到Real Server2。Real Server2收到报文，拆掉报文以后发现了里面还有一个封装，它就知道了，这就是隧道。后续的过程就与DR一样了。LVS的调度算法LVS的调度算法是指LVS对于请求的分发方式。DS在向Real Server分发请求实现负载均衡时，有10种不同的算法：123456789101112轮询（Round Robin，rr）：在Real Server之间轮流分配请求。加权轮询（Weighted Round Robin，wrr）：有权重地进行轮询。最少链接（Least Connnections，lc）：将请求发送给连接数最少的RS。加权最少链接（Weighted Least Connnections，wlc）：分发给基于权重的最少链接。基于局部性的最少链接（Locality-Based Least Connections，lblc）：首先根据目标IP地址找出最近使用的服务器，如果该服务器可用且没有超载（一半的工作负荷），则将请求发送到该服务器。否则，使用“最少链接”原则。带复制的基于局部性最少链接（Locality-Based Least Connections withReplication，lblcr）：它与lblc的不同之处是它要维护从一个目标IP到一组服务器的映射，而lblc值维护从一个目标IP到一台服务器的映射。目标地址散列（Destination Hashing，dh）：使用请求的目标IP地址作为散列键，从静态分配的散列表找出对应的服务器，若该服务器可用且未超载，则转发请求，否则返回空。源地址散列（Source Hashing，sh）：使用请求的源IP地址作为散列键，从静态分配的散列表找出对应的服务器，若该服务器可用且未超载，则转发请求，否则返回空。最短期望延迟（Shortest Expecte Delay，sed）：“最短的期望的延迟”是基于WLC算法的，只是其计算方法不同。具体算法如下：（active+1)*256/weight最少队列调度（Never Queue Scheduling）：无需队列。如果有台realserver的连接数＝0就直接分配过去，不需要在进行SED运算。如果没有服务器连接数为空闲，则使用SED算法。LVS的优缺点是什么LVS的优点工作在网络层，可以实现高性能、高可用的服务器集群技术。廉价，可把许多低性能的服务器组合在一起形成一个水平扩展的集群服务器。易用，配置非常简单，且有多重负载均衡的方法稳定可靠，即使集群中的一台服务器挂掉，也不影响整体服务效果。可扩展性好，可以用户透明地进行水平扩展，加减机器非常方便。LVS的缺点由于是通用组件，因此不能对特定业务进行针对优化。对于长连接无法进行负载均衡。自身没有健康状态检查，需要结合脚本或者Keepalived等软件实现。LVS （Linux Virtual Server）跟SLB(Server Load Balancing)最大的区别就是，LVS是在网络层起作用的，而SLB是在应用层起作用的。]]></content>
      <categories>
        <category>技术</category>
        <category>LVS</category>
      </categories>
      <tags>
        <tag>LVS</tag>
        <tag>负责均衡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker Swarm集群管理]]></title>
    <url>%2F2023%2F04e8be827.html</url>
    <content type="text"><![CDATA[Swarm 在 Docker 1.12 版本之前属于一个独立的项目，在 Docker 1.12 版本发布之后，该项目合并到了 Docker 中，成为 Docker 的一个子命令。目前，Swarm 是 Docker 社区提供的唯一一个原生支持 Docker 集群管理的工具。它可以把多个 Docker 主机组成的系统转换为单一的虚拟 Docker 主机，使得容器可以组成跨主机的子网网络。Swarm 认识Swarm 是目前 Docker 官方唯一指定（绑定）的集群管理工具。Docker 1.12 内嵌了 swarm mode 集群管理模式。为了方便演示跨主机网络，我们需要用到一个工具——Docker Machine，这个工具与 Docker Compose、Docker Swarm 并称 Docker 三剑客，下面我们来看看如何安装 Docker Machine：123$ curl -L https://github.com/docker/machine/releases/download/v0.9.0-rc2/docker-machine-`uname -s`-`uname -m` &gt;/tmp/docker-machine &amp;&amp; chmod +x /tmp/docker-machine &amp;&amp; sudo cp /tmp/docker-machine /usr/local/bin/docker-machine安装过程和 Docker Compose 非常类似。现在 Docker 三剑客已经全部到齐了。在开始之前，我们需要了解一些基本概念，有关集群的 Docker 命令如下：docker swarm：集群管理，子命令有 init, join,join-token, leave, updatedocker node：节点管理，子命令有 demote, inspect,ls, promote, rm, ps, updatedocker service：服务管理，子命令有 create, inspect, ps, ls ,rm , scale, updatedocker stack/deploy：试验特性，用于多应用部署，等正式版加进来再说。创建集群首先使用 Docker Machine 创建一个虚拟机作为 manger 节点。1234567891011121314151617181920212223$ docker-machine create --driver virtualbox manager1 Running pre-create checks...(manager1) Unable to get the latest Boot2Docker ISO release version: Get https://api.github.com/repos/boot2docker/boot2docker/releases/latest: dial tcp: lookup api.github.com on [::1]:53: server misbehavingCreating machine...(manager1) Unable to get the latest Boot2Docker ISO release version: Get https://api.github.com/repos/boot2docker/boot2docker/releases/latest: dial tcp: lookup api.github.com on [::1]:53: server misbehaving(manager1) Copying /home/zuolan/.docker/machine/cache/boot2docker.iso to /home/zuolan/.docker/machine/machines/manager1/boot2docker.iso...(manager1) Creating VirtualBox VM...(manager1) Creating SSH key...(manager1) Starting the VM...(manager1) Check network to re-create if needed...(manager1) Found a new host-only adapter: "vboxnet0"(manager1) Waiting for an IP...Waiting for machine to be running, this may take a few minutes...Detecting operating system of created instance...Waiting for SSH to be available...Detecting the provisioner...Provisioning with boot2docker...Copying certs to the local machine directory...Copying certs to the remote machine...Setting Docker configuration on the remote daemon...Checking connection to Docker...Docker is up and running!To see how to connect your Docker Client to the Docker Engine running on this virtual machine, run: docker-machine env manager1查看虚拟机的环境变量等信息，包括虚拟机的 IP 地址：1234567$ docker-machine env manager1export DOCKER_TLS_VERIFY="1"export DOCKER_HOST="tcp://192.168.99.100:2376"export DOCKER_CERT_PATH="/home/zuolan/.docker/machine/machines/manager1"export DOCKER_MACHINE_NAME="manager1"# Run this command to configure your shell: # eval $(docker-machine env manager1)然后再创建一个节点作为 work 节点。1$ docker-machine create --driver virtualbox worker1现在我们有了两个虚拟主机，使用 Machine 的命令可以查看：1234$ docker-machine ls NAME ACTIVE DRIVER STATE URL SWARM DOCKER ERRORSmanager1 - virtualbox Running tcp://192.168.99.100:2376 v1.12.3 worker1 - virtualbox Running tcp://192.168.99.101:2376 v1.12.3但是目前这两台虚拟主机并没有什么联系，为了把它们联系起来，我们需要 Swarm 登场了。因为我们使用的是 Docker Machine 创建的虚拟机，因此可以使用 docker-machine ssh 命令来操作虚拟机，在实际生产环境中，并不需要像下面那样操作，只需要执行 docker swarm 即可。把 manager1 加入集群：12345678910$ docker-machine ssh manager1 docker swarm init --listen-addr 192.168.99.100:2377 --advertise-addr 192.168.99.100Swarm initialized: current node (23lkbq7uovqsg550qfzup59t6) is now a manager.To add a worker to this swarm, run the following command: docker swarm join \ --token SWMTKN-1-3z5rzoey0u6onkvvm58f7vgkser5d7z8sfshlu7s4oz2gztlvj-c036gwrakjejql06klrfc585r \ 192.168.99.100:2377To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.用 –listen-addr 指定监听的 ip 与端口，实际的 Swarm 命令格式如下，本例使用 Docker Machine 来连接虚拟机而已：1$ docker swarm init --listen-addr &lt;MANAGER-IP&gt;:&lt;PORT&gt;接下来，再把 work1 加入集群中：1234$ docker-machine ssh worker1 docker swarm join --token \ SWMTKN-1-3z5rzoey0u6onkvvm58f7vgkser5d7z8sfshlu7s4oz2gztlvj-c036gwrakjejql06klrfc585r \ 192.168.99.100:2377This node joined a swarm as a worker.上面 join 命令中可以添加 –listen-addr $WORKER1_IP:2377 作为监听准备，因为有时候可能会遇到把一个 work 节点提升为 manger 节点的可能，当然本例子没有这个打算就不添加这个参数了。注意：如果你在新建集群时遇到双网卡情况，可以指定使用哪个 IP，例如上面的例子会有可能遇到下面的错误。123$ docker-machine ssh manager1 docker swarm init --listen-addr $MANAGER1_IP:2377Error response from daemon: could not choose an IP address to advertise since this system has multiple addresses on different interfaces (10.0.2.15 on eth0 and 192.168.99.100 on eth1) - specify one with --advertise-addrexit status 1发生错误的原因是因为有两个 IP 地址，而 Swarm 不知道用户想使用哪个，因此要指定 IP。12345678910$ docker-machine ssh manager1 docker swarm init --advertise-addr 192.168.99.100 --listen-addr 192.168.99.100:2377 Swarm initialized: current node (ahvwxicunjd0z8g0eeosjztjx) is now a manager.To add a worker to this swarm, run the following command: docker swarm join \ --token SWMTKN-1-3z5rzoey0u6onkvvm58f7vgkser5d7z8sfshlu7s4oz2gztlvj-c036gwrakjejql06klrfc585r \ 192.168.99.100:2377To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.集群初始化成功。现在我们新建了一个有两个节点的“集群”，现在进入其中一个管理节点使用 docker node 命令来查看节点信息：1234$ docker-machine ssh manager1 docker node lsID HOSTNAME STATUS AVAILABILITY MANAGER STATUS23lkbq7uovqsg550qfzup59t6 * manager1 Ready Active Leaderdqb3fim8zvcob8sycri3hy98a worker1 Ready Active现在每个节点都归属于 Swarm，并都处在了待机状态。Manager1 是领导者，work1 是工人。现在，我们继续新建虚拟机 manger2、worker2、worker3，现在已经有五个虚拟机了，使用 docker-machine ls 来查看虚拟机：123456NAME ACTIVE DRIVER STATE URL SWARM DOCKER ERRORSmanager1 - virtualbox Running tcp://192.168.99.100:2376 v1.12.3 manager2 - virtualbox Running tcp://192.168.99.105:2376 v1.12.3 worker1 - virtualbox Running tcp://192.168.99.102:2376 v1.12.3 worker2 - virtualbox Running tcp://192.168.99.103:2376 v1.12.3 worker3 - virtualbox Running tcp://192.168.99.104:2376 v1.12.3然后我们把剩余的虚拟机也加到集群中。添加 worker2 到集群中：1234$ docker-machine ssh worker2 docker swarm join \ --token SWMTKN-1-3z5rzoey0u6onkvvm58f7vgkser5d7z8sfshlu7s4oz2gztlvj-c036gwrakjejql06klrfc585r \ 192.168.99.100:2377This node joined a swarm as a worker.添加 worker3 到集群中：1234$ docker-machine ssh worker3 docker swarm join \ --token SWMTKN-1-3z5rzoey0u6onkvvm58f7vgkser5d7z8sfshlu7s4oz2gztlvj-c036gwrakjejql06klrfc585r \ 192.168.99.100:2377This node joined a swarm as a worker.添加 manager2 到集群中：先从 manager1 中获取 manager 的 token：123456$ docker-machine ssh manager1 docker swarm join-token managerTo add a manager to this swarm, run the following command: docker swarm join \ --token SWMTKN-1-3z5rzoey0u6onkvvm58f7vgkser5d7z8sfshlu7s4oz2gztlvj-8tn855hkjdb6usrblo9iu700o \192.168.99.100:2377然后添加 manager2 到集群中：1234$ docker-machine ssh manager2 docker swarm join \ --token SWMTKN-1-3z5rzoey0u6onkvvm58f7vgkser5d7z8sfshlu7s4oz2gztlvj-8tn855hkjdb6usrblo9iu700o \ 192.168.99.100:2377This node joined a swarm as a manager.现在再来查看集群信息：1234567$ docker-machine ssh manager2 docker node lsID HOSTNAME STATUS AVAILABILITY MANAGER STATUS16w80jnqy2k30yez4wbbaz1l8 worker1 Ready Active 2gkwhzakejj72n5xoxruet71z worker2 Ready Active 35kutfyn1ratch55fn7j3fs4x worker3 Ready Active a9r21g5iq1u6h31myprfwl8ln * manager2 Ready Active Reachabledpo7snxbz2a0dxvx6mf19p35z manager1 Ready Active Leader建立跨主机网络为了演示更清晰，下面我们把宿主机也加入到集群之中，这样我们使用 Docker 命令操作会清晰很多。直接在本地执行加入集群命令：1234$ docker swarm join \ --token SWMTKN-1-3z5rzoey0u6onkvvm58f7vgkser5d7z8sfshlu7s4oz2gztlvj-8tn855hkjdb6usrblo9iu700o \ 192.168.99.100:2377This node joined a swarm as a manager.现在我们有三台 manager，三台 worker。其中一台是宿主机，五台虚拟机。12345678$ docker node lsID HOSTNAME STATUS AVAILABILITY MANAGER STATUS6z2rpk1t4xucffzlr2rpqb8u3 worker3 Ready Active 7qbr0xd747qena4awx8bx101s * user-pc Ready Active Reachable9v93sav79jqrg0c7051rcxxev manager2 Ready Active Reachablea1ner3zxj3ubsiw4l3p28wrkj worker1 Ready Active a5w7h8j83i11qqi4vlu948mad worker2 Ready Active d4h7vuekklpd6189fcudpfy18 manager1 Ready Active Leader查看网络状态：123456$ docker network lsNETWORK ID NAME DRIVER SCOPE764ff31881e5 bridge bridge local fbd9a977aa03 host host local 6p6xlousvsy2 ingress overlay swarm e81af24d643d none null local可以看到在 swarm 上默认已有一个名为 ingress 的 overlay 网络, 默认在 swarm 里使用，本例子中会创建一个新的 overlay 网络。123456789$ docker network create --driver overlay swarm_test4dm8cy9y5delvs5vd0ghdd89s$ docker network lsNETWORK ID NAME DRIVER SCOPE764ff31881e5 bridge bridge localfbd9a977aa03 host host local6p6xlousvsy2 ingress overlay swarme81af24d643d none null local4dm8cy9y5del swarm_test overlay swarm这样一个跨主机网络就搭建好了，但是现在这个网络只是处于待机状态，下一小节我们会在这个网络上部署应用。在跨主机网络上部署应用首先我们上面创建的节点都是没有镜像的，因此我们要逐一 pull 镜像到节点中，这里我们使用前面搭建的私有仓库。1234567891011121314151617181920212223242526272829303132333435$ docker-machine ssh manager1 docker pull reg.example.com/library/nginx:alpine alpine: Pulling from library/nginxe110a4a17941: Pulling fs layer... ...7648f5d87006: Pull completeDigest: sha256:65063cb82bf508fd5a731318e795b2abbfb0c22222f02ff5c6b30df7f23292feStatus: Downloaded newer image for reg.example.com/library/nginx:alpine$ docker-machine ssh manager2 docker pull reg.example.com/library/nginx:alpinealpine: Pulling from library/nginxe110a4a17941: Pulling fs layer... ...7648f5d87006: Pull completeDigest: sha256:65063cb82bf508fd5a731318e795b2abbfb0c22222f02ff5c6b30df7f23292feStatus: Downloaded newer image for reg.example.com/library/nginx:alpine$ docker-machine ssh worker1 docker pull reg.example.com/library/nginx:alpine alpine: Pulling from library/nginxe110a4a17941: Pulling fs layer... ...7648f5d87006: Pull completeDigest: sha256:65063cb82bf508fd5a731318e795b2abbfb0c22222f02ff5c6b30df7f23292feStatus: Downloaded newer image for reg.example.com/library/nginx:alpine$ docker-machine ssh worker2 docker pull reg.example.com/library/nginx:alpinealpine: Pulling from library/nginxe110a4a17941: Pulling fs layer... ...7648f5d87006: Pull completeDigest: sha256:65063cb82bf508fd5a731318e795b2abbfb0c22222f02ff5c6b30df7f23292feStatus: Downloaded newer image for reg.example.com/library/nginx:alpine$ docker-machine ssh worker3 docker pull reg.example.com/library/nginx:alpinealpine: Pulling from library/nginxe110a4a17941: Pulling fs layer... ...7648f5d87006: Pull completeDigest: sha256:65063cb82bf508fd5a731318e795b2abbfb0c22222f02ff5c6b30df7f23292feStatus: Downloaded newer image for reg.example.com/library/nginx:alpine上面使用 docker pull 分别在五个虚拟机节点拉取 nginx:alpine 镜像。接下来我们要在五个节点部署一组 Nginx 服务。部署的服务使用 swarm_test 跨主机网络。12$ docker service create --replicas 2 --name helloworld --network=swarm_test nginx:alpine5gz0h2s5agh2d2libvzq6bhgs查看服务状态：123$ docker service lsID NAME REPLICAS IMAGE COMMAND5gz0h2s5agh2 helloworld 0/2 nginx:alpine查看 helloworld 服务详情（为了方便阅读，已调整输出内容）：1234$ docker service ps helloworldID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERRORay081uome3 helloworld.1 nginx:alpine manager1 Running Preparing 2 seconds ago 16cvore0c96 helloworld.2 nginx:alpine worker2 Running Preparing 2 seconds ago可以看到两个实例分别运行在两个节点上。进入两个节点，查看服务状态（为了方便阅读，已调整输出内容）：123456$ docker-machine ssh manager1 docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES119f787622c2 nginx:alpine "nginx -g ..." 4 minutes ago Up 4 minutes 80/tcp, 443/tcp hello ...$ docker-machine ssh worker2 docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES5db707401a06 nginx:alpine "nginx -g ..." 4 minutes ago Up 4 minutes 80/tcp, 443/tcp hello ...上面输出做了调整，实际的 NAMES 值为：12helloworld.1.ay081uome3eejeg4mspa8pdlxhelloworld.2.16cvore0c96rby1vp0sny3mvt记住上面这两个实例的名称。现在我们来看这两个跨主机的容器是否能互通：首先使用 Machine 进入 manager1 节点，然后使用 docker exec -i 命令进入 helloworld.1 容器中 ping 运行在 worker2 节点的 helloworld.2 容器。12345678$ docker-machine ssh manager1 docker exec -i helloworld.1.ay081uome3eejeg4mspa8pdlx \ ping helloworld.2.16cvore0c96rby1vp0sny3mvtPING helloworld.2.16cvore0c96rby1vp0sny3mvt (10.0.0.4): 56 data bytes64 bytes from 10.0.0.4: seq=0 ttl=64 time=0.591 ms64 bytes from 10.0.0.4: seq=1 ttl=64 time=0.594 ms64 bytes from 10.0.0.4: seq=2 ttl=64 time=0.624 ms64 bytes from 10.0.0.4: seq=3 ttl=64 time=0.612 ms^C然后使用 Machine 进入 worker2 节点，然后使用 docker exec -i 命令进入 helloworld.2 容器中 ping 运行在 manager1 节点的 helloworld.1 容器。12345678$ docker-machine ssh worker2 docker exec -i helloworld.2.16cvore0c96rby1vp0sny3mvt \ ping helloworld.1.ay081uome3eejeg4mspa8pdlx PING helloworld.1.ay081uome3eejeg4mspa8pdlx (10.0.0.3): 56 data bytes64 bytes from 10.0.0.3: seq=0 ttl=64 time=0.466 ms64 bytes from 10.0.0.3: seq=1 ttl=64 time=0.465 ms64 bytes from 10.0.0.3: seq=2 ttl=64 time=0.548 ms64 bytes from 10.0.0.3: seq=3 ttl=64 time=0.689 ms^C可以看到这两个跨主机的服务集群里面各个容器是可以互相连接的。为了体现 Swarm 集群的优势，我们可以使用虚拟机的 ping 命令来测试对方虚拟机内的容器。1234567891011121314$ docker-machine ssh worker2 ping helloworld.1.ay081uome3eejeg4mspa8pdlxPING helloworld.1.ay081uome3eejeg4mspa8pdlx (221.179.46.190): 56 data bytes64 bytes from 221.179.46.190: seq=0 ttl=63 time=48.651 ms64 bytes from 221.179.46.190: seq=1 ttl=63 time=63.239 ms64 bytes from 221.179.46.190: seq=2 ttl=63 time=47.686 ms64 bytes from 221.179.46.190: seq=3 ttl=63 time=61.232 ms^C$ docker-machine ssh manager1 ping helloworld.2.16cvore0c96rby1vp0sny3mvtPING helloworld.2.16cvore0c96rby1vp0sny3mvt (221.179.46.194): 56 data bytes64 bytes from 221.179.46.194: seq=0 ttl=63 time=30.150 ms64 bytes from 221.179.46.194: seq=1 ttl=63 time=54.455 ms64 bytes from 221.179.46.194: seq=2 ttl=63 time=73.862 ms64 bytes from 221.179.46.194: seq=3 ttl=63 time=53.171 ms^C上面我们使用了虚拟机内部的 ping 去测试容器的延迟，可以看到延迟明显比集群内部的 ping 值要高。Swarm 集群负载现在我们已经学会了 Swarm 集群的部署方法，现在来搭建一个可访问的 Nginx 集群吧。体验最新版的 Swarm 所提供的自动服务发现与集群负载功能。首先删掉上一节我们启动的 helloworld 服务：12$ docker service rm helloworld helloworld然后在新建一个服务，提供端口映射参数，使得外界可以访问这些 Nginx 服务：12$ docker service create --replicas 2 --name helloworld -p 7080:80 --network=swarm_test nginx:alpine9gfziifbii7a6zdqt56kocyun查看服务运行状态：123$ docker service ls ID NAME REPLICAS IMAGE COMMAND9gfziifbii7a helloworld 2/2 nginx:alpine不知你有没有发现，虽然我们使用 –replicas 参数的值都是一样的，但是上一节中获取服务状态时，REPLICAS 返回的是 0/2，现在的 REPLICAS 返回的是 2/2。同样使用 docker service ps 查看服务详细状态时（下面输出已经手动调整为更易读的格式），可以看到实例的 CURRENT STATE 中是 Running 状态的，而上一节中的 CURRENT STATE 中全部是处于 Preparing 状态。1234$ docker service ps helloworldID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR9ikr3agyi... helloworld.1 nginx:alpine user-pc Running Running 13 seconds ago 7acmhj0u... helloworld.2 nginx:alpine worker2 Running Running 6 seconds ago这就涉及到 Swarm 内置的发现机制了，目前 Docker 1.12 中 Swarm 已经内置了服务发现工具，我们不再需要像以前使用 Etcd 或者 Consul 这些工具来配置服务发现。对于一个容器来说如果没有外部通信但又是运行中的状态会被服务发现工具认为是 Preparing 状态，本小节例子中因为映射了端口，因此有了 Running 状态。现在我们来看 Swarm 另一个有趣的功能，当我们杀死其中一个节点时，会发生什么。首先 kill 掉 worker2 的实例：12$ docker-machine ssh worker2 docker kill helloworld.2.7acmhj0udzusv1d7lu2tbuhu4helloworld.2.7acmhj0udzusv1d7lu2tbuhu4稍等几秒，再来看服务状态：12345678$ docker service ps helloworldID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR9ikr3agyi... helloworld.1 nginx:alpine zuolan-pc Running Running 19 minutes ago 8f866igpl... helloworld.2 nginx:alpine manager1 Running Running 4 seconds ago 7acmhj0u... \_ helloworld.2 nginx:alpine worker2 Shutdown Failed 11 seconds ago ...exit...$ docker service ls ID NAME REPLICAS IMAGE COMMAND9gfziifbii7a helloworld 2/2 nginx:alpine可以看到即使我们 kill 掉其中一个实例，Swarm 也会迅速把停止的容器撤下来，同时在节点中启动一个新的实例顶上来。这样服务依旧还是两个实例在运行。此时如果你想添加更多实例可以使用 scale 命令：12$ docker service scale helloworld=3helloworld scaled to 3查看服务详情，可以看到有三个实例启动了：1234$ docker service ps helloworldID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR9ikr3agyi... helloworld.1 nginx:alpine user-pc Running Running 30 minutes ago 8f866igpl... helloworld.2 nginx:alpine manager1 Running Running 11 minutes ago 7acmhj0u... \_ helloworld.2 nginx:alpine worker2 Shutdown Failed 11 minutes ago exit1371vexr1jm... helloworld.3 nginx:alpine worker2 Running Running 4 seconds ago现在如果想减少实例数量，一样可以使用 scale 命令：12$ docker service scale helloworld=2helloworld scaled to 2至此，Swarm的主要用法都已经介绍完了，主要讲述了 Swarm 集群网络的创建与部署。介绍了 Swarm 的常规应用，包括 Swarm 的服务发现、负载均衡等，然后使用 Swarm 来配置跨主机容器网络，并在上面部署应用。]]></content>
      <categories>
        <category>技术</category>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker Swarm</tag>
        <tag>Docker集群管理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka部署]]></title>
    <url>%2F2023%2F04b8893609.html</url>
    <content type="text"><![CDATA[什么是 KafkaKafka 是一个分布式流式平台，它有三个关键能力订阅发布记录流，它类似于企业中的消息队列 或 企业消息传递系统以容错的方式存储记录流实时记录流Kafka 的应用作为消息系统作为存储系统作为流处理器Kafka 可以建立流数据管道，可靠性的在系统或应用之间获取数据。建立流式应用传输和响应数据。Kafka 作为消息系统Kafka 作为消息系统，它有三个基本组件Producer : 发布消息的客户端Broker：一个从生产者接受并存储消息的客户端Consumer : 消费者从 Broker 中读取消息在大型系统中，会需要和很多子系统做交互，也需要消息传递，在诸如此类系统中，你会找到源系统（消息发送方）和 目的系统（消息接收方）。为了在这样的消息系统中传输数据，你需要有合适的数据管道这种数据的交互看起来就很混乱，如果我们使用消息传递系统，那么系统就会变得更加简单和整洁Kafka 运行在一个或多个数据中心的服务器上作为集群运行Kafka 集群存储消息记录的目录被称为 topics每一条消息记录包含三个要素：键（key）、值（value）、时间戳（Timestamp）核心 APIKafka 有四个核心API，它们分别是Producer API，它允许应用程序向一个或多个 topics 上发送消息记录Consumer API，允许应用程序订阅一个或多个 topics 并处理为其生成的记录流Streams API，它允许应用程序作为流处理器，从一个或多个主题中消费输入流并为其生成输出流，有效的将输入流转换为输出流。Connector API，它允许构建和运行将 Kafka 主题连接到现有应用程序或数据系统的可用生产者和消费者。例如，关系数据库的连接器可能会捕获对表的所有更改Kafka 基本概念Kafka 作为一个高度可扩展可容错的消息系统，它有很多基本概念，下面就来认识一下这些 Kafka 专属的概念topicTopic 被称为主题，在 kafka 中，使用一个类别属性来划分消息的所属类，划分消息的这个类称为 topic。topic 相当于消息的分配标签，是一个逻辑概念。主题好比是数据库的表，或者文件系统中的文件夹。partitionpartition 译为分区，topic 中的消息被分割为一个或多个的 partition，它是一个物理概念，对应到系统上的就是一个或若干个目录，一个分区就是一个 提交日志。消息以追加的形式写入分区，先后以顺序的方式读取。注意：由于一个主题包含无数个分区，因此无法保证在整个 topic 中有序，但是单个 Partition 分区可以保证有序。消息被迫加写入每个分区的尾部。Kafka 通过分区来实现数据冗余和伸缩性分区可以分布在不同的服务器上，也就是说，一个主题可以跨越多个服务器，以此来提供比单个服务器更强大的性能。segmentSegment 被译为段，将 Partition 进一步细分为若干个 segment，每个 segment 文件的大小相等。brokerKafka 集群包含一个或多个服务器，每个 Kafka 中服务器被称为 broker。broker 接收来自生产者的消息，为消息设置偏移量，并提交消息到磁盘保存。broker 为消费者提供服务，对读取分区的请求作出响应，返回已经提交到磁盘上的消息。broker 是集群的组成部分，每个集群中都会有一个 broker 同时充当了 集群控制器(Leader)的角色，它是由集群中的活跃成员选举出来的。每个集群中的成员都有可能充当 Leader，Leader 负责管理工作，包括将分区分配给 broker 和监控 broker。集群中，一个分区从属于一个 Leader，但是一个分区可以分配给多个 broker（非Leader），这时候会发生分区复制。这种复制的机制为分区提供了消息冗余，如果一个 broker 失效，那么其他活跃用户会重新选举一个 Leader 接管。producer生产者，即消息的发布者，其会将某 topic 的消息发布到相应的 partition 中。生产者在默认情况下把消息均衡地分布到主题的所有分区上，而并不关心特定消息会被写到哪个分区。不过，在某些情况下，生产者会把消息直接写到指定的分区。consumer消费者，即消息的使用者，一个消费者可以消费多个 topic 的消息，对于某一个 topic 的消息，其只会消费同一个 partition 中的消息在了解完 Kafka 的基本概念之后，我们通过搭建 Kafka 集群来进一步深刻认识一下 Kafka。确保安装环境安装 Java 环境在安装 Kafka 之前，先确保Linux 环境上是否有 Java 环境，使用 java -version 命令查看 Java 版本，推荐使用Jdk 1.8 ，如果没有安装 Java 环境的话，可以按照这篇文章进行安装。安装 Zookeeper 环境Kafka 的底层使用 Zookeeper 储存元数据，确保一致性，所以安装 Kafka 前需要先安装 Zookeeper，Kafka 的发行版自带了 Zookeeper ，可以直接使用脚本来启动，不过安装一个 Zookeeper 也不费劲Zookeeper 单机搭建Zookeeper 单机搭建比较简单，直接从官网下载一个稳定版本的 Zookeeper ，这里我使用的是 3.4.10，下载完成后，在 Linux 系统中的 /usr/local 目录下创建 zookeeper 文件夹，使用xftp 工具(xftp 和 xshell 工具都可以在官网申请免费的家庭版)把下载好的 zookeeper 压缩包放到 /usr/local/zookeeper 目录下。如果下载的是一个 tar.gz 包的话，直接使用 tar -zxvf zookeeper-3.4.10.tar.gz解压即可。如果下载的是 zip 包的话，还要检查一下 Linux 中是否有 unzip 工具，如果没有的话，使用 yum install unzip 安装 zip 解压工具，完成后使用 unzip zookeeper-3.4.10.zip 解压即可。解压完成后，cd 到 /usr/local/zookeeper/zookeeper-3.4.10 ，创建一个 data 文件夹，然后进入到 conf 文件夹下，使用 mv zoo_sample.cfg zoo.cfg 进行重命名操作。然后使用 vi 打开 zoo.cfg ，更改一下dataDir = /usr/local/zookeeper/zookeeper-3.4.10/data ，保存。进入bin目录，启动服务输入命令./zkServer.sh start 输出下面内容表示搭建成功关闭服务输入命令：1./zkServer.sh stop使用 ./zkServer.sh status 可以查看状态信息。Zookeeper 集群搭建准备条件准备条件：需要三个服务器，这里我使用了CentOS7 并安装了三个虚拟机，并为各自的虚拟机分配了1GB的内存，在每个 /usr/local/ 下面新建 zookeeper 文件夹，把 zookeeper 的压缩包挪过来，解压，完成后会有 zookeeper-3.4.10 文件夹，进入到文件夹，新建两个文件夹，分别是 data 和log文件夹注：上一节单机搭建中已经创建了一个data 文件夹，就不需要重新创建了，直接新建一个 log 文件夹，对另外两个新增的服务需要新建这两个文件夹。设置集群新建完成后，需要编辑 conf/zoo.cfg 文件，三个文件的内容如下123456789tickTime=2000initLimit=10syncLimit=5dataDir=/usr/local/zookeeper/zookeeper-3.4.10/datadataLogDir=/usr/local/zookeeper/zookeeper-3.4.10/logclientPort=12181server.1=192.168.1.7:12888:13888server.2=192.168.1.8:12888:13888server.3=192.168.1.9:12888:13888server.1 中的这个 1 表示的是服务器的标识也可以是其他数字，表示这是第几号服务器，这个标识要和下面我们配置的 myid 的标识一致可以。192.168.1.7:12888:13888 为集群中的 ip 地址，第一个端口表示的是 master 与 slave 之间的通信接口，默认是 2888，第二个端口是leader选举的端口，集群刚启动的时候选举或者leader挂掉之后进行新的选举的端口，默认是 3888现在对上面的配置文件进行解释tickTime: 这个时间是作为 Zookeeper 服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每个 tickTime 时间就会发送一个心跳。initLimit：这个配置项是用来配置 Zookeeper 接受客户端（这里所说的客户端不是用户连接 Zookeeper 服务器的客户端，而是 Zookeeper 服务器集群中连接到 Leader 的 Follower 服务器）初始化连接时最长能忍受多少个心跳时间间隔数。当已经超过 5个心跳的时间（也就是 tickTime）长度后 Zookeeper 服务器还没有收到客户端的返回信息，那么表明这个客户端连接失败。总的时间长度就是 5*2000=10 秒syncLimit: 这个配置项标识 Leader 与Follower 之间发送消息，请求和应答时间长度，最长不能超过多少个 tickTime 的时间长度，总的时间长度就是5*2000=10秒dataDir: 快照日志的存储路径dataLogDir: 事务日志的存储路径，如果不配置这个那么事务日志会默认存储到dataDir指定的目录，这样会严重影响zk的性能，当zk吞吐量较大的时候，产生的事务日志、快照日志太多clientPort: 这个端口就是客户端连接 Zookeeper 服务器的端口，Zookeeper 会监听这个端口，接受客户端的访问请求。创建 myid 文件在了解完其配置文件后，现在来创建每个集群节点的 myid ，我们上面说过，这个 myid 就是 server.1 的这个 1 ，类似的，需要为集群中的每个服务都指定标识，使用 echo 命令进行创建123456# server.1echo "1" &gt; /usr/local/zookeeper/zookeeper-3.4.10/data/myid# server.2echo "2" &gt; /usr/local/zookeeper/zookeeper-3.4.10/data/myid# server.3echo "3" &gt; /usr/local/zookeeper/zookeeper-3.4.10/data/myid启动服务并测试配置完成，为每个 zk 服务启动并测试，我的测试结果如下启动服务（每台都需要执行）1cd /usr/local/zookeeper/zookeeper-3.4.10/bin./zkServer.sh start检查服务状态使用 ./zkServer.sh status 命令检查服务状态192.168.1.7 — follower192.168.1.8 — leader192.168.1.9 — followerzk集群一般只有一个leader，多个follower，主一般是相应客户端的读写请求，而从主同步数据，当主挂掉之后就会从follower里投票选举一个leader出来。Kafka 集群搭建准备条件搭建好的 Zookeeper 集群Kafka 压缩包在 /usr/local 下新建 kafka 文件夹，然后把下载完成的 tar.gz 包移到 /usr/local/kafka 目录下，使用 tar -zxvf 压缩包 进行解压，解压完成后，进入到 kafka_2.12-2.3.0 目录下，新建 log 文件夹，进入到 config 目录下我们可以看到有很多 properties 配置文件，这里主要关注 server.properties 这个文件即可。kafka 启动方式有两种，一种是使用 kafka 自带的 zookeeper 配置文件来启动（可以按照官网来进行启动，并使用单个服务多个节点来模拟集群），一种是通过使用独立的zk集群来启动，这里推荐使用第二种方式，使用 zk 集群来启动修改配置项需要为每个服务都修改一下配置项，也就是server.properties， 需要更新和添加的内容有12345678910broker.id=0 //初始是0，每个 server 的broker.id 都应该设置为不一样的，就和 myid 一样 我的三个服务分别设置的是 1,2,3log.dirs=/usr/local/kafka/kafka_2.12-2.3.0/log#在log.retention.hours=168 下面新增下面三项message.max.byte=5242880default.replication.factor=2replica.fetch.max.bytes=5242880#设置zookeeper的连接端口zookeeper.connect=192.168.1.7:2181,192.168.1.8:2181,192.168.1.9:2181配置项的含义123456789101112131415161718broker.id=0 #当前机器在集群中的唯一标识，和zookeeper的myid性质一样port=9092 #当前kafka对外提供服务的端口默认是9092host.name=192.168.1.7 #这个参数默认是关闭的，在0.8.1有个bug，DNS解析问题，失败率的问题。num.network.threads=3 #这个是borker进行网络处理的线程数num.io.threads=8 #这个是borker进行I/O处理的线程数log.dirs=/usr/local/kafka/kafka_2.12-2.3.0/log #消息存放的目录，这个目录可以配置为“，”逗号分割的表达式，上面的num.io.threads要大于这个目录的个数这个目录，如果配置多个目录，新创建的topic他把消息持久化的地方是，当前以逗号分割的目录中，那个分区数最少就放那一个socket.send.buffer.bytes=102400 #发送缓冲区buffer大小，数据不是一下子就发送的，先回存储到缓冲区了到达一定的大小后在发送，能提高性能socket.receive.buffer.bytes=102400 #kafka接收缓冲区大小，当数据到达一定大小后在序列化到磁盘socket.request.max.bytes=104857600 #这个参数是向kafka请求消息或者向kafka发送消息的请请求的最大数，这个值不能超过java的堆栈大小num.partitions=1 #默认的分区数，一个topic默认1个分区数log.retention.hours=168 #默认消息的最大持久化时间，168小时，7天message.max.byte=5242880 #消息保存的最大值5Mdefault.replication.factor=2 #kafka保存消息的副本数，如果一个副本失效了，另一个还可以继续提供服务replica.fetch.max.bytes=5242880 #取消息的最大直接数log.segment.bytes=1073741824 #这个参数是：因为kafka的消息是以追加的形式落地到文件，当超过这个值的时候，kafka会新起一个文件log.retention.check.interval.ms=300000 #每隔300000毫秒去检查上面配置的log失效时间（log.retention.hours=168 ），到目录查看是否有过期的消息如果有，删除log.cleaner.enable=false #是否启用log压缩，一般不用启用，启用的话可以提高性能zookeeper.connect=192.168.1.7:2181,192.168.1.8:2181,192.168.1.9:2181 #设置zookeeper的连接端口启动 Kafka 集群并测试启动服务，进入到 /usr/local/kafka/kafka_2.12-2.3.0/bin 目录下1./kafka-server-start.sh -daemon ../config/server.properties # 启动后台进程检查服务是否启动1234jps # 执行命令 6201 QuorumPeerMain7035 Jps6972 Kafkakafka 已经启动创建 Topic 来验证是否创建成功12cd .. # 往回退一层 到 /usr/local/kafka/kafka_2.12-2.3.0 目录下bin/kafka-topics.sh --create --zookeeper 192.168.1.7:2181 --replication-factor 2 --partitions 1 --topic cxuan对上面的解释–replication-factor 2 复制两份–partitions 1 创建1个分区–topic 创建主题查看我们的主题是否创建成功1bin/kafka-topics.sh --list --zookeeper 192.168.1.7:2181启动一个服务就能把集群启动起来在一台机器上创建一个发布者1./kafka-console-producer.sh --broker-list 192.168.1.7:9092 --topic cxuantopic # 创建一个broker发布者在一台服务器上创建一个订阅者1bin/kafka-console-consumer.sh --bootstrap-server 192.168.1.7:9092 --topic cxuantopic --from-beginning # 创建一个consumer消费者注意：这里使用 –zookeeper 的话可能出现 zookeeper is not a recognized option 的错误，这是因为 kafka 版本太高，需要使用 --bootstrap-server 指令测试结果发布消费其他命令显示 topic12bin/kafka-topics.sh --list --zookeeper 192.168.1.7:2181cxuantopic #显示查看 topic 状态12345678bin/kafka-topics.sh --describe --zookeeper 192.168.1.7:2181 --topic cxuantopic# 下面是显示的详细信息Topic:cxuantopic PartitionCount:1 ReplicationFactor:2 Configs:Topic: cxuantopic Partition: 0 Leader: 1 Replicas: 1,2 Isr: 1,2# 分区为为1 复制因子为2 主题 cxuantopic 的分区为0 # Replicas: 0,1 复制的为1，2Leader 负责给定分区的所有读取和写入的节点，每个节点都会通过随机选择成为 leader。Replicas 是为该分区复制日志的节点列表，无论它们是 Leader 还是当前处于活动状态。Isr 是同步副本的集合。它是副本列表的子集，当前仍处于活动状态并追随Leader。至此，kafka 集群搭建完毕。验证多节点接收数据刚刚我们都使用的是 相同的ip 服务，下面使用其他集群中的节点，验证是否能够接受到服务在另外两个节点上使用1bin/kafka-console-consumer.sh --bootstrap-server 192.168.1.7:9092 --topic cxuantopic --from-beginning然后再使用 broker 进行消息发送，经测试三个节点都可以接受到消息。配置详解在搭建 Kafka 的时候我们简单介绍了一下 server.properties 中配置的含义，现在我们来详细介绍一下参数的配置和概念常规配置这些参数是 kafka 中最基本的配置broker.id每个 broker 都需要有一个标识符，使用 broker.id 来表示。它的默认值是 0，它可以被设置成其他任意整数，在集群中需要保证每个节点的 broker.id 都是唯一的。port如果使用配置样本来启动 kafka ，它会监听 9092 端口，修改 port 配置参数可以把它设置成其他任意可用的端口。zookeeper.connect用于保存 broker 元数据的地址是通过 zookeeper.connect 来指定。localhost:2181 表示运行在本地 2181 端口。该配置参数是用逗号分隔的一组 hostname:port/path 列表，每一部分含义如下：hostname 是 zookeeper 服务器的服务名或 IP 地址port 是 zookeeper 连接的端口/path 是可选的 zookeeper 路径，作为 Kafka 集群的 chroot 环境。如果不指定，默认使用跟路径log.dirsKafka 把消息都保存在磁盘上，存放这些日志片段的目录都是通过 log.dirs 来指定的。它是一组用逗号分隔的本地文件系统路径。如果指定了多个路径，那么 broker 会根据 “最少使用” 原则，把同一分区的日志片段保存到同一路径下。要注意，broker 会向拥有最少数目分区的路径新增分区，而不是向拥有最小磁盘空间的路径新增分区。num.recovery.threads.per.data.dir对于如下 3 种情况，Kafka 会使用可配置的线程池来处理日志片段服务器正常启动，用于打开每个分区的日志片段；服务器崩溃后启动，用于检查和截断每个分区的日志片段；服务器正常关闭，用于关闭日志片段默认情况下，每个日志目录只使用一个线程。因为这些线程只是在服务器启动和关闭时会用到，所以完全可以设置大量的线程来达到井行操作的目的。特别是对于包含大量分区的服务器来说，一旦发生崩愤，在进行恢复时使用井行操作可能会省下数小时的时间。设置此参数时需要注意，所配置的数字对应的是 log.dirs 指定的单个日志目录。也就是说，如果 num.recovery.threads.per.data.dir 被设为 8，并且 log.dir 指定了 3 个路径，那么总共需要 24 个线程。auto.create.topics.enable默认情况下，Kafka 会在如下 3 种情况下创建主题当一个生产者开始往主题写入消息时当一个消费者开始从主题读取消息时当任意一个客户向主题发送元数据请求时delete.topic.enable如果你想要删除一个主题，你可以使用主题管理工具。默认情况下，是不允许删除主题的，delete.topic.enable 的默认值是 false 因此你不能随意删除主题。这是对生产环境的合理性保护，但是在开发环境和测试环境，是可以允许你删除主题的，所以，如果你想要删除主题，需要把 delete.topic.enable 设为 true。主题默认配置Kafka 为新创建的主题提供了很多默认配置参数，下面就来一起认识一下这些参数num.partitionsnum.partitions 参数指定了新创建的主题需要包含多少个分区。如果启用了主题自动创建功能（该功能是默认启用的），主题分区的个数就是该参数指定的值。该参数的默认值是 1。要注意，我们可以增加主题分区的个数，但不能减少分区的个数。default.replication.factor这个参数比较简单，它表示 kafka保存消息的副本数，如果一个副本失效了，另一个还可以继续提供服务default.replication.factor 的默认值为1，这个参数在你启用了主题自动创建功能后有效。log.retention.msKafka 通常根据时间来决定数据可以保留多久。默认使用 log.retention.hours 参数来配置时间，默认是 168 个小时，也就是一周。除此之外，还有两个参数 log.retention.minutes 和 log.retentiion.ms 。这三个参数作用是一样的，都是决定消息多久以后被删除，推荐使用 log.retention.ms。log.retention.bytes另一种保留消息的方式是判断消息是否过期。它的值通过参数 log.retention.bytes 来指定，作用在每一个分区上。也就是说，如果有一个包含 8 个分区的主题，并且 log.retention.bytes 被设置为 1GB，那么这个主题最多可以保留 8GB 数据。所以，当主题的分区个数增加时，整个主题可以保留的数据也随之增加。log.segment.bytes上述的日志都是作用在日志片段上，而不是作用在单个消息上。当消息到达 broker 时，它们被追加到分区的当前日志片段上，当日志片段大小到达 log.segment.bytes 指定上限（默认为 1GB）时，当前日志片段就会被关闭，一个新的日志片段被打开。如果一个日志片段被关闭，就开始等待过期。这个参数的值越小，就越会频繁的关闭和分配新文件，从而降低磁盘写入的整体效率。log.segment.ms上面提到日志片段经关闭后需等待过期，那么 log.segment.ms 这个参数就是指定日志多长时间被关闭的参数和，log.segment.ms 和 log.retention.bytes 也不存在互斥问题。日志片段会在大小或时间到达上限时被关闭，就看哪个条件先得到满足。message.max.bytesbroker 通过设置 message.max.bytes 参数来限制单个消息的大小，默认是 1000 000， 也就是 1MB，如果生产者尝试发送的消息超过这个大小，不仅消息不会被接收，还会收到 broker 返回的错误消息。跟其他与字节相关的配置参数一样，该参数指的是压缩后的消息大小，也就是说，只要压缩后的消息小于 mesage.max.bytes，那么消息的实际大小可以大于这个值这个值对性能有显著的影响。值越大，那么负责处理网络连接和请求的线程就需要花越多的时间来处理这些请求。它还会增加磁盘写入块的大小，从而影响 IO 吞吐量。]]></content>
      <categories>
        <category>技术</category>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>kafka原理</tag>
        <tag>消息队列</tag>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka相关讨论]]></title>
    <url>%2F2023%2F04e6ad34c9.html</url>
    <content type="text"><![CDATA[Kafka 是一个消息系统，原本开发自 LinkedIn，用作 LinkedIn 的活动流（Activity Stream）和运营数据处理管道（Pipeline）的基础。现在它已被多家不同类型的公司 作为多种类型的数据管道和消息系统使用。活动流数据是几乎所有站点在对其网站使用情况做报表时都要用到的数据中最常规的部分。活动数据包括页面访问量（Page View）、被查看内容方面的信息以及搜索情况等内容。这种数据通常的处理方式是先把各种活动以日志的形式写入某种文件，然后周期性地对这些文件进行统计分析。运营数据指的是服务器的性能数据（CPU、IO 使用率、请求时间、服务日志等等数据)。运营数据的统计方法种类繁多。近年来，活动和运营数据处理已经成为了网站软件产品特性中一个至关重要的组成部分，这就需要一套稍微更加复杂的基础设施对其提供支持。Kafka 简介Kafka 是一种分布式的，基于发布 / 订阅的消息系统。主要设计目标如下：以时间复杂度为 O(1) 的方式提供消息持久化能力，即使对 TB 级以上数据也能保证常数时间复杂度的访问性能。高吞吐率。即使在非常廉价的商用机器上也能做到单机支持每秒 100K 条以上消息的传输。支持 Kafka Server 间的消息分区，及分布式消费，同时保证每个 Partition 内的消息顺序传输。同时支持离线数据处理和实时数据处理。Scale out：支持在线水平扩展。Kafka 基础概念概念一：生产者与消费者对于 Kafka 来说客户端有两种基本类型：生产者（Producer）和消费者（Consumer）。除此之外，还有用来做数据集成的 Kafka Connect API 和流式处理的 Kafka Streams 等高阶客户端，但这些高阶客户端底层仍然是生产者和消费者API，它们只不过是在上层做了封装。这很容易理解，生产者（也称为发布者）创建消息，而消费者（也称为订阅者）负责消费or读取消息。概念二：主题（Topic）与分区（Partition）在 Kafka 中，消息以主题（Topic）来分类，每一个主题都对应一个「消息队列」，这有点儿类似于数据库中的表。但是如果我们把所有同类的消息都塞入到一个“中心”队列中，势必缺少可伸缩性，无论是生产者/消费者数目的增加，还是消息数量的增加，都可能耗尽系统的性能或存储。我们使用一个生活中的例子来说明：现在 A 城市生产的某商品需要运输到 B 城市，走的是公路，那么单通道的高速公路不论是在「A 城市商品增多」还是「现在 C 城市也要往 B 城市运输东西」这样的情况下都会出现「吞吐量不足」的问题。所以我们现在引入分区（Partition）的概念，类似“允许多修几条道”的方式对我们的主题完成了水平扩展。概念三：Broker 和集群（Cluster）一个 Kafka 服务器也称为 Broker，它接受生产者发送的消息并存入磁盘；Broker 同时服务消费者拉取分区消息的请求，返回目前已经提交的消息。使用特定的机器硬件，一个 Broker 每秒可以处理成千上万的分区和百万量级的消息。（现在动不动就百万量级..我特地去查了一把，好像确实集群的情况下吞吐量挺高的..摁..）若干个 Broker 组成一个集群（Cluster），其中集群内某个 Broker 会成为集群控制器（Cluster Controller），它负责管理集群，包括分配分区到 Broker、监控 Broker 故障等。在集群内，一个分区由一个 Broker 负责，这个 Broker 也称为这个分区的 Leader；当然一个分区可以被复制到多个 Broker 上来实现冗余，这样当存在 Broker 故障时可以将其分区重新分配到其他 Broker 来负责。下图是一个样例：Kafka 的一个关键性质是日志保留（retention），我们可以配置主题的消息保留策略，譬如只保留一段时间的日志或者只保留特定大小的日志。当超过这些限制时，老的消息会被删除。我们也可以针对某个主题单独设置消息过期策略，这样对于不同应用可以实现个性化。概念四：多集群随着业务发展，我们往往需要多集群，通常处于下面几个原因：基于数据的隔离；基于安全的隔离；多数据中心（容灾）当构建多个数据中心时，往往需要实现消息互通。举个例子，假如用户修改了个人资料，那么后续的请求无论被哪个数据中心处理，这个更新需要反映出来。又或者，多个数据中心的数据需要汇总到一个总控中心来做数据分析。上面说的分区复制冗余机制只适用于同一个 Kafka 集群内部，对于多个 Kafka 集群消息同步可以使用 Kafka 提供的 MirrorMaker 工具。本质上来说，MirrorMaker 只是一个 Kafka 消费者和生产者，并使用一个队列连接起来而已。它从一个集群中消费消息，然后往另一个集群生产消息。Kafka 的设计与实现上面我们知道了 Kafka 中的一些基本概念，但作为一个成熟的「消息队列」中间件，其中有许多有意思的设计值得我们思考，下面我们简单列举一些。讨论一：Kafka 存储在文件系统上是的，您首先应该知道 Kafka 的消息是存在于文件系统之上的。Kafka 高度依赖文件系统来存储和缓存消息，一般的人认为 “磁盘是缓慢的”，所以对这样的设计持有怀疑态度。实际上，磁盘比人们预想的快很多也慢很多，这取决于它们如何被使用；一个好的磁盘结构设计可以使之跟网络速度一样快。现代的操作系统针对磁盘的读写已经做了一些优化方案来加快磁盘的访问速度。比如，预读会提前将一个比较大的磁盘快读入内存。后写会将很多小的逻辑写操作合并起来组合成一个大的物理写操作。并且，操作系统还会将主内存剩余的所有空闲内存空间都用作磁盘缓存，所有的磁盘读写操作都会经过统一的磁盘缓存（除了直接 I/O 会绕过磁盘缓存）。综合这几点优化特点，如果是针对磁盘的顺序访问，某些情况下它可能比随机的内存访问都要快，甚至可以和网络的速度相差无几。上述的 Topic 其实是逻辑上的概念，面相消费者和生产者，物理上存储的其实是 Partition，每一个 Partition 最终对应一个目录，里面存储所有的消息和索引文件。默认情况下，每一个 Topic 在创建时如果不指定 Partition 数量时只会创建 1 个 Partition。比如，我创建了一个 Topic 名字为 test ，没有指定 Partition 的数量，那么会默认创建一个 test-0 的文件夹，这里的命名规则是：&lt;topic_name&gt;-&lt;partition_id&gt;。任何发布到 Partition 的消息都会被追加到 Partition 数据文件的尾部，这样的顺序写磁盘操作让 Kafka 的效率非常高（经验证，顺序写磁盘效率比随机写内存还要高，这是 Kafka 高吞吐率的一个很重要的保证）。每一条消息被发送到 Broker 中，会根据 Partition 规则选择被存储到哪一个 Partition。如果 Partition 规则设置的合理，所有消息可以均匀分布到不同的 Partition中。讨论二：Kafka 中的底层存储设计假设我们现在 Kafka 集群只有一个 Broker，我们创建 2 个 Topic 名称分别为：「topic1」和「topic2」，Partition 数量分别为 1、2，那么我们的根目录下就会创建如下三个文件夹：123| --topic1-0| --topic2-0| --topic2-1在 Kafka 的文件存储中，同一个 Topic 下有多个不同的 Partition，每个 Partition 都为一个目录，而每一个目录又被平均分配成多个大小相等的 Segment File 中，Segment File 又由 index file 和 data file 组成，他们总是成对出现，后缀 “.index” 和 “.log” 分表表示 Segment 索引文件和数据文件。现在假设我们设置每个 Segment 大小为 500 MB，并启动生产者向 topic1 中写入大量数据，topic1-0 文件夹中就会产生类似如下的一些文件：1234567891011| --topic1-0 | --00000000000000000000.index | --00000000000000000000.log | --00000000000000368769.index | --00000000000000368769.log | --00000000000000737337.index | --00000000000000737337.log | --00000000000001105814.index | --00000000000001105814.log| --topic2-0| --topic2-1Segment 是 Kafka 文件存储的最小单位。Segment 文件命名规则：Partition 全局的第一个 Segment 从 0 开始，后续每个 Segment 文件名为上一个 Segment 文件最后一条消息的 offset 值。数值最大为 64 位 long 大小，19 位数字字符长度，没有数字用0填充。如 00000000000000368769.index 和 00000000000000368769.log。以上面的一对 Segment File 为例，说明一下索引文件和数据文件对应关系：其中以索引文件中元数据 &lt;3, 497&gt; 为例，依次在数据文件中表示第 3 个 message（在全局 Partition 表示第 368769 + 3 = 368772 个 message）以及该消息的物理偏移地址为 497。注意该 index 文件并不是从0开始，也不是每次递增1的，这是因为 Kafka 采取稀疏索引存储的方式，每隔一定字节的数据建立一条索引，它减少了索引文件大小，使得能够把 index 映射到内存，降低了查询时的磁盘 IO 开销，同时也并没有给查询带来太多的时间消耗。因为其文件名为上一个 Segment 最后一条消息的 offset ，所以当需要查找一个指定 offset 的 message 时，通过在所有 segment 的文件名中进行二分查找就能找到它归属的 segment ，再在其 index 文件中找到其对应到文件上的物理位置，就能拿出该 message 。由于消息在 Partition 的 Segment 数据文件中是顺序读写的，且消息消费后不会删除（删除策略是针对过期的 Segment 文件），这种顺序磁盘 IO 存储设计师 Kafka 高性能很重要的原因。Kafka 是如何准确的知道 message 的偏移的呢？这是因为在 Kafka 定义了标准的数据存储结构，在 Partition 中的每一条 message 都包含了以下三个属性：offset：表示 message 在当前 Partition 中的偏移量，是一个逻辑上的值，唯一确定了 Partition 中的一条 message，可以简单的认为是一个 id；MessageSize：表示 message 内容 data 的大小；data：message 的具体内容讨论三：生产者设计概要当我们发送消息之前，先问几个问题：每条消息都是很关键且不能容忍丢失么？偶尔重复消息可以么？我们关注的是消息延迟还是写入消息的吞吐量？举个例子，有一个信用卡交易处理系统，当交易发生时会发送一条消息到 Kafka，另一个服务来读取消息并根据规则引擎来检查交易是否通过，将结果通过 Kafka 返回。对于这样的业务，消息既不能丢失也不能重复，由于交易量大因此吞吐量需要尽可能大，延迟可以稍微高一点。再举个例子，假如我们需要收集用户在网页上的点击数据，对于这样的场景，少量消息丢失或者重复是可以容忍的，延迟多大都不重要只要不影响用户体验，吞吐则根据实时用户数来决定。不同的业务需要使用不同的写入方式和配置。具体的方式我们在这里不做讨论，现在先看下生产者写消息的基本流程：流程如下：首先，我们需要创建一个ProducerRecord，这个对象需要包含消息的主题（topic）和值（value），可以选择性指定一个键值（key）或者分区（partition）。发送消息时，生产者会对键值和值序列化成字节数组，然后发送到分配器（partitioner）。如果我们指定了分区，那么分配器返回该分区即可；否则，分配器将会基于键值来选择一个分区并返回。选择完分区后，生产者知道了消息所属的主题和分区，它将这条记录添加到相同主题和分区的批量消息中，另一个线程负责发送这些批量消息到对应的Kafka broker。当broker接收到消息后，如果成功写入则返回一个包含消息的主题、分区及位移的RecordMetadata对象，否则返回异常。生产者接收到结果后，对于异常可能会进行重试。讨论四：消费者设计概要消费者与消费组假设这么个场景：我们从Kafka中读取消息，并且进行检查，最后产生结果数据。我们可以创建一个消费者实例去做这件事情，但如果生产者写入消息的速度比消费者读取的速度快怎么办呢？这样随着时间增长，消息堆积越来越严重。对于这种场景，我们需要增加多个消费者来进行水平扩展。Kafka消费者是消费组的一部分，当多个消费者形成一个消费组来消费主题时，每个消费者会收到不同分区的消息。假设有一个T1主题，该主题有4个分区；同时我们有一个消费组G1，这个消费组只有一个消费者C1。那么消费者C1将会收到这4个分区的消息，如下所示：如果我们增加新的消费者C2到消费组G1，那么每个消费者将会分别收到两个分区的消息，如下所示：如果增加到4个消费者，那么每个消费者将会分别收到一个分区的消息，如下所示：但如果我们继续增加消费者到这个消费组，剩余的消费者将会空闲，不会收到任何消息：总而言之，我们可以通过增加消费组的消费者来进行水平扩展提升消费能力。这也是为什么建议创建主题时使用比较多的分区数，这样可以在消费负载高的情况下增加消费者来提升性能。另外，消费者的数量不应该比分区数多，因为多出来的消费者是空闲的，没有任何帮助。Kafka一个很重要的特性就是，只需写入一次消息，可以支持任意多的应用读取这个消息。换句话说，每个应用都可以读到全量的消息。为了使得每个应用都能读到全量消息，应用需要有不同的消费组。对于上面的例子，假如我们新增了一个新的消费组G2，而这个消费组有两个消费者，那么会是这样的：在这个场景中，消费组G1和消费组G2都能收到T1主题的全量消息，在逻辑意义上来说它们属于不同的应用。最后，总结起来就是：如果应用需要读取全量消息，那么请为该应用设置一个消费组；如果该应用消费能力不足，那么可以考虑在这个消费组里增加消费者。消费组与分区重平衡可以看到，当新的消费者加入消费组，它会消费一个或多个分区，而这些分区之前是由其他消费者负责的；另外，当消费者离开消费组（比如重启、宕机等）时，它所消费的分区会分配给其他分区。这种现象称为重平衡（rebalance）。重平衡是 Kafka 一个很重要的性质，这个性质保证了高可用和水平扩展。不过也需要注意到，在重平衡期间，所有消费者都不能消费消息，因此会造成整个消费组短暂的不可用。而且，将分区进行重平衡也会导致原来的消费者状态过期，从而导致消费者需要重新更新状态，这段期间也会降低消费性能。消费者通过定期发送心跳（hearbeat）到一个作为组协调者（group coordinator）的 broker 来保持在消费组内存活。这个 broker 不是固定的，每个消费组都可能不同。当消费者拉取消息或者提交时，便会发送心跳。如果消费者超过一定时间没有发送心跳，那么它的会话（session）就会过期，组协调者会认为该消费者已经宕机，然后触发重平衡。可以看到，从消费者宕机到会话过期是有一定时间的，这段时间内该消费者的分区都不能进行消息消费；通常情况下，我们可以进行优雅关闭，这样消费者会发送离开的消息到组协调者，这样组协调者可以立即进行重平衡而不需要等待会话过期。在 0.10.1 版本，Kafka 对心跳机制进行了修改，将发送心跳与拉取消息进行分离，这样使得发送心跳的频率不受拉取的频率影响。另外更高版本的 Kafka 支持配置一个消费者多长时间不拉取消息但仍然保持存活，这个配置可以避免活锁（livelock）。活锁，是指应用没有故障但是由于某些原因不能进一步消费。Partition 与消费模型上面提到，Kafka 中一个 topic 中的消息是被打散分配在多个 Partition(分区) 中存储的， Consumer Group 在消费时需要从不同的 Partition 获取消息，那最终如何重建出 Topic 中消息的顺序呢？答案是：没有办法。Kafka 只会保证在 Partition 内消息是有序的，而不管全局的情况。下一个问题是：Partition 中的消息可以被（不同的 Consumer Group）多次消费，那 Partition中被消费的消息是何时删除的？ Partition 又是如何知道一个 Consumer Group 当前消费的位置呢？无论消息是否被消费，除非消息到期 Partition 从不删除消息。例如设置保留时间为 2 天，则消息发布 2 天内任何 Group 都可以消费，2 天后，消息自动被删除。Partition 会为每个 Consumer Group 保存一个偏移量，记录 Group 消费到的位置。 如下图：为什么 Kafka 是 pull 模型消费者应该向 Broker 要数据（pull）还是 Broker 向消费者推送数据（push）？作为一个消息系统，Kafka 遵循了传统的方式，选择由 Producer 向 broker push 消息并由 Consumer 从 broker pull 消息。一些 logging-centric system，比如 Facebook 的Scribe和 Cloudera 的Flume，采用 push 模式。事实上，push 模式和 pull 模式各有优劣。push 模式很难适应消费速率不同的消费者，因为消息发送速率是由 broker 决定的。push 模式的目标是尽可能以最快速度传递消息，但是这样很容易造成 Consumer 来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。而 pull 模式则可以根据 Consumer 的消费能力以适当的速率消费消息。对于 Kafka 而言，pull 模式更合适。pull 模式可简化 broker 的设计，Consumer 可自主控制消费消息的速率，同时 Consumer 可以自己控制消费方式——即可批量消费也可逐条消费，同时还能选择不同的提交方式从而实现不同的传输语义。讨论五：Kafka 如何保证可靠性当我们讨论可靠性的时候，我们总会提到*保证**这个词语。可靠性保证是基础，我们基于这些基础之上构建我们的应用。比如关系型数据库的可靠性保证是ACID，也就是原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）和持久性（Durability）。Kafka 中的可靠性保证有如下四点：对于一个分区来说，它的消息是有序的。如果一个生产者向一个分区先写入消息A，然后写入消息B，那么消费者会先读取消息A再读取消息B。当消息写入所有in-sync状态的副本后，消息才会认为已提交（committed）。这里的写入有可能只是写入到文件系统的缓存，不一定刷新到磁盘。生产者可以等待不同时机的确认，比如等待分区主副本写入即返回，后者等待所有in-sync状态副本写入才返回。一旦消息已提交，那么只要有一个副本存活，数据不会丢失。消费者只能读取到已提交的消息。使用这些基础保证，我们构建一个可靠的系统，这时候需要考虑一个问题：究竟我们的应用需要多大程度的可靠性？可靠性不是无偿的，它与系统可用性、吞吐量、延迟和硬件价格息息相关，得此失彼。因此，我们往往需要做权衡，一味的追求可靠性并不实际。动手搭一个 Kafka通过上面的描述，我们已经大致了解到了「Kafka」是何方神圣了，现在我们开始尝试自己动手本地搭一个来实际体验一把。下载 Kafka这里以 Mac OS 为例，在安装了 Homebrew 的情况下执行下列代码：1brew install kafka由于 Kafka 依赖了 Zookeeper，所以在下载的时候会自动下载。启动服务我们在启动之前首先需要修改 Kafka 的监听地址和端口为 localhost:9092：1vi /usr/local/etc/kafka/server.properties然后修改成下图的样子：依次启动 Zookeeper 和 Kafka：12brew services start zookeeperbrew services start kafka然后执行下列语句来创建一个名字为 “test” 的 Topic：1kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test我们可以通过下列的命令查看我们的 Topic 列表：1kafka-topics --list --zookeeper localhost:2181发送消息然后我们新建一个控制台，运行下列命令创建一个消费者关注刚才创建的 Topic：1kafka-console-consumer --bootstrap-server localhost:9092 --topic test --from-beginning用控制台往刚才创建的 Topic 中添加消息，并观察刚才创建的消费者窗口：1kafka-console-producer --broker-list localhost:9092 --topic test能通过消费者窗口观察到正确的消息：]]></content>
      <categories>
        <category>技术</category>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>kafka原理</tag>
        <tag>消息队列</tag>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka原理详解]]></title>
    <url>%2F2023%2F04f0560a40.html</url>
    <content type="text"><![CDATA[Kafka是最初由Linkedin公司开发，是一个分布式、支持分区的（partition）、多副本的（replica），基于zookeeper协调的分布式消息系统，它的最大的特性就是可以实时的处理大量数据以满足各种需求场景：比如基于hadoop的批处理系统、低延迟的实时系统、storm/Spark流式处理引擎，web/nginx日志、访问日志，消息服务等等，用scala语言编写，Linkedin于2010年贡献给了Apache基金会并成为顶级开源 项目。概念理解产生背景当今社会各种应用系统诸如商业、社交、搜索、浏览等像信息工厂一样不断的生产出各种信息，在大数据时代，我们面临如下几个挑战：如何收集这些巨大的信息如何分析它如何及时做到如上两点以上几个挑战形成了一个业务需求模型，即生产者生产（produce）各种信息，消费者消费（consume）（处理分析）这些信息，而在生产者与消费者之间，需要一个沟通两者的桥梁-消息系统。从一个微观层面来说，这种需求也可理解为不同的系统之间如何传递消息。Kafka诞生Kafka由 linked-in 开源kafka-即是解决上述这类问题的一个框架，它实现了生产者和消费者之间的无缝连接。kafka-高产出的分布式消息系统(A high-throughput distributed messaging system)Kafka的特性高吞吐量、低延迟：kafka每秒可以处理几十万条消息，它的延迟最低只有几毫秒可扩展性：kafka集群支持热扩展持久性、可靠性：消息被持久化到本地磁盘，并且支持数据备份防止数据丢失容错性：允许集群中节点失败（若副本数量为n,则允许n-1个节点失败）高并发：支持数千个客户端同时读写Kafka场景应用日志收集：一个公司可以用Kafka可以收集各种服务的log，通过kafka以统一接口服务的方式开放给各种consumer，例如hadoop、Hbase、Solr等。消息系统：解耦和生产者和消费者、缓存消息等。用户活动跟踪：Kafka经常被用来记录web用户或者app用户的各种活动，如浏览网页、搜索、点击等活动，这些活动信息被各个服务器发布到kafka的topic中，然后订阅者通过订阅这些topic来做实时的监控分析，或者装载到hadoop、数据仓库中做离线分析和挖掘。运营指标：Kafka也经常用来记录运营监控数据。包括收集各种分布式应用的数据，生产各种操作的集中反馈，比如报警和报告。流式处理：比如spark streaming和storm事件源Kafka一些重要设计思想Consumergroup：各个consumer可以组成一个组，每个消息只能被组中的一个consumer消费，如果一个消息可以被多个consumer消费的话，那么这些consumer必须在不同的组。消息状态：在Kafka中，消息的状态被保存在consumer中，broker不会关心哪个消息被消费了被谁消费了，只记录一个offset值（指向partition中下一个要被消费的消息位置），这就意味着如果consumer处理不好的话，broker上的一个消息可能会被消费多次。消息持久化：Kafka中会把消息持久化到本地文件系统中，并且保持极高的效率。消息有效期：Kafka会长久保留其中的消息，以便consumer可以多次消费，当然其中很多细节是可配置的。批量发送：Kafka支持以消息集合为单位进行批量发送，以提高push效率。push-and-pull :Kafka中的Producer和consumer采用的是push-and-pull模式，即Producer只管向broker push消息，consumer只管从broker pull消息，两者对消息的生产和消费是异步的。Kafka集群中broker之间的关系：不是主从关系，各个broker在集群中地位一样，我们可以随意的增加或删除任何一个broker节点。负载均衡方面： Kafka提供了一个 metadata API来管理broker之间的负载（对Kafka0.8.x而言，对于0.7.x主要靠zookeeper来实现负载均衡）。同步异步：Producer采用异步push方式，极大提高Kafka系统的吞吐率（可以通过参数控制是采用同步还是异步方式）。分区机制partition：Kafka的broker端支持消息分区，Producer可以决定把消息发到哪个分区，在一个分区中消息的顺序就是Producer发送消息的顺序，一个主题中可以有多个分区，具体分区的数量是可配置的。分区的意义很重大，后面的内容会逐渐体现。离线数据装载：Kafka由于对可拓展的数据持久化的支持，它也非常适合向Hadoop或者数据仓库中进行数据装载。插件支持：现在不少活跃的社区已经开发出不少插件来拓展Kafka的功能，如用来配合Storm、Hadoop、flume相关的插件。消息队列通信的模式点对点模式如上图所示，点对点模式通常是基于拉取或者轮询的消息传送模型，这个模型的特点是发送到队列的消息被一个且只有一个消费者进行处理。生产者将消息放入消息队列后，由消费者主动的去拉取消息进行消费。点对点模型的的优点是消费者拉取消息的频率可以由自己控制。但是消息队列是否有消息需要消费，在消费者端无法感知，所以在消费者端需要额外的线程去监控。发布订阅模式如上图所示，发布订阅模式是一个基于消息送的消息传送模型，改模型可以有多种不同的订阅者。生产者将消息放入消息队列后，队列会将消息推送给订阅过该类消息的消费者（类似微信公众号)。由于是消费者被动接收推送，所以无需感知消息队列是否有待消费的消息！但是consumer1、consumer2、consumer3由于机器性能不一样，所以处理消息的能力也会不一样，但消息队列却无法感知消费者消费的速度！所以推送的速度成了发布订阅模模式的一个问题！假设三个消费者处理速度分别是8M/s、5M/s、2M/s，如果队列推送的速度为5M/s，则consumer3无法承受！如果队列推送的速度为2M/s，则consumer1、consumer2会出现资源的极大浪费！Kafka的架构原理上面简单的介绍了为什么需要消息队列以及消息队列通信的两种模式，下面主角介绍Kafka。Kafka是一种高吞吐量的分布式发布订阅消息系统，它可以处理消费者规模的网站中的所有动作流数据，具有高性能、持久化、多副本备份、横向扩展能力。基础架构与名词解释Producer：Producer即生产者，消息的产生者，是消息的入口。Broker：Broker是kafka实例，每个服务器上有一个或多个kafka的实例，我们姑且认为每个broker对应一台服务器。每个kafka集群内的broker都有一个不重复的编号，如图中的broker-0、broker-1等……Topic：消息的主题，可以理解为消息的分类，kafka的数据就保存在topic。在每个broker上都可以创建多个topic。Partition：Topic的分区，每个topic可以有多个分区，分区的作用是做负载，提高kafka的吞吐量。同一个topic在不同的分区的数据是不重复的，partition的表现形式就是一个一个的文件夹！Replication:每一个分区都有多个副本，副本的作用是做备胎。当主分区（Leader）故障的时候会选择一个备胎（Follower）上位，成为Leader。在kafka中默认副本的最大数量是10个，且副本的数量不能大于Broker的数量，follower和leader绝对是在不同的机器，同一机器对同一个分区也只可能存放一个副本（包括自己）。Message：每一条发送的消息主体。Consumer：消费者，即消息的消费方，是消息的出口。Consumer Group：我们可以将多个消费组组成一个消费者组，在kafka的设计中同一个分区的数据只能被消费者组中的某一个消费者消费。同一个消费者组的消费者可以消费同一个topic的不同分区的数据，这也是为了提高kafka的吞吐量！Zookeeper：kafka集群依赖zookeeper来保存集群的的元信息，来保证系统的可用性。工作流程分析发送数据我们看上面的架构图中，producer就是生产者，是数据的入口。注意看图中的红色箭头，Producer在写入数据的时候永远的找leader，不会直接将数据写入follower！那leader怎么找呢？写入的流程又是什么样的呢？我们看下图：发送的流程就在图中已经说明了，就不单独在文字列出来了！需要注意的一点是，消息写入leader后，follower是主动的去leader进行同步的！producer采用push模式将数据发布到broker，每条消息追加到分区中，顺序写入磁盘，所以保证同一分区内的数据是有序的！写入示意图如下：上面说到数据会写入到不同的分区，那kafka为什么要做分区呢？相信大家应该也能猜到，分区的主要目的是：方便扩展：因为一个topic可以有多个partition，所以我们可以通过扩展机器去轻松的应对日益增长的数据量。提高并发：以partition为读写单位，可以多个消费者同时消费数据，提高了消息的处理效率。熟悉负载均衡的朋友应该知道，当我们向某个服务器发送请求的时候，服务端可能会对请求做一个负载，将流量分发到不同的服务器，那在kafka中，如果某个topic有多个partition，producer又怎么知道该将数据发往哪个partition呢？kafka中有几个原则：partition在写入的时候可以指定需要写入的partition，如果有指定，则写入对应的partition。如果没有指定partition，但是设置了数据的key，则会根据key的值hash出一个partition。如果既没指定partition，又没有设置key，则会轮询选出一个partition。保证消息不丢失是一个消息队列中间件的基本保证，那producer在向kafka写入消息的时候，怎么保证消息不丢失呢？其实上面的写入流程图中有描述出来，那就是通过ACK应答机制！在生产者向队列写入数据的时候可以设置参数来确定是否确认kafka接收到数据，这个参数可设置的值为0、1、all。0代表producer往集群发送数据不需要等到集群的返回，不确保消息发送成功。安全性最低但是效率最高。1代表producer往集群发送数据只要leader应答就可以发送下一条，只确保leader发送成功。all代表producer往集群发送数据需要所有的follower都完成从leader的同步才会发送下一条，确保leader发送成功和所有的副本都完成备份。安全性最高，但是效率最低。最后要注意的是，如果往不存在的topic写数据，能不能写入成功呢？kafka会自动创建topic，分区和副本的数量根据默认配置都是1。保存数据Producer将数据写入kafka后，集群就需要对数据进行保存了！kafka将数据保存在磁盘，可能在我们的一般的认知里，写入磁盘是比较耗时的操作，不适合这种高并发的组件。Kafka初始会单独开辟一块磁盘空间，顺序写入数据（效率比随机写入高）。Partition 结构前面说过了每个topic都可以分为一个或多个partition，如果你觉得topic比较抽象，那partition就是比较具体的东西了！Partition在服务器上的表现形式就是一个一个的文件夹，每个partition的文件夹下面会有多组segment文件，每组segment文件又包含.index文件、.log文件、.timeindex文件（早期版本中没有）三个文件， log文件就实际是存储message的地方，而index和timeindex文件为索引文件，用于检索消息。如上图，这个partition有三组segment文件，每个log文件的大小是一样的，但是存储的message数量是不一定相等的（每条的message大小不一致)。文件的命名是以该segment最小offset来命名的，如000.index存储offset为0~368795的消息，kafka就是利用分段+索引的方式来解决查找效率的问题。Message结构上面说到log文件就实际是存储message的地方，我们在producer往kafka写入的也是一条一条的message，那存储在log中的message是什么样子的呢？消息主要包含消息体、消息大小、offset、压缩类型……等等！我们重点需要知道的是下面三个：offset：offset是一个占8byte的有序id号，它可以唯一确定每条消息在parition内的位置！消息大小：消息大小占用4byte，用于描述消息的大小。消息体：消息体存放的是实际的消息数据（被压缩过），占用的空间根据具体的消息而不一样。存储策略无论消息是否被消费，kafka都会保存所有的消息。那对于旧数据有什么删除策略呢？基于时间，默认配置是168小时（7天）。基于大小，默认配置是1073741824。需要注意的是，kafka读取特定消息的时间复杂度是O(1)，所以这里删除过期的文件并不会提高kafka的性能！消费数据消息存储在log文件后，消费者就可以进行消费了。在讲消息队列通信的两种模式的时候讲到过点对点模式和发布订阅模式。Kafka采用的是发布订阅模式，消费者主动的去kafka集群拉取消息，与producer相同的是，消费者在拉取消息的时候也是找leader去拉取。多个消费者可以组成一个消费者组（consumer group），每个消费者组都有一个组id！同一个消费组者的消费者可以消费同一topic下不同分区的数据，但是不会组内多个消费者消费同一分区的数据！！！我们看下图：图示是消费者组内的消费者小于partition数量的情况，所以会出现某个消费者消费多个partition数据的情况，消费的速度也就不及只处理一个partition的消费者的处理速度！如果是消费者组的消费者多于partition的数量，那会不会出现多个消费者消费同一个partition的数据呢？上面已经提到过不会出现这种情况！多出来的消费者不消费任何partition的数据。所以在实际的应用中，建议消费者组的consumer的数量与partition的数量一致！在保存数据的小节里面，我们聊到了partition划分为多组segment，每个segment又包含.log、.index、.timeindex文件，存放的每条message包含offset、消息大小、消息体……我们多次提到segment和offset，查找消息的时候是怎么利用segment+offset配合查找的呢？假如现在需要查找一个offset为368801的message是什么样的过程呢？我们先看看下面的图：先找到offset的368801message所在的segment文件（利用二分法查找），这里找到的就是在第二个segment文件。打开找到的segment中的.index文件（也就是368796.index文件，该文件起始偏移量为368796+1，我们要查找的offset为368801的message在该index内的偏移量为368796+5=368801，所以这里要查找的相对offset为5）。由于该文件采用的是稀疏索引的方式存储着相对offset及对应message物理偏移量的关系，所以直接找相对offset为5的索引找不到，这里同样利用二分法查找相对offset小于或者等于指定的相对offset的索引条目中最大的那个相对offset，所以找到的是相对offset为4的这个索引。根据找到的相对offset为4的索引确定message存储的物理偏移位置为256。打开数据文件，从位置为256的那个地方开始顺序扫描直到找到offset为368801的那条Message。这套机制是建立在offset为有序的基础上，利用segment+有序offset+稀疏索引+二分查找+顺序查找等多种手段来高效的查找数据！至此，消费者就能拿到需要处理的数据进行处理了。那每个消费者又是怎么记录自己消费的位置呢？在早期的版本中，消费者将消费到的offset维护zookeeper中，consumer每间隔一段时间上报一次，这里容易导致重复消费，且性能不好！在新的版本中消费者消费到的offset已经直接维护在kafk集群的__consumer_offsets这个topic中！]]></content>
      <categories>
        <category>技术</category>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>kafka原理</tag>
        <tag>消息队列</tag>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[磁盘阵列的创建]]></title>
    <url>%2F2023%2F0498e1822.html</url>
    <content type="text"><![CDATA[常见raid有0、1和5，以下操作在虚拟机下模拟，学会这招在自己电脑做个raid也未尝不可啊~Windows 创建RaidRAID 0 创建添加两块硬盘，联机并初始化（2T以下选MBR，以上选GPT）右键选择新建带区卷（RAID 0） raid-0至少要两块硬盘以上才能创建，其容量是所有raid硬盘容量之和，传输速度也是所有容量速度之和，是所有raid中速度最快使用效率最高的，但没有冗余，如果其中一块硬盘坏了，整个raid就挂了。raid 1创建添加两块硬盘,联机并初始化（2T以下选MBR，以上选GPT）,选择新建镜像卷 raid-1有且仅有两块硬盘（此处的硬盘也可以是由多个硬盘组成的阵列）才能创建，其容量是raid中容量最小的硬盘的容量（因为raid-1的硬盘的一半是用来备份的，空间利用率只有50%），传输速度也是单个硬盘的速度，是所有raid中速度最慢使用效率最低的，但是有冗余，如果其中一块硬盘坏了，数据还能正常使用不影响，只需换上新硬盘修复下即可。RAID 5创建添加三块硬盘，联机并初始化（2T以下选MBR，以上选GPT） raid-5至少要三块硬盘以上才能创建，其容量是N-1块硬盘的容量（N指raid中硬盘的个数，因为raid-5中的所有硬盘是环环相扣建立关系的），传输速度是N-1硬盘的速度，速度比raid-0慢，比raid-1快，使用效率也是介于两者之间，也是有冗余，最多只能坏一块盘，如果其中一块硬盘坏了，数据还能正常使用不影响，只需换上新硬盘修复下即可。部分主板也可在BIOS下设置RAID组建RAID的关键是硬盘的容量，目前，INTEL的RAID最大可以识别3.6TB（也就是十进制的4TB），就是2个2TB的硬盘组建RAID 0。不要用2个3TB的硬盘组RAID 0，因为只能用到4TB，有2TB白扔了。多个硬盘尽量接在同样的SATA口，比如都接SATA2.0（一般为黑色或蓝色接口）或都接SATA3.0（一般为白色接口）。在BIOS的SATA Configuration 中设置SATA Mode 为RAID Mode ,保存重启后，可以看到RAID ROM 启动并检测到2个2TB的硬盘，注意ROM采用16进制表示容量，2TB的硬盘显示为1.8TB。按CTRL-I进入RAID ROM 配置RAID，第一项Create RAID Volume ,就是建立RAID卷，回车进入。从Name : Volume0开始敲回车确认（默认就可以），到Capacity:(容量)时要注意。Linux创建Raid创建四个相同大小的硬盘打开虚拟机，点击箭头所在位置点击添加硬盘一直点下一步，到出现磁盘大小（我这里定的是200G）点击下一步，然后点完成第一个硬盘就建好了，以此类推，再建三个，最后一定不要忘了点确定开启虚拟机，打开终端将四个硬盘逐个分区并改格式查看磁盘分区表创建硬盘分区(每个硬盘分两个区)将硬盘格式改为RAID格式（ｔ 为改格式命令, fd 为RAID格式）依次类推，把四个硬盘都分成两个区，并把格式改为RAID格式，每个硬盘操作过后记得保存四个硬盘分区完成后，查看一下分区情况 创建RAID0和RAID5raid0 四个磁盘，raide5 三个磁盘，一个备份磁盘1mdadm 命令用法：mdadm [mode] [options]123456789101112131415-C: 创建模式-v 显示创建过程中的详细信息 -n #: 使用#个块设备来创建此RAID -l #：指明要创建的RAID的级别 -a &#123;yes|no&#125;：自动创建目标RAID设备的设备文件 -c CHUNK_SIZE: 指明块大小,单位k -x #: 指明空闲盘的个数-D：显示raid的详细信息 mdadm-D /dev/md#管理模式： -f: 标记指定磁盘为损坏 -a: 添加磁盘 -r: 移除磁盘查看RAID0和RAID5信息 格式化和挂载先对磁盘进行格式化 进行挂载]]></content>
      <categories>
        <category>桌面</category>
        <category>系统</category>
      </categories>
      <tags>
        <tag>磁盘阵列</tag>
        <tag>Raid</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[磁盘阵列详解]]></title>
    <url>%2F2023%2F04e4b5c2f3.html</url>
    <content type="text"><![CDATA[磁盘阵列（Redundant Arrays of Independent Drives，RAID），有“独立磁盘构成的具有冗余能力的阵列”之意。磁盘阵列是由很多块独立的磁盘，组合成一个容量巨大的磁盘组，利用个别磁盘提供数据所产生加成效果提升整个磁盘系统效能。利用这项技术，将数据切割成许多区段，分别存放在各个硬盘上。RAID0定义：RAID 0又称为Stripe或Striping，它代表了所有RAID级别中最高的存储性能。RAID 0提高存储性能的原理是把连续的数据分散到多个磁盘上存取，这样，系统有数据请求就可以被多个磁盘并行的执行，每个磁盘执行属于它自己的那部分数据请求。这种数据上的并行操作可以充分利用总线的带宽，显著提高磁盘整体存取性能。工作原理：系统向三个磁盘组成的逻辑硬盘（RAID0 磁盘组）发出的I/O数据请求被转化为3项操作，其中的每一项操作都对应于一块物理硬盘。通过建立RAID 0，原先顺序的数据请求被分散到所有的三块硬盘中同时执行。从理论上讲，三块硬盘的并行操作使同一时间内磁盘读写速度提升了3倍。 但由于总线带宽等多种因素的影响，实际的提升速率肯定会低于理论值，但是，大量数据并行传输与串行传输比较，提速效果显著显然毋庸置疑。优缺点：优点：读写性能是所有RAID级别中最高的。缺点：RAID 0的缺点是不提供数据冗余，因此一旦用户数据损坏，损坏的数据将无法得到恢复。RAID0运行时只要其中任一块硬盘出现问题就会导致整个数据的故障。一般不建议企业用户单独使用。总结：磁盘空间使用率：100%，故成本最低。读性能：N*单块磁盘的读性能写性能：N*单块磁盘的写性能冗余：无，任何一块磁盘损坏都将导致数据不可用。RAID1定义：RAID 1通过磁盘数据镜像实现数据冗余，在成对的独立磁盘上产生互为备份的数据。当原始数据繁忙时，可直接从镜像拷贝中读取数据，因此RAID 1可以提高读取性能。RAID 1是磁盘阵列中单位成本最高的，但提供了很高的数据安全性和可用性。当一个磁盘失效时，系统可以自动切换到镜像磁盘上读写，而不需要重组失效的数据。工作原理：RAID1是将一个两块硬盘所构成RAID磁盘阵列，其容量仅等于一块硬盘的容量，因为另一块只是当作数据“镜像”。RAID1磁盘阵列显然是最可靠的一种阵列，因为它总是保持一份完整的数据备份。它的性能自然没有RAID0磁盘阵列那样好，但其数据读取确实较单一硬盘来的快，因为数据会从两块硬盘中较快的一块中读出。RAID1磁盘阵列的写入速度通常较慢，因为数据得分别写入两块硬盘中并做比较。RAID1磁盘阵列一般支持“热交换”，就是说阵列中硬盘的移除或替换可以在系统运行时进行，无须中断退出系统。RAID1磁盘阵列是十分安全的，不过也是较贵一种RAID磁盘阵列解决方案，因为两块硬盘仅能提供一块硬盘的容量。RAID1磁盘阵列主要用在数据安全性很高，而且要求能够快速恢复被破坏的数据的场合。在这里，需要注意的是，读只能在一块磁盘上进行，并不会进行并行读取，性能取决于硬盘中较快的一块。写的话通常比单块磁盘要慢，虽然是并行写，即对两块磁盘的写入是同时进行的，但因为要比较两块硬盘中的数据，所以性能比单块磁盘慢。优缺点：RAID1通过硬盘数据镜像实现数据的冗余，保护数据安全，在两块盘上产生互为备份的数据，当原始数据繁忙时，可直接从镜像备份中读取数据，因此RAID1可以提供读取性能。RAID1是硬盘中单位成本最高的，但提供了很高的数据安全性和可用性，当一个硬盘失效时，系统可以自动切换到镜像硬盘上读/写，并且不需要重组失效的数据。总结：磁盘空间使用率：50%，故成本最高。读性能：只能在一个磁盘上读取，取决于磁盘中较快的那块盘写性能：两块磁盘都要写入，虽然是并行写入，但因为要比对，故性能单块磁盘慢。冗余：只要系统中任何一对镜像盘中有一块磁盘可以使用，甚至可以在一半数量的硬盘出现问题时系统都可以正常运行。RAID 5定义：RAID 5是RAID 0和RAID 1的折中方案。RAID 5具有和RAID0相近似的数据读取速度，只是多了一个奇偶校验信息，写入数据的速度比对单个磁盘进行写入操作稍慢。同时由于多个数据对应一个奇偶校验信息，RAID5的磁盘空间利用率要比RAID 1高，存储成本相对较低，是目前运用较多的一种解决方案。工作原理：RAID5把数据和相对应的奇偶校验信息存储到组成RAID5的各个磁盘上，并且奇偶校验信息和相对应的数据分别存储于不同的磁盘上，其中任意N-1块磁盘上都存储完整的数据，也就是说有相当于一块磁盘容量的空间用于存储奇偶校验信息。因此当RAID5的一个磁盘发生损坏后，不会影响数据的完整性，从而保证了数据安全。当损坏的磁盘被替换后，RAID还会自动利用剩下奇偶校验信息去重建此磁盘上的数据，来保持RAID5的高可靠性。做raid 5阵列所有磁盘容量必须一样大，当容量不同时，会以最小的容量为准。 最好硬盘转速一样，否则会影响性能，而且可用空间=磁盘数n-1，Raid 5 没有独立的奇偶校验盘，所有校验信息分散放在所有磁盘上， 只占用一个磁盘的容量。总结：磁盘空间利用率：(N-1)/N，即只浪费一块磁盘用于奇偶校验。读性能：(n-1)*单块磁盘的读性能，接近RAID0的读性能。写性能：比单块磁盘的写性能要差（这点不是很明白，不是可以并行写入么？）冗余：只允许一块磁盘损坏。RAID10定义：RAID10也被称为镜象阵列条带。像RAID0一样，数据跨磁盘抽取；像RAID1一样，每个磁盘都有一个镜象磁盘, 所以RAID 10的另一种会说法是 RAID 0+1。RAID10提供100%的数据冗余，支持更大的卷尺寸，但价格也相对较高。对大多数只要求具有冗余度而不必考虑价格的应用来说，RAID10提供最好的性能。使用RAID10，可以获得更好的可靠性，因为即使两个物理驱动器发生故障（每个阵列中一个），数据仍然可以得到保护。RAID10需要4 + 2*N 个磁盘驱动器（N &gt;=0)， 而且只能使用其中一半(或更小, 如果磁盘大小不一)的磁盘用量, 例如 4 个 250G 的硬盘使用RAID10 阵列， 实际容量是 500G。实现原理：Raid10其实结构非常简单，首先创建2个独立的Raid1，然后将这两个独立的Raid1组成一个Raid0，当往这个逻辑Raid中写数据时，数据被有序的写入两个Raid1中。磁盘1和磁盘2组成一个Raid1，磁盘3和磁盘4又组成另外一个Raid1;这两个Raid1组成了一个新的Raid0。如写在硬盘1上的数据1、3、5、7，写在硬盘2中则为数据1、3、5、7，硬盘中的数据为0、2、4、6，硬盘4中的数据则为0、2、4、6，因此数据在这四个硬盘上组合成Raid10，且具有raid0和raid1两者的特性。虽然Raid10方案造成了50%的磁盘浪费，但是它提供了200%的速度和单磁盘损坏的数据安全性，并且当同时损坏的磁盘不在同一Raid1中，就能保证数据安全性。假如磁盘中的某一块盘坏了，整个逻辑磁盘仍能正常工作的。当我们需要恢复RAID10中损坏的磁盘时，只需要更换新的硬盘，按照RAID10的工作原理来进行数据恢复，恢复数据过程中系统仍能正常工作。原先的数据会同步恢复到更换的硬盘中。总结：磁盘空间利用率：50%。读性能：N/2*单块硬盘的读性能写性能：N/2*单块硬盘的写性能冗余：只要一对镜像盘中有一块磁盘可以使用就没问题。]]></content>
      <categories>
        <category>桌面</category>
        <category>系统</category>
      </categories>
      <tags>
        <tag>磁盘阵列</tag>
        <tag>Raid</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS7内核升级]]></title>
    <url>%2F2023%2F048b137609.html</url>
    <content type="text"><![CDATA[Linux的内核概念不用说大家也很清楚，正是内核版本的不同，才有Linux发行版本的说法，现在主流的centos应该都是centos7了，从centos7.2开始，内核版本为3.10，越往后内核版本越高。高版本的内核修复了许多的低版本内核的bug，因此，系统是需要提高内核版本的，从而提高安全性，稳定性，并增加更多的功能。通常来说，Linux是支持多版本内核共存的，无非是系统启动的时候应用哪个版本内核而已。关于内核：Linux 内核分两种：官方内核（通常是内核开发人员用）和各大 Linux 发行版内核（一般用户常用）。关于Linux内核版本号：12[root@centos7 ~]# uname -r3.10.0-1127.19.1.el7.x86_64查询得到的版本号为：3.10.0-1127.19.1.el7.x86_64第一个组数字：3, 主版本号第二个组数字：10, 次版本号，当前为稳定版本，一般这个数字为偶数表示稳定，奇数表示在开发版本，通常这样的不做生产使用。第三个组数字：0, 修订版本号第四个组数字：1127.19.1，表示发型版本的补丁版本el7：则表示我正在使用的内核是 RedHat / CentOS 系列发行版专用内核 ，centos7x86_64：采用的是适用64位的CPU的操作系统。内核版本的分类：查看内核的种类在官网：The Linux Kernel ArchivesPrepatch：Prepatch 或 “RC” 内核是主要的内核预发行版本，主要针对内核开发人员和 Linux 爱好者。必须从源代码进行编译，并且通常包含必须在可以放入稳定版本之前进行测试的新功能。Prepatch 内核由 Linus Torvalds 维护和发布。Mainline：Mainline 主线树由 Linus Torvalds 维护。这个版本的内核会引入所有新功能。新的 Mainline 内核每 2-3 个月发布一次。Stable：每个主线内核被发布后，即被认为是“stable”。任何对 stable 内核的 BUG 修复都会从 Mainline 主线树中回溯并由指定的 stable 内核维护人员使用。 在下一个主线内核可用之前，通常只有几个 BUG 修复内核版本 - 除非它被指定为“longterm maintenance kernel（长期维护内核）”。stable 内核更新按需发布，通常每月 2-3 次。Longterm：通常会提供几个“longterm maintenance”内核版本，用于修复旧版内核的 BUG。这些内核只会修复重大 BUG，并且不会频繁发布版本。下载地址为官网网站：如下图 内核的选择：根据上面的内核的种类的介绍，我们可以知道稳定的并且大部分bug已经修复的内核种类为longterm，因此在上面的第二章表内挑选一个longterm种类的内核即可，下面的实验是使用的5.4.69的tarball源码包。（顺带一说，上表也说了，下一个版本内核发布时间为2020-10-02）。选择的理由是：longterm是长期稳定版本，并且大部分bug也修复了，主版本也够高，内核是向下兼容的，并且可以体验到更多的新功能。（第一张表说的是对应版本提供的发行人员名字，发行日期，最终支持时间–过了那个时间就没人管了）内核的升级：（两种方法：内核编译和yum更新）通常，centos是可以多内核共存的，因此不建议删除旧版本的内核，仅仅安装新版本内核后，grub选择新版本内核进入系统并使用即可，（谁也不敢保证新的内核就一定好用，毕竟留个旧的，以后想反悔也简单一点）第一种方法：yum 升级安装的方式（简单）步骤 1：检查已安装的内核版本uname -rs,给出的结果为：Linux 3.10.0-1127.19.1.el7.x86_64步骤 2：在 CentOS 7 中升级内核CentOS 允许使用 ELRepo，这是一个第三方仓库，可以将内核升级到最新版本。在 CentOS 7 上启用 ELRepo 仓库，运行如下命令：123rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org #导入该源的秘钥rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm #启用该源仓库yum --disablerepo="\*" --enablerepo="elrepo-kernel" list available #查看有哪些内核版本可供安装输出如下：123456789101112131415161718192021[root@centos7 ~]# yum --disablerepo="\*" --enablerepo="elrepo-kernel" list availableLoaded plugins: fastestmirrorLoading mirror speeds from cached hostfile \* elrepo-kernel: mirrors.tuna.tsinghua.edu.cnAvailable Packageselrepo-release.noarch 7.0-5.el7.elrepo elrepo-kernelkernel-lt.x86_64 4.4.238-1.el7.elrepo elrepo-kernelkernel-lt-devel.x86_64 4.4.238-1.el7.elrepo elrepo-kernelkernel-lt-doc.noarch 4.4.238-1.el7.elrepo elrepo-kernelkernel-lt-headers.x86_64 4.4.238-1.el7.elrepo elrepo-kernelkernel-lt-tools.x86_64 4.4.238-1.el7.elrepo elrepo-kernelkernel-lt-tools-libs.x86_64 4.4.238-1.el7.elrepo elrepo-kernelkernel-lt-tools-libs-devel.x86_64 4.4.238-1.el7.elrepo elrepo-kernelkernel-ml-devel.x86_64 5.8.13-1.el7.elrepo elrepo-kernelkernel-ml-doc.noarch 5.8.13-1.el7.elrepo elrepo-kernelkernel-ml-headers.x86_64 5.8.13-1.el7.elrepo elrepo-kernelkernel-ml-tools.x86_64 5.8.13-1.el7.elrepo elrepo-kernelkernel-ml-tools-libs.x86_64 5.8.13-1.el7.elrepo elrepo-kernelkernel-ml-tools-libs-devel.x86_64 5.8.13-1.el7.elrepo elrepo-kernelperf.x86_64 5.8.13-1.el7.elrepo elrepo-kernelpython-perf.x86_64开始安装：12yum --enablerepo=elrepo-kernel install kernel-ml -y #安装的是主线版本，该版本比较激进，慎重选择。版本号5.8.13yum --enablerepo=elrepo-kernel install kernel-lt -y #安装的长期稳定版本，稳定可靠，版本为4.4.238两个yum命令选择一个安装。安装完毕后，重启机器，手动选择新安装的新版本哦！！！步骤 3：设置 GRUB 默认的内核版本为了让新安装的内核成为默认启动选项，你需要如下修改 GRUB 配置：打开并编辑 /etc/default/grub 并设置 GRUB_DEFAULT=0。意思是 GRUB 初始化页面的第一个内核将作为默认内核。1234567GRUB_TIMEOUT=5GRUB_DISTRIBUTOR="$(sed 's, release .\*$,,g' /etc/system-release)"GRUB_DEFAULT=saved #这里的saved改为0即可，重启后，默认就是你上次所选的版本了。GRUB_DISABLE_SUBMENU=trueGRUB_TERMINAL_OUTPUT="console"GRUB_CMDLINE_LINUX="rhgb quiet"GRUB_DISABLE_RECOVERY="true"执行命令：1grub2-mkconfig -o /boot/grub2/grub.cfg第二种方法：使用官网源码包编译安装root账号操作，编译这么重大的事，别的普通用户不用想的。1wget https://cdn.kernel.org/pub/linux/kernel/v5.x/linux-5.4.69.tar.xz #下载得到这么一个xz包，解压它1tar -xf linux-5.4.69.tar.xz进入解压出来的东西的目录内：1cd linux-5.4.69编译前的环境准备（安装编译依赖并升级所有软件）：12yum install gcc make ncurses-devel openssl-devel flex bison elfutils-libelf-devel -yyum upgrade -y查看现有的内核配置文件（我这里是已经rpm方式升级过了，因此，有两个内核配置文件，如果是yum upgrade -y 后，那么，应该有一个文件是config-3.10.0-693.el7.x86_64）：1234[root@master boot]# pwd/boot[root@master boot]# ls config-*config-3.10.0-693.el7.x86_64 config-5.16.9-1.el7.elrepo.x86_64cp /boot/config-3.10.0-693.el7.x86_64 .config #复制内核配置文件到前面解压出来的目录内，这个文件是发行版自带的文件，比如我的内核原来查询出的是3.10.0-1127.19.1.el7.x86_64，这个是由于我使用命令 yum upgrade -y 升级的，最初的版本是3.10.0-693.el7.x86_64 （这里注意一下，这个命令是复制并改名， 是. config）OK，环境准备完毕，开始正式安装了。1234#make config（基于文本的最为传统的配置界面，不推荐使用） #make menuconfig（基于文本选单的配置界面，字符终端下推荐使用） #make xconfig（基于图形窗口模式的配置界面，Xwindow下推荐使用） #make oldconfig（如果只想在原来内核配置的基础上修改一些小地方，会省去不少麻烦）下面的安装是选择使用 make menuconfig（上面的四个命令选择使用第二个）。make menuconfig #该命令会出现一个简陋的菜单，类似bios菜单，其中可以进行功能挑选，具体的功能太多太多，不能一一介绍，懒人办法-默认即可。选择save，选择ok即可。 1make -j `nproc` &amp;&amp; make modules_install &amp;&amp; make install #内核全开，编译三连编译时间比较长，我的四核CPU大概一个小时。编译完毕后， echo $? 下，看看是否正常，如返回0，重启机器，如别的数字，根据报错更正即可。开机菜单顺序问题参照第一种方法的最后处理步骤，不在多说了。两种方法的优劣：第一种方法简单，这是唯一优点。缺点是只能选择4.4.238和5.8.13这两种内核（源里有什么才可以选择，没有神仙都没办法）(2021年九月以后，此网络源里只有5.14和5.4两种内核)第二种方法，编译，可选择的余地更多，并且更为灵活，可以选自己需要的模块编译进去嘛，就是难度比较高，需要比较了解内核才可以。现在网络源的内核版本只有5.14和5.4啦1234567891011121314151617181920[root@overseas-net ~]# yum --disablerepo="*" --enablerepo="elrepo-kernel" list availableLoaded plugins: fastestmirrorRepodata is over 2 weeks old. Install yum-cron? Or run: yum makecache fastDetermining fastest mirrors * elrepo-kernel: hkg.mirror.rackspace.comAvailable Packageskernel-lt-doc.noarch 5.4.92-1.el7.elrepo elrepo-kernelkernel-lt-headers.x86_64 5.4.92-1.el7.elrepo elrepo-kernelkernel-lt-tools.x86_64 5.4.92-1.el7.elrepo elrepo-kernelkernel-lt-tools-libs.x86_64 5.4.92-1.el7.elrepo elrepo-kernelkernel-lt-tools-libs-devel.x86_64 5.4.92-1.el7.elrepo elrepo-kernelkernel-ml.x86_64 5.10.10-1.el7.elrepo elrepo-kernelkernel-ml-devel.x86_64 5.10.10-1.el7.elrepo elrepo-kernelkernel-ml-doc.noarch 5.10.10-1.el7.elrepo elrepo-kernelkernel-ml-headers.x86_64 5.10.10-1.el7.elrepo elrepo-kernelkernel-ml-tools.x86_64 5.10.10-1.el7.elrepo elrepo-kernelkernel-ml-tools-libs.x86_64 5.10.10-1.el7.elrepo elrepo-kernelkernel-ml-tools-libs-devel.x86_64 5.10.10-1.el7.elrepo elrepo-kernelperf.x86_64 5.10.10-1.el7.elrepo elrepo-kernelpython-perf.x86_64 5.10.10-1.el7.elrepo elrepo-kernel]]></content>
      <categories>
        <category>桌面</category>
        <category>系统</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>内核升级</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux性能调优]]></title>
    <url>%2F2023%2F044fbb5713.html</url>
    <content type="text"><![CDATA[调优是一件因地制宜的事情，生搬硬套可能适得其反。互联网企业，生产环境大多数都是linux操作系统，Linux服务器性能调优，是一个重要的课题。linux有成百上千的参数可以调整，对这些参数的理解，可以帮助我们更好的理解服务器工作的原理，帮助更快的找到瓶颈和解决问题。为了帮助理解，先上一个Linux网络收包图：DMA：是一种无需CPU的参与就可以让外设和系统内存之间进行双向数据传输的硬件机制。使用DMA可以使系统CPU从实际的I/O数据传输过程中摆脱出来，从而大大提高系统的吞吐率中断：在传统模式下，收到一个网络包，CPU会产生一次硬中断，切换上下文来处理网络包。但是过多的中断会影响CPU的执行效率，所以就有来类似于中断聚合的机制或者软中断方式来处理，来提高效率。一般在硬件drvier层面作为应用运维基本上做不了优化，本文探讨的核心是kernel这个层面。特别注意，我们今天不讲TCP相关的原理，具体的细节可以自行google，我们直接上实战参数。backlogbacklog其实是一个连接队列，backlog大小包括半连接状态和全连接状态两种队列大小net.core.netdev_max_backlog上图中的 recv_backlog，进入网卡还没被内核协议栈处理的报文最大长度。所有网络协议栈的收包队列，网卡收到的所有报文都在 netdev backlog 队列中等待软中断处理，和中断频率一起影响收包速度从而影响收包带宽，以 netdev_backlog=400, 中断频率=100HZ 为例：1234400 * 100 = 40000packets HZ(Timeslice freq) packets/s40000 * 1000 = 40 Mpackets average(Bytes/packet) throughput Bytes/snet.ipv4.tcp_max_syn_backlogtcp_max_syn_backlog 是内核保持的未被 ACK 的 SYN 包最大队列长度，超过这个数值后，多余的请求会被丢弃。对于服务器而言默认值不够大（通常为128），高并发服务有必要将netdev_max_backlog和此参数调整到比较大的值。net.core.somaxconnsomaxconn 是一个 socket 上等待应用程序 accept() 的最大队列长度，默认值通常为128。在一个 socket 进行 listen(int sockfd, int backlog) 时需要指定 backlog 值作为参数，如果这个 backlog 值大于 somaxconn 的值，最大队列长度将以 somaxconn 为准，多余的连接请求将被放弃，此时客户端可能收到一个 ECONNREFUSED 或忽略此连接并重传。将此参数调大可以一定程度上避免高并发服务遇到突发流量导致丢包，但是如果应用程序处理速度跟不上收包速度，调大 somaxconn 是没有意义的，只会导致 client 端虽然不会因为拥塞而被断开连接，但是请求依旧没有被处理，某些情况下甚至会造成反效果（如负载均衡器后的一台后端遇到瓶颈无法处理更多请求时，应该快速将连接断开使客户端重连到其他后端，而不是将请求堆积在这台满载的后端上）TIME_WAITTIME_WAIT 状态原本是为避免连接没有可靠断开而和后续新建的连接的数据混淆，TIME_WAIT 中的 peer 会给所有来包回 RST，对于 Windows，TIME_WAIT 状态持续的 2MSL 可以通过注册表配置，而 Linux 则是写死在内核源码里的60秒。这个参数也不是盲目的改，要结合具体的场景。对于会主动关闭请求的服务端（典型应用：non-keepalive HTTP，服务端发送所有数据后直接关闭连接），实际上并不会出现在主动关闭之后再向那个客户端发包的情况，所以 TIME_WAIT 会出现在服务端的80端口上，正常情况下，由于客户端的（source IP, source port）二元组在短时间内几乎不会重复，因此这个 TIME_WAIT 的累积基本不会影响后续连接的建立。有些例外情况如压力测试且客户端只有少数几台机器的时候，连接建立和断开过快会导致客户端二元组在短时间内循环，若此时服务器端口上的 socket 仍处于 TIME_WAIT 状态则会无法建立连接 。所以，对于一些生产环境中的服务器应用，出现大量TIME_WAIT不必太过担心。但在反向代理中，TIME_WAIT 问题则会非常明显，如 nginx 默认行为下会对于 client 传来的每一个 request 都向 upstream server 打开一个新连接，高 QPS 的反向代理将会快速积累 TIME_WAIT 状态的 socket，直到没有可用的本地端口，无法继续向 upstream 打开连接，此时服务将不可用。实践中，服务端使用 RST 关闭连接可以避免服务端积累 TIME_WAIT，但更优的设计是服务端告知客户端什么时候应该关闭连接，然后由客户端主动关闭。net.ipv4.tcp_max_tw_buckets此数值定义系统在同一时间最多能有多少 TIME_WAIT 状态，当超过这个值时，系统会直接删掉这个 socket 而不会留下 TIME_WAIT 的状态。net.ipv4.tcp_tw_reuse依赖 TCP 时间戳，即 net.ipv4.tcp_timestamps = 1，tw_reuse 会根据 TCP 时间戳决定是否复用 TIME_WAIT socket，在 tw_buckets 满的时候，会根据 TCP 时间戳决定是否复用 TIME WAIT socket，选取一个已经持续1秒以上的连接，复用这个五元组，这个选项只对客户端（反代服务器连接 upstream 时也可认为是客户端）有效。net.ipv4.ip_local_port_rangeTCP 建立连接时 client 会随机从该参数定义的端口范围中选择一个作为源端口，这个端口范围一般被称作临时端口（ephemeral ports）或动态端口（dynamic ports），更具体地说，是以下几种情况之一会分配一个临时端口，如果不可用临时端口，下述系统调用将返回错误TCP keepalive 相关TCP keepalive 是建立 TCP 连接是分配一个计数器，当计数器归零时，发送一个空 ACK（dup ack 是被允许的），主要有两大目的：探测对端存活避免网络 idle 超时net.ipv4.tcp_keepalive_time最大闲置时间，从最后一个 data packet（空 ACK 不算 data）之后多长时间开始发送探测包，单位是秒net.ipv4.tcp_keepalive_intvl发送探测包的时间间隔，在此期间连接上传输了任何内容都不影响探测的发送，单位是秒net.ipv4.tcp_keepalive_probes最大失败次数，超过此值后将通知应用层连接失效nf_conntrack连接跟踪表：nf_conntrack，是一个链表。系统最大允许连接跟踪数= 连接跟踪表大小(HASHSIZE) * Bucket大小(bucket size)该模块并不是所有 Linux 内核都会加载，最常见是使用了 iptables、lvs 等内核态 NAT/防火墙导致内核需要对连接表进行追踪，iptable_nat、ip_vs 等多个内核模块都依赖 nf_conntrack， 但是 nf_conntrack 的存在会影响高并发下的内核收包性能连接跟踪表获取bucket是hash操作时间很短，而遍历bucket相对费时，因此为了conntrack性能考虑，bucket size越小越好，设置为8或者4。同时设定了 conntrack_max 和 hashsize 时，bucket_size会调整为其比值关于Linux内核方面的一些知识就罗列完了，对于一些TCP窗口相关的，也不太清楚里面的细节，所以下面就结合调研的情况，补充一些其他参数。文件相关#系统所有进程能打开的最大文件数量1fs.file-max = 11000000# 单个进程最大文件数1fs.nr_open = 10000000#同一用户同时可以添加的watch数目（watch一般是针对目录，决定了同时同一用户可以监控的目录数量)1fs.inotify.max_user_watches = 1048576TCP参数# 是否忽略ICMP_ECHO requests，简单理解就是是否禁止ping，0不禁止1net.ipv4.icmp_echo_ignore_all = 0# 开启TCP转发1net.ipv4.ip_forward = 1#过载之后是否关闭端口发送RST，0表示不关闭1net.ipv4.tcp_abort_on_overflow = 0TCP窗口相关#用于计算读缓冲区中应用缓存的大小# 方法就是 应用缓存大小 = （读缓冲区大小）* (1-2^ tcp_adv_win_scale)# 配置为1的话就表示有一半的读缓冲区大小用于存放应用缓存1net.ipv4.tcp_adv_win_scale = 1#tcp缓冲区相关,读写缓冲区的设置，通常无需配置，而且发现线上环境设置得较大了一些。#单位是页，在Linux上就是4k分别是无压力模式，压力模式，最大值，超过最大值，系统将不能分配出内存用于TCP连接1net.ipv4.tcp_mem = 764346 1019129 1528692#TCP拥塞控制相关，最好开启BBR算法123net.ipv4.tcp_allowed_congestion_control = bbr cubic reno #支持的拥塞控制算法net.ipv4.tcp_available_congestion_control = bbr cubic renonet.ipv4.tcp_congestion_control = bbr#实际应用的拥塞控制算法，默认是开启BBR算法，线上内核版本超过4.9，可以开启。#是否开启窗口扩大因子选项，默认开启1net.ipv4.tcp_window_scaling = 1用于解决默认窗口最大只有65535字节的问题。在三次握手的时候会协商双方的窗口扩大因子，实际窗口=window_size2^窗口扩大因子，例如下面窗口扩大因子是7，如果对方tcp window_size=235,那么对方的接收窗口实际上就是2352^7=235*128=30080,在wireshark中，也会显示窗口大小和计算出的实际窗口大小。TCP缓冲区#读缓冲区，最大值覆盖下面的值，单位字节12net.ipv4.tcp_rmem = 4096 87380 33554432net.core.rmem_max = 16777216#写缓冲区，覆盖下面的值，单位字节12net.ipv4.tcp_wmem = 4096 65536 33554432net.core.wmem_max = 16777216TCP重传#FIN-WAIT-1 -&gt; FIN-WAIT-2 之间超时时间，15s是合适的，不是MSL的值1net.ipv4.tcp_fin_timeout = 15#选择性确认，以支持选择性重传，即接收端在一段失序报文中确认收到的报文段，发送端只发送丢失的报文段，而不是基于悲观态度全部重传，提高重传效率。开启，默认开启1net.ipv4.tcp_sack = 1#这个值表示超过次数之后需要更新路由表1net.ipv4.tcp_retries1 = 3#这个值表示超过次数之后会断开连接，不再进行重试，建议改成5，及时释放连接。1net.ipv4.tcp_retries2 = 15# 主动连接方syn的重试次数，3较为合适，默认是6，内网环境下建议配置较少以便及时暴露问题。3次retries表示加上第一次一共四次，时间间隔分别按照1，2，4 ，8…指数倍递增，3次也就是间隔15s会放弃重传（实际时间可能稍高，因为需要包括报文传输和处理的时间）1net.ipv4.tcp_syn_retries = 3# 被连接方synack的重试次数，时间间隔计算同上。2也较为合适。1net.ipv4.tcp_synack_retries = 2#是否开启tcp_syncookies 这个在syn_backlog队列满了之后才会触发，但是由于会占用tcp options（因为options部分最大长度是40字节）导致一些其他的重要的options选项无法使用。且需要耗费cpu资源计算cookie，是否开启有待商榷，建议关闭1net.ipv4.tcp_syncookies = 1#是否开启tcp timestamp选项，通常需要开启，用于精确计算RTT，避免序列号回绕时序列号碰撞而产生错误，而且tw_reuse也是需要基于tcp timestamp开启来使用。1net.ipv4.tcp_timestamps = 1内核参数列表最后奉上我们调整过后的调优内核参数列表，将如下内容写入：/etc/sysctl.conf12345678910111213141516171819202122232425kernel.core_pattern=/home/cores/core.%P.%e.%tnet.ipv4.tcp_tw_reuse = 1fs.inotify.max_user_watches = 1000000net.ipv4.tcp_max_tw_buckets = 1000000net.ipv4.tcp_max_syn_backlog=262144net.core.somaxconn=163840net.ipv4.tcp_syn_retries=3net.core.netdev_max_backlog=163840net.ipv4.tcp_timestamps=1net.ipv4.tcp_keepalive_time = 1200fs.file-max=11000000fs.nr_open=10000000net.ipv4.icmp_echo_ignore_all=0net.ipv4.ip_forward=1net.ipv4.tcp_moderate_rcvbuf=1net.ipv4.tcp_fin_timeout=15net.ipv4.tcp_sack=1net.ipv4.tcp_retries1=3net.ipv4.tcp_retries2=5net.ipv4.tcp_synack_retries=2net.ipv4.tcp_window_scaling=1net.ipv4.tcp_adv_win_scale=1net.ipv4.tcp_mem=764346 1019129 1528692net.ipv4.tcp_rmem=4096 87380 33554432net.ipv4.tcp_wmem=4096 65536 3355443Limit参数这部分参数关系到systemd和用户相关管理进程的文件具柄，防止too many files错误总结内核参数千千万，不能盲目的一键配置，可能会适得其反。大多数情况下，高并发的场景还是很少的。对于业务层面的SRE来说，熟悉一些常见的TCP相关的参数，以及文件limit就足够解决大部分问题了。]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>性能调优</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux系统优化思路]]></title>
    <url>%2F2023%2F04b206e26e.html</url>
    <content type="text"><![CDATA[Linux操作系统是一个开源产品，也是一个开源软件的实践和应用平台，在这个平台下有无数的开源软件支撑，我们常见的apache、tomcat、mysql、php等等，开源软件的最大理念是自由、开放，那么linux作为一个开源平台，最终要实现的是通过这些开源软件的支持，以最低廉的成本，达到应用最优的性能。因此，谈到性能问题，主要实现的是linux操作系统和应用程序的最佳结合。性能问题综述系统的性能是指操作系统完成任务的有效性、稳定性和响应速度。Linux系统管理员可能经常会遇到系统不稳定、响应速度慢等问题，例如在linux上搭建了一个web服务，经常出现网页无法打开、打开速度慢等现象，而遇到这些问题，就有人会抱怨linux系统不好，其实这些都是表面现象。操作系统完成一个任务时，与系统自身设置、网络拓朴结构、路由设备、路由策略、接入设备、物理线路等多个方面都密切相关，任何一个环节出现问题，都会影响整个系统的性能。因此当linux应用出现问题时，应当从应用程序、操作系统、服务器硬件、网络环境等方面综合排查，定位问题出现在哪个部分，然后集中解决。在应用程序、操作系统、服务器硬件、网络环境等方面，影响性能最大的是应用程序和操作系统两个方面，因为这两个方面出现的问题不易察觉，隐蔽性很强。而硬件、网络方面只要出现问题，一般都能马上定位。下面主要讲解操作系统方面的性能调优思路，应用程序方面需要具体问题具体对待。以下从影响Linux性能的因素、分析性能涉及的人员、系统性能优化工具、系统性能评价标准四个方面介绍优化Linux的一般思路和方法。影响Linux性能的因素2.1系统硬件资源1．CPUCPU是操作系统稳定运行的根本，CPU的速度与性能在很大程度上决定了系统整体的性能，因此，CPU数量越多、主频越高，服务器性能也就相对越好。但事实并非完全如此。目前大部分CPU在同一时间内只能运行一个线程，超线程的处理器可以在同一时间运行多个线程，因此，可以利用处理器的超线程特性提高系统性能。在Linux系统下，只有运行SMP内核才能支持超线程，但是，安装的CPU数量越多，从超线程获得的性能方面的提高就越少。另外，Linux内核会把多核的处理器当作多个单独的CPU来识别，例如两个4核的CPU，在Lnux系统下会被当作8个单核CPU。但是从性能角度来讲，两个4核的CPU和8个单核的CPU并不完全等价，根据权威部门得出的测试结论，前者的整体性能要比后者低25％~30％。可能出现CPU瓶颈的应用有db服务器、动态Web服务器等，对于这类应用，要把CPU的配置和性能放在主要位置。2．内存内存的大小也是影响Linux性能的一个重要的因素，内存太小，系统进程将被阻塞，应用也将变得缓慢，甚至失去响应；内存太大，导致资源浪费。Linux系统采用了物理内存和虚拟内存两种方式，虚拟内存虽然可以缓解物理内存的不足，但是占用过多的虚拟内存，应用程序的性能将明显下降，要保证应用程序的高性能运行，物理内存一定要足够大；但是过大的物理内存，会造成内存资源浪费，例如，在一个32位处理器的Linux操作系统上，超过8GB的物理内存都将被浪费。因此，要使用更大的内存，建议安装64位的操作系统，同时开启Linux的大内存内核支持。由于处理器寻址范围的限制，在32位Linux操作系统上，应用程序单个进程最大只能使用4GB的内存，这样以来，即使系统有更大的内存，应用程序也无法“享”用，解决的办法就是使用64位处理器，安装64位操作系统。在64位操作系统下，可以满足所有应用程序对内存的使用需求 ，几乎没有限制。可能出现内存性能瓶颈的应用有NOSQL服务器、数据库服务器、缓存服务器等，对于这类应用要把内存大小放在主要位置。3．磁盘I/O性能磁盘的I/O性能直接影响应用程序的性能，在一个有频繁读写的应用中，如果磁盘I/O性能得不到满足，就会导致应用停滞。好在现今的磁盘都采用了很多方法来提高I/O性能，比如常见的磁盘RAID技术。通过RAID技术组成的磁盘组，就相当于一个大硬盘，用户可以对它进行分区格式化、建立文件系统等操作，跟单个物理硬盘一模一样，唯一不同的是RAID磁盘组的I/O性能比单个硬盘要高很多，同时在数据的安全性也有很大提升。根据磁盘组合方式的不同，RAID可以分为RAID0，RAID1、RAID2、RAID3、RAID4、RAID5、RAID6、RAID7、RAID0+1、RAID10等级别，常用的RAID级别有RAID0、RAID1、RAID5、RAID0+1，这里进行简单介绍。RAID 0：通过把多块硬盘粘合成一个容量更大的硬盘组，提高了磁盘的性能和吞吐量。这种方式成本低，要求至少两个磁盘，但是没有容错和数据修复功能，因而只能用在对数据安全性要求不高的环境中。RAID 1：也就是磁盘镜像，通过把一个磁盘的数据镜像到另一个磁盘上，最大限度地保证磁盘数据的可靠性和可修复性，具有很高的数据冗余能力，但磁盘利用率只有50%，因而，成本最高，多用在保存重要数据的场合。RAID5：采用了磁盘分段加奇偶校验技术，从而提高了系统可靠性，RAID5读出效率很高，写入效率一般，至少需要3块盘。允许一块磁盘故障，而不影响数据的可用性。RAID0+1：把RAID0和RAID1技术结合起来就成了RAID0+1，至少需要4个硬盘。此种方式的数据除分布在多个盘上外，每个盘都有其镜像盘，提供全冗余能力，同时允许一个磁盘故障，而不影响数据可用性，并具有快速读/写能力。通过了解各个RAID级别的性能，可以根据应用的不同特性，选择适合自身的RAID级别，从而保证应用程序在磁盘方面达到最优性能。4．网络宽带Linux下的各种应用，一般都是基于网络的，因此网络带宽也是影响性能的一个重要因素，低速的、不稳定的网络将导致网络应用程序的访问阻塞，而稳定、高速的网络带宽，可以保证应用程序在网络上畅通无阻地运行。幸运的是，现在的网络一般都是千兆带宽或光纤网络，带宽问题对应用程序性能造成的影响也在逐步降低。2.2 操作系统相关资源基于操作系统的性能优化也是多方面的，可以从系统安装、系统内核参数、网络参数、文件系统等几个方面进行衡量，下面依次进行简单介绍。1．系统安装优化系统优化可以从安装操作系统开始，当安装Linux系统时，磁盘的划分，SWAP内存的分配都直接影响以后系统的运行性能。例如，磁盘分配可以遵循应用的需求：对于对写操作频繁而对数据安全性要求不高的应用，可以把磁盘做成RAID 0；而对于对数据安全性较高，对读写没有特别要求的应用，可以把磁盘做成RAID 1；对于对读操作要求较高，而对写操作无特殊要求，并要保证数据安全性的应用，可以选择RAID 5；对于对读写要求都很高，并且对数据安全性要求也很高的应用，可以选择RAID10/01。这样通过不同的应用需求设置不同的RAID级别，在磁盘底层对系统进行优化操作。随着内存价格的降低和内存容量的日益增大，对虚拟内存SWAP的设定，现在已经没有了所谓虚拟内存是物理内存两倍的要求，但是SWAP的设定还是不能忽略，根据经验：如果内存较小（物理内存小于4GB），一般设置SWAP交换分区大小为内存的2倍；如果物理内存大于8GB小于16GB，可以设置SWAP大小等于或略小于物理内存即可；如果内存大小在16GB以上，原则上可以设置SWAP为0，但并不建议这么做，因为设置一定大小的SWAP还是有一定作用的。2．内核参数优化系统安装完成后，优化工作并没有结束，接下来还可以对系统内核参数进行优化，不过内核参数的优化要和系统中部署的应用结合起来整体考虑。例如，如果系统部署的是Oracle数据库应用，那么就需要对系统共享内存段（kernel.shmmax、kernel.shmmni、kernel.shmall）、系统信号量（kernel.sem）、文件句柄（fs.file-max）等参数进行优化设置；如果部署的是Web应用，那么就需要根据Web应用特性进行网络参数的优化，例如修改net.ipv4.ip_local_port_range、net.ipv4.tcp_tw_reuse、net.core.somaxconn等网络内核参数。3．文件系统优化文件系统的优化也是系统资源优化的一个重点，在Linux下可选的文件系统有ext2、ext3、ReiserFS、ext4、xfs，根据不同的应用，选择不同的文件系统。Linux标准文件系统是从VFS开始的，然后是ext，接着就是ext2，应该说，ext2是Linux上标准的文件系统，ext3是在ext2基础上增加日志形成的，从VFS到ext4，其设计思想没有太大变化，都是早期UNIX家族基于超级块和inode的设计理念。XFS文件系统是一个高级日志文件系统，XFS通过分布处理磁盘请求、定位数据、保持Cache 的一致性来提供对文件系统数据的低延迟、高带宽的访问，因此，XFS极具伸缩性，非常健壮，具有优秀的日志记录功能、可扩展性强、快速写入性能等优点。目前服务器端ext4和xfs是主流文件系统，如何选择合适的文件系统，需要根据文件系统的特点加上业务的需求综合来定。2.3 、应用程序软件资源应用程序的优化其实是整个优化工程的核心，如果一个应用程序存在BUG，那么即使所有其他方面都达到了最优状态，整个应用系统还是性能低下，所以，对应用程序的优化是性能优化过程的重中之重，这就对程序架构设计人员和程序开发人员提出了更高的要求。分析系统性能涉及的人员3.1、Linux运维人员在做性能优化过程中，Linux运维人员承担着很重要的任务。首先，Linux运维人员要了解和掌握操作系统的当前运行状态，例如系统负载、内存状态、进程状态、CPU负荷等信息，这些信息是检测和判断系统性能的基础和依据；其次，Linux运维人员还有掌握系统的硬件信息，例如磁盘I/O、CPU型号、内存大小、网卡带宽等参数信息，然后根据这些信息综合评估系统资源的使用情况；第三，作为一名Linux运维人员，还要掌握应用程序对系统资源的使用情况，更深入的一点就是要了解应用程序的运行效率，例如是否有程序BUG、内存溢出等问题，通过对系统资源的监控，就能发现应用程序是否存在异常，如果确实是应用程序存在问题，需要把问题立刻反映给程序开发人员，进而改进或升级程序。性能优化本身就是一个复杂和繁琐的过程，Linux运维人员只有了解了系统硬件信息、网络信息、操作系统配置信息和应用程序信息才能有针对性地的展开对服务器性能优化，这就要求Linux运维人员有充足的理论知识、丰富的实战经验以及缜密分析问题的头脑。3.2、系统架构设计人员系统性能优化涉及的第二类人员就是应用程序的架构设计人员。如果Linux运维人员在经过综合判断后，发现影响性能的是应用程序的执行效率，那么程序架构设计人员就要及时介入，深入了解程序运行状态。首先，系统架构设计人员要跟踪了解程序的执行效率，如果执行效率存在问题，要找出哪里出现了问题；其次，如果真的是架构设计出现了问题，那么就要马上优化或改进系统架构，设计更好的应用系统架构。3.3、软件开发人员系统性能优化最后一个环节涉及的是程序开发人员，在Linux运维人员或架构设计人员找到程序或结构瓶颈后，程序开发人员要马上介入进行相应的程序修改。修改程序要以程序的执行效率为基准，改进程序的逻辑，有针对性地进行代码优化。例如，Linux运维人员在系统中发现有条SQL语句耗费大量的系统资源，抓取这条执行的SQL语句，发现此SQL语句的执行效率太差，是开发人员编写的代码执行效率低造成的，这就需要把这个信息反馈给开发人员，开发人员在收到这个问题后，可以有针对性的进行SQL优化，进而实现程序代码的优化。从上面这个过程可以看出，系统性能优化一般遵循的流程是：首先Linux运维人员查看系统的整体状况，主要从系统硬件、网络设备、操作系统配置、应用程序架构和程序代码五个方面进行综合判断，如果发现是系统硬件、网络设备或者操作系统配置问题，Linux运维人员可以根据情况自主解决；如果发现是程序结构问题，就需要提交给程序架构设计人员；如果发现是程序代码执行问题，就交给开发人员进行代码优化。这样就完成了一个系统性能优化的过程。调优总结系统性能优化是个涉及面广、繁琐、长久的工作，寻找出现性能问题的根源往往是最难的部分，一旦找到出现问题的原因，性能问题也就迎刃而解。因此，解决问题的思路变得非常重要。例如，linux系统下的一个网站系统，用户反映，网站访问速度很慢，有时无法访问。针对这个问题:第一步要做的是检测网络，可以通过ping命令检查网站的域名解析是否正常，同时，ping服务器地址的延时是否过大等等，通过这种方式，首先排除网络可能出现的问题；如果网络没有问题接着进入第二步，对linux系统的内存使用状况进行检查，因为网站响应速度慢，一般跟内存关联比较大，通过free、vmstat等命令判断内存资源是否紧缺，如果内存资源不存在问题进入第三步，检查系统CPU的负载状况，可以通过sar、vmstat、top等命令的输出综合判断CPU是否存在过载问题，如果CPU没有问题继续进入第四步，检查系统的磁盘I/O是否存在瓶颈，可以通过iostat、vmstat等命令检查磁盘的读写性能，如果磁盘读写也没有问题，linux系统自身的性能问题基本排除，最后要做的是检查程序本身是否存在问题。通过这样的思路，层层检测，步步排查，性能问题就“无处藏身”，查找出现性能问题的环节也就变得非常简单]]></content>
      <categories>
        <category>桌面</category>
        <category>系统</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>系统优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper详解]]></title>
    <url>%2F2023%2F04f855b121.html</url>
    <content type="text"><![CDATA[今天聊聊zookeeper的理论知识。zookeeper是什么？有什么用？ZooKeeper，它是一个开放源码的「分布式协调服务」，它是一个集群的管理者，它将简单易用的接口提供给用户。可以基于Zookeeper 实现诸如数据发布/订阅、负载均衡、命名服务、分布式协调/通知、集群管理、Master 选举、分布式锁和分布式队列等功能。Zookeeper的用途：命名服务、配置管理、集群管理、分布式锁、队列管理什么是命名服务，什么是配置管理，什么是集群管理？「命名服务」：命名服务是指通过「指定的名字」来获取资源或者服务地址。Zookeeper可以创建一个「全局唯一的路径」，这个路径就可以作为一个名字。被命名的实体可以是「集群中的机器，服务的地址，或者是远程的对象」等。一些分布式服务框架（RPC、RMI）中的服务地址列表，通过使用命名服务，客户端应用能够根据特定的名字来获取资源的实体、服务地址和提供者信息等。「配置管理：」 ：实际项目开发中，我们经常使用.properties或者xml需要配置很多信息，如数据库连接信息、fps地址端口等等。因为你的程序一般是分布式部署在不同的机器上（如果你是单机应用当我没说），如果把程序的这些配置信息「保存在zk的znode节点」下，当你要修改配置，即znode会发生变化时，可以通过改变zk中某个目录节点的内容，利用「watcher通知给各个客户端」，从而更改配置。「集群管理」集群管理包括集群监控和集群控制，其实就是监控集群机器状态，剔除机器和加入机器。zookeeper可以方便集群机器的管理，它可以实时监控znode节点的变化，一旦发现有机器挂了，该机器就会与zk断开连接，对用的临时目录节点会被删除，其他所有机器都收到通知。新机器加入也是类似酱紫，所有机器收到通知：有新兄弟目录加入啦。zookeeper的数据模型是怎样的？znode有几种类型？zookeeper的数据模型ZooKeeper的视图数据结构，很像Unix文件系统，也是树状的，这样可以确定每个路径都是唯一的。zookeeper的节点统一叫做「znode」，它是可以通过「路径来标识」，结构图如下：znode的4种类型根据节点的生命周期，znode可以分为4种类型，分别是持久节点（PERSISTENT）、持久顺序节点（PERSISTENT_SEQUENTIAL）、临时节点（EPHEMERAL）、临时顺序节点（EPHEMERAL_SEQUENTIAL）持久节点（PERSISTENT）这类节点被创建后，就会一直存在于Zk服务器上。直到手动删除。持久顺序节点（PERSISTENT_SEQUENTIAL）它的基本特性同持久节点，不同在于增加了顺序性。父节点会维护一个自增整性数字，用于子节点的创建的先后顺序。临时节点（EPHEMERAL）临时节点的生命周期与客户端的会话绑定，一旦客户端会话失效（非TCP连接断开），那么这个节点就会被自动清理掉。zk规定临时节点只能作为叶子节点。临时顺序节点（EPHEMERAL_SEQUENTIAL）基本特性同临时节点，添加了顺序的特性。znode节点里面存储的是什么？每个节点的数据最大不能超过多少？znode节点里面存储的是什么？Znode数据节点的代码如下123456public class DataNode implements Record &#123; byte data[]; Long acl; public StatPersisted stat; private Set&lt;String&gt; children = null; &#125;Znode包含了「存储数据、访问权限、子节点引用、节点状态信息」，如图：「data:」 znode存储的业务数据信息「ACL:」 记录客户端对znode节点的访问权限，如IP等。「child:」 当前节点的子节点引用「stat:」 包含Znode节点的状态信息，比如「事务id、版本号、时间戳」等等。每个节点的数据最大不能超过多少？为了保证高吞吐和低延迟，以及数据的一致性，znode只适合存储非常小的数据，不能超过1M，最好都小于1K。znode节点上的监听机制Watcher监听机制Zookeeper 允许客户端向服务端的某个Znode注册一个Watcher监听，当服务端的一些指定事件触发了这个Watcher，服务端会向指定客户端发送一个事件通知来实现分布式的通知功能，然后客户端根据 Watcher通知状态和事件类型做出业务上的改变。可以把Watcher理解成客户端注册在某个Znode上的触发器，当这个Znode节点发生变化时（增删改查），就会触发Znode对应的注册事件，注册的客户端就会收到异步通知，然后做出业务的改变。Watcher监听机制的工作原理ZooKeeper的Watcher机制主要包括客户端线程、客户端 WatcherManager、Zookeeper服务器三部分。客户端向ZooKeeper服务器注册Watcher的同时，会将Watcher对象存储在客户端的WatchManager中。当zookeeper服务器触发watcher事件后，会向客户端发送通知， 客户端线程从 WatcherManager 中取出对应的 Watcher 对象来执行回调逻辑。Watcher特性总结「一次性:」 一个Watch事件是一个一次性的触发器。一次性触发，客户端只会收到一次这样的信息。「异步的：」 Zookeeper服务器发送watcher的通知事件到客户端是异步的，不能期望能够监控到节点每次的变化，Zookeeper只能保证最终的一致性，而无法保证强一致性。「轻量级：」 Watcher 通知非常简单，它只是通知发生了事件，而不会传递事件对象内容。「客户端串行：」 执行客户端 Watcher 回调的过程是一个串行同步的过程。注册 watcher用getData、exists、getChildren方法触发 watcher用create、delete、setData方法Zookeeper的特性Zookeeper 保证了如下分布式一致性特性：「顺序一致性」：从同一客户端发起的事务请求，最终将会严格地按照顺序被应用到 ZooKeeper 中去。「原子性」：所有事务请求的处理结果在整个集群中所有机器上的应用情况是一致的，也就是说，要么整个集群中所有的机器都成功应用了某一个事务，要么都没有应用。「单一视图」：无论客户端连到哪一个 ZooKeeper 服务器上，其看到的服务端数据模型都是一致的。「可靠性：」 一旦服务端成功地应用了一个事务，并完成对客户端的响应，那么该事务所引起的服务端状态变更将会被一直保留下来。「实时性（最终一致性）：」 Zookeeper 仅仅能保证在一定的时间段内，客户端最终一定能够从服务端上读取到最新的数据状态。zookeeper是如何保证事务的顺序一致性？需要了解事务ID，即zxid。ZooKeeper的在选举时通过比较各结点的zxid和机器ID选出新的主结点的。zxid由Leader节点生成，有新写入事件时，Leader生成新zxid并随提案一起广播，每个结点本地都保存了当前最近一次事务的zxid，zxid是递增的，所以谁的zxid越大，就表示谁的数据是最新的。ZXID的生成规则如下：ZXID有两部分组成：任期：完成本次选举后，直到下次选举前，由同一Leader负责协调写入；事务计数器：单调递增，每生效一次写入，计数器加一。ZXID的低32位是计数器，所以同一任期内，ZXID是连续的，每个结点又都保存着自身最新生效的ZXID，通过对比新提案的ZXID与自身最新ZXID是否相差“1”，来保证事务严格按照顺序生效的。Zookeeper的服务器有几种角色？Zookeeper下Server工作状态有几种？Zookeeper 服务器角色Zookeeper集群中，有Leader、Follower和Observer三种角色「Leader」Leader服务器是整个ZooKeeper集群工作机制中的核心，其主要工作：事务请求的唯一调度和处理者，保证集群事务处理的顺序性集群内部各服务的调度者「Follower」Follower服务器是ZooKeeper集群状态的跟随者，其主要工作：处理客户端非事务请求，转发事务请求给Leader服务器参与事务请求Proposal的投票参与Leader选举投票「Observer」Observer是3.3.0 版本开始引入的一个服务器角色，它充当一个观察者角色——观察ZooKeeper集群的最新状态变化并将这些状态变更同步过来。其工作：处理客户端的非事务请求，转发事务请求给 Leader 服务器不参与任何形式的投票Zookeeper下Server工作状态服务器具有四种状态，分别是 LOOKING、FOLLOWING、LEADING、OBSERVING。1.LOOKING：寻找Leader状态。当服务器处于该状态时，它会认为当前集群中没有 Leader，因此需要进入 Leader 选举状态。2.FOLLOWING：跟随者状态。表明当前服务器角色是Follower。3.LEADING：领导者状态。表明当前服务器角色是Leader。4.OBSERVING：观察者状态。表明当前服务器角色是Observer。ZooKeeper集群部署图？ZooKeeper是如何保证主从节点数据一致性？ZooKeeper集群部署图ZooKeeper集群是一主多从的结构：如果是写入数据，先写入主服务器（主节点），再通知从服务器。如果是读取数据，既读主服务器的，也可以读从服务器的。ZooKeeper如何保证主从节点数据一致性我们知道集群是主从部署结构，要保证主从节点一致性问题，无非就是两个主要问题：「主服务器挂了，或者重启了」「主从服务器之间同步数据」Zookeeper是采用ZAB协议（Zookeeper Atomic Broadcast，Zookeeper原子广播协议）来保证主从节点数据一致性的，ZAB协议支持「崩溃恢复和消息广播」两种模式，很好解决了这两个问题：崩溃恢复：Leader挂了，进入该模式，选一个新的leader出来消息广播： 把更新的数据，从Leader同步到所有FollowerLeader服务器挂了，所有集群中的服务器进入LOOKING状态，首先，它们会选举产生新的Leader服务器；接着，新的Leader服务器与集群中Follower服务进行数据同步，当集群中超过半数机器与该 Leader服务器完成数据同步之后，退出恢复模式进入消息广播模式。Leader 服务器开始接收客户端的事务请求生成事务Proposal进行事务请求处理。ZooKeeper选举机制服务器启动或者服务器运行期间（Leader挂了），都会进入Leader选举，我们来看一下~假设现在ZooKeeper集群有五台服务器，它们myid分别是服务器1、2、3、4、5，如图：服务器启动的Leader选举zookeeper集群初始化阶段，服务器（myid=1-5）「依次」启动，开始zookeeper选举Leader服务器1（myid=1）启动，当前只有一台服务器，无法完成Leader选举服务器2（myid=2）启动，此时两台服务器能够相互通讯，开始进入Leader选举阶段每个服务器发出一个投票服务器1 和 服务器2都将自己作为Leader服务器进行投票，投票的基本元素包括：服务器的myid和ZXID，我们以（myid，ZXID）形式表示。初始阶段，服务器1和服务器2都会投给自己，即服务器1的投票为（1,0），服务器2的投票为（2,0），然后各自将这个投票发给集群中的其他所有机器。接受来自各个服务器的投票每个服务器都会接受来自其他服务器的投票。同时，服务器会校验投票的有效性，是否本轮投票、是否来自LOOKING状态的服务器。处理投票收到其他服务器的投票，会将被人的投票跟自己的投票PK，PK规则如下：优先检查ZXID。ZXID比较大的服务器优先作为leader。如果ZXID相同的话，就比较myid，myid比较大的服务器作为leader。 服务器1的投票是（1,0），它收到投票是（2,0），两者zxid都是0，因为收到的myid=2，大于自己的myid=1，所以它更新自己的投票为（2,0），然后重新将投票发出去。对于服务器2呢，即不再需要更新自己的投票，把上一次的投票信息发出即可。统计投票每次投票后，服务器会统计所有投票，判断是否有过半的机器接受到相同的投票信息。服务器2收到两票，少于3（n/2+1,n为总服务器），所以继续保持LOOKING状态服务器3（myid=3）启动，继续进入Leader选举阶段跟前面流程一致，服务器1和2先投自己一票，因为服务器3的myid最大，所以大家把票改投给它。此时，服务器为3票（大于等于n/2+1）,所以服务器3当选为Leader。 服务器1，2更改状态为FOLLOWING，服务器3更改状态为LEADING；服务器4启动，发起一次选举。此时服务器1，2，3已经不是LOOKING状态，不会更改选票信息。选票信息结果：服务器3为3票，服务器4为1票。服务器4并更改状态为FOLLOWING；服务器5启动，发起一次选举。同理，服务器也是把票投给服务器3，服务器5并更改状态为FOLLOWING；投票结束，服务器3当选为Leader服务器运行期间的Leader选举zookeeper集群的五台服务器（myid=1-5）正在运行中，突然某个瞬间，Leader服务器3挂了，这时候便开始Leader选举1.变更状态Leader 服务器挂了之后，余下的非Observer服务器都会把自己的服务器状态更改为LOOKING，然后开始进入Leader选举流程。2.每个服务器发起投票每个服务器都把票投给自己，因为是运行期间，所以每台服务器的ZXID可能不相同。假设服务1,2,4,5的zxid分别为333,666,999,888，则分别产生投票（1,333），（2，666），（4,999）和（5,888），然后各自将这个投票发给集群中的其他所有机器。3.接受来自各个服务器的投票4.处理投票投票规则是跟Zookeeper集群启动期间一致的，优先检查ZXID，大的优先作为Leader，所以显然服务器zxid=999具有优先权。5.统计投票6.改变服务器状态zookeeper分布式锁的实现原理Zookeeper就是使用临时顺序节点特性实现分布式锁的。获取锁过程 （创建临时节点，检查序号最小）释放锁 （删除临时节点，监听通知）获取锁过程当第一个客户端请求过来时，Zookeeper客户端会创建一个持久节点/locks。如果它（Client1）想获得锁，需要在locks节点下创建一个顺序节点lock1.如图接着，客户端Client1会查找locks下面的所有临时顺序子节点，判断自己的节点lock1是不是排序最小的那一个，如果是，则成功获得锁。这时候如果又来一个客户端client2前来尝试获得锁，它会在locks下再创建一个临时节点lock2客户端client2一样也会查找locks下面的所有临时顺序子节点，判断自己的节点lock2是不是最小的，此时，发现lock1才是最小的，于是获取锁失败。获取锁失败，它是不会甘心的，client2向它排序靠前的节点lock1注册Watcher事件，用来监听lock1是否存在，也就是说client2抢锁失败进入等待状态。此时，如果再来一个客户端Client3来尝试获取锁，它会在locks下再创建一个临时节点lock3同样的，client3一样也会查找locks下面的所有临时顺序子节点，判断自己的节点lock3是不是最小的，发现自己不是最小的，就获取锁失败。它也是不会甘心的，它会向在它前面的节点lock2注册Watcher事件，以监听lock2节点是否存在。释放锁我们再来看看释放锁的流程，zookeeper的「客户端业务完成或者故障」，都会删除临时节点，释放锁。如果是任务完成，Client1会显式调用删除lock1的指令如果是客户端故障了，根据临时节点得特性，lock1是会自动删除的lock1节点被删除后，Client2可开心了，因为它一直监听着lock1。lock1节点删除，Client2立刻收到通知，也会查找locks下面的所有临时顺序子节点，发下lock2是最小，就获得锁。同理，Client2获得锁之后，Client3也对它虎视眈眈。]]></content>
      <categories>
        <category>技术</category>
        <category>Zookeeper</category>
      </categories>
      <tags>
        <tag>Zookeeper</tag>
        <tag>zk原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper的安装与部署]]></title>
    <url>%2F2023%2F04f855b122.html</url>
    <content type="text"><![CDATA[本文讲述如何安装和部署ZooKeeper。系统要求ZooKeeper可以运行在多种系统平台上面，表1展示了zk支持的系统平台，以及在该平台上是否支持开发环境或者生产环境。表1：ZooKeeper支持的运行平台系统开发环境生产环境Linux支持支持Solaris支持支持FreeBSD支持支持Windows支持不支持MacOS支持不支持ZooKeeper是用Java编写的，运行在Java环境上，因此，在部署zk的机器上需要安装Java运行环境。为了正常运行zk，我们需要JRE1.6或者以上的版本。对于集群模式下的ZooKeeper部署，3个ZooKeeper服务进程是建议的最小进程数量，而且不同的服务进程建议部署在不同的物理机器上面，以减少机器宕机带来的风险，以实现ZooKeeper集群的高可用。ZooKeeper对于机器的硬件配置没有太大的要求。例如，在Yahoo!内部，ZooKeeper部署的机器其配置通常如下：双核处理器，2GB内存，80GB硬盘。下载可以从 https://zookeeper.apache.org/releases.html 下载ZooKeeper，目前最新的稳定版本为 3.4.8 版本，用户可以自行选择一个速度较快的镜像来下载即可。目录下载并解压ZooKeeper软件压缩包后，可以看到zk包含以下的文件和目录：bin目录zk的可执行脚本目录，包括zk服务进程，zk客户端，等脚本。其中，.sh是Linux环境下的脚本，.cmd是Windows环境下的脚本。conf目录配置文件目录。zoo_sample.cfg为样例配置文件，需要修改为自己的名称，一般为zoo.cfg。log4j.properties为日志配置文件。libzk依赖的包。contrib目录一些用于操作zk的工具包。recipes目录zk某些用法的代码示例单机模式ZooKeeper的安装包括单机模式安装，以及集群模式安装。单机模式较简单，是指只部署一个zk进程，客户端直接与该zk进程进行通信。在开发测试环境下，通过来说没有较多的物理资源，因此我们常使用单机模式。当然在单台物理机上也可以部署集群模式，但这会增加单台物理机的资源消耗。故在开发环境中，我们一般使用单机模式。但是要注意，生产环境下不可用单机模式，这是由于无论从系统可靠性还是读写性能，单机模式都不能满足生产的需求。运行配置上面提到，conf目录下提供了配置的样例zoo_sample.cfg，要将zk运行起来，需要将其名称修改为zoo.cfg。打开zoo.cfg，可以看到默认的一些配置。tickTime时长单位为毫秒，为zk使用的基本时间度量单位。例如，1 tickTime是客户端与zk服务端的心跳时间，2 tickTime是客户端会话的超时时间。tickTime的默认值为2000毫秒，更低的tickTime值可以更快地发现超时问题，但也会导致更高的网络流量（心跳消息）和更高的CPU使用率（会话的跟踪处理）。clientPortzk服务进程监听的TCP端口，默认情况下，服务端会监听2181端口。dataDir无默认配置，必须配置，用于配置存储快照文件的目录。如果没有配置dataLogDir，那么事务日志也会存储在此目录。启动在Windows环境下，直接双击zkServer.cmd即可。在Linux环境下，进入bin目录，执行命令1./zkServer.sh start这个命令使得zk服务进程在后台进行。如果想在前台中运行以便查看服务器进程的输出日志，可以通过以下命令运行：1./zkServer.sh start-foreground执行此命令，可以看到大量详细信息的输出，以便允许查看服务器发生了什么。使用文本编辑器打开zkServer.cmd或者zkServer.sh文件，可以看到其会调用zkEnv.cmd或者zkEnv.sh脚本。zkEnv脚本的作用是设置zk运行的一些环境变量，例如配置文件的位置和名称等。连接如果是连接同一台主机上的zk进程，那么直接运行bin/目录下的zkCli.cmd（Windows环境下）或者zkCli.sh（Linux环境下），即可连接上zk。直接执行zkCli.cmd或者zkCli.sh命令默认以主机号 127.0.0.1，端口号 2181 来连接zk，如果要连接不同机器上的zk，可以使用 -server 参数，例如：1bin/zkCli.sh -server 192.168.0.1:2181集群模式单机模式的zk进程虽然便于开发与测试，但并不适合在生产环境使用。在生产环境下，我们需要使用集群模式来对zk进行部署。注意在集群模式下，建议至少部署3个zk进程，或者部署奇数个zk进程。如果只部署2个zk进程，当其中一个zk进程挂掉后，剩下的一个进程并不能构成一个quorum的大多数。因此，部署2个进程甚至比单机模式更不可靠，因为2个进程其中一个不可用的可能性比一个进程不可用的可能性还大。5. 1 运行配置在集群模式下，所有的zk进程可以使用相同的配置文件（是指各个zk进程部署在不同的机器上面），例如如下配置：12345678tickTime=2000dataDir=/home/myname/zookeeperclientPort=2181initLimit=5syncLimit=2server.1=192.168.229.160:2888:3888server.2=192.168.229.161:2888:3888server.3=192.168.229.162:2888:3888initLimitZooKeeper集群模式下包含多个zk进程，其中一个进程为leader，余下的进程为follower。当follower最初与leader建立连接时，它们之间会传输相当多的数据，尤其是follower的数据落后leader很多。initLimit配置follower与leader之间建立连接后进行同步的最长时间。syncLimit配置follower和leader之间发送消息，请求和应答的最大时间长度。tickTimetickTime则是上述两个超时配置的基本单位，例如对于initLimit，其配置值为5，说明其超时时间为 2000ms * 5 = 10秒。server.id=host:port1:port2其中id为一个数字，表示zk进程的id，这个id也是dataDir目录下myid文件的内容。host是该zk进程所在的IP地址，port1表示follower和leader交换消息所使用的端口，port2表示选举leader所使用的端口。dataDir其配置的含义跟单机模式下的含义类似，不同的是集群模式下还有一个myid文件。myid文件的内容只有一行，且内容只能为1 - 255之间的数字，这个数字亦即上面介绍server.id中的id，表示zk进程的id。注意如果仅为了测试部署集群模式而在同一台机器上部署zk进程，server.id=host:port1:port2配置中的port参数必须不同。但是，为了减少机器宕机的风险，强烈建议在部署集群模式时，将zk进程部署不同的物理机器上面。启动假如我们打算在三台不同的机器 192.168.229.160，192.168.229.161，192.168.229.162上各部署一个zk进程，以构成一个zk集群。三个zk进程均使用相同的 zoo.cfg 配置：12345678tickTime=2000dataDir=/home/myname/zookeeperclientPort=2181initLimit=5syncLimit=2server.1=192.168.229.160:2888:3888server.2=192.168.229.161:2888:3888server.3=192.168.229.162:2888:3888在三台机器dataDir目录（ /home/myname/zookeeper 目录）下，分别生成一个myid文件，其内容分别为1，2，3。然后分别在这三台机器上启动zk进程，这样我们便将zk集群启动了起来。连接可以使用以下命令来连接一个zk集群：1bin/zkCli.sh -server 192.168.229.160:2181,192.168.229.161:2181,192.168.229.162:2181成功连接后，可以看到如下输出：12345678910112016-06-28 19:29:18,074 [myid:] - INFO [main:ZooKeeper@438] - Initiating client connection, connectString=192.168.229.160:2181,192.168.229.161:2181,192.168.229.162:2181 sessionTimeout=30000 watcher=org.apache.zookeeper.ZooKeeperMain$MyWatcher@770537e4Welcome to ZooKeeper!2016-06-28 19:29:18,146 [myid:] - INFO [main-SendThread(192.168.229.162:2181):ClientCnxn$SendThread@975] - Opening socket connection to server 192.168.229.162/192.168.229.162:2181. Will not attempt to authenticate using SASL (unknown error)JLine support is enabled2016-06-28 19:29:18,161 [myid:] - INFO [main-SendThread(192.168.229.162:2181):ClientCnxn$SendThread@852] - Socket connection established to 192.168.229.162/192.168.229.162:2181, initiating session2016-06-28 19:29:18,199 [myid:] - INFO [main-SendThread(192.168.229.162:2181):ClientCnxn$SendThread@1235] - Session establishment complete on server 192.168.229.162/192.168.229.162:2181, sessionid = 0x3557c39d2810029, negotiated timeout = 30000WATCHER::WatchedEvent state:SyncConnected type:None path:null[zk: 192.168.229.160:2181,192.168.229.161:2181,192.168.229.162:2181(CONNECTED) 0]从日志输出可以看到，客户端连接的是192.168.229.162:2181进程（连接上哪台机器的zk进程是随机的），客户端已成功连接上zk集群。]]></content>
      <categories>
        <category>技术</category>
        <category>Zookeeper</category>
      </categories>
      <tags>
        <tag>Zookeeper</tag>
        <tag>zk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL主从结构详解]]></title>
    <url>%2F2023%2F0417f29fe3.html</url>
    <content type="text"><![CDATA[今天聊一下数据库主从结构数据库主从概念、优点、用途主从数据库是什么意思呢，主是主库的意思，从是从库的意思。数据库主库对外提供读写的操作，从库对外提供读的操作。数据库为什么需要主从架构呢？高可用，实时灾备，用于故障切换。比如主库挂了，可以切从库。读写分离，提供查询服务，减少主库压力，提升性能备份数据，避免影响业务。数据库主从复制原理主从复制原理，简言之，分三步曲进行：主数据库有个bin log二进制文件，记录了所有增删改SQL语句。（binlog线程）从数据库把主数据库的bin log文件的SQL 语句复制到自己的中继日志 relay log（io线程）从数据库的relay log重做日志文件，再执行一次这些sql语句。（Sql执行线程）详细的主从复制过程如图：上图主从复制过程分了五个步骤进行：主库的更新SQL(update、insert、delete)被写到binlog从库发起连接，连接到主库。此时主库创建一个binlog dump thread，把bin log的内容发送到从库。从库启动之后，创建一个I/O线程，读取主库传过来的bin log内容并写入到relay log从库还会创建一个SQL线程，从relay log里面读取内容，从ExecMasterLog_Pos位置开始执行读取到的更新事件，将更新内容写入到slave的db主主、主从、主备的区别数据库主主：两台都是主数据库，同时对外提供读写操作。客户端访问任意一台。数据存在双向同步。数据库主从：一台是主数据库，同时对外提供读写操作。一台是从数据库，对外提供读的操作。数据从主库同步到从库。数据库主备：一台是主数据库，同时对外提供读写操作。一台是备库，只作为备份作用，不对外提供读写，主机挂了它就取而代之。数据从主库同步到备库。从库和备库，就是slave库功能不同因此叫法才不一样而已。一般slave库都会对外提供读的功能的，因此，大家日常听得比较多就是主从。MySQL是怎么保证主从一致的我们学习数据库的主从复制原理后，了解到从库拿到并执行主库的binlog日志，就可以保持数据与主库一致了。这是为什么呢？哪些情况会导致不一致呢？长链接主库和从库在同步数据的过程中断怎么办呢，数据不就会丢失了嘛。因此主库与从库之间维持了一个长链接，主库内部有一个线程，专门服务于从库的这个长链接的。binlog格式binlog 日志有三种格式，分别是statement，row和mixed。如果是statement格式，binlog记录的是SQL的原文，如果主库和从库选的索引不一致，可能会导致主库不一致。我们来分析一下。假设主库执行删除这个SQL（其中a和create_time都有索引）如下：12delete from t where a &gt; '666' and create_time&lt;'2022-03-01' limit 1;复制代码我们知道，数据选择了a索引和选择create_time索引，最后limit 1出来的数据一般是不一样的。所以就会存在这种情况：在binlog = statement格式时，主库在执行这条SQL时，使用的是索引a，而从库在执行这条SQL时，使用了索引create_time。最后主从数据不一致了。如何解决这个问题呢？可以把binlog格式修改为row。row格式的binlog日志，记录的不是SQL原文，而是两个event:Table_map 和 Delete_rows。Table_map event说明要操作的表，Delete_rows event用于定义要删除的行为，记录删除的具体行数。row格式的binlog记录的就是要删除的主键ID信息，因此不会出现主从不一致的问题。但是如果SQL删除10万行数据，使用row格式就会很占空间的，10万条数据都在binlog里面，写binlog的时候也很耗IO。但是statement格式的binlog可能会导致数据不一致，因此设计MySQL的大叔想了一个折中的方案，mixed格式的binlog。所谓的mixed格式其实就是row和statement格式混合使用，当MySQL判断可能数据不一致时，就用row格式，否则使用就用statement格式。数据库主从延迟的原因与解决方案主从延迟是怎么定义的呢？ 与主从数据同步相关的时间点有三个主库执行完一个事务，写入binlog，我们把这个时刻记为T1；主库同步数据给从库，从库接收完这个binlog的时刻，记录为T2；从库执行完这个事务，这个时刻记录为T3。所谓主从延迟，其实就是指同一个事务，在从库执行完的时间和在主库执行完的时间差值，即T3-T1。哪些情况会导致主从延迟呢？如果从库所在的机器比主库的机器性能差，会导致主从延迟，这种情况比较好解决，只需选择主从库一样规格的机器就好。如果从库的压力大，也会导致主从延迟。比如主库直接影响业务的，大家可能使用会比较克制，因此一般查询都打到从库了，结果导致从库查询消耗大量CPU，影响同步速度，最后导致主从延迟。这种情况的话，可以搞了一主多从的架构，即多接几个从库分摊读的压力。另外，还可以把binlog接入到Hadoop这类系统，让它们提供查询的能力。大事务也会导致主从延迟。如果一个事务执行就要10分钟，那么主库执行完后，给到从库执行，最后这个事务可能就会导致从库延迟10分钟啦。日常开发中，我们为什么特别强调，不要一次性delete太多SQL，需要分批进行，其实也是为了避免大事务。另外，大表的DDL语句，也会导致大事务，大家日常开发关注一下哈。网络延迟也会导致主从延迟，这种情况你只能优化你的网络啦，比如带宽20M升级到100M类似意思等。如果从数据库过多也会导致主从延迟，因此要避免复制的从节点数量过多。从库数据一般以3-5个为宜。低版本的MySQL只支持单线程复制，如果主库并发高，来不及传送到从库，就会导致延迟。可以换用更高版本的Mysql，可以支持多线程复制。聊聊数据的库高可用方案双机主备高可用一主一从一主多从MariaDB同步多主机集群数据库中间件双机主备高可用架构描述：两台机器A和B，A为主库，负责读写，B为备库，只备份数据。如果A库发生故障，B库成为主库负责读写。修复故障后，A成为备库，主库B同步数据到备库A优点：一个机器故障了可以自动切换，操作比较简单。缺点：只有一个库在工作，读写压力大，未能实现读写分离，并发也有一定限制一主一从架构描述: 两台机器A和B，A为主库，负责读写，B为从库，负责读数据。如果A库发生故障，B库成为主库负责读写。修复故障后，A成为从库，主库B同步数据到从库A。优点： 从库支持读，分担了主库的压力，提升了并发度。一个机器故障了可以自动切换，操作比较简单。缺点： 一台从库，并发支持还是不够，并且一共两台机器，还是存在同时故障的机率，不够高可用。一主多从架构描述: 一台主库多台从库，A为主库，负责读写，B、C、D为从库，负责读数据。如果A库发生故障，B库成为主库负责读写，C、D负责读。修复故障后，A也成为从库，主库B同步数据到从库A。优点： 多个从库支持读，分担了主库的压力，明显提升了读的并发度。缺点： 只有台主机写，因此写的并发度不高MariaDB同步多主机集群架构描述:有代理层实现负载均衡，多个数据库可以同时进行读写操作；各个数据库之间可以通过Galera Replication方法进行数据同步，每个库理论上数据是完全一致的。优点： 读写的并发度都明显提升，可以任意节点读写，可以自动剔除故障节点，具有较高的可靠性。缺点： 数据量不支持特别大。要避免大事务卡死，如果集群节点一个变慢，其他节点也会跟着变慢。数据库中间件架构描述：mycat分片存储，每个分片配置一主多从的集群。优点：解决高并发高数据量的高可用方案缺点：维护成本比较大。]]></content>
      <categories>
        <category>技术</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>主从结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL主从同步详解与配置]]></title>
    <url>%2F2023%2F0490be6400.html</url>
    <content type="text"><![CDATA[MySQL数据库自身提供的主从复制功能可以方便的实现数据的多处自动备份，实现数据库的拓展。多个数据备份不仅可以加强数据的安全性，通过实现读写分离还能进一步提升数据库的负载性能。下图就描述了一个多个数据库间主从复制与读写分离的模型。web服务器(tomcat,apache,iis,weblogic等)从多个slave数据库进行读操作，从master数据库进行写操作，如图：第一部分【原理解析】在一主多从的数据库体系中，多个从服务器采用异步的方式更新主数据库的变化，业务服务器在执行写或者相关修改数据库的操作是在主服务器上进行的，读操作则是在各从服务器上进行。MySQL主从复制原理为什么要做主从复制在业务复杂的系统中，有这么一个情景，有一句sql语句需要锁表，导致暂时不能使用读的服务，那么就很影响运行中的业务，使用主从复制，让主库负责写，从库负责读，这样，即使主库出现了锁表的情景，通过读从库也可以保证业务的正常运行。做数据的热备，主库宕机后能够及时替换主库，保证业务可用性。架构的扩展。业务量越来越大，I/O访问频率过高，单机无法满足，此时做多库的存储，降低磁盘I/O访问的频率，提高单个机器的I/O性能。MySQL主从复制的流程主库db的更新事件(update、insert、delete)被写到binlog主库创建一个binlog dump thread，把binlog的内容发送到从库从库启动并发起连接，连接到主库从库启动之后，创建一个I/O线程，读取主库传过来的binlog内容并写入到relay log从库启动之后，创建一个SQL线程，从relay log里面读取内容，从Exec_Master_Log_Pos位置开始执行读取到的更新事件，将更新内容写入到slave的db注：上述流程为相对流程，并非绝对流程MySQL 主从复制主要用途读写分离在开发工作中，有时候会遇见某个sql 语句需要锁表，导致暂时不能使用读的服务，这样就会影响现有业务，使用主从复制，让主库负责写，从库负责读，这样，即使主库出现了锁表的情景，通过读从库也可以保证业务的正常运作。数据实时备份，当系统中某个节点发生故障时，可以方便的故障切换高可用HA架构扩展随着系统中业务访问量的增大，如果是单机部署数据库，就会导致I/O访问频率过高。有了主从复制，增加多个数据存储节点，将负载分布在多个从节点上，降低单机磁盘I/O访问的频率，提高单个机器的I/O性能。MySQL 主从形式一主一从一主多从，提高系统的读性能一主一从和一主多从是最常见的主从架构，实施起来简单并且有效，不仅可以实现HA，而且还能读写分离，进而提升集群的并发能力。多主一从 （从5.7开始支持）多主一从可以将多个mysql数据库备份到一台存储性能比较好的服务器上。双主复制双主复制，也就是互做主从复制，每个master既是master，又是另外一台服务器的slave。这样任何一方所做的变更，都会通过复制应用到另外一方的数据库中。级联复制级联复制模式下，部分slave的数据同步不连接主节点，而是连接从节点。因为如果主节点有太多的从节点，就会损耗一部分性能用于replication，那么我们可以让3~5个从节点连接主节点，其它从节点作为二级或者三级与从节点连接，这样不仅可以缓解主节点的压力，并且对数据一致性没有负面影响。MySQL 主从复制原理详解MySQL主从复制涉及到三个线程，一个运行在主节点（log dump thread），其余两个(I/O thread, SQL thread)运行在从节点，如下图所示:主节点 binary log dump 线程当从节点连接主节点时，主节点会创建一个log dump 线程，用于发送bin-log的内容。在读取bin-log中的操作时，此线程会对主节点上的bin-log加锁，当读取完成，甚至在发动给从节点之前，锁会被释放。从节点I/O线程当从节点上执行start slave命令之后，从节点会创建一个I/O线程用来连接主节点，请求主库中更新的bin-log。I/O线程接收到主节点binlog dump 进程发来的更新之后，保存在本地relay-log中。从节点SQL线程SQL线程负责读取relay log中的内容，解析成具体的操作并执行，最终保证主从数据的一致性。对于每一个主从连接，都需要三个进程来完成。当主节点有多个从节点时，主节点会为每一个当前连接的从节点建一个binary log dump 进程，而每个从节点都有自己的I/O进程，SQL进程。从节点用两个线程将从主库拉取更新和执行分成独立的任务，这样在执行同步数据任务的时候，不会降低读操作的性能。比如，如果从节点没有运行，此时I/O进程可以很快从主节点获取更新，尽管SQL进程还没有执行。如果在SQL进程执行之前从节点服务停止，至少I/O进程已经从主节点拉取到了最新的变更并且保存在本地relay日志中，当服务再次起来之后，就可以完成数据的同步。要实施复制，首先必须打开Master 端的binary log（bin-log）功能，否则无法实现。因为整个复制过程实际上就是Slave 从Master 端获取该日志然后再在自己身上完全顺序的执行日志中所记录的各种操作。如下图所示：复制的基本过程如下：从节点上的I/O 进程连接主节点，并请求从指定日志文件的指定位置（或者从最开始的日志）之后的日志内容；主节点接收到来自从节点的I/O请求后，通过负责复制的I/O进程根据请求信息读取指定日志指定位置之后的日志信息，返回给从节点。返回信息中除了日志所包含的信息之外，还包括本次返回的信息的bin-log file 的以及bin-log position；从节点的I/O进程接收到内容后，将接收到的日志内容更新到本机的relay log中，并将读取到的binary log文件名和位置保存到master-info 文件中，以便在下一次读取的时候能够清楚的告诉Master“我需要从某个bin-log 的哪个位置开始往后的日志内容，请发给我”；Slave 的 SQL线程检测到relay-log 中新增加了内容后，会将relay-log的内容解析成在祝节点上实际执行过的操作，并在本数据库中执行。MySQL 主从复制模式MySQL 主从复制默认是异步的模式。MySQL增删改操作会全部记录在binary log中，当slave节点连接master时，会主动从master处获取最新的bin log文件。并把bin log中的sql relay。异步模式（mysql async-mode）异步模式如下图所示，这种模式下，主节点不会主动push bin log到从节点，这样有可能导致failover的情况下，也许从节点没有即时地将最新的bin log同步到本地。半同步模式(mysql semi-sync)这种模式下主节点只需要接收到其中一台从节点的返回信息，就会commit；否则需要等待直到超时时间然后切换成异步模式再提交；这样做的目的可以使主从数据库的数据延迟缩小，可以提高数据安全性，确保了事务提交后，binlog至少传输到了一个从节点上，不能保证从节点将此事务更新到db中。性能上会有一定的降低，响应时间会变长。如下图所示：半同步模式不是mysql内置的，从mysql 5.5开始集成，需要master 和slave 安装插件开启半同步模式。全同步模式全同步模式是指主节点和从节点全部执行了commit并确认才会向客户端返回成功。binlog记录格式MySQL 主从复制有三种方式：基于SQL语句的复制（statement-based replication，SBR），基于行的复制（row-based replication，RBR)，混合模式复制（mixed-based replication,MBR)。对应的binlog文件的格式也有三种：STATEMENT,ROW,MIXED。Statement-base Replication (SBR)就是记录sql语句在bin log中，Mysql 5.1.4 及之前的版本都是使用的这种复制格式。优点是只需要记录会修改数据的sql语句到binlog中，减少了binlog日质量，节约I/O，提高性能。缺点是在某些情况下，会导致主从节点中数据不一致（比如sleep(),now()等）。Row-based Relication(RBR)是mysql master将SQL语句分解为基于Row更改的语句并记录在bin log中，也就是只记录哪条数据被修改了，修改成什么样。优点是不会出现某些特定情况下的存储过程、或者函数、或者trigger的调用或者触发无法被正确复制的问题。缺点是会产生大量的日志，尤其是修改table的时候会让日志暴增,同时增加bin log同步时间。也不能通过bin log解析获取执行过的sql语句，只能看到发生的data变更。Mixed-format Replication(MBR)，MySQL NDB cluster 7.3 和7.4 使用的MBR。是以上两种模式的混合，对于一般的复制使用STATEMENT模式保存到binlog，对于STATEMENT模式无法复制的操作则使用ROW模式来保存，MySQL会根据执行的SQL语句选择日志保存方式。GTID复制模式在传统的复制里面，当发生故障，需要主从切换，需要找到binlog和pos点，然后将主节点指向新的主节点，相对来说比较麻烦，也容易出错。在MySQL 5.6里面，不用再找binlog和pos点，我们只需要知道主节点的ip，端口，以及账号密码就行，因为复制是自动的，MySQL会通过内部机制GTID自动找点同步。@ 多线程复制（基于库），在MySQL 5.6以前的版本，slave的复制是单线程的。一个事件一个事件的读取应用。而master是并发写入的，所以延时是避免不了的。唯一有效的方法是把多个库放在多台slave，这样又有点浪费服务器。在MySQL 5.6里面，我们可以把多个表放在多个库，这样就可以使用多线程复制。基于GTID复制实现的工作原理主节点更新数据时，会在事务前产生GTID，一起记录到binlog日志中。从节点的I/O线程将变更的bin log，写入到本地的relay log中。SQL线程从relay log中获取GTID，然后对比本地binlog是否有记录（所以MySQL从节点必须要开启binary log）。如果有记录，说明该GTID的事务已经执行，从节点会忽略。如果没有记录，从节点就会从relay log中执行该GTID的事务，并记录到bin log。在解析过程中会判断是否有主键，如果没有就用二级索引，如果有就用全部扫描。总结Mysql 主从复制是mysql 高可用，高性能的基础，有了这个基础，mysql 的部署会变得简单、灵活并且具有多样性，从而可以根据不同的业务场景做出灵活的调整。第二部分【主从同步部署设置详解】MySQL数据库主从同步配置详解一：准备事项两台服务器，mysql版本必须一致，不然会出现问题注意：在进行主从同步之前先确保要备份的数据库和表在从库存在，主从复制不会创建库和表，如果没有会出现不能同步的错误，可以自己通过mysqldump进行数据导出，然后再用source进行数据恢复到从库中去。实验环境：centos7-2003 64位，mysql5.7.25-el7【一主一从-主从同步配置，手动指定二进制文件位置】主服务器配置：1 安装mysql数据库【本文使用的是mysql5.7.25版本】2 操作一些数据到mysql数据库3 vim /etc/my.cnf ——-修改my.cnf文件配置以下参数[mysqld]log_bin ——开启二进制日志 server_id=1 —–设置服务器id，这个id是唯一，不能跟别的服务器重复12345[root@mysql51 ~]# vim /etc/my.cnf[mysqld]log_bin # 开启二进制日志server_id=1 # 设置服务器id[root@localhost tmp]# systemctl restart mysqld # 重启mysql使设置生效4 备份数据库123456789101112[root@localhost tmp]# mysqldump -u'root' -p'123456' --all-databases --single-transaction --master-data=2 --flush-logs &gt; /tmp/mysqlbackup/`date +%F_%H-%M-%S`-mysql-all.sql # 备份数据库# 下面的命令是发送数据备份文件到从服务器的/tmp/目录下[root@localhost tmp]# scp /tmp/2020-12-09_13-18-55-mysql-all.sql 192.168.137.101:/tmp/The authenticity of host '192.168.137.101 (192.168.137.101)' can't be established.ECDSA key fingerprint is SHA256:XGNB4Tf/YmQOhuzY8BXxSIEbRbKli30IAJLfR5UM4VI.ECDSA key fingerprint is MD5:96:ef:27:6c:35:16:f0:99:5c:4b:1d:4c:27:b1:56:bf.Are you sure you want to continue connecting (yes/no)? yes # 这里输入yes表示需要连接Warning: Permanently added '192.168.137.101' (ECDSA) to the list of known hosts.root@192.168.137.101's password: # 这里输入从服务器密码2020-12-09_13-18-55-mysql-all.sql 100% 777KB 89.1MB/s 00:00 # 看到100%说明传输成功[root@localhost tmp]#参数说明：mysqldump ——–mysql自带的备份数据库命令-u’root’ ————–登陆用户名（实验用的是root用户，生产环境下只能用普通用户）-p’123456’———–登陆密码–all-databases ——-备份所有库 （如果只是备份单独一个数据库，写数据库名即可）（如果备份多个数据库 –databases database1 database2 database3 ）–single-transaction ——– 一致性服务可用性，锁表机制 , 热备份–master-data=2——该选项将会记录binlog的日志位置与文件名，可以选择1或者2，效果一样–flush-logs————–自动刷新日志5 在主服务器上创建并授权从服务器使用的mysql同步数据使用的mysql账户mysql -u’root’ -p’123456’ —登陆主服务器mysql数据库mysql&gt; create user ‘rep‘@’%’ identified by ‘1234567’; —-创建从服务器同步用的mysql用户mysql&gt; grant replication slave,replication client on . to ‘rep‘@’%’; —-授权创建的用户权限12345678910111213141516171819[root@localhost tmp]# mysql -uroot -p'123456' # 登陆mysqlmysql: [Warning] Using a password on the command line interface can be insecure.Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 3Server version: 5.7.25-log MySQL Community Server (GPL)Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.mysql&gt; create user 'rep'@'%' identified by 'test@A123456'; # 创建rep用户Query OK, 0 rows affected (0.00 sec)mysql&gt; grant replication slave,replication client on *.* to 'rep'@'%'; # 授权rep权限Query OK, 0 rows affected (0.01 sec)从服务器配置：1 安装mysql数据库（mysql版本必须跟主服务器版本一致）2 测试rep远程登录是否可用 mysql -h 192.168.137.100 -u’rep’ -p’1234567’ 能登陆到mysql说明测试ok12345678910111213141516# 远程登陆主服务器mysql测试rep账户可用[root@localhost tmp]# mysql -h'192.168.137.100' -u'rep' -p'test@A123456' mysql: [Warning] Using a password on the command line interface can be insecure.Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 4Server version: 5.7.25-log MySQL Community Server (GPL)Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.mysql&gt;3 vim /etc/my.cnf 设置mysql文件[mysqld]server_id=2 设置服务器id，注意id的唯一性4 重启mysql 执行命令 systemctl restart mysqld1[root@localhost tmp]# systemctl restart mysqld5 手动同步数据 把上面使用mysqldump备份的sql文件导入到从服务器mysql数据库执行命令 mysql -u’root’ -p’数据库密码’ &lt; /tmp/2020-12-09_13-18-55-mysql-all.sql123[root@localhost tmp]# mysql -uroot -p'123456' &lt; /tmp/2020-12-09_13-18-55-mysql-all.sql mysql: [Warning] Using a password on the command line interface can be insecure.[root@localhost tmp]#6 登陆mysql设置主从关系1mysql -u'root' -p'数据库密码'1mysql&gt; change master to master_host=&apos;192.168.137.100&apos;,master_user=&apos;rep&apos;,master_password=&apos;test@A123456&apos;,master_log_file=&apos;localhost-bin.000002&apos;,master_log_pos=154;123422 -- CHANGE MASTER TO MASTER_LOG_FILE='localhost-bin.000002', MASTER_LOG_POS=154;说明：这里的MASTER_LOG_FILE及MASTER_LOG_POS参数的来源就是上面2020-12-09_13-18-55-mysql-all.sql文件里的 使用vim 2020-12-09_13-18-55-mysql-all.sql找到第22行就能看到这里是我们自己手动指定文件位置，【这个位置也可以设置成自动的，下面的内容还会单独列出来】参数解释：MASTER_HOST : 设置要连接的主服务器的ip地址 MASTER_USER : 设置要连接的主服务器的用户名 MASTER_PASSWORD : 设置要连接的主服务器的密码 MASTER_LOG_FILE : 设置要连接的主服务器的bin日志的日志名称，即第5步得到的信息 MASTER_LOG_POS : 设置要连接的主服务器的bin日志的记录位置，即第5步得到的信息，（这里注意，最后一项不需要加引号。否则配置失败）7 启动从服务器 mysql&gt;start slave;12mysql&gt; start slave;Query OK, 0 rows affected (0.00 sec)8 查看启动状态 mysql&gt;show slave status\G;12345# 下面两项必须都是yes才表示配置成功，否则必须stop slave 检查配置修改正确后再次start slave# 直到都是yes为止Slave_IO_Running: Yes Slave_SQL_Running: Yes123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960mysql&gt; show slave status\G;*************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 192.168.137.100 Master_User: rep Master_Port: 3306 Connect_Retry: 60 Master_Log_File: localhost-bin.000002 Read_Master_Log_Pos: 613 Relay_Log_File: localhost-relay-bin.000002 Relay_Log_Pos: 783 Relay_Master_Log_File: localhost-bin.000002 Slave_IO_Running: Yes Slave_SQL_Running: Yes Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: Last_Errno: 0 Last_Error: Skip_Counter: 0 Exec_Master_Log_Pos: 613 Relay_Log_Space: 994 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: 0Master_SSL_Verify_Server_Cert: No Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: Replicate_Ignore_Server_Ids: Master_Server_Id: 1 Master_UUID: d4194c0e-35e2-11eb-a021-000c291c0d9f Master_Info_File: /var/lib/mysql/master.info SQL_Delay: 0 SQL_Remaining_Delay: NULL Slave_SQL_Running_State: Slave has read all relay log; waiting for more updates Master_Retry_Count: 86400 Master_Bind: Last_IO_Error_Timestamp: Last_SQL_Error_Timestamp: Master_SSL_Crl: Master_SSL_Crlpath: Retrieved_Gtid_Set: Executed_Gtid_Set: Auto_Position: 0 Replicate_Rewrite_DB: Channel_Name: Master_TLS_Version: 1 row in set (0.00 sec)注意事项：如果主库里面有初始化数据，那么在主从复制前需要把数据先导入到从库以保证初始一致性！【关于验证主从复制是否有效，方法是，往主服务器mysql里写数据，然后在从服务器查询数据】左边是主服务器插入数据，右边是从服务同步写入并查询数据上面的日志文件位置是需要手动指定的，实际生产环境中可能出现二进制文件非常多的情况，再用手动指定会很不方便下面介绍如自动指定文件位置【一主一从-主从同步配置，自动指定二进制文件位置】操作步骤跟上一致，只是参数有所变化，主从服务同时开启gtid模式从MySQL 5.6.5 开始新增了一种基于 GTID 的复制方式。通过 GTID 保证了每个在主库上提交的事务在集群中有一个唯一的ID。这种方式强化了数据库的主备一致性，故障恢复以及容错能力。GTID (Global Transaction ID)是全局事务ID,当在主库上提交事务或者被从库应用时，可以定位和追踪每一个事务，对DBA来说意义就很大了，我们可以适当的解放出来，不用手工去可以找偏移量的值了，而是通过CHANGE MASTER TO MASTER_HOST=’xxx’, MASTER_AUTO_POSITION=1的即可方便的搭建从库，在故障修复中也可以采用MASTER_AUTO_POSITION=‘X’的方式。可能大多数人第一次听到GTID的时候会感觉有些突兀，但是从架构设计的角度，GTID是一种很好的分布式ID实践方式，通常来说，分布式ID有两个基本要求：1）全局唯一性2）趋势递增这个ID因为是全局唯一，所以在分布式环境中很容易识别，因为趋势递增，所以ID是具有相应的趋势规律，在必要的时候方便进行顺序提取，行业内适用较多的是基于Twitter的ID生成算法snowflake,所以换一个角度来理解GTID，其实是一种优雅的分布式设计。1.如何开启GTID如何开启GTID呢，我们先来说下基础的内容，然后逐步深入，通常来说，需要在my.cnf中配置如下的几个参数：server_id=1①log-bin②gtid_mode=ON③enforce_gtid_consistency=1GTID工作原理简单介绍1.master更新数据的时候，会在事务前产生GTID，一同记录到binlog日志中。2.slave端的io线程将binlog写入到本地relay log中。3.然后SQL线程从relay log中读取GTID，设置gtid_next的值为该gtid，然后对比slave端的binlog是否有记录4.如果有记录的话，说明该GTID的事务已经运行，slave会忽略5.如果没有记录的话，slave就会执行该GTID对应的事务，并记录到binlog中GTID工作原理深入理解gtid在master和slave上持久化保存，即使删除了日志，也会记录到previous_gtid中。1.binlog文件中记录的格式是先记录gtid，然后再记录事务相关的操作。2.gtid_next 是基于会话的，不同会话的gtid_next不同。3.如果mysql是5.6版本，那么主从必须开启log_slave_updates参数，此时slave对比自己的binlog查看是否有记录。如果mysql是5.7版本，那么主从不需要开启此参数（级联主从除外），mysql5.7提供了gtid_excuted系统表来记录复制的信息，以此减少从库的压力。主服务器配置gtid操作12345678[root@localhost tmp]# vim /etc/my.cnf # 编辑配置文件[mysqld]log_binserver_id=1gtid_mode=ONenforce_gtid_consistency=1[root@localhost tmp]# systemctl restart mysqld # 重启mysql生效从服务器配置gtid操作 同主服务器12345678[root@localhost tmp]# vim /etc/my.cnf # 编辑配置文件[mysqld]# log_bin # 从服务器log_bin可开可不开，不影响复制功能server_id=2 # 注意id的唯一性gtid_mode=ONenforce_gtid_consistency=1[root@localhost tmp]# systemctl restart mysqld # 重启mysql生效设置从服务器主从关系1change master to master_host='192.168.137.100',master_user='rep',master_password='test@A123456',master_auto_position=1;二进制文件位置由master_auto_position=1参数自动确定123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566mysql&gt; change master to master_host=&apos;192.168.137.100&apos;,master_user=&apos;rep&apos;,master_password=&apos;test@A123456&apos;,master_auto_position=1;Query OK, 0 rows affected, 2 warnings (0.00 sec)mysql&gt; start slave;Query OK, 0 rows affected (0.00 sec)mysql&gt; show slave status\G;*************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 192.168.137.100 Master_User: rep Master_Port: 3306 Connect_Retry: 60 Master_Log_File: localhost-bin.000003 Read_Master_Log_Pos: 154 Relay_Log_File: localhost-relay-bin.000002 Relay_Log_Pos: 375 Relay_Master_Log_File: localhost-bin.000003 Slave_IO_Running: Yes Slave_SQL_Running: Yes Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: Last_Errno: 0 Last_Error: Skip_Counter: 0 Exec_Master_Log_Pos: 154 Relay_Log_Space: 586 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: 0Master_SSL_Verify_Server_Cert: No Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: Replicate_Ignore_Server_Ids: Master_Server_Id: 1 Master_UUID: d4194c0e-35e2-11eb-a021-000c291c0d9f Master_Info_File: /var/lib/mysql/master.info SQL_Delay: 0 SQL_Remaining_Delay: NULL Slave_SQL_Running_State: Slave has read all relay log; waiting for more updates Master_Retry_Count: 86400 Master_Bind: Last_IO_Error_Timestamp: Last_SQL_Error_Timestamp: Master_SSL_Crl: Master_SSL_Crlpath: Retrieved_Gtid_Set: Executed_Gtid_Set: Auto_Position: 1 Replicate_Rewrite_DB: Channel_Name: Master_TLS_Version: 1 row in set (0.00 sec)以上是一主一从的配置方法及操作流程=================================================================================下面介绍一下多主一从，从服务器的配置参数从服务器 配置my.cnf文件123456789[root@localhost tmp]# vim /etc/my.cnf # 编辑配置文件[mysql]server_id=6gtid_mode=ONenforce_gtid_consiscenty=1master-info-repository=TABLErelay-log-info-repository=TABLE[root@localhost tmp]# systemctl restart mysqld # 重启mysql生效配置从服务器主从关系123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566[root@localhost tmp]# mysql -uroot -p&apos;123456&apos;mysql&gt; change master to master_host=&apos;192.168.137.88&apos;,master_user=&apos;rep&apos;,master_password=&apos;test@A123456&apos;,master_auto_position=1 for channel &apos;192.168.137.88&apos;# 注意语法for channel &apos;192.168.137.88&apos; 是设置192.168.137.88为主服务器Query OK, 0 rows affected, 2 warnings (0.00 sec)mysql&gt; change master to master_host=&apos;192.168.137.89&apos;,master_user=&apos;rep&apos;,master_password=&apos;test@A123456&apos;,master_auto_position=1 for channel &apos;192.168.137.89&apos;# 注意语法for channel &apos;192.168.137.89&apos; 是设置192.168.137.89为主服务器Query OK, 0 rows affected, 2 warnings (0.00 sec)mysql&gt; change master to master_host=&apos;192.168.137.90&apos;,master_user=&apos;rep&apos;,master_password=&apos;test@A123456&apos;,master_auto_position=1 for channel &apos;192.168.137.90&apos;# 注意语法for channel &apos;192.168.137.90&apos; 是设置192.168.137.90为主服务器Query OK, 0 rows affected, 2 warnings (0.00 sec)mysql&gt; start slave; # 启动slave服务Query OK, 0 rows affected (0.00 sec)mysql&gt; show slave status\G; # 查看状态 【下面这个两个参数有三对，必须都是yes才配置成功】Slave_IO_Running: Yes Slave_SQL_Running: Yes下面是成功配置后的部分复制内容*************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 192.168.137.88 Master_User: rep Master_Port: 3306 Connect_Retry: 60 Master_Log_File: localhost-bin.000003 Read_Master_Log_Pos: 154 Relay_Log_File: localhost-relay-bin.000002 Relay_Log_Pos: 375 Relay_Master_Log_File: localhost-bin.000003 Slave_IO_Running: Yes Slave_SQL_Running: Yes*************************** 2. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 192.168.137.89 Master_User: rep Master_Port: 3306 Connect_Retry: 60 Master_Log_File: localhost-bin.000003 Read_Master_Log_Pos: 154 Relay_Log_File: localhost-relay-bin.000002 Relay_Log_Pos: 375 Relay_Master_Log_File: localhost-bin.000003 Slave_IO_Running: Yes Slave_SQL_Running: Yes*************************** 3. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 192.168.137.90 Master_User: rep Master_Port: 3306 Connect_Retry: 60 Master_Log_File: localhost-bin.000003 Read_Master_Log_Pos: 154 Relay_Log_File: localhost-relay-bin.000002 Relay_Log_Pos: 375 Relay_Master_Log_File: localhost-bin.000003 Slave_IO_Running: Yes Slave_SQL_Running: Yes关于多主多从的配置，实际上就是主服务器之间互相配置主从，然后从服务器配置主服务器即可，如果看到这里你能够配置一主多从，那么多主多从就不是问题了第三部分-拓展【mysql一键安装脚本】可复制直接用以下是一个mysql的一键安装脚本，可以直接在CentOS-7上直接使用123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141#!/bin/bash#这是一个在centOS 7 2003版本上配置mysql5.7.25版本全自动脚本 #在centOS 8 2004版本也可以完成###安装mysql后设置的新密码newpsd="test@A123456"###安装mysql后设置的新密码# 进入脚本运行的目录并打印路径basedir=`dirname $0`cd $basedirecho `pwd`##------下载mysql四个基本rpm包到脚本运行目录下-----------####------下载mysql四个基本rpm包-----------###mysql-community-client-5.7.25-1.el7.x86_64.rpm#mysql-community-common-5.7.25-1.el7.x86_64.rpm#mysql-community-libs-5.7.25-1.el7.x86_64.rpm#mysql-community-server-5.7.25-1.el7.x86_64.rpm#将下载的包名放到数组，通过循环遍历检测是否下载成功packge=(mysql-community-client-5.7.25-1.el7.x86_64.rpm mysql-community-common-5.7.25-1.el7.x86_64.rpm mysql-community-libs-5.7.25-1.el7.x86_64.rpm mysql-community-server-5.7.25-1.el7.x86_64.rpm mysql-community-libs-compat-5.7.25-1.el7.x86_64.rpm)commurl="http://mirrors.ustc.edu.cn/mysql-repo/yum/mysql-5.7-community/el/7/x86_64/"##------构建函数判断下载包是否成功------##jud () &#123;for i in $*do if [ ! -f $&#123;i&#125; ] then echo "$&#123;i&#125;未下载，即将下载，请稍等..." wget $&#123;commurl&#125;$&#123;i&#125; #else fidone&#125;##------构建函数判断下载包是否成功------####------构建函数判断yum安装状态是否成功------##yumisntalljud ()&#123;if [ $? -eq 0 ] then echo "已安装成功" #elsefi&#125;##------构建函数判断yum安装是否成功------###1 执行函数下载mysql包jud $&#123;packge[*]&#125;while : do l=0 k=0 packge2=() for pac in $&#123;packge[*]&#125; do if [ -f $&#123;pac&#125; ] then packge2[++n]=$&#123;pac&#125; else echo "正在下载$&#123;pac&#125;请稍后.....`date`" wget $&#123;commurl&#125;$&#123;pac&#125; fi done##--------------判断包是否都下载好了-----------------------## for x in in $&#123;packge2[*]&#125; do let l++ done if [ $&#123;l&#125; -ge 4 ] then for pac2 in $&#123;packge2[*]&#125; do let k++ echo "$&#123;k&#125;: $&#123;pac2&#125;已下载" done echo "共安下载了$&#123;k&#125;个包`date`" break #else fi done#2 安装依赖包yum install -y net-tools.x86_64 libaio.x86_64 perl.x86_64yumisntalljud#3 删除冲突包yum remove -y postfix.x86_64 mariadb-libs.x86_64 yumisntalljud#4 安装mysql包yum install -y mysql-community*yumisntalljud#5 启动mysqlsystemctl start mysqldif [ $? -eq 0 ] then echo "mysql启动成功" else echo "mysql启动失败"fi#6 mysql加入开机启动systemctl enable mysqldif [ $? -eq 0 ] then echo "已设置mysql开机启动" else echo "开始启动设置失败"fi#7 搜索mysql初始密码并赋值给psd变量#特征文字2020-09-26T05:52:07.040901Z 1 [Note] A temporary password is generated for root@localhost: qCpjvSsj!3Nopsd=`grep 'password' /var/log/mysqld.log | sed -r 's/.*root@localhost/initialpassword/' | awk '&#123;print $2&#125;'`#8 设置mysql初始密码mysqladmin -uroot -p$&#123;psd&#125; password $&#123;newpsd&#125;#9 自动查询数据库信息mysql -uroot -p$&#123;newpsd&#125; mysql -e "show databases;"if [ $? -eq 0 ] then echo "mysql安装成功" else echo "设置出错或者安装失败，请检查......"fi转载自：MySQL主从同步详解与配置 - 知乎 (zhihu.com)]]></content>
      <categories>
        <category>技术</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ELK实践]]></title>
    <url>%2F2023%2F02294d67f2.html</url>
    <content type="text"><![CDATA[本篇文章主要介绍elk的一些框架组成，原理和实践，采用的ELK本版为7.7.0版本转载自：https://www.cnblogs.com/zsql/p/13164414.html#_label0ELK介绍ELK简介 ELK是Elasticsearch、Logstash、Kibana三大开源框架首字母大写简称(但是后期出现的filebeat(beats中的一种)可以用来替代logstash的数据收集功能，比较轻量级)。市面上也被成为Elastic Stack。 Filebeat是用于转发和集中日志数据的轻量级传送工具。Filebeat监视您指定的日志文件或位置，收集日志事件，并将它们转发到Elasticsearch或 Logstash进行索引。Filebeat的工作方式如下：启动Filebeat时，它将启动一个或多个输入，这些输入将在为日志数据指定的位置中查找。对于Filebeat所找到的每个日志，Filebeat都会启动收集器。每个收集器都读取单个日志以获取新内容，并将新日志数据发送到libbeat，libbeat将聚集事件，并将聚集的数据发送到为Filebeat配置的输出。 Logstash是免费且开放的服务器端数据处理管道，能够从多个来源采集数据，转换数据，然后将数据发送到您最喜欢的“存储库”中。Logstash能够动态地采集、转换和传输数据，不受格式或复杂度的影响。利用Grok从非结构化数据中派生出结构，从IP地址解码出地理坐标，匿名化或排除敏感字段，并简化整体处理过程。 Elasticsearch是Elastic Stack核心的分布式搜索和分析引擎,是一个基于Lucene、分布式、通过Restful方式进行交互的近实时搜索平台框架。Elasticsearch为所有类型的数据提供近乎实时的搜索和分析。无论您是结构化文本还是非结构化文本，数字数据或地理空间数据，Elasticsearch都能以支持快速搜索的方式有效地对其进行存储和索引。 Kibana是一个针对Elasticsearch的开源分析及可视化平台，用来搜索、查看交互存储在Elasticsearch索引中的数据。使用Kibana，可以通过各种图表进行高级数据分析及展示。并且可以为 Logstash 和 ElasticSearch 提供的日志分析友好的 Web 界面，可以汇总、分析和搜索重要数据日志。还可以让海量数据更容易理解。它操作简单，基于浏览器的用户界面可以快速创建仪表板（dashboard）实时显示Elasticsearch查询动态为什么要使用ELK 日志主要包括系统日志、应用程序日志和安全日志。系统运维和开发人员可以通过日志了解服务器软硬件信息、检查配置过程中的错误及错误发生的原因。经常分析日志可以了解服务器的负荷，性能安全性，从而及时采取措施纠正错误。 往往单台机器的日志我们使用grep、awk等工具就能基本实现简单分析，但是当日志被分散的储存不同的设备上。如果你管理数十上百台服务器，你还在使用依次登录每台机器的传统方法查阅日志。这样是不是感觉很繁琐和效率低下。当务之急我们使用集中化的日志管理，例如：开源的syslog，将所有服务器上的日志收集汇总。集中化管理日志后，日志的统计和检索又成为一件比较麻烦的事情，一般我们使用grep、awk和wc等Linux命令能实现检索和统计，但是对于要求更高的查询、排序和统计等要求和庞大的机器数量，依然使用这样的方法难免有点力不从心。 一般大型系统是一个分布式部署的架构，不同的服务模块部署在不同的服务器上，问题出现时，大部分情况需要根据问题暴露的关键信息，定位到具体的服务器和服务模块，构建一套集中式日志系统，可以提高定位问题的效率。完整日志系统基本特征收集：能够采集多种来源的日志数据传输：能够稳定的把日志数据解析过滤并传输到存储系统存储：存储日志数据分析：支持 UI 分析警告：能够提供错误报告，监控机制ELK架构分析beats+elasticsearch+kibana模式 如上图所示，该ELK框架由beats（日志分析我们通常使用filebeat）+elasticsearch+kibana构成，这个框架比较简单，入门级的框架。其中filebeat也能通过module对日志进行简单的解析和索引。并查看预建的Kibana仪表板。该框架适合简单的日志数据，一般可以用来玩玩，生产环境建议接入logstashbeats+logstash+elasticsearch+kibana模式该框架是在上面的框架的基础上引入了logstash，引入logstash带来的好处如下：Logstash具有基于磁盘的自适应缓冲系统，该系统将吸收传入的吞吐量，从而减轻背压从其他数据源（例如数据库，S3或消息传递队列）中提取将数据发送到多个目的地，例如S3，HDFS或写入文件使用条件数据流逻辑组成更复杂的处理管道filebeat结合logstash带来的优势：1、水平可扩展性，高可用性和可变负载处理：filebeat和logstash可以实现节点之间的负载均衡，多个logstash可以实现logstash的高可用2、消息持久性与至少一次交付保证：使用Filebeat或Winlogbeat进行日志收集时，可以保证至少一次交付。从Filebeat或Winlogbeat到Logstash以及从Logstash到Elasticsearch的两种通信协议都是同步的，并且支持确认。Logstash持久队列提供跨节点故障的保护。对于Logstash中的磁盘级弹性，确保磁盘冗余非常重要。3、具有身份验证和有线加密的端到端安全传输：从Beats到Logstash以及从 Logstash到Elasticsearch的传输都可以使用加密方式传递 。与Elasticsearch进行通讯时，有很多安全选项，包括基本身份验证，TLS，PKI，LDAP，AD和其他自定义领域当然在该框架的基础上还可以引入其他的输入数据的方式：比如：TCP，UDP和HTTP协议是将数据输入Logstash的常用方法（如下图所示）：beats+缓存/消息队列+logstash+elasticsearch+kibana模式在如上的基础上我们可以在beats和logstash中间添加一些组件redis、kafka、RabbitMQ等，添加中间件将会有如下好处：第一，降低对日志所在机器的影响，这些机器上一般都部署着反向代理或应用服务，本身负载就很重了，所以尽可能的在这些机器上少做事；第二，如果有很多台机器需要做日志收集，那么让每台机器都向Elasticsearch持续写入数据，必然会对Elasticsearch造成压力，因此需要对数据进行缓冲，同时，这样的缓冲也可以一定程度的保护数据不丢失；第三，将日志数据的格式化与处理放到Indexer中统一做，可以在一处修改代码、部署，避免需要到多台机器上去修改配置ELK部署elk各个组件的网址可以在官网下载：https://www.elastic.co/cn/或者在中文社区下载：https://elasticsearch.cn/download/注：本次安装都是采用压缩包的方式安装filebeat的安装介绍原理 Filebeat的工作方式如下：启动Filebeat时，它将启动一个或多个输入，这些输入将在为日志数据指定的位置中查找。对于Filebeat所找到的每个日志，Filebeat都会启动收集器。每个收集器都读取单个日志以获取新内容，并将新日志数据发送到libbeat，libbeat将聚集事件，并将聚集的数据发送到为Filebeat配置的输出 Filebeat结构：由两个组件构成，分别是inputs（输入）和harvesters（收集器），这些组件一起工作来跟踪文件并将事件数据发送到您指定的输出，harvester负责读取单个文件的内容。harvester逐行读取每个文件，并将内容发送到输出。为每个文件启动一个harvester。harvester负责打开和关闭文件，这意味着文件描述符在harvester运行时保持打开状态。如果在收集文件时删除或重命名文件，Filebeat将继续读取该文件。这样做的副作用是，磁盘上的空间一直保留到harvester关闭。默认情况下，Filebeat保持文件打开，直到达到close_inactive简单安装本文采用压缩包的方式安装，linux版本，filebeat-7.7.0-linux-x86_64.tar.gz12curl-L-Ohttps://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.7.0-linux-x86_64.tar.gztar -xzvf filebeat-7.7.0-linux-x86_64.tar.gz配置示例文件：filebeat.reference.yml（包含所有未过时的配置项）配置文件：filebeat.yml启动命令：./filebeat -e具体的原理、使用、实例见文：一篇文章搞懂filebeat（ELK）logstash的安装介绍基本原理logstash分为三个步骤：inputs（必须的）→ filters（可选的）→ outputs（必须的），inputs生成时间，filters对其事件进行过滤和处理，outputs输出到输出端或者决定其存储在哪些组件里。inputs和outputs支持编码和解码Logstash管道中的每个input阶段都在自己的线程中运行。将写事件输入到内存（默认）或磁盘上的中心队列。每个管道工作线程从该队列中取出一批事件，通过配置的filter处理该批事件，然后通过output输出到指定的组件存储。管道处理数据量的大小和管道工作线程的数量是可配置的简单安装下载地址1：https://www.elastic.co/cn/downloads/logstash下载地址2：https://elasticsearch.cn/download/这里需要安装jdk，我使用的是elasticsearch7.7.0自带的jdk：解压即安装：1tar -zxvf logstash-7.7.0.tar.gz来个logstash版本的HelloWorld：1./bin/logstash -e 'input &#123; stdin &#123; &#125; &#125; output &#123; stdout &#123;&#125; &#125;'具体的原理、使用、实例见文：从0到1学会logstash的玩法（ELK）elasticsearch的安装介绍基本介绍 Elasticsearch（ES）是一个基于Lucene构建的开源、分布式、RESTful接口的全文搜索引擎。Elasticsearch还是一个分布式文档数据库，其中每个字段均可被索引，而且每个字段的数据均可被搜索，ES能够横向扩展至数以百计的服务器存储以及处理PB级的数据。可以在极短的时间内存储、搜索和分析大量的数据。 基本概念有：Cluster 集群、Node节点、Index索引、Document文档、Shards &amp; Replicas分片与副本等elasticsearch的优势：分布式：横向扩展非常灵活全文检索：基于lucene的强大的全文检索能力；近实时搜索和分析：数据进入ES，可达到近实时搜索，还可进行聚合分析高可用：容错机制，自动发现新的或失败的节点，重组和重新平衡数据模式自由：ES的动态mapping机制可以自动检测数据的结构和类型，创建索引并使数据可搜索。RESTful API：JSON + HTTPlinux系统参数设置1234567891011121314151617181920212223242526272829303132333435361、设置系统配置ulimit #暂时修改，切换到该用户es，ulimit -n 65535 /etc/security/limits.conf #永久修改 es - nofile 65535ulimit -a #查看当前用户的资源限制2、禁用sawpping方式一：swapoff -a #临时禁用所有的swap文件vim /etc/fstab #注释掉所有的swap相关的行，永久禁用方式二：cat /proc/sys/vm/swappiness #查看该值sysctl vm.swappiness=1 #临时修改该值为1vim /etc/sysctl.conf #修改文件 永久生效vm.swappiness = 1 #如果有该值，则修改该值，若没有，则追加该选项，sysctl -p生效命令方式三：配置elasticsearch.yml文件，添加如下配置：bootstrap.memory_lock: trueGET _nodes?filter_path=**.mlockall #检查如上配置是否成功注意：如果试图分配比可用内存更多的内存，mlockall可能会导致JVM或shell会话退出!3、配置文件描述符ulimit -n 65535 #临时修改vim /etc/security/limits.conf #永久修改es soft nproc 65535es hard nproc 655354、配置虚拟内存sysctl -w vm.max_map_count=262144 #临时修改该值vim /etc/sysctl.conf #永久修改vm.max_map_count=2621445、配置线程数ulimit -u 4096 #临时修改vim /etc/security/limits.conf #永久修改elasticsearch安装elasticsearch是需要其他用户启动的，所以需要先创建一个新的用户elk：123groupadd elasticuseradd elk -d /data/hd05/elk -g elasticecho '2edseoir@' | passwd elk --stdin下载：https://elasticsearch.cn/download/也可以去官网下载：wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.7.0-linux-x86_64.tar.gz12解压：tar -zxvf elasticsearch-7.7.0-linux-x86_64.tar.gz建立软链接：ln –s elasticsearch-7.7.0 es目录介绍：123456789$ES_HOME：/data/hd05/elk/elasticsearch-7.7.0bin: $ES_HOME/bin #es启动命令和插件安装命令conf：$ES_HOME/conf #elasticsearch.yml配置文件目录data：$ES_HOME/data #对应的参数path.data，用于存放索引分片数据文件logs：$ES_HOME/logs #对应的参数path.logs，用于存放日志jdk：$ES_HOME/jdk #自带支持该es版本的jdkplugins： $ES_HOME/jplugins #插件存放目录lib： $ES_HOME/lib #存放依赖包，比如java类库modules： $ES_HOME/modules #包含所有的es模块配置自带的java环境：Vim ~/.bashrc############往后面添加如下内容######################export JAVA_HOME=/data/hd05/elk/es/jdkexport PATH=$JAVA_HOME/bin:PATHexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar\:/lib/tools.jarjvm.options文件说明：1234567891011配置java参数一种是通过修改/data/hd05/elk/elasticsearch-7.7.0/config/jvm.options文件修改jvm参数，一个使用过一个变量ES_JAVA_OPTS来声明jvm参数/data/hd05/elk/elasticsearch-7.7.0/config/jvm.options介绍：8:-Xmx2g #表示只适合java88-:-Xmx2g #表示适合高于java8的版本8-9:-Xmx2g #表示适合java8，和java9其他配置，都是jvm的相关参数，如果要想明白，得去看java虚拟机通过变量ES_JAVA_OPTS来声明jvm参数：例如：export ES_JAVA_OPTS="$ES_JAVA_OPTS -Djava.io.tmpdir=/path/to/temp/dir"./bin/elasticsearch配置config/jvm.options12345678910111213141516171819202122[elk@lgh config]$ cat jvm.options | egrep -v '^$|#' -Xms2g-Xmx2g8-13:-XX:+UseConcMarkSweepGC8-13:-XX:CMSInitiatingOccupancyFraction=758-13:-XX:+UseCMSInitiatingOccupancyOnly14-:-XX:+UseG1GC14-:-XX:G1ReservePercent=2514-:-XX:InitiatingHeapOccupancyPercent=30-Djava.io.tmpdir=$&#123;ES_TMPDIR&#125;-XX:+HeapDumpOnOutOfMemoryError-XX:HeapDumpPath=data-XX:ErrorFile=logs/hs_err_pid%p.log8:-XX:+PrintGCDetails8:-XX:+PrintGCDateStamps8:-XX:+PrintTenuringDistribution8:-XX:+PrintGCApplicationStoppedTime8:-Xloggc:logs/gc.log8:-XX:+UseGCLogFileRotation8:-XX:NumberOfGCLogFiles=328:-XX:GCLogFileSize=64m9-:-Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m配置加密通信证书生成证书：方法一：1./bin/elasticsearch-certutil ca -out config/elastic-certificates.p12 -pass "password"查看config目录，有elastic-certificates.p12文件生成方法二：12345./bin/elasticsearch-certutil ca #创建集群认证机构，需要交互输入密码./bin/elasticsearch-certutil cert --ca elastic-stack-ca.p12 #为节点颁发证书，与上面密码一样执行./bin/elasticsearch-keystore add xpack.security.transport.ssl.keystore.secure_password 并输入第一步输入的密码 执行./bin/elasticsearch-keystore add xpack.security.transport.ssl.truststore.secure_password 并输入第一步输入的密码 将生成的elastic-certificates.p12、elastic-stack-ca.p12文件移动到config目录下配置config/elasticsearch.yml12345678910111213141516171819202122232425[elk@lgh config]$ cat elasticsearch.yml | egrep -v '^$|#'cluster.name: my_clusternode.name: lgh01node.data: truenode.master: truepath.data: /data/hd05/elk/elasticsearch-7.7.0/datapath.logs: /data/hd05/elk/elasticsearch-7.7.0/logsnetwork.host: 192.168.110.130http.port: 9200transport.tcp.port: 9300discovery.seed_hosts: ["192.168.110.130","192.168.110.131","192.168.110.132","192.168.110.133"]cluster.initial_master_nodes: ["lgh01","lgh02","lgh03"]cluster.routing.allocation.cluster_concurrent_rebalance: 32cluster.routing.allocation.node_concurrent_recoveries: 32cluster.routing.allocation.node_initial_primaries_recoveries: 32http.cors.enabled: truehttp.cors.allow-origin: '*'#下面的是配置x-pack和tsl/ssl加密通信的xpack.security.enabled: truexpack.license.self_generated.type: basicxpack.security.transport.ssl.enabled: truexpack.security.transport.ssl.verification_mode: certificatexpack.security.transport.ssl.keystore.path: elastic-certificates.p12xpack.security.transport.ssl.truststore.path: elastic-certificates.p12bootstrap.memory_lock: false #centos6需要配置bootstrap.system_call_filter: false #centos6需要配置然后通过scp到其他的节点，修改上面的node.name和node.master参数，然后要删除data目标，不然会存在报错然后使用./bin/elasticsearch -d 后台启动elasticsearch，去掉-d则是前端启动elasticsearch然后./bin/elasticsearch-setup-passwords interactive 配置默认用户的密码：（有如下的交互），可以使用auto自动生成。123456789101112131415161718192021222324[elk@lgh elasticsearch-7.7.0]$ ./bin/elasticsearch-setup-passwords interactiveEnter password for the elasticsearch keystore : Initiating the setup of passwords for reserved users elastic,apm_system,kibana,logstash_system,beats_system,remote_monitoring_user.You will be prompted to enter passwords as the process progresses.Please confirm that you would like to continue [y/N]yEnter password for [elastic]: Reenter password for [elastic]: Enter password for [apm_system]: Reenter password for [apm_system]: Enter password for [kibana]: Reenter password for [kibana]: Enter password for [logstash_system]: Reenter password for [logstash_system]: Enter password for [beats_system]: Reenter password for [beats_system]: Enter password for [remote_monitoring_user]: Reenter password for [remote_monitoring_user]: 1qaz@WSXChanged password for user [apm_system]Changed password for user [kibana]Changed password for user [logstash_system]Changed password for user [beats_system]Changed password for user [remote_monitoring_user]然后可以登录http://192.168.110.130:9200/ 需要输入密码，输入elastic/passwd即可登录head插件安装https://github.com/mobz/elasticsearch-head #head官网https://nodejs.org/zh-cn/download/ #nodejs下载官方说明，elasticsearch7有三种方式使用head插件，这里我只试过两种：第一种：使用谷歌浏览器head插件，这个直接在谷歌浏览器上面安装插件就可以使用了第二种：使用head服务（把head当做一个服务来使用），安装如下123456#Running with built in servergit clone git://github.com/mobz/elasticsearch-head.gitcd elasticsearch-headnpm installnpm run startopen http://localhost:9100/如果在如上的安装过程中报错，可以尝试下这个命令再继续安装npm install phantomjs-prebuilt@2.1.16 –ignore-scriptskibana的安装介绍下载地址：https://elasticsearch.cn/download/也可以去官网下载解压后修改kibana.yml文件123456789[elk@lgh config]$ cat kibana.yml | egrep -v "^$|#"server.port: 5601server.host: "0.0.0.0"server.name: "my-kibana"elasticsearch.hosts: ["http://192.168.110.130:9200","http://192.168.110.131:9200","http://192.168.110.132:9200"]elasticsearch.preserveHost: truekibana.index: ".kibana"elasticsearch.username: "elastic"elasticsearch.password: "password" #或者使用keystore的保存的密码"$&#123;ES_PWD&#125;"./bin/kibana 启动访问网址：http://192.168.110.130:5601/ 并使用elastic/password 登录实例分析一篇文章搞懂filebeat（ELK） 该文章中有beats+elasticsearch+kibana的实例从0到1学会logstash的玩法（ELK） 该文章中有beats+logstash+elasticsearch+kibana实例现在我们弄一个beats+缓存/消息队列+logstash+elasticsearch+kibana的实例：中间组件我们使用kafka，我们看下filebeat把kafka作为output的官网：https://www.elastic.co/guide/en/beats/filebeat/7.7/kafka-output.html这里要注意kafka的版本，我试过两个都是极端的版本，坑了自己一把。假如你已经有kafka集群了，我这里安装的是一个单机版本（1.1.1）：数据集我们采用apache的日志格式，下载地址：https://download.elastic.co/demos/logstash/gettingstarted/logstash-tutorial.log.gz日志格式如下：1234[elk@lgh ~]$ tail -3 logstash-tutorial.log 86.1.76.62 - - [04/Jan/2015:05:30:37 +0000] "GET /projects/xdotool/ HTTP/1.1" 200 12292 "http://www.haskell.org/haskellwiki/Xmonad/Frequently_asked_questions" "Mozilla/5.0 (X11; Linux x86_64; rv:24.0) Gecko/20140205 Firefox/24.0 Iceweasel/24.3.0"86.1.76.62 - - [04/Jan/2015:05:30:37 +0000] "GET /reset.css HTTP/1.1" 200 1015 "http://www.semicomplete.com/projects/xdotool/" "Mozilla/5.0 (X11; Linux x86_64; rv:24.0) Gecko/20140205 Firefox/24.0 Iceweasel/24.3.0"86.1.76.62 - - [04/Jan/2015:05:30:37 +0000] "GET /style2.css HTTP/1.1" 200 4877 "http://www.semicomplete.com/projects/xdotool/" "Mozilla/5.0 (X11; Linux x86_64; rv:24.0) Gecko/20140205 Firefox/24.0 Iceweasel/24.3.0"首先我们配置filebeat的配置文件filebeat.yml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960#=========================== Filebeat inputs =============================filebeat.inputs:# Each - is an input. Most options can be set at the input level, so# you can use different inputs for various configurations.# Below are the input specific configurations.- type: log # Change to true to enable this input configuration. enabled: true # Paths that should be crawled and fetched. Glob based paths. paths: - /data/elk/logstash-tutorial.log #这里使用的是apache的日志格式 #- c:\programdata\elasticsearch\logs\* # Exclude lines. A list of regular expressions to match. It drops the lines that are # matching any regular expression from the list. #exclude_lines: ['^DBG'] # Include lines. A list of regular expressions to match. It exports the lines that are # matching any regular expression from the list. #include_lines: ['^ERR', '^WARN'] # Exclude files. A list of regular expressions to match. Filebeat drops the files that # are matching any regular expression from the list. By default, no files are dropped. #exclude_files: ['.gz$'] # Optional additional fields. These fields can be freely picked # to add additional information to the crawled log files for filtering #fields: # level: debug # review: 1 ### Multiline options # Multiline can be used for log messages spanning multiple lines. This is common # for Java Stack Traces or C-Line Continuation # The regexp Pattern that has to be matched. The example pattern matches all lines starting with [ #multiline.pattern: ^\[ # Defines if the pattern set under pattern should be negated or not. Default is false. #multiline.negate: false # Match can be set to "after" or "before". It is used to define if lines should be append to a pattern # that was (not) matched before or after or as long as a pattern is not matched based on negate. # Note: After is the equivalent to previous and before is the equivalent to to next in Logstash #multiline.match: after#================================ Outputs =====================================output.kafka: hosts: ["192.168.110.130:9092"] #配置kafka的broker topic: 'filebeat_test' #配置topic 名字 partition.round_robin: reachable_only: false required_acks: 1 compression: gzip max_message_bytes: 1000000然后使用命令后台启动：1cd filebeat-7.7.0-linux-x86_64 &amp;&amp; nohup ./filebeat -e &amp;接下来我们配置logstash的配置文件12345678910111213141516171819202122232425262728293031cd logstash-7.7.0/ &amp;&amp; mkidr conf.dcd conf.dvim apache.conf ################apache.conf文件中填入如下内容##############################input &#123; kafka&#123; bootstrap_servers =&gt; "192.168.110.130:9092" topics =&gt; ["filebeat_test"] group_id =&gt; "test123" auto_offset_reset =&gt; "earliest" &#125; &#125;filter &#123;json &#123; source =&gt; "message" &#125; grok &#123; match =&gt; &#123; "message" =&gt; "%&#123;COMBINEDAPACHELOG&#125;"&#125; remove_field =&gt; "message" &#125;&#125;output &#123; stdout &#123; codec =&gt; rubydebug &#125; elasticsearch &#123; hosts =&gt; ["192.168.110.130:9200","192.168.110.131:9200","10.18.126.224:9200","192.168.110.132:9200"] index =&gt; "test_kakfa" user =&gt; "elastic" password =&gt; "$&#123;ES_PWD&#125;" &#125;&#125;然后后台启动logstash命令1cd logstash-7.7.0/ &amp;&amp; nohup ./bin/logstash -f conf.d/apache.conf &amp;然后我们查看elasticsearch集群查看该索引 接下来我们登录到kibana查看该索引的分析]]></content>
      <categories>
        <category>技术</category>
        <category>ELK</category>
      </categories>
      <tags>
        <tag>ELK</tag>
        <tag>ES</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[开发者平台冷知识]]></title>
    <url>%2F2023%2F017f1808df.html</url>
    <content type="text"><![CDATA[Google Play和Apple Store是海外的主流应用商店，今天详细说说一些比较冷门问题。常见问题：Google Play平台：首发没有灰度，只有全量发布无法回滚版本，只能创建新版本覆盖未灰度完创建新版本会自动暂停灰度，Google市场最新版为上一个版本灰度比例只能上升，不能降低，灰度期间有问题可暂停灰度，但已安装该版本的用户仍然是此版本，全量发布后，该版本不能再暂停灰度用户均随机选择，可选择特定地区进行灰度商店截图，文案等信息可随时修改，需审核，可能会因为包体有问题而拒绝有侵权风险，但已取得授权文件的应用，提审前可通过以下链接先提交授权文件再提审：前往提交提审后无法撤回审核商品详情实验（A/B测试）：Google Play 上的商品详情，可以通过开展实验来找到对应用而言最富有成效的图片和本地化文本。可以针对自己的主要商品详情页面和自定义商品详情页面开展实验。对于已发布的应用，可以测试各种变体，并与目前的版本进行对比，从而根据安装数据找到效果最好的变体。APP最低支持系统是4.4，下个版本的app改为最低支持系统为5.0，那4.4系统的用户无法接收相关更新通知或是查看到最新版本，查找该应用时，将会看到与他们的装置相匹配的旧版本。如需取消发布应用，需先关闭自管式发布应用。Google商店发布手表包时，不会像Automotive包一样，有单独的渠道分发，手表包跟手机包是在同一渠道，所以按照Google官方文档操作加入Wear OS计划后，需要把手机包和手表包一起提审才行（同一版本，上传两个包），不然一直审核驳回。​ iOS平台：灰度：只针对打开了自动更新的用户，手动到商店搜索都是最新版本App 处在分阶段发布过程中，可以选择暂停发布，总计 30 天，暂停次数没有限制。如果App 下架，当前的分阶段发布会停止，且不再对该版本可用。​ 2. 更改截图、文案等信息需创建新版本才可编辑提审后可撤回审核无法回滚版本，只能创建新版本覆盖]]></content>
      <categories>
        <category>应用市场</category>
      </categories>
      <tags>
        <tag>应用上架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Google开发者账号申请流程]]></title>
    <url>%2F2023%2F011df77120.html</url>
    <content type="text"><![CDATA[Google商店作为海外最大的应用商店，今天来说说如何申请Google开发者账号，上架我们的APP到谷歌商店。1.首先我们要先申请一个Google账号：前往申请2.验证手机号码，填写国内的手机号码就行：ps：如果输入号码后提示：“此电话号码无法用于进行验证” ：把Google浏览器设置为英文后重启浏览器重新注册： 3.注册完成后打开Google开发者后台：选择个人或公司：填写相关信息：付款：购买完成后即可进入Google Play后台。4.绑定付款资料（财务提供），Google会往你提供的银行打一笔款，收到款后需填写具体金额进行验证。验证成功后，需要财务同事协助填写美国税务信息。5.账号产生收益后，Google会再次要求验证收款账户，需提供公司注册证书，法人身份证（护照），手持法人身份证（护照）带上Google字眼和时间的标签，银行流水等信息：一定要处理，两周到一个月内（时间不定）如果没有完成认证，会导致用户支付失败。账号产生收益，但未绑定银行账号也会出现红色警报]]></content>
      <categories>
        <category>应用市场</category>
      </categories>
      <tags>
        <tag>应用上架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Google应用商店上架流程]]></title>
    <url>%2F2023%2F011a846b2.html</url>
    <content type="text"><![CDATA[国内的应用商店上架流程都大同小异，今天来说一下Google应用商店的上架流程。应用首发首先，得先申请一个Google开发者账号，申请流程点此查看（需花费25美元）。打开Google Play控制台：点此前往1.点击右上角创建应用：2.填写相关信息：3.点击创建后回跳转到信息中心，需要按照他的提示全部完成任务，完成之后回打钩：全部完成之后，可以先在测试渠道上传包，先测试APP登录，支付等是否有问题，如无问题可直接把包推到正式版，或者在正式版渠道提交包进行审核，两种方式皆需要审核，审核通过后即可在Google应用商店发布。应用更新版本1.应用在Google商店发布之后，有更新版本的需求，登录到Google Play控制台。点击需要更新的APP，点击左边的导航栏：正式版，确认是手机渠道后，点击右上角的创建新的发布版本：2.点击上传需要更新的APP（因为手机和手表在同一渠道，如有手表包，需要等手机包上传完毕后，再点击上传手表包）：3.填写更新版本日志（替换中文字体就行，其他字符不可删除，如有多语言，需全部语言替换）：4.点击检查发布版本，警告可忽略，填写灰度比例，点击发布版本即可提交到Google审核。5.审核通过后，如果开启了自管式发布，不会自动发布到Google商店，需要到发布概览处点击发布（如需自动发布可在发布概览处关闭自管式发布）：6.观察数据无异常后，可加大灰度比例，如数据有异常，也可暂停灰度：]]></content>
      <categories>
        <category>应用市场</category>
      </categories>
      <tags>
        <tag>应用上架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[远程挂载Linux目录]]></title>
    <url>%2F2022%2F11a4e0a084.html</url>
    <content type="text"><![CDATA[Linux现有比较成熟的解决方案有两种，一种是NFS远程挂载，另一种是Samba共享目录。下面就这两方式的配置进行说明。环境：机器A：192.168.1.1 共享目录 /data/share 机器B：192.168.1.2 关联目录 /data/store大致逻辑是这样：将A机器的目录/data/share设置为共享目录，机器B通过mount的方式和A机器的共享文件夹进行连接。一、NFS远程挂载1、概念NFS：即网络文件系统（Network File System）分布式文件系统协议2、操作步骤[机器A]安装NFS123#由于NFS是依赖于RPC协议来进行的协议传输，所以，此时需同时安装，NFS 和 RPC 两个应用程序#安装NFS和RPC（安装nfs-utils，rpcbind） yum -y install nfs-utils rpcbind设置共享目录NFS的配置文件在/etc/exports，内容默认为空。配置格式为目录位置 客户机地址(权限选项)123456789101112vim /etc/exports/data/share 192.168.1.2(rw,sync,no_root_squash) #客户机地址 可以是 ： 主机名、IP地址、网段地址、或者&quot;*、?&quot;通配符；#权限选项：rw表示允许读写(ro为只读)# sync表示同步写# no_root_squash表示当前客户机以root身份访问时，赋予本地root权限(默认是root_squash,将作为nfsnobody用户降权对待) （NFS 服务器共享目录用户的属性，如果用户是 root，那么对于这个共享目录来说就具有 root 的权限。） #给多个地址授权/data/share 192.168.1.2(rw,sync,no_root_squash) 192.168.1.3(rw,sync,no_root_squash)#给某个网段内所有IP授权/data/share 192.168.1.*(rw,sync,no_root_squash)启动NFS服务配置完上述的目录文件配置后，则启动NFS服务；先启动 RPC服务，再启动 NFS 服务1234567891011#启动rpc服务systemctl start rpcbind#启动nfs服务systemctl start nfs#查看rpc服务状态systemctl status rpcbind#查看nfs服务状态systemctl status nfs#查看对应进程信息ps -ef | grep rpcbind ps -ef | grep nfs查看当前机器已经发布的NFS共享目录1234showmount -e 192.168.1.1显示Export list for 192.168.1.1:/data/share 192.168.1.2此时共享机器A的配置已经完成，可直接在机器B进行目录的挂载操作[机器B]安装RPC服务目录的挂载于共享是基于RPC协议进行的，所以B服务器作为挂载方，也应同时具备RPC的应用功能，所以也应同时安装对应的 rpcbind 服务插件。（安装rpcbind时，最好也可以直接把 nfs-utils 同步安装下，后续再次作为共享方时，则也会方便很多）1yum -y install rpcbind nfs-utils挂载123使用mount命令，此处表示将IP为：192.168.1.1所共享的/data/share目录，挂载到当前服务的 /data/store 目录下 mount -t nfs 192.168.1.1:/data/share /data/store开机自动挂载123vim /etc/fstab192.168.1.1:/data/share /data/store nfs defaults,_netdev 0 0开机自动启动12systemctl enable rpcbind.servicesystemctl enable nfs-server.service查看当前机器挂载点1df -h二、Samba共享目录1、概念和NFS不同的是，NFS解决的是UNIX/Linux之间的资源共享，而Samba除了Linux之间，也可用于Windows和Linux之间的资源共享，所以配置相对于NFS来说麻烦一些。2、操作步骤【服务端】Linux作为服务端安装Samba安装samba程序包123456#安装samba的主程序包、共享程序包、Linux作为客户端访问windows的客户端包yum -y intall samba samba-common samba-client systemctl status firewalld #查看防火墙状态setenforce 0 # 关闭SELinux 1启用 0 告警不启用getenforce # 查看Selinux状态 disabled 关闭 enforcing 打开配置共享目录123456vim /etc/samba/smb.conf[sambdir] public = yes path = /data/samba writable = yes启动Samba服务1234systemctl restart smbsystemctl enable smb systemctl restart nmbsystemctl enable nmb【客户端】安装Samba客户端包1yum -y intall samba samba-common samba-client目录挂载1mount -t cifs -o &quot;rw,dir_mode=0644,file_mode=0644,username=username,password=yourpassword&quot; //192.168.1.100/yourshare_folder_name /usr/local/your_server_folder]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>挂载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Windows图文界面使用SVN]]></title>
    <url>%2F2022%2F1154f25ee7.html</url>
    <content type="text"><![CDATA[之前已经出过一篇Linux系统使用SVN的教程，可点此查看，今天出一下Windows图形界面的教程。需要使用TortoiseSVN软件。TortoiseSVN软件下载：https://tortoisesvn.net/downloads.html1、 根据自己电脑位数下载、安装TortoiseSVN客户端（按提示一步步安装即可）：2、安装完成后，新建一个文件夹（你自己想要放文件的地方创建即可）3、右键上一步创建的文件夹—选择SVN Checkout，输入url，确定，后面根据提示，输账号密码即可（url格式为：svn://ip:端口/目录，如：svn://192.168.0.0:8080/code，其中ip、端口、目录根据自己公司的进行替换）：输入账号密码，点击OK：仓库如果有文件的话，会把文件拉到本地，因为我这个是新建的仓库，里面没有东西，所以点击OK后，文件夹是空的：4、进入文件夹，创建你需要上传到仓库的文件（这里我随便复制一个文件过来），然后右键文件，选择TortoiseSVN-Add，然后再右键文件，会多出一个SVN Commit的选项，点击SVN Commit，可以在Message处填写提交的版本信息，备注一下，然后点击OK，就把文件上传到仓库了成功提交后，会多出一个绿色的勾：此时，其他有权限的用户就可以把这个文件拉到他们的电脑上了。如需从仓库删除文件，在电脑上删除文件后，需要commit，不然只是在你电脑本地删除该文件，文件还是存在仓库： 你在电脑上修改了文件的话，文件会出现一个红色感叹号：如果在本地更新了文件，需要同步给别人的话，也需要commit到仓库，不然别人拉取的还是旧版本的文件： 如果你需要拉取别人更新后的文件的话，右键空白处，选择SVN Update即可。注：每次修改commit前，最好先Update一下（防止多人提交，先把别人提交的更新下来），不然如果你跟别人修改了同一个文件的话，你commit会提示有冲突。]]></content>
      <categories>
        <category>技术</category>
        <category>SVN</category>
      </categories>
      <tags>
        <tag>TortoiseSVN</tag>
        <tag>SVN客户端</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac部署nexus服务及nexus迁移]]></title>
    <url>%2F2022%2F1134aa5b02.html</url>
    <content type="text"><![CDATA[旧的nexus服务器机器配置不行，顺便和Jenkins一起迁移至新的macmini机器上。下载nexus并解压，官网地址：https://www.sonatype.com/download-oss-sonatype解压后有两个目录：nexus 基础命令：命令行进入本地nexus-3.43.0-01的bin目录，然后使用以下命令：123$ ./nexus start 启动$ ./nexus stop 停止$ ./nexus restart 重启启动 nexus 时报错：123No suitable Java Virtual Machine could be found on your system.The version of the JVM must be 1.8.Please define INSTALL4J_JAVA_HOME to point to a suitable JVM.错误原因：当前 nexus 要求使用 JVM 1.8;但是当前系统 jdk 环境配置不是1.8版本；解决方法：下载 jdk 8并安装；官方下载地址：https://www.oracle.com/java/technologies/javase/javase-jdk8-downloads.html安装后 jdk 目录为：1/Library/Java/JavaVirtualMachines/jdk1.8.0_251.jdk/Contents/Home进入nexus-3.24.0-02 的 bin 目录下，修改其中的 nexus 文件:1$ vim nexus取消INSTALL4J_JAVA_HOME_OVERRIDE的注释，并填入jdk 1.8的环境目录：保存并退出，重新启动nexus即可：1./nexus start至此，nexus服务已部署完成，需要把原来nexus服务器的仓库迁移过来：把sonatype-work目录备份：1mv sonatype-work sonatype-work.bak把原来服务器上的sonatype-work目录拷贝过来就行：1scp -r /data/src/sonatype-work/ stary@172.22.13.45:/Users/stary/nexus/把nexus服务重启即可：]]></content>
      <categories>
        <category>技术</category>
        <category>nexus</category>
      </categories>
      <tags>
        <tag>nexus</tag>
        <tag>nexus迁移</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac部署Jenkins服务及Jenkins迁移]]></title>
    <url>%2F2022%2F1189a4bfe3.html</url>
    <content type="text"><![CDATA[由于老的Jenkins服务器配置跟不上时代的发展了，需要把服务迁移至新Macmini机器上。通过brew安装，首先安装brew：1/bin/zsh -c "$(curl -fsSL https://gitee.com/cunkai/HomebrewCN/raw/master/Homebrew.sh)"安装好brew后，用brew命令安装Jenkins：1brew install jenkins安装完成后使用一下命令启动Jenkins：1/opt/homebrew/opt/jenkins/bin/jenkins --httpListenAddress=0.0.0.0 --httpPort=8080可用以下命令查看端口是否在监听：1netstat -AaLlnW | grep 8080或者：1sudo lsof -nP |grep LISTEN打开浏览器，IP+8080访问：找到页面提示的路径中的文件，输入文件中的密码信息：勾选需要安装的插件，等待配置：来到创建用户的界面，设置管理员用户名和密码：按照提示继续，就能来到这个界面啦：至此，Jenkins服务器部署好了，需要把旧服务器数据迁移过来，把Jenkins整个目录拷贝过来就行：备份.jenkins目录：1mv .jenkins .jenkins.bak拷贝目录：1scp -r ./.jenkins/ stary@172.22.13.45:/Users/stary/重启Jenkins即可：]]></content>
      <categories>
        <category>技术</category>
        <category>Jenkins</category>
      </categories>
      <tags>
        <tag>Jenkins</tag>
        <tag>Jenkins迁移</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jira解决issue界面添加自定义字段]]></title>
    <url>%2F2022%2F113409a5de.html</url>
    <content type="text"><![CDATA[接上篇，研发要求问题单点击已解决后需要强制填写“云帆服务器”字段才可以提交。实现方法：1.点解项目设置-工作流，编辑对应问题类型的工作流：2.点击链接为已解决的工作流动作：3.点击编辑：4.链接到对应的界面（需提前在界面处创建好对应的界面，此处已创建）：5.校验条件处添加校验器：6.点击发布：此时点击解决问题会弹出界面，必须填写云帆服务器名称才可以提交单子：]]></content>
      <categories>
        <category>技术</category>
        <category>jira</category>
      </categories>
      <tags>
        <tag>jira</tag>
        <tag>自定义字段</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jira问题单添加自定义字段]]></title>
    <url>%2F2022%2F1174c5f91.html</url>
    <content type="text"><![CDATA[研发要求要在缺陷类的问题单里加一个”云帆服务器“的字段。具体实现方法如下：1.进入需要设置的jira项目，点击项目设置-字段-编辑2.在自定义字段里添加云帆服务器：3.点击字段配置-增加字段配置-配置，云帆服务器选项处可以选择是否显示以及是否必选（为了不影响其他项目，可以复制一个默认的配置，然后修改配置，再把项目关联到这个方案）。 4.点击字段配置方案，创建一个方案，引用刚刚创建的字段配置：5.字段配置了，需要在创建问题单的时候显示出来，找到项目关联的方案（也可复制一个方案，单独修改，以免修改到其他项目），点击界面，找到需要配置的界面-配置，再加入刚创建的字段 6.在创建问题的时候就会显示对应的字段了：]]></content>
      <categories>
        <category>技术</category>
        <category>jira</category>
      </categories>
      <tags>
        <tag>jira</tag>
        <tag>自定义字段</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jira修改时间显示格式]]></title>
    <url>%2F2022%2F11d46c8f26.html</url>
    <content type="text"><![CDATA[由于jira系统默认时间会采用“14/Jul/22 2:02 PM”的样式，跟我们日常所使用的格式不太一致，看起很不习惯 ，在这里说明一下在JIRA中如何进行日期格式的调整：想调整2018-12-31和2018-12-31 23:22:29，可以按以下步骤进行设置1.系统管理员登录系统后台，点击系统-外观2.在日期/时间格式中将时间格式进行调整更新完之后时间显示如下：还有在创建问题单的时候，需要选择日期，选择完之后还是显示之前默认的“14/Jul/22 2:02 PM”的格式：如果需要2018-12-31和2018-12-31 23:22:29，可以按以下步骤进行设置：管理员登录系统后台&gt;&gt;系统&gt;&gt;般配置&gt;&gt;高级设置修改以下项：更新完后显示如下：另外，开发要求jira平台问题单时间节点显示为具体的时间，而不是几天前：解决方法：1.编辑/usr/local/jira/atlassian-jira/WEB-INF/classes/jpm.xml文件2.找到对应的参数(key)：jira.lf.date.relativize3.将对应的值(default-value)改为：false4.重新启动JIRA可以看到时间格式已修改：]]></content>
      <categories>
        <category>技术</category>
        <category>jira</category>
      </categories>
      <tags>
        <tag>jira</tag>
        <tag>时间格式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jira插件破解]]></title>
    <url>%2F2022%2F11473314d.html</url>
    <content type="text"><![CDATA[我使用的jira版本是7.13.0，与confluence的破解方式不一样。破解jira服务：jira服务器授权更新：将atlassian-extras-3.2.jar替换掉/usr/local/jira/atlassian/jira/atlassian-jira/WEB-INF/lib/目录下对应jar包，重启jira服务即可。jira插件授权更新：使用的破解文件是：atlassian-universal-plugin-manager-plugin-2.22.9.jarJIRA V7.13安装后默认的atlassian-universal-plugin-manager-plugin版本就是2.22.9方法很简单，直接将这个破解文件覆盖原来的文件，路径在：/usr/local/jira/atlassian-jira/WEB-INF/atlassian-bundled-plugins然后重启jira服务。插件破解原理：atlassian-universal-plugin-manager-plugin插件是进行插件管理的，只需要破解了这个插件，剩下的所有插件都自动破解完成了上面两个文件已配置好，直接到插件市场找你想要的插件找到想要的插件后，免费试用，转到官网得到一个试用的license，把这个license直接黏贴到JIRA后台的插件管理中输入插件license的地方，会发现就破解了，插件的deadline就是你jira应用的deadline：2033年6月5日（前提是jira已经破解了）注意，不要去升级插件：atlassian-universal-plugin-manager-plugin，就让版本一直是2.22.9，升级了插件就用不了了如果升级了，可以用如下方法：12#cd /#find . -name *manager-pl* |grep jira返回如下：12345678910find: ‘./proc/67008’: No such file or directoryfind: ‘./proc/67009’: No such file or directoryfind: ‘./proc/67010’: No such file or directoryfind: ‘./proc/67011’: No such file or directory./var/atlassian/application-data/jira/plugins/installed-plugins/plugin.7291572728482458613.atlassian-universal-plugin-manager-plugin-3.0.1.jar./var/atlassian/application-data/jira/plugins/.osgi-plugins/transformed-plugins/atlassian-universal-plugin-manager-plugin-2.22.9_1543375156000.jar./var/atlassian/application-data/jira/plugins/.osgi-plugins/transformed-plugins/plugin.7291572728482458613.atlassian-universal-plugin-manager-plugin-3.0.1_1545962501000.jar./var/atlassian/application-data/jira/plugins/.osgi-plugins/transformed-plugins/atlassian-universal-plugin-manager-plugin-2.22.9_1545974690000.jar./opt/atlassian/jira/atlassian-jira/WEB-INF/atlassian-bundled-plugins/atlassian-universal-plugin-manager-plugin-2.22.9.jar./opt/atlassian/jira/temp/plugin.7291572728482458613.atlassian-universal-plugin-manager-plugin-3.0.1.jar把这些jar文件都删除，再把文件：atlassian-universal-plugin-manager-plugin-2.22.9.jar 放/opt/atlassian/jira/atlassian-jira/WEB-INF/atlassian-bundled-plugins 目录下，重启jira.如果想修改Jira破解方式跟Confluence一样，需要先从数据库中得到 licences ，用破解包生成的 licences 替换。生成 licenses 进入后台管理面板页，在系统信息中获得服务器ID (管理——系统——系统信息——服务器 ID) 之后，参照wiki插件破解方式获取激活码(license)，保存备用。修改 licenses 我们需要进入数据库，使用上文得到的激活码（license）来替换原有的 license。登陆数据库123456789101112# 执行SELECT * FROM productlicense\G获得原来的激活信息（别忘了备份旧的license信息，方便回退）： MySQL [jiradb]&gt; SELECT * FROM productlicense\G*************************** 1. row *************************** ID: 10200LICENSE: AAABZg0xxxx*************************** 2. row *************************** ID: 10201LICENSE: AAABcQ0ODAoxxxx # 利用上面获取licences的方式获取licences后，更新记录update productlicense set license ='xxxxx' WHERE id=10200;update productlicense set license ='xxxxx' WHERE id=10201;修改启动命令停止 jira 后，在 setenv.sh 文件末尾加入：1export JAVA_OPTS="-javaagent:/usr/local/confluence/ $&#123;JAVA_OPTS&#125;"重新启动即可。]]></content>
      <categories>
        <category>技术</category>
        <category>jira</category>
      </categories>
      <tags>
        <tag>插件破解</tag>
        <tag>jira</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Confluence插件破解]]></title>
    <url>%2F2022%2F111980c424.html</url>
    <content type="text"><![CDATA[本章说一下confluence的插件如何破解。环境配置1.下载atlassian-agent.jar放置在你不会删除的任意位置。全局设置：123[root@nbu upgrade-confluence]# tailf -1 /etc/profileexport JAVA_OPTS="-javaagent:/usr/local/confluence/atlassian-agent.jar $&#123;JAVA_OPTS&#125;"[root@nbu upgrade-confluence]# source /etc/profile或者：可以把：export JAVA_OPTS=”-javaagent:/usr/local/confluence/ ${JAVA_OPTS}”这样的命令放到服务安装所在bin目录下的setenv.sh或setenv.bat（供windows使用）中。或者：还可以直接命令行执行：JAVA_OPTS=”-javaagent:/usr/local/confluence/atlassian-agent.jar” /path/to/start-confluence.sh来启动服务。3.重启服务4.查看是否配置成功，使用ps -ef |grep java查看是否带有 -javaagent参数，且查看日志catalina.out，搜索agent working，无报错情况。破解步骤1.在确认环境配置好的前提下，执行以下命令，这里以confluence的mohamicorp-render-markdown 插件为例。1java -jar atlassian-agent.jar -p mohamicorp-render-markdown -m admin@chuangyue.cn -n name -o https://mohami.io/ -s B93T-TNVL-Q1QC-LZ3N关于参数详情可以运行java -jar atlassian-agent.jar查看。-p 指的是应用密钥/插件关键字，这里mohamicorp-render-markdown-m 指的是邮箱，随便填写即可。-n 指的是名字，随意填写-o 指的是提供提供该产品的开发者，具体指的是上图中的开发者。-s 是你的server-id,可以在的授权细节中查看。运行后会得到如下所示：1234567891011121314151617[root@nbu upgrade-confluence]# java -jar atlassian-agent.jar -p mohamicorp-render-markdown -m admin@chuangyue.cn -n name -o https://mohami.io/ -s B93T-TNVL-Q1QC-LZ3N =========================================================== Atlassian Crack Agent ============== https://zhile.io ============== QQ Group: 30347511 =========================================================== Your license code(Don't copy this line!!!): AAABTQ0ODAoPeJx9kUFvgjAYhu/9FSQ7VylkTk2abEMOJohzoIfdavmUZlDI1+LGfv06wcuSLWn7HZo+75und3kHXgatx3yPzZYsWIYzL8pyL/DZgqRdfQTcnvYG0HDKyEYobUELLSH+bBX2K2GBB/58Qf0Ht0iiJGjzx2XdlKJWssGWIugCkNYC34vmQ0/AYbFFZYBb7ICswEhUrVWN5ntdqVpZKLxqoHvH3iutbc1yOv0qVQUT1fwHz6xAh+cnURkgUaOtkDYVNXDtDpIBXgDXK/68CHOap4eE7tguoslbmJItnoVWRlyb3EKHLJc6vdFiZ6bioqiVfpRlJ/S572AiNYkQrm9HF8y5WFDmkyxOuds0YfezOfNDN/wwIPFFVN0QNrQdheZ9C9fG0XaziV+j9VNCXjqUpTDwG/2PCddUXUbFo8yD+9qfuOAb+2SxgDAtAhUAkNaZ1D/IFPRzMP4dpn4dsrEtQ60CFHDHThIq23PE4kdA6aHvKzM9SM7wX02gg然后复制生成的license，完成破解。此方法破解可以在插件升级后依旧是破解状态。异常处理如果你之前不是一个干净的jira或confluence，你加了java环境变量后可能会起不来，通常看日志里会报License问题，原因在于你之前已经录进了一些license，而这license与现在的agent冲突，解决的办法是：根据日志里报出的license，你去你的数据库里搜，找到并删除，之后重启进入系统后，用它的keygen重新算license，然后输入就可以了。]]></content>
      <categories>
        <category>技术</category>
        <category>Confluence</category>
      </categories>
      <tags>
        <tag>wiki</tag>
        <tag>插件破解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Google浏览器翻译失败]]></title>
    <url>%2F2022%2F104415a2fd.html</url>
    <content type="text"><![CDATA[谷歌浏览器默认使用的翻译网站域名为translate.google.com IP为203.208.46.200请求超时是很正常的，毕竟网站服务器是国外谷歌设有国内的翻译网站translate.google.cn IP为 203.208.40.66这个网站是能够正常ping同和请求访问的解决方法：打开C:\Windows\System32\drivers\etc目录下的hosts文件。该文件需要管理员权限，或者是只读。无法更改内容。此时进行文件的属性权限修改让管理员拿到所有权就可以进行修改或者把文件复制到桌面，修改完后再覆盖回去。在hosts文件中增加以下两行内容即可:12203.208.40.66 translate.google.com203.208.40.66 translate.googleapis.com使用win + R键运行cmd 窗口输入指令1ipconfig /flushdns至此，就可以使用翻译功能。]]></content>
      <categories>
        <category>桌面</category>
      </categories>
      <tags>
        <tag>google</tag>
        <tag>翻译</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openVPN客户端安装失败]]></title>
    <url>%2F2022%2F10d0bf260f.html</url>
    <content type="text"><![CDATA[安装openVPN时进度条卡在“tap install (tap0901) (may require confirmation)”，最后弹出错误“an error ocurred installing the TAP device driver”查看日志提示：1There are no TAP-Windows adapters on this system. You should be able to create a TAP-Windows adapter by going to Start -&gt; All Programs -&gt; TAP-Windows -&gt; Utilities -&gt; Add a new TAP-Windows virtual ethernet adapter.此问题是由于TAP网络适配器没有安装成功所导致的，单独安装TAP适配器也是报错。解决办法：1.重置网络（可能会导致断网可提前下载ccleaner）2.运行ccleaner，扫描注册表并选择修复所有问题。3.重新安装openVPN。]]></content>
      <categories>
        <category>技术</category>
        <category>openVPN</category>
      </categories>
      <tags>
        <tag>openVPN</tag>
        <tag>Windows</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Gitlab服务迁移]]></title>
    <url>%2F2022%2F1045efe774.html</url>
    <content type="text"><![CDATA[由于Gitlab自身的兼容性问题，高版本的Gitlab无法恢复低版本备份的数据，所以新旧服务器Gitlab版本必须一致。查看GitLab版本：1cat /opt/gitlab/embedded/service/gitlab-rails/VERSION新机器安装Gitlab服务，在此不再赘述，可参考此文章。备份旧服务器数据：1gitlab-rake gitlab:backup:create默认将会在 /var/opt/gitlab/backups/ 目录下生成备份文件。使用scp命令从旧服务器复制文件到新服务器：1scp /nfsdrive/backup/gitlab/1665250409_2022_10_09_12.10.14_gitlab_backup.tar root@172.31.0.123:/var/opt/gitlab/backups/新服务器上将备份文件权限修改为777，避免出现权限不够的问题：12cd /var/opt/gitlab/backupschmod 777 1665250409_2022_10_09_12.10.14_gitlab_backup.tar新服务器上停止数据连接服务：12gitlab-ctl stop unicorngitlab-ctl stop sidekiq新服务器上恢复备份文件至Gitlab：1gitlab-rake gitlab:backup:restore BACKUP=备份文件编号例如：备份文件名为1665250409_2022_10_09_12.10.14_gitlab_backup.tar，则编号为1665250409_2022_10_09_12.10.14。在提示中敲入“yes”继续。完成后会看到提示：/etc/gitlab/gitlab-secrets.json和/etc/gitlab/gitlab.rb文件包含敏感数据，需要手动恢复这些文件。如果不覆盖这两个文件会造成CI/CD配置页报500错误：需要旧服务器上把这两个文件拷贝至新服务器上覆盖：12scp /etc/gitlab/gitlab-secrets.json root@172.31.0.123:/etc/gitlab/gitlab-secrets.jsonscp /etc/gitlab/gitlab.rb root@172.31.0.123:/etc/gitlab/gitlab.rb重新配置并重启：1gitlab-ctl reconfigure &amp;&amp; gitlab-ctl restart检测恢复数据情况：1gitlab-rake gitlab:check SANITIZE=truePS：如果网站配置了证书，需把证书放到对应路径：至此，Gitlab服务迁移完成。]]></content>
      <categories>
        <category>技术</category>
        <category>Gitlab</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Gitlab</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openVPN证书到期]]></title>
    <url>%2F2022%2F10c7d8d8c.html</url>
    <content type="text"><![CDATA[今天连接openVPN一直报错，连接失败，查看连接错误日志如下：123456789Sat Oct 08 11:04:24 2022 VERIFY ERROR: depth=0, error=certificate has expired: CN=serverSat Oct 08 11:04:24 2022 OpenSSL: error:14090086:SSL routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify failedSat Oct 08 11:04:24 2022 TLS_ERROR: BIO read tls_read_plaintext errorSat Oct 08 11:04:24 2022 TLS Error: TLS object -&gt; incoming plaintext read errorSat Oct 08 11:04:24 2022 TLS Error: TLS handshake failedSat Oct 08 11:04:24 2022 SIGUSR1[soft,tls-error] received, process restartingSat Oct 08 11:04:24 2022 MANAGEMENT: &gt;STATE:1665198264,RECONNECTING,tls-error,,Sat Oct 08 11:04:24 2022 Restart pause, 2 second(s)Sat Oct 08 11:04:26 2022 WARNING: No server certificate verification method has been enabled.从日志消息得知证书过期，到服务器查看证书到期时间：1openssl x509 -noout -text -in /etc/openvpn/server/easy-rsa/pki/issued/server.crt看到证书确实到期了：解决方法1：续期证书：1./easyrsa renew server nopass重新查看证书到期时间：验证证书有效性：123[root@xxx cert_new]# openssl verify -CAfile ca.crt -purpose sslserver server.crt server.crt: OK证书续期成功，重启服务重新连接即可。ps：Windows连接面板没反应可把log文件删除再重新连接。解决方法2：把相关证书文件重新生成一次（可生成长时间不到期的证书）：首先新建一个空文件夹，用于存放新证书相关文件:1mkdir /etc/openvpn/cert_new当时搭建OpenVPN时，使用的是easy-rsa进行证书生成，因此将之前的easy-rsa拷贝至cert_new文件夹：1cp -r /etc/openvpn/easy-rsa /etc/openvpn/cert_new进入新的easy-rsa文件夹，删除旧的pki文件夹：1rm -rf pki使用“easyrsa”命令新建pki目录：1234[root@xxx easy-rsa]# ./easyrsa init-pki init-pki complete; you may now create a CA or requests.Your newly created PKI dir is: /etc/openvpn/cert_new/easy-rsa/pki生成ca证书：123456[root@xxx easy-rsa]# ./easyrsa --batch build-ca nopass Generating RSA private key, 2048 bit long modulus...............................................................................................................+++...................+++e is 65537 (0x10001)生成服务端证书（前面的环境变量代表证书超时天数为3650天）：123456789101112131415161718[root@xxx easy-rsa]# EASYRSA_CERT_EXPIRE=3650 ./easyrsa build-server-full server nopass Using SSL: openssl OpenSSL 1.0.2k-fips 26 Jan 2017Generating a 2048 bit RSA private key................+++......+++writing new private key to '/etc/openvpn/cert_new/easy-rsa/pki/private/server.key.c8ybv0BPWo'-----Using configuration from /etc/openvpn/cert_new/easy-rsa/pki/safessl-easyrsa.cnfCheck that the request matches the signatureSignature okThe Subject's Distinguished Name is as followscommonName :ASN.1 12:'server'Certificate is to be certified until Apr 26 07:27:46 2032 GMT (3650 days) Write out database with 1 new entriesData Base Updated生成客户端证书：123456789101112131415161718[root@xxx easy-rsa]# EASYRSA_CERT_EXPIRE=3650 ./easyrsa build-client-full client nopass Using SSL: openssl OpenSSL 1.0.2k-fips 26 Jan 2017Generating a 2048 bit RSA private key.........+++.............+++writing new private key to '/etc/openvpn/cert_new/easy-rsa/pki/private/client.key.nsEQpRpArw'-----Using configuration from /etc/openvpn/cert_new/easy-rsa/pki/safessl-easyrsa.cnfCheck that the request matches the signatureSignature okThe Subject's Distinguished Name is as followscommonName :ASN.1 12:'client'Certificate is to be certified until Apr 26 07:30:30 2032 GMT (3650 days) Write out database with 1 new entriesData Base Updated生成crl.pem文件：1234567[root@xxx easy-rsa]# EASYRSA_CRL_DAYS=3650 ./easyrsa gen-crl Using SSL: openssl OpenSSL 1.0.2k-fips 26 Jan 2017Using configuration from /etc/openvpn/cert_new/easy-rsa/pki/safessl-easyrsa.cnf An updated CRL has been created.CRL file: /etc/openvpn/cert_new/easy-rsa/pki/crl.pem将这些文件统一复制到/etc/openvpn/cert_new目录：1cp pki/ca.crt pki/private/ca.key pki/issued/server.crt pki/private/server.key pki/crl.pem /etc/openvpn/cert_new为避免权限问题，将crl.pem的所有者改为nobody：1chown nobody:nobody crl.pem进入/etc/openvpn/cert_new目录，使用openssl命令验证证书有效性：123[root@xxx cert_new]# openssl verify -CAfile ca.crt -purpose sslserver server.crt server.crt: OK生成OpenVPN所需的secret文件ta.key：1openvpn --genkey --secret ta.key将所有需要的文件复制到/etc/openvpn：1cp ca.crt ca.key crl.pem easy-rsa server.crt server.key ta.key /etc/openvpn重启OpenVPN服务，即可使OpenVPN加载新的证书文件。]]></content>
      <categories>
        <category>技术</category>
        <category>openVPN</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>openVPN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[phabricator服务迁移]]></title>
    <url>%2F2022%2F09a696f486.html</url>
    <content type="text"><![CDATA[在新服务器上部署好phabricator服务，在此不再赘述，可参考此文章。原服务器上把数据库备份：1./bin/storage dump | gzip &gt; /tmp/backup.sql.gz把备份数据传至新服务器上：1scp /tmp/backup.sql.gz root@172.31.0.146:/root/把备份数据导入新服务器数据库：1gunzip -c /root/backup.sql.gz | mysql -u root -p数据过大可能会报MySQL server has gone away错误，可把max_allowed_packet参数设置大一点，编辑my.cnf文件：1max_allowed_packet = 20M查看托管存储库：1./bin/repository list-paths把/var/repo目录整个复制到新服务器对应的托管存储库路径下：1scp -r /var/repo/ root@172.31.0.146:/var/修改配置文件：不可直接修改，需用以下命令设置：1234./bin/config set mysql.host 'localhost'./bin/config set mysql.port '3306'./bin/config set mysql.user 'root'./bin/config set mysql.pass 'root的密码'设置完后，执行:1./bin/storage upgrade测试邮件是否配置正确：1./bin/mail send-test --to chenmingchang@stary.com --subject hello &lt; README.md在web端访问成功：]]></content>
      <categories>
        <category>技术</category>
        <category>phabricator</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>phabricator</tag>
        <tag>code review</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL异常关机数据恢复]]></title>
    <url>%2F2022%2F0932c87208.html</url>
    <content type="text"><![CDATA[MySQL因为停电异常关机，导致数据库无法启动成功，出现如下报错：123456789101112131415161718192021222021-03-15T01:50:21.759534Z 0 [Note] /usr/sbin/mysqld (mysqld 5.7.20) starting as process 22291 …2021-03-15T01:50:21.767474Z 0 [Note] InnoDB: PUNCH HOLE support available2021-03-15T01:50:21.767508Z 0 [Note] InnoDB: Mutexes and rw_locks use GCC atomic builtins2021-03-15T01:50:21.767521Z 0 [Note] InnoDB: Uses event mutexes2021-03-15T01:50:21.767530Z 0 [Note] InnoDB: GCC builtin __sync_synchronize() is used for memory barrier2021-03-15T01:50:21.767538Z 0 [Note] InnoDB: Compressed tables use zlib 1.2.32021-03-15T01:50:21.767549Z 0 [Note] InnoDB: Using Linux native AIO2021-03-15T01:50:21.768478Z 0 [Note] InnoDB: Number of pools: 12021-03-15T01:50:21.768631Z 0 [Note] InnoDB: Using CPU crc32 instructions2021-03-15T01:50:21.770450Z 0 [Note] InnoDB: Initializing buffer pool, total size = 2G, instances = 8, chunk size = 128M2021-03-15T01:50:21.925994Z 0 [Note] InnoDB: Completed initialization of buffer pool2021-03-15T01:50:21.949851Z 0 [Note] InnoDB: If the mysqld execution user is authorized, page cleaner thread priority can be changed. See the man page of setpriority().2021-03-15T01:50:21.962020Z 0 [Note] InnoDB: Highest supported file format is Barracuda.2021-03-15T01:50:21.977821Z 0 [Note] InnoDB: Log scan progressed past the checkpoint lsn 7018727888602021-03-15T01:50:22.006445Z 0 [Note] InnoDB: Doing recovery: scanned up to log sequence number 7018732975692021-03-15T01:50:22.008894Z 0 [Note] InnoDB: Database was not shutdown normally!2021-03-15T01:50:22.008927Z 0 [Note] InnoDB: Starting crash recovery.2021-03-15T01:50:22.027234Z 0 [ERROR] InnoDB: Trying to access page number 3841987584 in space 0, space name innodb_system, which is outside the tablespace bounds. Byte offset 0, len 16384, i/o type read. If you get this error at mysqld startup, please check that your my.cnf matches the ibdata files that you have in the MySQL server.2021-03-15T01:50:22.027297Z 0 [ERROR] InnoDB: Server exits.2021-03-15T01:56:15.606226Z 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use –explicit_defaults_for_timestamp server option (see documentation for more details).2021-03-15T01:56:15.606344Z 0 [Warning] ‘NO_ZERO_DATE’, ‘NO_ZERO_IN_DATE’ and ‘ERROR_FOR_DIVISION_BY_ZERO’ sql modes should be used with strict mode. They will be merged with strict mode in a future release.2021-03-15T01:56:15.606352Z 0 [Warning] ‘NO_AUTO_CREATE_USER’ sql mode was not set.解决方法：修改/etc/my.cnf文件，添加innodb_force_recovery=6；重启MySQL服务，能够顺利启动MySQL服务；使用mysqldump将数据导出到他处；删除原来的数据文件/var/lib/mysql，并重新初始化数据库；将mysqldump导出的文件再导入回来。注意：在执行上述操作之前，记得先备份一下MySQL的数据文件，以免出现innodb_force_recovery=6都无法启动MySQL的情况。经验：虽然使用innodb_force_recovery可以进行innodb数据恢复，但是还是有可能丢失数据，所以如果是关键数据库，建议还是定期进行备份比较稳妥，备份脚本可查看此文章。]]></content>
      <categories>
        <category>技术</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql授权和撤销授权]]></title>
    <url>%2F2022%2F09ee8bdffb.html</url>
    <content type="text"><![CDATA[对某个用户授予对某个表的某种操作的权限，并可限制具体的网段1GRANT SELECT,INSERT ON book_review.* TO 'test'@'10.100.100.%' IDENTIFIED BY 'test000';查询某个网段的用户信息，因为授权或者禁用是根据某个网段的主机进行操作:1SELECT User,Host from `mysql`.user where Host like '10.100.100.%'再根据上面查到的信息，查看已经授权的项:1SHOW grants for test@10.100.100.%接下来就可以根据上面查到的选项进行撤销授权：1revoke ALL ON `test_db`.* FROM 'test'@'10.100.100.%';]]></content>
      <categories>
        <category>技术</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DNS服务器部署]]></title>
    <url>%2F2022%2F098d4f1456.html</url>
    <content type="text"><![CDATA[dns服务器就是域名服务器，它可以把用户的网站地址解析成相应的IP地址。DNS服务部署：1.安装LnmOS初始化环境：1curl -o - 'https://raw.githubusercontent.com/fxtxkktv/fxtxkktv.github.io/master/files/Install_LnmOS_env.sh'|bashPython安装失败可手动安装，Python包会下载到/tmp目录，需要先安装依赖libdb4-4.8.30-13.el7.x86_64.rpm，再安装Python，再用yum安装python-pip。2.获取主程序：1git clone https://github.com/fxtxkktv/lnmDNS.git进入程序目录：1cd lnmDNS创建程序独立运行Python环境：1virtualenv -p /opt/Py27lnmos/bin/python venv进入virtualenv环境：1source venv/bin/activate安装DNS服务：1yum -y install bind bind-sdb bind-utilsnamed服务自启动：1systemctl enable named安装Python程序扩展包：1pip install -r readme/requirements.txt3.创建数据库并恢复数据模版[创建数据库]:1mysql -u root -p -e "create database lnmdns"[恢复数据模版]:1mysql -u root -p lnmdns &lt; readme/db_schema.sql[配置数据库连接及其他]:1vim config/config.inilnmDNS服务默认不支持特殊符号解析，只支持字母和数字，需修改一下/root/lnmDNS/views/recordconf.tpl文件：12345&lt;input type="text" class="form-control" onkeyup="value=value.replace(/[^\w\.\@]/ig,'')" id="host" name="host" placeholder="" required&gt; 改为 &lt;input type="text" class="form-control" οnkeyup="value=value.replace(//s/g,'')" id="host" name="host" placeholder="" required&gt;4.正式运行程序：[程序调试]：1python main.py[后台运行]:1startweb.sh restart[前端访问]：主机ip:8443默认用户名：admin 密码: admin启停命令：启动服务命令：1/root/lnmDNS/startweb.sh start关闭服务命令：1/root/lnmDNS/startweb.sh stop状态监控：1/root/lnmDNS/startweb.sh status]]></content>
      <categories>
        <category>技术</category>
        <category>DNS</category>
      </categories>
      <tags>
        <tag>DNS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用EwoMail搭建邮件服务器]]></title>
    <url>%2F2022%2F09cfa5bedf.html</url>
    <content type="text"><![CDATA[随着移动互联网的普及，邮件服务使用的场景越来越少。但是注册帐号的时候，还是需要邮箱地址。市面上免费的邮箱很多，不过一般需要绑定手机号，如果想在同一个邮箱服务商那注册多个邮箱帐号，需要准备多个手机号，这非常的烦人。接下来我们自己动手搭建邮件服务器吧。所需配置搭建一个邮箱服务器,并不是无成本的.我们需要以下硬件即可云服务器域名进入教程主要分以下步骤：购买云服务器Linux和域名安装xshell和xftp在Linux环境下安装EwoMail解析域名配置EwoMail（非常重要）测试WebMail邮件发送测试购买云服务器和域名云服务器购买域名购买安装xshell和xftp这两个软件是用来玩Linux的,通俗点的解释一下这两个软件是用来干嘛的：xshell是用来操作Linux服务器的,和cmd控制的输入命令式相似；xftp是用来传输文件的,你也可以理解为手机的usb数据传输线,它可以让本地电脑和云服务器交换数据,因为是可视化界面,所以非常简单；官网安装地址： xshell和xftp下载不出意外在你邮箱里,会收到两封附有下载链接的邮件,之后就请自行下载安装。安装EwoMail在第三步开始前,请检查你的xshell是否能打开,然后你还需要两个信息：云服务器的公网IP和登录密码：如果没问题,我们就开始使用xshell工具配置EwoMail了：1.打开xshell,连接云服务器2.安裝EwoMail这里按照官方文档的步骤来：关闭selinux12vi /etc/sysconfig/selinuxSELINUX=enforcing 改为 SELINUX=disabled小白指令：修改完之后,按键盘ESC,再按键盘 ：,输入wq ,回车；3.设置主机名12#mail.symail.club 换成 mail.你的域名 hostnamectl set-hostname mail.symail.club修改hosts12vi /etc/hosts#把symail.club 全换成你的域名4.执行安装123456yum -y install gitcd /rootgit clone https://github.com/gyxuehu/EwoMail.gitcd /root/EwoMail/install#下面的symail.club请换成你注册的域名sh ./start.sh symail.club等待安装,大概10分钟左右,解析域名在解析域名之前,先确保你之前是安装成功的,怎么确认呢？很简单,你只需要访问以下,两个端口即可。8000端口：即WebMail页面, 请在浏览器中输入 你云服务器的 公网IP：8000 即可, 如http://129.123.123.123:8000/8010端口：即邮件管理后台, 地址同上,将那个8000改成8010即可。如果你能够成功正确访问,那说明你之前三步,已经成功了,但是别高兴的太早,下面两步,是非常需要细心的。腾讯云的25端口解封25端口是发送邮件的关键,我们需要前往腾讯云-&gt;我的域名中点击进入25端口解封,进行解封即可。解析域名配置登录ewomail的后台管理系统，第四步有提到过后台系统的访问。12管理员默认账号：admin默认密码：ewomail123登录成功后,进入邮件系统设置：将框起来的全换成你的域名。换完之后,开始添加邮箱账号,邮箱管理&gt;域名添加：添加完你的域名之后,邮箱管理&gt;邮箱添加：如果你都按照以上要求完成了配置,那么我们的EwoMail，已经完成了百分之90%了。接下来我们测试一下,是否能够连上webmail测试WebMail我们进入 您的域名或IP:8000端口,访问WebMail，进入之后是这个也页面：输入你在第五步,添加的邮箱账号。接下来，可能会有三种错误：认证失败(账号密码错误)域不允许(域名解析错误)无法连接服务器（域名解析中的IP地址可能填错了）如果你没有以上错误,并且成功登录了,那么接下来进行发收邮件测试。邮件收发测试发送一封邮件给你其他邮箱：如果发送成功,那么恭喜你已经成功了95%,如果提示无法连接服务器,那么请回到第二步,修改你的hosts。给自己邮箱发一封邮件：如果收件也成功了,那么恭喜你，你的EwoMail，已经搭建完成了。本文章主要是介绍安装教程,实现效果即可,如果需要更深入了解EwoMail： EwoMail官方文档]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>邮箱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS Docker 安装]]></title>
    <url>%2F2022%2F098df5126b.html</url>
    <content type="text"><![CDATA[Docker支持以下的CentOS版本：CentOS 7 (64-bit)CentOS 6.5 (64-bit) 或更高的版本前提条件目前，CentOS 仅发行版本中的内核支持 Docker。Docker 运行在 CentOS 7 上，要求系统为64位、系统内核版本为 3.10 以上。Docker 运行在 CentOS-6.5 或更高的版本的 CentOS 上，要求系统为64位、系统内核版本为 2.6.32-431 或者更高版本。使用 yum 安装（CentOS 7下）Docker 要求 CentOS 系统的内核版本高于 3.10 ，查看本页面的前提条件来验证你的CentOS 版本是否支持 Docker 。通过 uname -r 命令查看你当前的内核版本12[root@VM_0_9_centos ~]# uname -r3.10.0-514.26.2.el7.x86_64安装 Docker从 2017 年 3 月开始 docker 在原来的基础上分为两个分支版本: Docker CE 和 Docker EE。Docker CE 即社区免费版，Docker EE 即企业版，强调安全，但需付费使用。本文介绍 Docker CE 的安装使用。移除旧的版本：12345678910$ sudo yum remove docker \ docker-client \ docker-client-latest \ docker-common \ docker-latest \ docker-latest-logrotate \ docker-logrotate \ docker-selinux \ docker-engine-selinux \ docker-engine安装一些必要的系统工具：1sudo yum install -y yum-utils device-mapper-persistent-data lvm2添加软件源信息：1sudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo更新 yum 缓存：1sudo yum makecache fast安装 Docker-ce：1sudo yum -y install docker-ce启动 Docker 后台服务：1sudo systemctl start docker测试运行 hello-world1docker run hello-world由于本地没有hello-world这个镜像，所以会下载一个hello-world的镜像，并在容器内运行。使用脚本安装 Docker使用 sudo 或 root 权限登录 Centos。确保 yum 包更新到最新：1sudo yum update执行 Docker 安装脚本：12curl -fsSL https://get.docker.com -o get-docker.shsudo sh get-docker.sh执行这个脚本会添加 docker.repo 源并安装 Docker。启动 Docker 进程：1sudo systemctl start docker验证 docker 是否安装成功并在容器中执行一个测试的镜像。1sudo docker run hello-world可以看出来控制台有输出“Hello from Docker!“等字样，到此，Docker 在 CentOS 系统的安装完成。镜像加速鉴于国内网络问题，后续拉取 Docker 镜像十分缓慢，我们可以需要配置加速器来解决，我使用的是网易的镜像地址： http://hub-mirror.c.163.com。新版的 Docker 使用 /etc/docker/daemon.json（Linux） 或者 %programdata%\docker\config\daemon.json（Windows） 来配置 Daemon。请在该配置文件中加入（没有该文件的话，请先建一个）：123&#123; "registry-mirrors": ["http://hub-mirror.c.163.com"]&#125;检查加速器是否生效：1docker info如果从结果中看到了如下内容，说明配置成功。Registry Mirrors:https://hub-mirror.c.163.com/安装Docker ComposeCompose 简介Compose 是用于定义和运行多容器 Docker 应用程序的工具。通过 Compose，您可以使用 YML 文件来配置应用程序需要的所有服务。然后，使用一个命令，就可以从 YML 文件配置中创建并启动所有服务。Compose 使用的三个步骤：使用 Dockerfile 定义应用程序的环境。使用 docker-compose.yml 定义构成应用程序的服务，这样它们可以在隔离环境中一起运行。最后，执行 docker-compose up 命令来启动并运行整个应用程序。Compose 安装通过yum安装组件变得都很方便，比如compose1yum install docker-compose删除 Docker CE执行以下命令来删除 Docker CE：12sudo yum remove docker-cesudo rm -rf /var/lib/docker]]></content>
      <categories>
        <category>技术</category>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[访问共享文件夹错误]]></title>
    <url>%2F2022%2F0954da8c84.html</url>
    <content type="text"><![CDATA[你不能访问此共享文件夹，因为你组织的安全策略阻止未经身份验证的来宾访问。访问共享文档挂了…方法1:2020之前的版本1、首先按window+R键打开运行键入gpedit.msc启动本地组策略编辑器。2、在组策略编辑器中找到“计算机配置”–“管理模板”–“网络”–“Lanman工作站”3、“Lanman工作站”这个节点，在右侧内容区可以看到“启用不安全的来宾登录”这一条策略设置。状态是“未配置”。双击“启用不安全的来宾登录”这一条策略设置，将其状态修改为“已启用”并单击确定按钮。再次尝试,无效,据说2020版之后就不能用了方法2:20201、打开编辑器。2、进入地址计算机\HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Services\LanmanWorkstation\Parameters 将AllowInsecureGuestAuth设置为13、进入计算机\HKEY_LOCAL_MACHINE\SOFTWARE\Policies\Microsoft\Windows\LanmanWorkstation，将AllowInsecureGuestAuth设置为1]]></content>
      <categories>
        <category>桌面</category>
      </categories>
      <tags>
        <tag>共享</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos设置中国时间]]></title>
    <url>%2F2022%2F09e356f91e.html</url>
    <content type="text"><![CDATA[中国标准时间(CST)比世界协调时间(UTC)早08:00小时。该时区为标准时区时间，主要用于亚洲。直接输入以下命令：1tzselect然后选择5然后选择9然后选择1然后继续选择1最后执行下面的命令：1234567891011121314TZ='Asia/Shanghai';export TZ#修改配置文件的时区vi /etc/sysconfig/clock#填写如下信息ZONE=Asia/Shanghai#删除localtimerm -f /etc/localtime#链接文件ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime#查看时间命令，如果是当前时间，则表示时间同步成功date#查看当地时间和世界时间timedatectl status]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>CST时间</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell脚本实现一对一群发带附件邮件]]></title>
    <url>%2F2022%2F098e42df72.html</url>
    <content type="text"><![CDATA[有时候我们有一对一发送邮件的需求，但是一个一个发太耗时间了，而这种耗时间的操作我们都可以交给机器来完成。没接触过shell语言以及对Linux系统不熟悉的可以看我另一篇文章，也可以实现需求。环境：CentOS 7（自带postfix）1.修改/etc/postfix/main.cf文件：123456myhostname = sample.test.com ← 设置系统的主机名mydomain = test.com ← 设置域名（我们将让此处设置将成为E-mail地址“@”后面的部分）myorigin = $mydomain ← 将发信地址“@”后面的部分设置为域名（非系统主机名）inet_interfaces = all ← 接受来自所有网络的请求mydestination = $myhostname, localhost.$mydomain, localhost, $mydomain ← 指定发给本地邮件的域名home_mailbox = Maildir/ ← 指定用户邮箱目录2.安装邮件发送工具mailx1yum -y install mailx配置邮箱信息（修改/etc/mail.rc）：12345set from=123456@qq.com //你自己的真实邮箱地址set smtp=smtp.exmail.qq.com //在邮箱设置，客户端设置，设置方法，发送服务器里set smtp-auth-user=123456@qq.com //自己邮箱真实地址set smtp-auth-password=123456 //自己邮箱密码set smtp-auth=login重启postfix服务：1service postfix restart测试是否能发送邮件：1echo "Test Mail" | mail -s "Test" 123456@qq.com收件人可以收到邮件：3.编写shell脚本（vim email.sh）：12345678910111213141516171819202122232425262728#!/bin/shcat /root/emailreceiver.txt | while read linedo from="yixian@qq.com" email_subject="IT服务申请" attachment1="/root/test.log" attachment2="/root/test.sh" name=$(echo $line | cut -d "," -f 1) to_all=$(echo $line | cut -d "," -f 2) account=$(echo $line | cut -d "," -f 3) password=$(echo $line | cut -d "," -f 4) email_content="Dear $name,账号已开通，账号信息如下，请查收登录链接：https://aaa.bbb.com账号：$account密码：$password"echo -e "$&#123;email_content&#125;" | mailx \-r "逸贤&lt;$&#123;from&#125;&gt;" \-a $&#123;attachment1&#125; \-a $&#123;attachment2&#125; \-s $&#123;email_subject&#125; $&#123;to_all&#125;done123-r 发送人邮箱-a 上传附件-s 邮件主题给email.sh 700权限：1chmod 700 emai.sh在/root/目录下创建emailreceiver.txt文件，并在里面写上对方的称呼，对方的邮箱，账号，密码，每一项以逗号隔开：执行脚本：1sh email.sh查看对应邮箱收取的邮件：]]></content>
      <categories>
        <category>技术</category>
        <category>脚本</category>
      </categories>
      <tags>
        <tag>脚本</tag>
        <tag>群发邮件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[甘特图制作]]></title>
    <url>%2F2022%2F093b14bb22.html</url>
    <content type="text"><![CDATA[甘特图又叫横道图、条状图，以图示的方式通过活动列表和时间刻度形象地表示出任何特定项目的活动顺序与持续时间。能够直观地表明任务计划在什么时候进行，及实际进展与计划要求的对比，管理者由此可便利地弄清一项任务（项目）还剩下哪些工作要做，并可评估工作进度。 静态设置数据源一个合格的工作进度规划，必须是要有任务开始时间以及任务持续时间的，因此在我们制作甘特图中，同样也需要这两组数据。建立工作进度规划表要求计划开始日期必须以日期格式储存计划持续天数必须以数字格式储存插入图表在我们excel中其实没有“甘特图”这种图表，然而我们为什么还能够用excel做出甘特图呢？在这里我们可以考虑利用其他图表，加以简单的修改，使他变成我们想要的“甘特图”。选中任务、计划开始日期、计划持续天数这三列数据点击插入选项卡选择插入柱形图或条形图在二维条形图中选择堆积条形图设置排列顺序在我们数据源中任务的先后顺序是从上往下排列的，然而我们刚刚生成的堆积条形图中，任务排列顺序却是反了过来。这样的逆序排列看起来并不是很美观，并且违背了人们从上往下阅读的习惯，因此我们需要对图表的垂直轴重新进行排列。点击垂直坐标轴点击右键并选择设置坐标轴格式在坐标轴选项中找到“逆序类别”并且勾选它设置起始日期我们的甘特图中持续天数是从第一天开始的，然而在刚刚生成的堆积条形图中，坐标轴边界的最小值并不是我们任务开始的这一天，因此我们需要手动设置坐标轴边界的最小值为任务开始的这一天。点击水平坐标轴单击右键并选择设置坐标轴格式在坐标轴边界最小值中，输入我们任务开始这一天的日期敲击回车后我们发现录入的日期变为了数值，这是因为在我们的excel中所有的日期都是以数值的形式储存的（1900/1/1这天数值为1，以此类推）隐藏系列在我们甘特图中只需要保留的是持续天数这个系列，而持续天数这条橙色的系列已经能够显示出开始日期的值，那么我们可以利用系列的填充格式，隐藏蓝色这条开始日期的系列，达到更加美观的效果。点击图表中蓝色系列数据条单击右键并选择设置数据系列格式在设置数据系列格式中选择第一个填充与线条展开填充选项，选择无填充美化系列整个甘特图做的已经差不多了，剩下的就是针对系列数据条进行一些简单的修改美化，可以根据自己的偏好，对系列数据条进行加粗，或者修改颜色。点击图表中橙色系列数据条单击右键并选择设置数据系列格式在系列选项中调整间隙宽度，缩小间隙宽度来加粗系列数据条同时可以修改数据系列条的填充色，使比较重要的任务突出显示出来最终的图表，已经基本完成了，就剩下最后的简单修饰，例如修改个标题，或者修改个图例等等，使得整个图表更加的简洁美观。甘特图的基本思想和固有的简单性是惊人的，以一个时间表，配合资源和颜色条，展示任务活动及其持续时间。从你的一个idea开始，很轻易的就可以开始制作一个甘特图。动态然而有的小伙伴们就说了，这图是静态的，而人物进度是动态的，如何才能让自己的甘特图动起来呢？我们插入一个滚动条控件，从而达到控制任务进度，达到动态管理的目的。接下来我们来一步一步学习如何绘制出一个这样的动态甘特图。设置数据源动态图的数据源需要在原有的基础上增加【计划已用天数】和【计划剩余天数】两列内容作为辅助列，同时增加【完成比例】一列作为进度展示。在C12单元格输入一个日期作为可变日期使用在E3单元格内输入公式（=MIN(MAX($C$12-C3,0),D3)）在F3单元格内输入公式（=D3-E3）在G3单元格内输入公式（=E3/D3）由于实际工作进度跟计划工作进度可能会有差别，因此这里以计划进度为例。插入滚动条控件利用滚动条控件来控制我们在C12单元格设置的日期，随着滚动条滚动，可以直接修改C12单元格内的日期数值，从而形成动态的数据表。在选项卡标签上单击右键打开自定义功能区在右侧功能区中勾选开发工具点击开发工具选项卡，找到控件选择插入在ActiveX控件中选择插入滚动条根据自己需求画一个控件矩形框设置控件数据控件默认最大值和最小值都是数字格式，因此我们需要把开始日期和结束日期转化为数字格式才能录入进控件的属性。选择控件后单击右键打开属性对话框在 LinkedCell 中输入C12，使控件链接到C12单元格在 Max 中输入值43100，2017/12/31转化为数字是43100在 Min 中输入值43070，2017/12/1转化为数字是43070关闭属性对话框后，点击一下开发工具中的设计模式，退出设计模式即可。插入图表同样的插入图表，然而在动态甘特图中，数据的选择是非常重要的，数据表中并不是所有的数据都需要被选择进去。选中任务、计划开始日期、计划已用天数、计划剩余天数这四列数据点击插入选项卡选择插入柱形图或条形图在二维条形图中选择堆积条形图，并适当调整格式。设置横纵坐标轴图表的纵坐标顺序并不是我们期望的顺序，我们期望能够从上到下排列任务的先后，因此在这里我们需要对类别重新排列，同时我们的横坐标起始日期应该是计划开始的第一天。点击垂直坐标轴单击右键并选择设置坐标轴格式在坐标轴选项中找到“逆序类别”并且勾选它点击水平坐标轴单击右键并选择设置坐标轴格式在坐标轴边界最小值中，输入我们任务开始这一天的日期优化图表整个动态甘特图框架已经完成，接下来可以对数据条进行一些简单的优化。点击图表中蓝色系列数据条并选择设置数据系列格式在设置数据系列格式中选择无填充点击图表中橙色系列数据条并选择设置数据系列格式在系列选项中调整间隙宽度，缩小间隙宽度来加粗系列数据条添加数据标签为了更直观的看出任务进行到哪里，我们可以选中已用时间系列，添加数据标签，借助数据标签来显示出任务完成比例。选中表中橙色系列，并勾选数据标签选中数据标签单击右键打开设置标签选项去掉勾选值【值】，只勾选【单元格中的值】在弹出的数据标签区域中选择完成比例区域打开填充与线条，对数据标签进行简单的美化即可图表美化最后可以插入一个矩形框，让我们的时间可以直接展示在图表中，同时也可以根据自己的需求进行简单的编辑美化。点击插入选项卡选择插入矩形在编辑栏中直接输入（=C12）选中矩形框，设置为无填充无边框组合最终的条形图、滚动条、矩形框，完成整个图表的制作。动态的甘特图能够更加直接的查阅到工作计划进度，再配合着自己实际工作进度，可以手动输入完成的比例，时刻提醒自己还有剩下哪些工作需要继续。]]></content>
      <categories>
        <category>桌面</category>
      </categories>
      <tags>
        <tag>甘特图</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux磁盘使用空间大小与显示不符]]></title>
    <url>%2F2022%2F09cea41083.html</url>
    <content type="text"><![CDATA[在服务器运维过程中，我们时常会遇到这样的情况，收到服务器磁盘空间告警。 登录服务器，通过 df -Hl 查看:和告警信息一致，接着我们就是要找到导致磁盘空间满的目录或文件。如何找到占用空间大的目录或文件？一种比较笨的方法是，在根目录下，通过 du -hs 命令，列出各目录所占空间大小：之后再用同样的方法继续到对应目录下去找。再相对高效一点的方法是通过 du的 -d 参数，或–max-depth，设置查询的目录深度，目录深度增加，所查询的目录，展示出来会很多，这个时候可以通过 grep 进行过滤，通过这样的方式，可以搜出以G或者T为单位的占用磁盘空间的大目录，并排序。或者可以通过find来查询1find / -type f -size +1G -exec du -h &#123;&#125; \;从效率上来说，find要比du要更快速、灵活通过这两种方法，我们可以快速找到占用磁盘空间的罪魁祸首你以为就这么简单？很多时候，你会发现，通过find或du查半天，发现所有加起来的占用空间，和df看到的磁盘空间占用，相差很大，就比如我上面的两张图通过df查看，磁盘使用37G，但是在根目录下通过du -hs 查看，总共加起来差不多10G，没有隐藏目录，那空间被谁吃了？很明显，有空间被已删除文件占用，文件删除了，但是资源没释放之前介绍过一个很好用的命令：lsof，我们可以通过以下命令去查看1lsof +L1或者1lsof |grep delete从结果可以看出，有一个 28G 左右的大日志文件，删除了，但是空间没释放，这可能是由于没停止服务，就把对应的文件删除了，导致空间没释放。对应的解决方法就是，重启 tomcat 应用，释放空间或者直接kill掉进程。磁盘空间莫名被吃？还有一种经常有人问的问题，就是，通过df查看到的磁盘会发现，Used 和 Avail 加起来不够 Size，莫名被吃掉一部分其实这是 Linux 文件系统的一种安全策略，它默认会为 root 用户保留 5% 的磁盘空间，留作紧急情况使用。这样能保证有些关键应用（比如数据库）在硬盘满的时候有点余地，不至于马上就 crash。我们可以通过 tune2fs 修改预留空间的比例：1tune2fs -m 1 /dev/vda1通过下图可以看到前后对比图片这样被吃掉的空间，就吐出来了。]]></content>
      <categories>
        <category>技术</category>
        <category>分区</category>
      </categories>
      <tags>
        <tag>磁盘</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker镜像的导入导出]]></title>
    <url>%2F2022%2F09153a44f6.html</url>
    <content type="text"><![CDATA[Docker镜像的导入导出本文介绍Docker镜像的导入导出，用于迁移、备份、升级等场景准备环境如下：CentOS 7.0Docker 1.18导入导出命令介绍涉及的命令有export、import、save、loadsave命令1docker save [options] images [images...]示例:1docker save -o nginx.tar nginx:latest或1docker save &gt; nginx.tar nginx:latest其中-o和&gt;表示输出到文件，nginx.tar为目标文件，nginx:latest是源镜像名（name:tag）load命令1docker load [options]示例:1docker load -i nginx.tar或1docker load &lt; nginx.tar其中-i和&lt;表示从文件输入。会成功导入镜像及相关元数据，包括tag信息export命令1docker export [options] container示例:1docker export -o nginx-test.tar nginx-test其中-o表示输出到文件，nginx-test.tar为目标文件，nginx-test是源容器名（name）import命令1docker import [options] file|URL|- [REPOSITORY[:TAG]]示例:1docker import nginx-test.tar nginx:imp或1cat nginx-test.tar | docker import - nginx:imp区别：export命令导出的tar文件略小于save命令导出的。export命令是从容器（container）中导出tar文件，而save命令则是从镜像（images）中导出。基于第二点，export导出的文件再import回去时，无法保留镜像所有历史（即每一层layer信息，不熟悉的可以去看Dockerfile），不能进行回滚操作；而save是依据镜像来的，所以导入时可以完整保留下每一层layer信息。如下图所示，nginx:latest是save导出load导入的，nginx:imp是export导出import导入的。建议可以依据具体使用场景来选择命令：​ &gt;若是只想备份images，使用save、load即可​ &gt;若是在启动容器后，容器内容有变化，需要备份，则使用export、import]]></content>
      <categories>
        <category>技术</category>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jumpserver服务迁移]]></title>
    <url>%2F2022%2F092b689438.html</url>
    <content type="text"><![CDATA[需求：把jumpserver服务搬迁到另一台服务器上原服务器ip：172.25.0.237新服务器ip：172.22.12.246由于jumpserver服务是在docker中启动的，所以直接把docker镜像搬到新服务器里跑就行。方法一：1.查看正在跑的容器：2.把容器打包为镜像：1docker commit -m "jumpserver backup" 67b073d01232 jumpserverbak:v2.7.03.导出刚打包的镜像：1docker save -o /tmp/jumpserverbak.tar jumpserverbak:v2.7.04.把镜像上传至新服务器：1scp /tmp/jumpserverbak.tar root@172.22.12.246:/data/backup/5.在新服务器安装docker1curl -sSL https://get.daocloud.io/docker | sh启动docker并设置开机自启动：12systemctl enable dockersystemctl start docker6.docker镜像导入：1docker load -i /data/backup/jumpserverbak.tar7.启动容器：1docker run -it -d --name jumpserverbak jumpserverbak方法二：1.查看正在跑的容器：2.导出镜像：1docker export -o jumpserver.tar jumpserver其中-o表示输出到文件，jumpserver.tar为目标文件，jumpserver是镜像名（name）3.进docker把数据库备份：123docker exec -it 67b073d01232 /bin/bashmysqldump -u jumpserver -h 127.0.0.1 -p jumpserver &gt; /opt/jumpserver.sqlexit把数据库备份文件拷贝到本地服务器：1docker cp 67b073d01232:/opt/jumpserver.sql /opt/4.把打包的docker镜像以及数据库备份文件拷贝到新服务器：1scp /opt/jumpserver.* root@172.22.12.246:/tmp/在新服务器查看是否拷贝成功：5.在新服务器安装docker1curl -sSL https://get.daocloud.io/docker | sh启动docker并设置开机自启动：12systemctl enable dockersystemctl start docker6.docker镜像导入：12cd /tmp/jumpserver/docker import jumpserver.tar test/jumpserver:v1.0查看是否导入成功：启动容器：1docker run --name jumpserver -d -p 81:80 -p 2222:2222 --restart=always -e SECRET_KEY=跟原服务器相同 -e BOOTSTRAP_TOKEN=跟原服务器相同 -e DB_HOST=127.0.0.1 -e DB_PORT=3306 -e DB_USER=jumpserver -e DB_PASSWORD=跟原服务器相同 -e DB_NAME=jumpserver -e REDIS_HOST=127.0.0.1 -e REDIS_PORT=6379 test/jummpserver:v1.0 /opt/entrypoint.sh查看容器是否在跑：把数据库备份文件拷贝到容器里：1docker cp /opt/jumpserver/jumpserver.sql d439a4275586:/opt/把数据库备份文件导入数据库：12345docker exec -it d439a427558 /bin/bashmysql -u jumpserver -h 127.0.0.1 -puse jumpserver;source /opt/jumpserver.sql;quit把/opt/koko/data/keys/.access_key文件删除（否则会验证不成功）并重启容器：12rm -f /opt/koko/data/keys/.access_keyexit重启容器：12docker stop d439a4275586docker start d439a4275586浏览器进入管理界面：至此，jumpserver服务搬迁完成。错误解决：进入资产列表测试资产可连接性的时候报错如下： 或者弹窗一直显示消息：……………………………解决方法：1234567docker exec -it d439a427558 /bin/bashcd /opt/jumpserversource /opt/py3/bin/activate./jms stopps aux | grep py3 | awk '&#123;print $2&#125;' |xargs kill -9rm -rf tmp/*.pid./jms start -d再到资产列表测试资产可连接性：]]></content>
      <categories>
        <category>技术</category>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>jumpserver</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阿里云Redis数据备份及恢复至自建数据库]]></title>
    <url>%2F2022%2F09e66e49e2.html</url>
    <content type="text"><![CDATA[1.登录阿里云控制台，点击云数据库Redis，点击对应实例，选择备份恢复，点击备份实例，下载备份文件。2.在本地服务器安装Redis（具体查看这篇文章，在此不再赘述）3.使用redis-port工具把下载的备份文件导进Redis数据库：3.1 下载redis-port1wget https://github.com/CodisLabs/redis-port赋予可执行权限：1chmod 777 redis-port把备份文件导入Redis：1./redis-port restore -i /data/backups/Redis/hy-redis02-hjl/hins2358867_data_20210821170224.rdb -t 127.0.0.1:6379导入完成：登入Redis查看数据：至此，数据导入成功。]]></content>
      <categories>
        <category>技术</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阿里云MongoDB物理备份文件恢复至自建数据库]]></title>
    <url>%2F2022%2F093109e84d.html</url>
    <content type="text"><![CDATA[1.登录阿里云，进入云数据库MongoDB模块，点击对应实例，选择备份与恢复 下载备份文件：2.在本地服务器安装MongoDB（版本需与阿里云MongoDB版本一致），具体安装方法请查看上一篇文章，在此不再赘述。3.解压下载的备份文件：12mkdir -p /mysql/mongotar -zxvf /data/backups/MongoDB/hins4463797_data_20210825030706.tar.gz -C /mysql/mongo/解压结果：4.以单节点模式恢复MongoDB物理备份的数据4.1 执行如下命令在/usr/local/mongodb文件夹中新建配置文件mongod.conf。1touch /usr/local/mongodb/mongod.conf4.2 在命令行中输入vi /usr/local/mongodb/mongod.conf打开mongod.conf文件，键盘输入i开启编辑模式。根据云数据库MongoDB版的存储引擎选择启动的配置模板，您可以将其复制到mongod.conf文件中:12345678910111213141516systemLog: destination: file path: /usr/local/mongodb/mongod.log logAppend: truesecurity: authorization: enabledstorage: dbPath: /mysql/mongo directoryPerDB: truenet: port: 27017 unixDomainSocket: enabled: falseprocessManagement: fork: true pidFilePath: /usr/local/mongodb/mongod.pid4.3 按Esc键退出编辑模式，键盘输入:wq保存并退出。5.指定新建的配置文件mongod.conf来启动MongoDB1mongod -f /usr/local/mongodb/mongod.conf等待启动完成后，执行如下命令登录MongoDB数据库，进入Mongo Shell。1mongo --host 127.0.0.1 -u &lt;username&gt; -p &lt;password&gt; --authenticationDatabase admin登录成功：在Mongo Shell中，执行show dbs查询当前本地MongoDB中所有的数据库，以验证是否恢复成功:至此恢复工作已成功完成，您可以在Mongo Shell中执行exit命令退出Mongo Shell。6.副本集模式启动MongoDB数据库6.1 在命令行中通过服务器的Mongo Shell使用root用户登录MongoDB数据库：（如果您的密码中包含特殊字符，则需要使用英文单引号（’）包裹密码，如：‘test123!@#’。否则可能会登录失败。）1mongo --host 127.0.0.1 -u root -p &lt;root用户密码&gt; --authenticationDatabase admin登录成功后，执行下方代码框中的命令完成如下动作：​ 1.在admin库中创建一个临时用户，赋予该用户临时的local库读写权限。​ 2.切换至临时用户移除local库中原有副本集配置。​ 3.切换回root用户删除临时用户和临时权限1234567891011121314151617181920212223242526272829303132333435363738use admindb.runCommand(&#123; createRole: "tmprole", roles: [ &#123; role: "root", db: "admin" &#125; ], privileges: [ &#123; resource: &#123; db: 'local', collection: 'system.replset' &#125;, actions: [ 'remove' ] &#125; ]&#125;)db.runCommand(&#123; createUser: "tmpuser", pwd: "tmppwd", roles: [ 'tmprole' ]&#125;)db.auth('tmpuser','tmppwd')use localdb.system.replset.remove(&#123;&#125;)use admindb.auth('root','&lt;root用户密码&gt;')db.dropRole('tmprole')db.dropUser('tmpuser')执行结果示例： (对于local库的system.replset集合，root用户只有只读权限，且由于root用户无法更改自身的权限，因此只能通过其他用户进行删除。)6.2 执行如下命令关闭MongoDB服务并退出Mongo Shell：123use admindb.shutdownServer()exit7.创建副本集认证文件（如需以副本集模式启动MongoDB，您需要创建一个key文件作为每个副本集节点之间的认证文件）：7.1 执行如下命令在mongo目录下创建keyFile文件夹作为认证文件的目录，并在该目录中创建一个key文件。1mkdir -p /usr/local/mongodb/keyFile &amp;&amp; touch /usr/local/mongodb/keyFile/mongodb.key执行vi /root/mongo/keyFile/mongodb.key打开mongodb.key文件，按键盘上的i进入编辑模式，输入加密内容。例如：1MongoDB Encrypting File加密内容有如下几个限制​ &gt;长度必须在6~1024个字符之间。​ &gt;只能包含base64编码中的字符。​ &gt;不能包含等号（=）。在命令行中执行如下命令将认证文件的权限修改为400，保证该文件内容仅对该文件所有者可见。1chmod 400 /usr/local/mongodb/keyFile/mongodb.key8.通过下列步骤为副本集准备两个空的节点。8.1 执行如下命令复制两份mongod.conf文件分别作为另外两个节点的启动配置文件。1cp /usr/local/mongodb/mongod.conf /usr/local/mongodb/mongod1.conf &amp;&amp; cp /usr/local/mongodb/mongod.conf /usr/local/mongodb/mongod2.conf执行如下命令分别为另外两个节点创建数据目录。1mkdir -p /mysql/mongo1 &amp;&amp; mkdir -p /mysql/mongo29.分别通过下列指示修改各节点的配置文件：执行vi /usr/local/mongodb/mongod.conf打开节点1的配置文件，并按照如下内容修改完成后保存退出。1234567891011121314151617181920systemLog: destination: file path: /usr/local/mongodb/mongod.log logAppend: truesecurity: authorization: enabled keyFile: /usr/local/mongodb/keyFile/mongodb.keystorage: dbPath: /mysql/mongo directoryPerDB: truenet: bindIp: 127.0.0.1 port: 27017 unixDomainSocket: enabled: falseprocessManagement: fork: true pidFilePath: /usr/local/mongodb/mongod.pidreplication: replSetName: "rs0"执行vi /usr/local/mongodb/mongod1.conf打开节点2的配置文件，并按照如下内容修改完成后保存退出。1234567891011121314151617181920systemLog: destination: file path: /usr/local/mongodb/mongod1.log logAppend: truesecurity: authorization: enabled keyFile: /usr/local/mongodb/keyFile/mongodb.keystorage: dbPath: /mysql/mongo1 directoryPerDB: truenet: bindIp: 127.0.0.1 port: 27018 unixDomainSocket: enabled: falseprocessManagement: fork: true pidFilePath: /usr/local/mongodb/mongod1.pidreplication: replSetName: "rs0"执行vi /usr/local/mongodb/mongod2.conf打开节点3的配置文件，并按照如下内容修改完成后保存退出。1234567891011121314151617181920systemLog: destination: file path: /usr/local/mongodb/mongod2.log logAppend: truesecurity: authorization: enabled keyFile: /usr/local/mongodb/keyFile/mongodb.keystorage: dbPath: /mysql/mongo2 directoryPerDB: truenet: bindIp: 127.0.0.1 port: 27019 unixDomainSocket: enabled: falseprocessManagement: fork: true pidFilePath: /usr/local/mongodb/mongod2.pidreplication: replSetName: "rs0"各重要参数说明如下：12345678systemLog.path下的path：当前节点的MongoDB日志文件路径，dbpath：当前节点的MongoDB数据文件路径。pidFilePath：当前节点的MongoDB的PID文件（记录进程ID的文件）路径。keyFile：副本集认证文件路径，所有节点必须使用同一个认证文件。bindIp：当前节点的IP地址。如果是在同一台服务器上部署副本集，所有节点可采用相同的IP地址。port：当前节点的端口号。如果是在同一台服务器上部署副本集，所有节点应采用不同的端口号。replication：副本集配置。replSetName：设置副本集的名称。执行如下命令启动3个节点。1mongod -f /usr/local/mongodb/mongod.conf &amp;&amp; mongod -f /usr/local/mongodb/mongod1.conf &amp;&amp; mongod -f /usr/local/mongodb/mongod2.conf等待启动完成后，使用root账号登录MongoDB数据库。1mongo --host 127.0.0.1 -u root -p &lt;root账号的密码&gt; --authenticationDatabase admin在Mongo Shell中通过如下命令将上述步骤中创建的副本集成员节点加入副本集并初始化。123456789rs.initiate( &#123; _id : "rs0", version : 1, members: [ &#123; _id: 0, host: "127.0.0.1:27017" , priority : 1&#125;, &#123; _id: 1, host: "127.0.0.1:27018" , priority : 0&#125;, &#123; _id: 2, host: "127.0.0.1:27019" , priority : 0&#125; ]&#125;)执行成功后，新加入的两个节点将会与主节点进行数据同步，注意此过程的耗时根据备份文件的大小会有较大差异。等待数据同步完成后，副本集模式启动完成。11.通过如下步骤验证是否启动成功。​ 1.执行exit退出Mongo Shell。​ 2.执行如下命令重新登录MongoDB数据库.1mongo -u &lt;username&gt; -p &lt;password&gt; --authenticationDatabase admin观察Mongo Shell命令行左侧，显示&lt;副本集名称&gt;:PRIMARY&gt;即代表副本集模式启动成功。]]></content>
      <categories>
        <category>技术</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux系统安装MongoDB]]></title>
    <url>%2F2022%2F09ce2728e7.html</url>
    <content type="text"><![CDATA[MongoDB 是由C++语言编写的，是一个基于分布式文件存储的开源数据库系统。在高负载的情况下，添加更多的节点，可以保证服务器性能。MongoDB 旨在为WEB应用提供可扩展的高性能数据存储解决方案。MongoDB 将数据存储为一个文档，数据结构由键值(key=&gt;value)对组成。MongoDB 文档类似于 JSON 对象。字段值可以包含其他文档，数组及文档数组。1、下载完安装包，并解压 tgz（以下演示的是 64 位 Linux上的安装）12wget https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-3.4.10.tgztar -zxvf mongodb-linux-x86_64-3.4.10.tgz 将Mongodb文件夹命名为mongdb文件夹1mv mongodb-linux-x86_64-3.4.10 mongodbMongoDB 的可执行文件位于 bin 目录下，所以可以将其添加到 PATH 路径中：打开/etc/profile文件，添加以下内容：1export PATH=&lt;mongodb-install-directory&gt;/bin:$PATH为你 MongoDB 的安装路径。如本文的 /usr/local/mongodb。1export PATH=/usr/local/mongodb/bin:$PATH保存后执行以下命令生效：1source /etc/profile2、创建数据库目录MongoDB的数据存储在data目录的db目录下，但是这个目录在安装过程不会自动创建，所以你需要手动创建data目录，并在data目录中创建db目录。以下实例中我们将data目录创建于根目录下(/)。注意：/data/db 是 MongoDB 默认的启动的数据库路径(–dbpath)。创建目录树：1mkdir -p /data/db3、命令行中运行 MongoDB 服务你可以在命令行中执行mongo安装目录中的bin目录执行mongod命令来启动mongdb服务。1./mongod --dbpath /data/db --fork关闭服务：1./mongod --dbpath /data/db --shutdown也可以在 mongo 的命令出口中实现：123&gt; use adminswitched to db admin&gt; db.shutdownServer()4、MongoDB后台管理 Shell 如果你需要进入MongoDB后台管理，你需要先打开mongodb装目录的下的bin目录，然后执行mongo命令文件。 MongoDB Shell是MongoDB自带的交互式Javascript shell,用来对MongoDB进行操作和管理的交互式环境。 当你进入mongoDB后台后，它默认会链接到 test 文档（数据库）插入一些简单的数据，并对插入的数据进行检索：5、创建用户和密码进入bin目录下， 输入./mongo 127.0.0.1:27017 连接到mongodb中建立系统管理员用户12345678use admindb.createUser( &#123; user: "myAdmin", pwd: "admin123", roles: [ &#123; role: "userAdminAnyDatabase", db: "admin" &#125; ] &#125;)mongod的启动和关闭 :mogodb 启动：1.缺省的启动方式1./mongod缺省的启动方式，数据存储目录里是/data/db，监听端口是270172.带基本参数的启动方式1./mongod --port 51017 --dbpath /home/mongo/data --logpath /home/mongo/log/log.log --logappend --fork --journal 1000123456789基本参数：-f 指定配置文件 （参考： http://www.mongodb.org/display/DOCS/File+Based+Configuration）--port 指定端口，默认是27017--dbpath 数据目录路径--logpath 日志文件路径--logappend 日志append而不是overwrite--fork 以创建子进程的方式运行--journal 日志提交间隔，默认100ms--nojournal 关闭日志功能，2.0版本以上是默认开启的mongodb关闭：123&gt; use admin&gt; db.shutdownServer()&gt; db.shutdownServer(&#123;force : true&#125;) 强制关闭Mongod，应对副本集中主从时间差超过10s时不允许关闭主库的情况不要使用kill直接杀mongo进程的方式关闭数据节点，会造成数据损坏。]]></content>
      <categories>
        <category>技术</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux系统安装Redis]]></title>
    <url>%2F2022%2F09ca3d3aa4.html</url>
    <content type="text"><![CDATA[REmote DIctionary Server(Redis) 是一个由 Salvatore Sanfilippo 写的 key-value 存储系统，是跨平台的非关系型数据库。Redis 是一个开源的使用 ANSI C 语言编写、遵守 BSD 协议、支持网络、可基于内存、分布式、可选持久性的键值对(Key-Value)存储数据库，并提供多种语言的 API。Redis 通常被称为数据结构服务器，因为值（value）可以是字符串(String)、哈希(Hash)、列表(list)、集合(sets)和有序集合(sorted sets)等类型。1.获取redis资源1wget http://download.redis.io/releases/redis-4.0.8.tar.gz2.解压1tar xzvf redis-4.0.8.tar.gz3.编译与安装1234cd redis-4.0.8makecd srcmake install PREFIX=/usr/local/redis4.移动配置文件到安装目录下123cd ../mkdir /usr/local/redis/etcmv redis.conf /usr/local/redis/etc5.配置redis为后台启动12vi /usr/local/redis/etc/redis.conf //将daemonize no 改成daemonize yes6.将redis加入到开机启动12vi /etc/rc.local//在里面添加内容：/usr/local/redis/bin/redis-server /usr/local/redis/etc/redis.conf7.开启redis1/usr/local/redis/bin/redis-server /usr/local/redis/etc/redis.conf8.将redis-cli,redis-server拷贝到bin下，让redis-cli指令可以在任意目录下直接使用12cp /usr/local/redis/bin/redis-server /usr/local/bin/cp /usr/local/redis/bin/redis-cli /usr/local/bin/9.设置redis密码 a.运行命令：1redis-cli b.查看现有的redis密码命令(可选操作，可以没有)1config get requirepass如果没有设置过密码的话运行结果会如下12requirepass ''c.设置redis密码运行命令：12config set requirepass ****(****为你要设置的密码)，设置成功的话会返回‘OK’字样d.测试连接1234 redis-cli -h 127.0.0.1 -p 6379 -a ****（****为你设置的密码） 输入 redis-cli 进入命令模式，使用 auth '\*\*\*\*\*' （****为你设置的密码）登陆redis-清理数据：1）首先通过密码登陆redis123&gt;redis-cli&gt;auth 密码&gt;2）执行清理前查看(若不需要清理全部则清理指定key即可)12&gt;keys * //查看所有key值&gt;3）清理redis1234&gt;del key //①删除指定key&gt;Flushdb //②删除当前数据库中的所有Key&gt;flushall //③删除所有数据库中的key&gt;10.让外网能够访问redisa.配置防火墙:12firewall-cmd --zone=public --add-port=6379/tcp --permanent（开放6379端口）systemctl restart firewalld（重启防火墙以使配置即时生效）查看系统所有开放的端口：1firewall-cmd --zone=public --list-portsb.此时 虽然防火墙开放了6379端口，但是外网还是无法访问的，因为redis监听的是127.0.0.1：6379，并不监听外网的请求。（一）把文件夹目录里的redis.conf配置文件里的bind 127.0.0.1前面加#注释掉（二）命令：redis-cli连接到redis后，输入命令 config get daemonize和config get protected-mode 是不是都为no，如果不是，就用config set 配置名 属性 改为no。常用命令12redis-server /usr/local/redis/etc/redis.conf //启动redispkill redis //停止redis卸载redis：12rm -rf /usr/local/redis //删除安装目录rm -rf /usr/bin/redis-* //删除所有redis相关命令脚本启动redis:1redis-server &amp;检测后台进程是否存在1ps -ef |grep redis检测6379端口是否在监听1netstat -lntp | grep 6379有时候会报异常原因: Redis已经启动解决: 关掉Redis,重启即可12redis-cli shutdownredis-server然后你就能看到Redis愉快的运行了.使用redis-cli客户端检测连接是否正常1234567redis-cli127.0.0.1:6379&gt; keys *(empty list or set)127.0.0.1:6379&gt; set key "hello world"OK127.0.0.1:6379&gt; get key"hello world"停止redis:使用客户端1redis-cli shutdown因为Redis可以妥善处理SIGTERM信号，所以直接kill -9也是可以的1kill -9 PID]]></content>
      <categories>
        <category>技术</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis-port工具]]></title>
    <url>%2F2022%2F0966cd2147.html</url>
    <content type="text"><![CDATA[Redis数据持久化方式有AOF和RDB两种，这两种方式各有优缺点。AOF方式会生成appendonly.aof文件，记录执行的命令操作日志。RDB方式会生成dump.rdb文件，按照配置条件触发保存数据快照。redis-port 是一个 Redis 工具，通过解析 rdb 文件，实现 Redis 之间的数据同步以及数据恢复。Github源码地址：https://github.com/CodisLabs/redis-port阿里云有已经编译好的：点此查看赋予可执行权限：1chmod 777 redis-port1、decode 是把rdb文件解码输出到文件1redis-port decode -i dump.rdb -o test.txt2、restore 是把rdb文件传输到指定redis实例（设置了密码的话需要用–auth=密码）1redis-port restore -i dump.rdb -t 127.0.0.1:7001restore前后redis数据变化:3、dump 是从redis实例数据转存为rdb文件1redis-port dump -f 127.0.0.1:7001 -o output_dump.rdb可以通过decode查看输出文件是否是redis实例的数据：4、sync 是两个redis实例间数据同步（rewrite存在相同的key覆盖）1redis-port sync -f 127.0.0.1:7001 -t 127.0.0.1:6379 --rewrite当看到sync rdb done 后就同步结束了，下边继续执行的是监听源实例数据的修改，并同步。数据同步完成.]]></content>
      <categories>
        <category>技术</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阿里云RDS数据备份及恢复至本地数据库]]></title>
    <url>%2F2022%2F09bd0cd4b1.html</url>
    <content type="text"><![CDATA[1.首先登陆阿里云平台，到云数据库RDS页面，点击对应实例，点击备份恢复。2.右上角点击备份实例，备份完成后下载。 3.把文件挂载到本地服务器：3.1在本地服务器上创建挂载目录1mkdir /data由于移动硬盘磁盘格式是NTFS，执行挂载命令后会报错：mount: unknown filesystem type ‘ntfs’，这是由于Linux上无法识别NTFS格式的分区的原因。解决办法：安装ntfs-3g12345wget https://tuxera.com/opensource/ntfs-3g_ntfsprogs-2017.3.23.tgztar -zxvf ntfs-3g_ntfsprogs-2017.3.23.tgzcd ntfs-3g_ntfsprogs-2017.3.23.tgz./configuremake &amp;&amp; make install4.把移动硬盘挂在到本地服务器上：1mount -t ntfs-3g /dev/sdb1 /data5.在本地服务器上，创建一个目录（例如/mysql/rds/hhm）用于存放解压后的文件。1mkdir -p /mysql/rds/hhm6.解压备份数据至刚才创建的目录1tar -izxvf /data/backups/RDS/HHM/hins17168727_data_20210820101803.tar.gz -C /mysql/rds/hhm解压后的文件如下：7.在本地服务器上安装MySQL（版本需与阿里云上的版本一致），由于阿里云上RDS的MySQL版本为5.6，故安装MySQL5.6：123wget http://repo.mysql.com/mysql-community-release-el6-5.noarch.rpmrpm -ivh mysql-community-release-el6-5.noarch.rpmyum install -y mysql-server mysql-devel8.在本地服务器上，安装Percona XtraBackup（恢复数据使用）：12yum install https://repo.percona.com/yum/percona-release-latest.noarch.rpmyum install percona-xtrabackup-249.执行如下命令，恢复解压好的备份文件。1innobackupex --defaults-file=/mysql/rds/hhm/backup-my.cnf --apply-log /mysql/rds/hhm恢复时请耐心等待，若系统返回如下类似结果，则说明备份文件已成功恢复到自建数据库。若系统返回如下报错，可以用rm -rf /var/lib/mysql命令清空文件夹内文件，然后用chown -R mysql:mysql /var/lib/mysql修改权限。10.启动MySQL：为避免版本问题，需修改/mysql/rds/hhm/backup-my.cnf文件：添加如下参数：1lower_case_table_names=1注释掉如下自建数据库不支持的参数：123456789#innodb_log_checksum_algorithm#innodb_fast_checksum#innodb_log_block_size#innodb_doublewrite_file#innodb_encrypt_algorithm#rds_encrypt_data#redo_log_version#master_key_id#server_uuid修改后文件如下：执行如下命令，修改文件属主，并确定文件所属为MySQL用户。1chown -R mysql:mysql /mysql/rds/hhm执行如下命令，启动MySQL进程。1mysqld --defaults-file=/mysql/rds/hhm/backup-my.cnf --user=mysql --datadir=/mysql/rds/hhm &amp;11.以原本的数据库账号密码登录验证是否成功：登录成功，查看数据库是否导入成功：可以看到，数据库已经成功导入，查看表是否成功导入：表也成功导入，随机查看一个表是否有数据：至此，阿里云RDS数据恢复至本地服务器完成。]]></content>
      <categories>
        <category>技术</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux挂载硬盘错误]]></title>
    <url>%2F2022%2F09e3336f7b.html</url>
    <content type="text"><![CDATA[Linux挂载U盘时，报错mount: unknown filesystem type ‘ntfs’ 错误。这是由于Linux上无法识别NTFS格式的分区的原因。解决办法：安装ntfs-3g12345wget https://tuxera.com/opensource/ntfs-3g_ntfsprogs-2017.3.23.tgztar -zxvf ntfs-3g_ntfsprogs-2017.3.23.tgzcd ntfs-3g_ntfsprogs-2017.3.23.tgz./configuremake &amp;&amp; make install执行挂载命令，挂载U盘：1mount -t ntfs-3g /dev/sdb1 /data至此，就解决了Linux系统无法识别ntfs格式的问题。]]></content>
      <categories>
        <category>技术</category>
        <category>分区</category>
      </categories>
      <tags>
        <tag>挂载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图床Fastdfs服务部署]]></title>
    <url>%2F2022%2F09e56d5322.html</url>
    <content type="text"><![CDATA[FastDFS是用c语言编写的一款开源的分布式文件系统。FastDFS为互联网量身定制，充分考虑了冗余备份、负载均衡、线性扩容等机制，并注重高可用、高性能等指标，使用FastDFS很容易搭建一套高性能的文件服务器集群提供文件上传、下载等服务。 Storage安装:123456789wget https://github.com/happyfish100/fastdfs/archive/V5.05.tar.gz tar -zxf V5.05.tar.gz cd fastdfs-5.05/./make.sh ./make.sh installmkdir -p /data/fastdfssed -i 's#/usr/local/bin/#/usr/bin/#g' /etc/init.d/fdfs_storaged cd /etc/fdfs/cp storage.conf.sample storage.conf修改配置：1vi storage.conf参考如下参数：1[root@localhost fdfs]# egrep -v "^$|#" storage.conf123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657disabled=falsegroup_name=group1bind_addr=client_bind=trueport=23000connect_timeout=30network_timeout=60heart_beat_interval=30stat_report_interval=60base_path=/data/fastdfsmax_connections=256buff_size = 256KBaccept_threads=1work_threads=4disk_rw_separated = truedisk_reader_threads = 1disk_writer_threads = 1sync_wait_msec=50sync_interval=0sync_start_time=00:00sync_end_time=23:59write_mark_file_freq=500store_path_count=1store_path0=/data/fastdfssubdir_count_per_path=256tracker_server=10.27.95.39:22122tracker_server=10.27.95.168:22122log_level=inforun_by_group=run_by_user=allow_hosts=*file_distribute_path_mode=0file_distribute_rotate_count=100fsync_after_written_bytes=0sync_log_buff_interval=10sync_binlog_buff_interval=10sync_stat_file_interval=300thread_stack_size=512KBupload_priority=10if_alias_prefix=check_file_duplicate=0file_signature_method=hashkey_namespace=FastDFSkeep_alive=0use_access_log = falserotate_access_log = falseaccess_log_rotate_time=00:00rotate_error_log = falseerror_log_rotate_time=00:00rotate_access_log_size = 0rotate_error_log_size = 0log_file_keep_days = 0file_sync_skip_invalid_record=falseuse_connection_pool = falseconnection_pool_max_idle_time = 3600http.domain_name=http.server_port=8080启动storage服务：12[root@HY-DFS-01 fdfs]# /etc/init.d/fdfs_storaged startStarting FastDFS storage server:查看程序运行情况：1234[root@HY-DFS-01 fdfs]# lsof -i :23000COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEfdfs_stor 10093 root 5u IPv4 133350 0t0 TCP *:inovaport1 (LISTEN)fdfs_stor 10093 root 20u IPv4 133361 0t0 TCP iZbp13mxkrdvotpxv3btaoZ:inovaport1-&gt;10.27.96.119:36215 (ESTABLISHED)12[root@HY-DFS-01 fdfs]# netstat -tunlp|grep 23000tcp 0 0 0.0.0.0:23000 0.0.0.0:* LISTEN 10093/fdfs_storaged123[root@HY-DFS-01 fdfs]# ps -ef|grep fdfsroot 10093 1 0 16:39 ? 00:00:00 /usr/bin/fdfs_storaged /etc/fdfs/storage.confroot 10111 9489 0 16:40 pts/0 00:00:00 grep fdfs安装nginx服务：所需文件：12345678910[root@HY-DFS-01 tools]# lltotal 3884drwxrwxr-x 10 root root 4096 Nov 30 13:45 fastdfs-5.05-rw-r--r-- 1 root root 20335 Dec 1 11:41 fastdfs-nginx-module-master.zipdrwxr-xr-x 5 root root 4096 Nov 29 15:42 libfastcommon-rw-r--r-- 1 root root 751033 Nov 29 15:31 libfastcommon.tar.gz-rw-r--r-- 1 root root 805253 Dec 1 13:44 nginx-1.6.3.tar.gz-rw-r--r-- 1 root root 2041593 Dec 1 11:41 pcre-8.37.tar.gzdrwxr-xr-x 2 root root 4096 Dec 1 11:42 scripts-rw-r--r-- 1 root root 336001 Nov 14 16:53 V5.05.tar.gz安装依赖包：1yum -y install gcc gcc-c++ autoconf libjpeg libjpeg-devel libpng libpng-devel freetype freetype-devel libxml2 libxml2-devel zlib zlib-devel glibc glibc-devel glib2 glib2-devel bzip2 bzip2-devel ncurses ncurses-devel curl curl-devel e2fsprogs e2fsprogs-devel krb5 krb5-devel libidn libidn-devel openssl openssl-devel openldap openldap-devel nss_ldap openldap-clients openldap-servers安装nginx需要安装以下依赖包：1yum -y install pcre-devel zlib-devel make创建用户：12useradd -u 602 -s /sbin/nologin -M nginxmkdir -p /usr/local/nginx/tmp/&#123;client,proxy,fcgi&#125;解压安装包：1234tar -zxf pcre-8.37.tar.gzunzip fastdfs-nginx-module-master.ziptar -zxf nginx-1.6.3.tar.gzcd nginx-1.6.3编译:1234567891011121314151617./configure \ --prefix=/usr/local/nginx \ --user=nginx \ --group=nginx \ --with-http_ssl_module \ --with-http_gzip_static_module \ --http-client-body-temp-path=/usr/local/nginx/tmp/client/ \ --http-proxy-temp-path=/usr/local/nginx/tmp/proxy/ \ --http-fastcgi-temp-path=/usr/local/nginx/tmp/fcgi/ \ --with-poll_module \ --with-file-aio \ --with-http_realip_module \ --with-http_addition_module \ --with-http_random_index_module \ --with-pcre=/opt/tools/pcre-8.37 \ --with-http_stub_status_module \ --add-module=/opt/tools/fastdfs-nginx-module-master/src/安装:12345678make &amp;&amp; make installchown -R nginx. /usr/local/nginxcd /opt/tools/fastdfs-nginx-module-master/src/cp mod_fastdfs.conf /etc/fdfs/ cp /opt/tools/fastdfs-5.05/conf/&#123;anti-steal.jpg,http.conf,mime.types&#125; /etc/fdfs/ touch /var/log/mod_fastdfs.log chown nginx:nginx /var/log/mod_fastdfs.log修改nginx配置文件：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394server &#123;listen 80;listen 443 ssl;ssl on;ssl_certificate /usr/local/nginx/certs/file_yixian12580_cn.crt;ssl_certificate_key /usr/local/nginx/certs/file_yixian12580_cn.key;ssl_protocols TLSv1 TLSv1.1 TLSv1.2;ssl_ciphers RC4-SHA:AES128-SHA:AES:!ADH:!aNULL:!DH:!EDH:!eNULL;ssl_prefer_server_ciphers on;ssl_session_cache shared:SSL-file:128m;ssl_session_timeout 10m;server_name file.yixian12580.cn;access_log logs/access.log main;access_log logs/file_yixian12580_cn.access.log main;error_log logs/file_yixian12580_cn.error.log;location /static &#123;alias /data/web/static;index index.html index.htm;&#125;location /download &#123;alias /data/web/download;index index.html index.htm;&#125;location /group1/M00 &#123;################Fastdfs服务部署在多台服务器时使用###################### proxy_next_upstream http_502 http_504 error timeout invalid_header;## proxy_cache http-cache; ## proxy_cache_valid 200 304 12h; ## proxy_cache_key $uri$is_args$args; ## proxy_pass http://fdfs_group1; ## expires 30d; #######################################################################alias /data/fastdfs/data;set $image_root "/data/fastdfs/data";if ($uri ~ "/([a-zA-Z0-9]+)/([a-zA-Z0-9]+)/([a-zA-Z0-9]+)/([a-zA-Z0-9]+)/(.*)") &#123;set $image_dir "$image_root/$3/$4/";set $image_name "$5";set $file "$image_dir$image_name";&#125;if (!-f $file) &#123;# 关闭lua代码缓存，方便调试lua脚本#lua_code_cache off;content_by_lua_file "/usr/local/nginx/lua/fastdfs.lua";#content_by_lua_file "/usr/local/nginx/lua/convert.lua";&#125;# ngx_fastdfs_module;&#125;location /group2/M00 &#123;################Fastdfs服务部署在多台服务器时使用###################### proxy_next_upstream http_502 http_504 error timeout invalid_header;## proxy_cache http-cache; ## proxy_cache_valid 200 304 12h; ## proxy_cache_key $uri$is_args$args; ## proxy_pass http://fdfs_group2; ## expires 30d; #######################################################################alias /data/fastdfs02/data;set $image_root "/data/fastdfs02/data";if ($uri ~ "/([a-zA-Z0-9]+)/([a-zA-Z0-9]+)/([a-zA-Z0-9]+)/([a-zA-Z0-9]+)/(.*)") &#123;set $image_dir "$image_root/$3/$4/";set $image_name "$5";set $file "$image_dir$image_name";&#125;if (!-f $file) &#123;# 关闭lua代码缓存，方便调试lua脚本#lua_code_cache off;content_by_lua_file "/usr/local/nginx/lua/fastdfs.lua";&#125;# ngx_fastdfs_module;&#125;###清除缓存模块###location ~ /purge(/.*) &#123;allow 127.0.0.1;allow 10.0.77.0/24;deny all;proxy_cache_purge http-cache $1$is_args$args;&#125;&#125;修改mod_fastdfs.conf文件（注意有两个group，Fastdfs服务部署在多台服务器时，要改成对应的group_name和 ip ）：1vi /etc/fdfs/mod_fastdfs.conf参考如下配置：1234567891011121314151617181920connect_timeout=2network_timeout=30base_path=/data/fastdfsload_fdfs_parameters_from_tracker=truestorage_sync_file_max_delay = 86400use_storage_id = falsestorage_ids_filename = storage_ids.conftracker_server=10.0.77.77:22122storage_server_port=23000group_name=group1url_have_group_name = truestore_path_count=1store_path0=/data/fastdfslog_level=infolog_filename=/var/log/mod_fastdfs.logresponse_mode=proxyif_alias_prefix=flv_support = trueflv_extension = flvgroup_count = 0上传文件测试：12[root@HY-DFS-01 ~]# fdfs_upload_file /etc/fdfs/client.conf /root/hello.txtgroup1/M00/00/00/ChtgcVg_yJOAaqgGAAAADVFCBW8206.txthttp访问测试：http://10.0.77.77:8080/group1/M00/00/00/ChtgcVg_yJOAaqgGAAAADVFCBW8206.txt开机自启动：123chkconfig --add fdfs_storagedchkconfig fdfs_storaged onecho '/usr/local/nginx/sbin/nginx' &gt;&gt;/etc/rc.local验证：1234567891011[root@HY-DFS-04 ~]# chkconfig --list|grep 3:on|grep fdfsfdfs_storaged 0:off 1:off 2:on 3:on 4:on 5:on 6:off[root@HY-DFS-04 ~]# cat /etc/rc.local#!/bin/sh# This script will be executed *after* all the other init scripts.# You can put your own initialization stuff in here if you don't# want to do the full Sys V style init stuff.touch /var/lock/subsys/local/usr/sbin/ntpdate time.nist.gov 210.72.145.44 64.147.116.229/usr/local/nginx/sbin/nginxTracker的安装：12345git clone https://github.com/happyfish100/libfastcommon.git tar -zxf libfastcommon.tar.gzcd libfastcommon./make.sh./make.sh install1mkdir -p /data/fastdfs/tracker123sed -i 's#/usr/local/bin/#/usr/bin/#g' /etc/init.d/fdfs_trackerdcd /etc/fdfscp tracker.conf.sample tracker.conf修改配置文件：1vi tracker.conf参考如下信息：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950disabled=falsebind_addr=port=22122connect_timeout=30network_timeout=60base_path=/data/fastdfs/trackermax_connections=256accept_threads=1work_threads=4store_lookup=2store_group=group2store_server=0store_path=0download_server=0reserved_storage_space = 10%log_level=inforun_by_group=run_by_user=allow_hosts=*sync_log_buff_interval = 10check_active_interval = 120thread_stack_size = 64KBstorage_ip_changed_auto_adjust = truestorage_sync_file_max_delay = 86400storage_sync_file_max_time = 300use_trunk_file = falseslot_min_size = 256slot_max_size = 16MBtrunk_file_size = 64MBtrunk_create_file_advance = falsetrunk_create_file_time_base = 02:00trunk_create_file_interval = 86400trunk_create_file_space_threshold = 20Gtrunk_init_check_occupying = falsetrunk_init_reload_from_binlog = falsetrunk_compress_binlog_min_interval = 0use_storage_id = falsestorage_ids_filename = storage_ids.confid_type_in_filename = ipstore_slave_file_use_link = falserotate_error_log = falseerror_log_rotate_time=00:00rotate_error_log_size = 0log_file_keep_days = 0use_connection_pool = falseconnection_pool_max_idle_time = 3600http.server_port=8080http.check_alive_interval=30http.check_alive_type=tcphttp.check_alive_uri=/status.html启动tracker服务：12[root@hy-lb-02 fdfs]# /etc/init.d/fdfs_trackerd startStarting FastDFS tracker server:查看程序运行情况：123[root@hy-lb-02 fdfs]# lsof -i :22122COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEfdfs_trac 11025 root 5u IPv4 105397 0t0 TCP *:22122 (LISTEN)12[root@hy-lb-02 fdfs]# netstat -tunlp|grep 22122tcp 0 0 0.0.0.0:22122 0.0.0.0:* LISTEN 11025/fdfs_trackerd123[root@hy-lb-02 fdfs]# ps -ef|grep fdfsroot 11025 1 0 16:36 ? 00:00:00 /usr/bin/fdfs_trackerd /etc/fdfs/tracker.confroot 11036 10358 0 16:37 pts/1 00:00:00 grep fdfs图床服务如需搬迁，可在别的服务器部署完Fastdfs后，把/data/fastdfs/data/目录拷贝到新服务器上Fastdfs服务指定的路径，并修改相应配置文件的域名，ip及文件路径信息：tracker.conf文件：1base_path=/data/fastdfs/trackerclient.conf：12base_path=/data/fastdfstracker_server=10.0.77.77:22122storage.conf文件：123base_path=/data/fastdfsstore_path0=/data/fastdfstracker_server=10.0.77.77:22122nginx配置文件： mod_fastdfs.conf文件：123tracker_server=10.0.77.77:22122url_have_group_name = truestore_path0=/data/fastdfs]]></content>
      <categories>
        <category>技术</category>
        <category>图床</category>
      </categories>
      <tags>
        <tag>Fastdfs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用office办公软件实现一对一群发邮件功能]]></title>
    <url>%2F2022%2F09f0f05856.html</url>
    <content type="text"><![CDATA[这里介绍运用office办公软件实现一对一邮件群发功能，也就是说你群发的邮件里面即使有好几十个收件人，收件人收到的你的邮件 他只会看到你的邮件只是发给他一个人，其余的收件人，他是无法看到的，所以邮件是一对一发送的。1.在Outlook登录自己要发送邮件的邮箱账号:2.打开Word文档，选择邮件-开始合并邮件–电子邮件：3.编写邮件内容：4.新建Excel表：填好变量:5.回到Word文档，关联刚创建的Excel表：在Word文档中选择邮件–选择收件人–使用现有列表然后在弹出框中选择刚刚新建的Excel表格，再选Sheet1 6.把光标停在需要使用变量的地方，点击插入合并域，选择需要插入的变量:可以点击预览效果查看效果:7.选择右侧的 完成并合并–发送电子邮件。在弹出框中选择好收件人（Excel表中的邮箱地址），填好主题，单击确定，便可将邮件群发至刚刚在Excel表中填写好的邮箱地址。]]></content>
      <categories>
        <category>桌面</category>
        <category>邮箱</category>
      </categories>
      <tags>
        <tag>群发邮件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac设置共享文件夹]]></title>
    <url>%2F2022%2F0920cd4380.html</url>
    <content type="text"><![CDATA[当我们在同一个局域网中有多台电脑，我们需要访问其他电脑上的文件的时候，我们就可以使用文件共享的功能。1.打开mac，新建一个文件夹专门用于存放需要共享的文件2.点击左上角苹果图标&gt;系统偏好设置，如下图：3.点击用户与群组，如下图：4.点击左下角锁头按钮，在弹出的窗口中输入本机密码，并确认以解锁，如下图：5.单击左下角的+，弹出创建用户窗口，输入用户名和密码后，单击创建用户，如下图：6.稍等一会就会看到新创建的用户了，如下图：7.单击左上角的 &lt; 按钮返回，单击共享，如下图：8.勾选左边的文件共享，如下图：9.在共享文件夹下单击+，选择之前设置好的共享文件夹添加进来，如下图： 10.单击用户下的+，弹出添加用户窗口，选择步骤5-6创建的用户选择进来，如下图：11.点击窗口右边的选项，勾选使用SMB来共享文件和文件夹，勾选要共享的账户，弹出输入共享账户密码窗口，输入密码后单击完成，则该共享账户拥有了权限。如下图：12.打开terminal，输入ifconfig，记录本机IP地址，如下图：13.此时可以告知使用win的同事，在他们的电脑上按住Windows+R，输入共享目录IP地址：\192.168.X.X（替换为共享文件所在mac的IP地址即可）回车，输入共享账号及其密码，点击确定即可进入共享目录，如下图：]]></content>
      <categories>
        <category>桌面</category>
        <category>Mac</category>
      </categories>
      <tags>
        <tag>共享</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos7部署phabricator服务]]></title>
    <url>%2F2022%2F09d5dcd440.html</url>
    <content type="text"><![CDATA[Phabricator是一套基于Web的软件开发协作工具，包括代码审查工具Differential，资源库浏览器Diffusion，变更监测工具Herald，Bug跟踪工具Maniphest和维基工具Phriction。Phabricator可与Git、Mercurial、Subversion集成使用。环境：首先搭建LNMP环境：1.安装MySQL1.1安装（需要5.5或更新的版本）：123#wget http://dev.mysql.com/get/mysql-community-release-el7-5.noarch.rpm#rpm -ivh mysql-community-release-el7-5.noarch.rpm#yum install mysql-community-server安装成功后重启mysql服务：1#service mysqld restart首次登录没有密码：1#mysql -u root设置密码，无需重启即可生效：1mysql&gt; set password for 'root'@'localhost' =password('password');2.安装PHP需要5.5或更新的版本2.1编译源码：新建一个目录,下载源码并解压：1234#mkdir /twy#cd /twy#wget http://cn2.php.net/distributions/php-5.6.30.tar.gz#tar zxvf php-5.6.30.tar.gz安装依赖库：12345678910#yum install gcc automake autoconf libtool make –y #yum install gcc gcc-c++ glibc –y #yum install libmcrypt-devel mhash-devel libxslt-devel –y #yum install libjpeg libjpeg-devel libpng libpng-devel freetype freetype-devel libxml2 libxml2-devel –y #yum install zlib zlib-devel glibc glibc-devel glib2 glib2-devel bzip2 bzip2-devel –y #yum install ncurses ncurses-devel curl curl-devel e2fsprogs e2fsprogs-devel -y#yum install php-devel #yum install krb5 krb5-devel libidn libidn-devel openssl openssl-devel -y #yum install mysql-devel（防止之后需要安装mysql扩展时找不到mysql.h）#yum install -y openssl openssl-devel在解压目录执行configure后执行make install：1234#cd /twy/php-5.6.30#./configure --prefix=/usr/local/php-5.6.30 --enable-fpm --with-mcrypt --enable-mbstring --disable-pdo --enable-pdo --with-curl --disable-debug --disable-rpath --enable-inline-optimization --with-bz2 --with-zlib --enable-sockets --enable-sysvsem --enable-sysvshm --enable-pcntl --enable-mbregex --with-mhash --enable-zip --with-pcre-regex --with-mysql --with-mysqli --with-gd --with-jpeg-dir -with-gettext --enable-xml --enable-freetype --with-openssl --enable-apcu --enable-opcache --with-mysql=mysqlnd --with-mysqli=mysqlnd --with-pdo-mysql=mysqlnd#make#make install2.2 php.ini配置首先将刚才解压的文件夹中将php.ini-development拷贝到/usr/local/php/lib目录并命名为php.ini：1#cp /twy/php-5.6.30/php.ini-development /usr/local/php-5.6.30/lib/php.ini打开配置文件1vim /usr/local/php-5.6.30/lib/php.ini修改以下内容：123always_populate_raw_post_data = -1post_max_size = 32Mdate.timezone =Asia/Shanghai2.3 php-fpm.conf配置首先将/usr/local/php-5.6.30/etc/php-fpm.conf.default拷贝一份并命名为php-fpm.conf:12#cd /usr/local/php-5.6.30 #cp etc/php-fpm.conf.default etc/php-fpm.conf接着对php-fpm.conf做如下更改（ 注意：不能另外加这几句，只能在原语句上去掉注释后修改）：12345;pid = run/php-fpm.pid这一句取消注释，并改为：pid = /usr/local/php-5.6.30/var/run/php-fpm.pid；;error_log = log/php-fpm.log这一句取消注释，并改为：error_log = /usr/local/php-5.6.30/var/log/error-log.loguser = nobody改为：user = nginxgroup = nobody改为：group = nginx;catch_workers_output = yes这一句取消注释，改为：catch_workers_output = yes2.4 启动PHP：1#/usr/local/php-5.6.30/sbin/php-fpm查看端口是否在监听：12[root@localhost nginx]# netstat -lntp | grep 9000 tcp 0 0 127.0.0.1:9000 0.0.0.0:* LISTEN 16124/php-fpm: pool关闭php-fpm：1kill -INT `cat /usr/local/php5/var/run/php-fpm.pid`或者1pkill php-fpmps：可以用PHP -v看下现在用的是不是你安装的php,有时用的是系统自带的，启动phabricator后还是不能查看那个访问页面。执行这条语句可以使用自己安装的：1ln -s /usr/local/php-5.6.30/bin/php /usr/bin/php3.安装nginx3.1 gcc 安装安装 nginx 需要先将官网下载的源码进行编译，编译依赖 gcc 环境，如果没有 gcc 环境，则需要安装：1yum install -y gcc-c++3.2 PCRE pcre-devel 安装PCRE(Perl Compatible Regular Expressions) 是一个Perl库，包括 perl 兼容的正则表达式库。nginx 的 http 模块使用 pcre 来解析正则表达式，所以需要在 linux 上安装 pcre 库，pcre-devel 是使用 pcre 开发的一个二次开发库。nginx也需要此库。1yum install -y pcre pcre-devel3.3 zlib 安装zlib 库提供了很多种压缩和解压缩的方式， nginx 使用 zlib 对 http 包的内容进行 gzip ，所以需要在 Centos 上安装 zlib 库。1yum install -y zlib zlib-devel3.4 OpenSSL 安装OpenSSL 是一个强大的安全套接字层密码库，囊括主要的密码算法、常用的密钥和证书封装管理功能及 SSL 协议，并提供丰富的应用程序供测试或其它目的使用。nginx 不仅支持 http 协议，还支持 https（即在ssl协议上传输http），所以需要在 Centos 安装 OpenSSL 库。1yum install -y openssl openssl-devel3.5下载nginx并编译123456#wget -c https://nginx.org/download/nginx-1.10.1.tar.gz#tar -zxvf nginx-1.10.1.tar.gz#cd nginx-1.10.1#./configure#make#make installnginx常用命令：12345678nginx -s stop 快速关闭Nginx，可能不保存相关信息，并迅速终止web服务。nginx -s quit 平稳关闭Nginx，保存相关信息，有安排的结束web服务。nginx -s reload 因改变了Nginx相关配置，需要重新加载配置而重载。nginx -s reopen 重新打开日志文件。nginx -c filename 为 Nginx 指定一个配置文件，来代替缺省的。nginx -t 不运行，而仅仅测试配置文件。nginx 将检查配置文件的语法的正确性，并尝试打开配置文件中所引用到的文件。nginx -v 显示 nginx 的版本。nginx -V 显示 nginx 的版本，编译器版本和配置参数3.6配置nginx：默认配置文件是nginx.conf,可以在这里面配置，但是如果把所有内容都放在nginx.conf里就太乱了，我们可以把其他的配置内容放在conf.d目录下。只要你的默认配置中把conf.d目录include进去就行了。 在conf.d中新建一个pha.example.com.conf，内容如下：server { server_name phabricator.example.com;#域名 listen 端口（记得加到防火墙里）; root 你下载phabricator的目录/phabricator/webroot; location / { index index.php; rewrite ^/(.*)$ /index.php?__path__=/$1 last; } location /index.php { fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; #required if PHP was built with --enable-force-cgi-redirect fastcgi_param REDIRECT_STATUS 200; #variables to make the $_SERVER populate in PHP fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; fastcgi_param QUERY_STRING $query_string; fastcgi_param REQUEST_METHOD $request_method; fastcgi_param CONTENT_TYPE $content_type; fastcgi_param CONTENT_LENGTH $content_length; fastcgi_param SCRIPT_NAME $fastcgi_script_name; fastcgi_param GATEWAY_INTERFACE CGI/1.1; fastcgi_param SERVER_SOFTWARE nginx/$nginx_version; fastcgi_param REMOTE_ADDR $remote_addr; } } 启动nginx1#/usr/sbin/nginx -c nginx.conf所在的目录/nginx.conf将phabricator下载到一个目录下，配置nginx和mysql会用到这个目录。123somewhere/ $ git clone https://github.com/phacility/libphutil.gitsomewhere/ $ git clone https://github.com/phacility/arcanist.gitsomewhere/ $ git clone https://github.com/phacility/phabricator.git4.1 phabricator连通mysql到phabricator的安装目录下：123456./bin/config set mysql.host 'localhost'./bin/config set mysql.port '3306'./bin/config set mysql.user 'root'./bin/config set mysql.pass 'root的密码'./bin/storage upgrade5.邮件设置5.1 配置邮件发送来源1bin/config set metamta.default-address admin@example.com5.2 生成 mailers.json 文件：123456789101112131415#cat &lt;&lt; EOF &gt; mailers.json[ &#123; "key": "stmp-mailer", "type": "smtp", "options": &#123; "host": "smtp.exmail.qq.com", "port": 465, "user": "admin@example.com", "password": "admin", "protocol": "ssl" &#125; &#125;]EOF5.3 导入邮件 smtp 配置1bin/config set cluster.mailers --stdin &lt; mailers.json5.4 发送测试邮件：1234#bin/mail send-test --to lb@example.com --subject hello &lt; mailers.jsonReading message body from stdin...Mail sent! You can view details by running this command:phabricator/ $ ./bin/mail show-outbound --id 126.设置管理员和认证方式这时，浏览器应该可以访问Phabricator了： http://mydomain.com作为第一个访问用户，可创建管理员账号。注意，管理员不是标准用户。如果能创建管理员，说明安装过程成功。这时可以添加认证方式（Auth Provider）。phabricator提供了多种认证方式，其中最基本的是用户名／密码的provider。作为管理员，在主页左侧栏选择Auth，访问Auth，选择Add Provider ，然后添加 Username/Password Provider即可。7.Phabricator中文汉化进入phabricator/src/extensions/目录下，rm -fr README，再执行下面git命令，然后在客户端设置里，选择语言中文即可．1git clone https://github.com/wanthings/phabricator-zh_CN.git ./8.配置完成后，进入web页面左上角有很多杂项，可根据提示修改即可：至此，Phabricator服务部署完成。]]></content>
      <categories>
        <category>技术</category>
        <category>phabricator</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>phabricator</tag>
        <tag>code review</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[远程桌面连接失败]]></title>
    <url>%2F2022%2F0929799bd2.html</url>
    <content type="text"><![CDATA[问题表现：远程桌面连接失败，显示出现了内部错误。解决方法：方法1：管理员身份运行cmd输入命令：netsh winsock reset重启电脑方法2：换个WiFi连接方法3：重启远程服务器]]></content>
      <categories>
        <category>桌面</category>
      </categories>
      <tags>
        <tag>插件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[谷歌浏览器安装插件]]></title>
    <url>%2F2022%2F09fc82d9ba.html</url>
    <content type="text"><![CDATA[问题表现：直接拖拽插件文件至谷歌浏览器扩展程序处安装失败。解决方法：1、 将XX.crx插件的扩展名改成.zip或者.rar并解压到文件夹XX；2、在扩展程序界面（chrome://extensions/或更多工具-扩展程序）右上方的开发者模式按钮上打勾；3、在左上方，点击“加载已解压的扩展程序”，选择第一步中的文件夹XX（注意是文件夹），即可安装。]]></content>
      <categories>
        <category>桌面</category>
      </categories>
      <tags>
        <tag>插件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[win10系统插入耳机没有声音]]></title>
    <url>%2F2022%2F095a7458cf.html</url>
    <content type="text"><![CDATA[1.下载安装并打开Realtek高清晰音频管理器：控制面板-查看方式：小图标，找到并打开Realtek高清晰音频管理器2点击右上角文件夹：3.禁用前面板插孔检测勾上：]]></content>
      <categories>
        <category>桌面</category>
        <category>故障</category>
      </categories>
      <tags>
        <tag>耳机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac设置静态IP]]></title>
    <url>%2F2022%2F099bc44327.html</url>
    <content type="text"><![CDATA[问题表现：Mac笔记本设置静态IP地址显示：无效的服务器地址 BasicIPv6ValidationError解决方法：打开终端：123networksetup -setv6off Ethernet（以太网名称）#关闭ipv6networksetup -setmanual Ethernet 192.168.31.2 255.255.255.0 192.168.1.1 #设置IP地址，对应IP地址、子网掩码、网关]]></content>
      <categories>
        <category>桌面</category>
        <category>Mac</category>
      </categories>
      <tags>
        <tag>静态ip</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux-GitLab安装及汉化]]></title>
    <url>%2F2022%2F099becdeed.html</url>
    <content type="text"><![CDATA[GitLab是一个用于仓库管理系统的开源项目。使用Git作为代码管理工具，并在此基础上搭建起来的Web服务。可通过Web界面进行访问公开的或者私人项目。它拥有GitHub类似的功能，能够浏览源代码，管理缺陷和注释。可以管理团队对仓库的访问，它非常易于浏览提交过的版本并提供一个文件历史库。团队成员可以利用内置的简单聊天程序（Wall）进行交流。它还提供了一个代码片段收集功能可以轻松实现代码复用。常用的网站：官网： https://about.gitlab.com/国内镜像： https://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/Gitlab相关操作及说明：1234567891011121314151617/etc/gitlab/gitlab.rb #gitlab配置文件/opt/gitlab #gitlab的程序安装目录/var/opt/gitlab #gitlab目录数据目录/var/opt/gitlab/git-data #存放仓库数据gitlab-ctl reconfigure #重新加载配置gitlab-ctl status #查看当前gitlab所有服务运行状态gitlab-ctl stop #停止gitlab服务gitlab-ctl stop nginx #单独停止某个服务gitlab-ctl tail #查看所有服务的日志Gitlab的服务构成：nginx： 静态web服务器gitlab-workhorse 轻量级反向代理服务器logrotate 日志文件管理工具postgresql 数据库redis 缓存数据库sidekiq 用于在后台执行队列任务（异步执行）安装环境：：（1）CentOS 6或者7 （此处使用7）（2）2G内存（实验）生产（至少4G），不然会很卡（3）安装包：gitlab-ce-10.2.2-ce（4）禁用防火墙，关闭selinux安装步骤：（1）安装软件123[root@gitlab ~]# yum install -y curl policycoreutils-python openssh-server #安装依赖[root@gitlab ~]# wget https://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el7/gitlab-ce-10.2.2-ce.0.el7.x86_64.rpm #下载软件包[root@gitlab ~]# rpm -ivh gitlab-ce-10.2.2-ce.0.el7.x86_64.rpm #安装gitlab（2）根据安装完成提示界面进行访问URL更改及重新加载配置文件 更改次选项为自己的域名或者IPexternal_url ‘ http://gitlab.example.com’123[root@gitlab ~]# vim /etc/gitlab/gitlab.rb #编辑配置文件 external_url 'http://192.168.1.21' #改为自己的IP地址[root@gitlab ~]# gitlab-ctl reconfigure #重新加载配置文件（3）重装完成访问http://192.168.1.21，会首先叫更改密码（root用户），改完后登录。如下界面：（4）汉化1、下载汉化补丁12[root@gitlab ~]# git clone https://gitlab.com/xhang/gitlab.git[root@gitlab ~]# cd gitlab2、查看全部分支版本1[root@gitlab ~]# git branch -a3、对比版本、生成补丁包1[root@gitlab ~]# git diff remotes/origin/10-2-stable remotes/origin/10-2-stable-zh &gt; /tmp/10.2.2-zh.diff4、停止服务器1[root@gitlab ~]# gitlab-ctl stop5、打补丁1[root@gitlab ~]# patch -d /opt/gitlab/embedded/service/gitlab-rails -p1 &lt; /tmp/10.2.2-zh.diff6、启动和重新配置12[root@gitlab ~]# gitlab-ctl start[root@gitlab ~]# gitlab-ctl reconfigure说明：这里如果使用的同样是gitlab10.2.2，下载汉化较慢的话，可以直接在这里下载10.2.2-zh.diff： https://pan.baidu.com/share/init?surl=wsUy66e4JW1wwEu7irYTSg 提取码：kaiw汉化完成后再次刷新页面如下：]]></content>
      <categories>
        <category>技术</category>
        <category>Gitlab</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Gitlab</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS 7 安装supervisor]]></title>
    <url>%2F2022%2F0918dcecaf.html</url>
    <content type="text"><![CDATA[1.什么是supervisorsupervisor是用python写的一个进程管理工具，用来启动，重启，关闭进程。环境：2.安装CentOS7扩展软件仓库：1yum install -y epel-release3.安装supervisor：1yum install -y supervisor查看安装了哪些文件以及所在的路径：1rpm -ql supervisor4.修改并定义ini文件：4.1找到supervisor 的配置的文件，可以使用例如： whereis supervisord.conf 或者 find / -name supervisor* 命令进行查询4.2修改配置文件：1vim /etc/supervisor/supervisord.conf4.3将配置文件最后一行修改ini 文件路径，意为：supervisor每次update都会加载这个自定义路径文件夹下的所有后缀为ini的文件。4.4 创建ini文件，加入supervisor的进程管理中， 实现服务器后台运行123456789101112131415[program:exmaple] # example是在supercisor中的进程名，随便取啥command=/usr/local/bin/gunicorn -w 4 -b 127.0.0.1:91 run:app #上述代码为使用gunicorn 运行flask。run是你的文件名 ：app指的是app = Flask(__name__)此处的app或者是command=/usr/local/bin/python3 /root/test/test.pynumprocs=1 ; number of processes copies to start (def 1) autostart = true ; 在 supervisord 启动的时候也自动启动 startsecs = 1 ; 启动 1 秒后没有异常退出，就当作已经正常启动了 autorestart = true ; 程序异常退出后自动重启 startretries = 3 ; 启动失败自动重试次数，默认是 3 user = root ; 用哪个用户启动 redirect_stderr = true ; 把 stderr 重定向到 stdout，默认 false stdout_logfile_maxbytes = 20MB ; stdout 日志文件大小，默认 50MB stdout_logfile_backups = 10 ; stdout 日志文件备份数 stdout_logfile=/**自定义路径/gunicorn.log ; log 日志stderr_logfile=/**自定义路径/gunicorn.error ; 错误日志的绝对路径5.启动supervisor：1supervisord -c /etc/supervisor/supervisord.conf关闭supervisor：1supervisorctl shutdown重启supervisor服务1supervisorctl restart6.管理Supervisor服务12345678910#开启所有supervisorctl start all#开启单个(配置文件里你起得进程名字--&gt;[program:myProgram])supervisorctl start myProgram#关闭所有supervisorctl stop all #关掉其中一个supervisorctl stop example #查看Supervisor 服务状态supervisorctl status6.2 修改了Supervisor ini文件或者conf文件配置后，需要更新服务：12supervisorctl update # 更新supervisor的配置supervisorctl reload # 重新加载项目到内存中7.查看Superviso日志1tail -f /var/log/supervisor/supervisord.log至此，supervisor安装完成。]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>supervisor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS 7部署frp]]></title>
    <url>%2F2022%2F096f50b0b4.html</url>
    <content type="text"><![CDATA[frp 是一个可用于内网穿透的高性能的反向代理应用，支持 tcp, udp, http, https 协议。 本文将基于 frp 来实现内网穿透。frp 的作用：1.利用处于内网或防火墙后的机器，对外网环境提供 http 或 https 服务。2.对于 http, https 服务支持基于域名的虚拟主机，支持自定义域名绑定，使多个域名可以共用一个80端口。3.利用处于内网或防火墙后的机器，对外网环境提供 tcp 和 udp 服务，例如在家里通过 ssh 访问处于公司内网环境内的主机。在具有公网IP的服务器或VPS上安装运行 frp 的服务端程序frps，并在处于内网的目标主机上面安装运行 frp 的客户端程序 frpc ，然后 User 就可以通过公网服务器来实现内网穿透从而访问内网主机。环境：配置 ssh 访问内网机器1.下载安装包：在公网服务器和内网机器上都要下载安装包并解压：12$ wget https://github.com/fatedier/frp/releases/download/v0.21.0/frp_0.21.0_linux_amd64.tar.gz$ tar -xzvf frp_0.21.0_linux_amd64.tar.gz2.在公网服务器上配置并启动：修改配置文件修改配置文件 $ vi frps.ini，如下：12[common]bind_port = 3389 #frp服务端端口（必须）配置很简单，然后启动：1$ nohup ./frps -c frps.ini &amp;查看 nohup.out 的信息，success：123456789# tail -f nohup.out 2018/09/14 05:33:15 [I] [service.go:130] frps tcp listen on 0.0.0.0:70002018/09/14 05:33:15 [I] [root.go:207] Start frps success2018/09/14 05:49:47 [I] [service.go:130] frps tcp listen on 0.0.0.0:70002018/09/14 05:49:47 [I] [root.go:207] Start frps success2018/09/14 06:28:59 [I] [service.go:319] client login info: ip [125.71.219.33:37092] version [0.21.0] hostname [] os [linux] arch [amd64]2018/09/14 06:28:59 [I] [proxy.go:217] [93eec0dde173fc68] [ssh] tcp proxy listen port [6000]2018/09/14 06:28:59 [I] [control.go:335] [93eec0dde173fc68] new proxy [ssh] success3.在内网机器上配置并启动：修改配置文件 $ vi frpc.ini，如下：1234567891011121314151617181920212223[common]server_addr = 0.0.0.0 #frp服务端地址，可以填ip或者域名，这里假设为0.0.0.0server_port = 3389 #frp服务端端口，即填写服务端配置中的 bind_port[ssh]type = tcp #连接类型，填tcp或udplocal_ip = 127.0.0.1 #填127.0.0.1或内网ip都可以local_port = 22 #需要转发到的端口，ssh端口是22remote_port = 6000 #frp服务端的远程监听端口，即你访问服务端的remote_port就相当于访 #问客户端的 local_port，如果填0则会随机分配一个端口 [jumpserver_web]type = tcplocal_ip = 127.0.0.1local_port = 81remote_port = 62226[jumpserver_ssh]type = tcplocal_ip = 127.0.0.1local_port = 2222remote_port = 2222启动客户端程序：：1$ nohup ./frpc -c frpc.ini &amp;查看 nohup.out 的信息，success：1234567$ tail -f nohup.out2018/09/14 14:28:58 [I] [proxy_manager.go:300] proxy removed: []2018/09/14 14:28:58 [I] [proxy_manager.go:310] proxy added: [ssh]2018/09/14 14:28:58 [I] [proxy_manager.go:333] visitor removed: []2018/09/14 14:28:58 [I] [proxy_manager.go:342] visitor added: []2018/09/14 14:28:59 [I] [control.go:246] [93eec0dde173fc68] login to server success, get run id [93eec0dde173fc68], server udp port [0]2018/09/14 14:29:00 [I] [control.go:169] [93eec0dde173fc68] [ssh] start proxy success4.登录：1ssh -oPort=6000 username@server_addr或者1ssh -p 6000 username@server_addr上面登录使用的 username 是内网机器的用户名，server_addr是公网服务器的IP，port 6000就是设置的 remote_port，最后的登录密码是内网机器的密码，而不是公网机器的密码，这一点一定要注意。问题解决:在启动服务端和客户端程序之后，可能发现还是无法登录到内网内网机器，在内网机器上面执行 tail -f nohup.out 查看启动命令的执行结果，可以发现以下的问题：12345$ tail -f nohup.out 2018/09/14 14:11:02 [I] [proxy_manager.go:333] visitor removed: []2018/09/14 14:11:02 [I] [proxy_manager.go:342] visitor added: []2018/09/14 14:13:09 [W] [control.go:113] login to server failed: dial tcp xxx.xxx.xxx.xxx:7000: connect: connection timed outdial tcp xxx.xxx.xxx.xxx:7000: connect: connection timed out仔细检查了一下，发现是公网服务器防火墙的原因，没有允许对应端口的流量通过，所以需要配置防火墙：123#firewall-cmd --zone=public --add-port=3389/tcp --permanent#firewall-cmd --zone=public --add-port=6000/tcp --permanent#firewall-cmd --reload阿里云服务器需要在esc管理中配置安全组规则中添加3389，6000端口]]></content>
      <categories>
        <category>技术</category>
        <category>frp</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>frp</tag>
        <tag>内网穿透</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openVPN客户端连接]]></title>
    <url>%2F2022%2F09776c38cc.html</url>
    <content type="text"><![CDATA[上一章已经部署好了openVPN服务器，现在我们说一下客户端如何连接服务器从而访问内网。Windows连接：下载OpenVPN GUI客户端：链接: https://pan.baidu.com/s/161nsft8FnVMz6vY_Xp0MXQ 提取码: qect默认下一步就可以安装完成（如果自定义安装的话要记住安装路径，一会会用到）。安装完成后，首先进入到安装目录下，默认安装目录为C:\Program Files\OpenVPN/config,创建一个ovpn后缀的配置文件：输入以下内容：123456789101112131415clientdev tunproto udpremote **** * #公网ip 端口resolv-retry infinitenobindpersist-keypersist-tun#ca ca.crtcomp-lzoverb 3auth-user-pass login.txt&lt;ca&gt;###服务器ca证书###&lt;/ca&gt;再在同目录下创建一个login.txt的文件，第一行写用户名，第二行写密码。然后运行Openvpn GUI客户端，在右下角任务栏会有图标:鼠标右键，连接：连接成功后，图标为绿色：Linux服务器连接：安装openVPN，前两章已经有教程，这里不再赘述。创建一个openvpn的文件夹并进入：12mkdir /etc/openvpncd /etc/openvpn/创建客户端配置文件1vim client.conf填写以下内容：123456789101112131415clientdev tunproto udpremote *.*.*.* * #公网IP和端口resolv-retry infinitenobindpersist-keypersist-tun#ca ca.crtcomp-lzoverb 3auth-user-pass login.txt&lt;ca&gt;####服务器ca证书####&lt;/ca&gt;再在同目录下创建一个login.txt的文件，第一行写用户名，第二行写密码。启动openvpn客户端：1[root@localhost ~]# openvpn --daemon --writepid /var/run/openvpn.pid --cd /etc/openvpn --config client.conf --script-security 2查看openvpn是否在运行：ping内网服务看是否能通：至此，openVPN客户端连接完成。]]></content>
      <categories>
        <category>技术</category>
        <category>openVPN</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>openVPN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openVPN通过用户名密码认证]]></title>
    <url>%2F2022%2F0964de3690.html</url>
    <content type="text"><![CDATA[紧接上一篇，让openvpn读取本地文件中的用户名密码，通过判断用户名密码是否存在文件中进行认证，搭建openvpn环境就不多做说明了，只要把openvpn搭建好，客户端能够连接就可以了。1、修改openvpn配置文件1vi /etc/server.conf编辑/etc/server.conf文件，并添加如下内容：123auth-user-pass-verify /etc/openvpn/checkpsw.sh via-envclient-cert-not-required username-as-common-name例如：12345678910111213141516171819202122[root@openVPN openvpn-2.0.9]# cat /etc/server.conf port 1194proto udpdev tunca /opt/tools/openvpn-2.0.9/easy-rsa/2.0/keys/ca.crtcert /opt/tools/openvpn-2.0.9/easy-rsa/2.0/keys/server.crtkey /opt/tools/openvpn-2.0.9/easy-rsa/2.0/keys/server.keydh /opt/tools/openvpn-2.0.9/easy-rsa/2.0/keys/dh1024.pemserver 10.8.0.0 255.255.255.0push "route 172.16.0.0.0 255.255.255.0"ifconfig-pool-persist ipp.txtkeepalive 10 120comp-lzopersist-keypersist-tunstatus openvpn-status.logverb 5####openvpn authenticated with user/passauth-user-pass-verify /etc/openvpn/checkpsw.sh via-envclient-cert-not-required username-as-common-name2、下载用户验证脚本checkpsw.sh文件的官方下载地址是： http://openvpn.se/files/other/checkpsw.sh但是可能现在已经无法下载了，如果无法下载就把下面的内容拷贝到一个文件中，然后改名为checkpw.sh即可，当然网上有很多地方有这个脚本，所以脚本的下载不是什么问题。123456789101112131415161718192021222324252627282930313233[root@openvpn ~]# cat checkpsw.sh #!/bin/sh############################################################ checkpsw.sh (C) 2004 Mathias Sundman &lt;mathias@openvpn.se&gt;## This script will authenticate Openvpn users against# a plain text file. The passfile should simply contain# one row per user with the username first followed by# one or more space(s) or tab(s) and then the password.PASSFILE="/etc/openvpn/psw-file"LOG_FILE="/var/log/openvpn/openvpn-password.log"TIME_STAMP=`date "+%Y-%m-%d %T"`###########################################################if [ ! -r "$&#123;PASSFILE&#125;" ]; then echo "$&#123;TIME_STAMP&#125;: Could not open password file \"$&#123;PASSFILE&#125;\" for reading." &gt;&gt; $&#123;LOG_FILE&#125; exit 1fiCORRECT_PASSWORD=`awk '!/^;/&amp;&amp;!/^#/&amp;&amp;$1=="'$&#123;username&#125;'"&#123;print $2;exit&#125;' $&#123;PASSFILE&#125;`if [ "$&#123;CORRECT_PASSWORD&#125;" = "" ]; then echo "$&#123;TIME_STAMP&#125;: User does not exist: username=\"$&#123;username&#125;\", password=\"$&#123;password&#125;\"." &gt;&gt; $&#123;LOG_FILE&#125; exit 1fiif [ "$&#123;password&#125;" = "$&#123;CORRECT_PASSWORD&#125;" ]; then echo "$&#123;TIME_STAMP&#125;: Successful authentication: username=\"$&#123;username&#125;\"." &gt;&gt; $&#123;LOG_FILE&#125; exit 0fiecho "$&#123;TIME_STAMP&#125;: Incorrect password: username=\"$&#123;username&#125;\", password=\"$&#123;password&#125;\"." &gt;&gt; $&#123;LOG_FILE&#125;exit 13、创建用户名密码文件12345678910111213141516171819cp /root/checkpsw.sh /etc/openvpn/chmod +x /etc/openvpn/checkpsw.sh# 给脚本添加执行权限，并将脚本拷贝到/etc/server.conf中指定的位置mkdir -p /var/log/openvpntouch /var/log/openvpn/openvpn-password.log# 创建目录以及日志文件，用来记录用户名密码认证产生的日志echo "test 123456" &gt;&gt;/etc/openvpn/psw-filechmod 400 /etc/openvpn/psw-file# 创建用户名密码文件，并修改权限[root@openvpn ~]# ll /etc/openvpn/psw-file -r-------- 1 root root 12 Sep 21 02:35 /etc/openvpn/psw-file[root@openvpn ~]# cat /etc/openvpn/psw-file test 1234564、修改客户端配置文件注释掉cert和key（客户端不需要crt和key文件，但是需要服务器的CA证书）12;cert eva.crt;key eva.key添加如下内容：1auth-user-pass例如：123456789101112131415clientdev tunproto udpremote 59.64.15.47 1194 #公网IPresolv-retry infinitenobindpersist-keypersist-tunca ca.crt;cert eva.crt;key eva.keyns-cert-type serverauth-user-passcomp-lzoverb 55、客户端连接测试创建一个新的名为“psw”的连接，里面的内容就是步骤4贴出来的，目录中只需要放服务器端CA证书即可：客户端点击连接之后，弹出窗口，这里输入pws-file文件中的用户名密码：认证通过，成功连接：6、检查日志：查看用户名密码认证产生的日志：123[root@open*** ~]# cat /var/log/openvpn/openvpn-password.log 2016-09-21 02:52:16: Successful authentication: username="test".查看openvpn状态日志：123456789101112[root@open*** open***-2.0.9]# tail -f /opt/tools/open***-2.0.9/easy-rsa/2.0/open***-status.logOpen××× CLIENT LISTUpdated,Wed Sep 21 02:58:31 2016Common Name,Real Address,Bytes Received,Bytes Sent,Connected Sincetest,192.168.49.1:7828,19111,5525,Wed Sep 21 02:52:15 2016ROUTING TABLEVirtual Address,Common Name,Real Address,Last Ref10.8.0.18,test,192.168.49.1:7828,Wed Sep 21 02:52:16 2016GLOBAL STATSMax bcast/mcast queue length,0END]]></content>
      <categories>
        <category>技术</category>
        <category>openVPN</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>openVPN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux部署openVPN以访问server端内网其他服务]]></title>
    <url>%2F2022%2F091f828d1f.html</url>
    <content type="text"><![CDATA[上一章已经部署了open服务，但那仅仅只能访问部署了open server的服务器，要想同时也能访问同网段的其他机器，还需配置一下。目前客户端都已经可以ping通OpenVPN Server的内网IP，那么想要和同在一个网段的其他地址通信，貌似只需要加一条路由就可以了，下面讨论一下让客户端可以同内网服务器IP段通信的方法：方法1、将内网的其他服务器的网关设置为OpenVPN Server的内网IP，即172.16.100.120Windows客户端可以ping通172.16.100.128：方法2、在OpenVPN Server上配置NAT转换，将到10.8.0.0/24的访问都转换为172.16.100.120在Open××× Server上添加一条防火墙规则：12/sbin/iptables -t nat -A POSTROUTING -s 10.8.0.0/24 -o eth1 -j SNAT --to-source 172.16.100.120iptables saveWindows客户端可以ping通172.16.100.128：方法3、在需要访问的内网服务器上配置一条到10.8.0.0段的静态路由：Windows客户端也可以ping通172.16.100.128：可以看到，这种方法也可以实现需求，但是不方便的是每一台内网服务器上都需要添加这样一条路由，操作起来可能稍显麻烦。]]></content>
      <categories>
        <category>技术</category>
        <category>openVPN</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>openVPN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux部署openVPN服务]]></title>
    <url>%2F2022%2F094db9fd21.html</url>
    <content type="text"><![CDATA[由于在家办公需要访问公司内网，故记录一下部署openVPN服务过程。环境：开始部署：1234mkdir -p /opt/toolscd /opt/tools/yum install openssl openssl-devel gcc -y# 因为openVPN依赖于openssl，所以这里需要安装openssl和openssl-devel安装lzo，lzo用于压缩隧道通信数据以加快传输速度123456wget http://www.oberhumer.com/opensource/lzo/download/lzo-2.03.tar.gz #下载lzo包mkdir -p /usr/local/lzotar -zxf lzo-2.03.tar.gzcd lzo-2.03./configure --prefix=/usr/local/lzo/make &amp;&amp; make install开始安装openVPN-2.0.9下载链接：链接: https://pan.baidu.com/s/1XD_pja7Cj4umOJngN92eIQ 提取码: m7dy1234tar -zxf openvpn-2.0.9.tar.gzcd openvpn-2.0.9./configure --with-lzo-headers=/usr/local/lzo/include --with-lzo-lib=/usr/local/lzo/lib --enable-password-savemake &amp;&amp; make installOpen安装完成，下面开始配置openVPN：1、建立CA的相关信息在open的源代码目录下有一个/easy-rsa/2.0目录，我的环境是/opt/tools/open-2.0.9/easy-rsa/2.0/，里面有一个vars文件存有CA的详细信息，这里需要编辑并修改相关配置。12345678910cd /opt/tools/openvpn-2.0.9/easy-rsa/2.0/vim vars #编辑vars修改为你自己的配置信息tail -5 vars #下面是我的配置信息export KEY_COUNTRY="CN" #定义你所在的国家export KEY_PROVINCE="GD" #定义你所在的省份export KEY_CITY="Shenzhen" #定义你所在的城市export KEY_ORG="contoso.com" #表示你所在的组织export KEY_EMAIL="admin@contoso.com" #表示你的邮件地址source vars./clean-all #初始化keys目录，会删除里面的所有文件2、创建根CA证书123456789101112131415161718192021222324252627282930[root@mylinux1 2.0]# ./build-ca Generating a 1024 bit RSA private key......++++++............++++++writing new private key to 'ca.key'-----You are about to be asked to enter information that will be incorporatedinto your certificate request.What you are about to enter is what is called a Distinguished Name or a DN.There are quite a few fields but you can leave some blankFor some fields there will be a default value,If you enter '.', the field will be left blank.-----Country Name (2 letter code) [CN]:State or Province Name (full name) [GD]:Locality Name (eg, city) [Shenzhen]:Organization Name (eg, company) [contoso.com]:Organizational Unit Name (eg, section) []:TechCommon Name (eg, your name or your server's hostname) [contoso.com CA]:Email Address [admin@contoso.com]:[root@mylinux1 2.0]# ll keys/total 12-rw-r--r-- 1 root root 1298 Sep 3 14:50 ca.crt-rw------- 1 root root 916 Sep 3 14:50 ca.key-rw-r--r-- 1 root root 0 Sep 3 14:45 index.txt-rw-r--r-- 1 root root 3 Sep 3 14:45 serial3、创建Diffie-Hellman（dh）文件123456789101112[root@mylinux1 2.0]# ./build-dh Generating DH parameters, 1024 bit long safe prime, generator 2This is going to take a long time..........+...............+........................................................+..................................................+.......+....................................................................+...+......................+....................................+...+.............................................+................................................................+...+..........+................+.....+..................................+.....................................................................+..................+....+...........................................+..............+...+.........................................................................................................................................................................................................................................+...................+.................+......................+...............................................................................................................................................+..............................................................................................................+..............................+..............................................................................+...............+................................+.......................................................................+...................................................+......................+.........................................+........................+..............................................+.........+............................................................................+....................................+.........................................................+.......+.............................+......................................................................+........................................................................................................................................................................................+.....................................................................................................................+................+.......+..........................+..........+................................................................................................................................................................................................+...................+.........................................................................+.............................................................................................................................................+..................................+...........................................................+............................................................................................................................+...........+.................................+.....+........................................................................................................................+........................+............................+..................+..............+...........................................................+.......................................................................................+........................+.................................................................................................................................+.......+................................+...+........................+...............................................................................................+...................................................................+...................+...........+.............................................................................................+..............................................................+.+.......+..+................+..............................................+................................+....+.....................................+..+.......................+............................................++*++*++*[root@mylinux1 2.0]# ll keys/dh*-rw-r--r-- 1 root root 245 Sep 3 14:51 dh1024.pem[root@mylinux1 2.0]# cat keys/dh1024.pem -----BEGIN DH PARAMETERS-----MIGHAoGBAO5vwKo4atqlMpkJfiJhpm2vgHApCiIWLkQAz+m8rj8B6QI+Lu6piSFPdAZ5drGmAZMhErx0zusS8IZRCQnw7kO7E+RTqw93T5PC3QjZhZNXc6RRpFOjLlkX6s+TjnNZS6EIP459i8TorZEVdwLD552duvaks21pLlJI8nOTEQyzAgEC-----END DH PARAMETERS-----4、为服务器生成证书和密钥12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758[root@mylinux1 2.0]# ./build-key-server serverGenerating a 1024 bit RSA private key..........++++++......++++++writing new private key to 'server.key'-----You are about to be asked to enter information that will be incorporatedinto your certificate request.What you are about to enter is what is called a Distinguished Name or a DN.There are quite a few fields but you can leave some blankFor some fields there will be a default value,If you enter '.', the field will be left blank.-----Country Name (2 letter code) [CN]:State or Province Name (full name) [GD]:Locality Name (eg, city) [Shenzhen]:Organization Name (eg, company) [contoso.com]:Organizational Unit Name (eg, section) []:TechCommon Name (eg, your name or your server's hostname) [server]:Email Address [admin@contoso.com]:Please enter the following 'extra' attributesto be sent with your certificate requestA challenge password []:123456An optional company name []:contoso.comUsing configuration from /opt/tools/open***-2.0.9/easy-rsa/2.0/openssl.cnfCheck that the request matches the signatureSignature okThe Subject's Distinguished Name is as followscountryName :PRINTABLE:'CN'stateOrProvinceName :PRINTABLE:'GD'localityName :PRINTABLE:'Shenzhen'organizationName :PRINTABLE:'contoso.com'organizationalUnitName:PRINTABLE:'Tech'commonName :PRINTABLE:'server'emailAddress :IA5STRING:'admin@contoso.com'Certificate is to be certified until Sep 1 06:54:39 2026 GMT (3650 days)Sign the certificate? [y/n]:y1 out of 1 certificate requests certified, commit? [y/n]yWrite out database with 1 new entriesData Base Updated[root@mylinux1 2.0]# ll keys/total 44-rw-r--r-- 1 root root 3972 Sep 3 14:54 01.pem-rw-r--r-- 1 root root 1298 Sep 3 14:50 ca.crt-rw------- 1 root root 916 Sep 3 14:50 ca.key-rw-r--r-- 1 root root 245 Sep 3 14:51 dh1024.pem-rw-r--r-- 1 root root 114 Sep 3 14:54 index.txt-rw-r--r-- 1 root root 21 Sep 3 14:54 index.txt.attr-rw-r--r-- 1 root root 0 Sep 3 14:45 index.txt.old-rw-r--r-- 1 root root 3 Sep 3 14:54 serial-rw-r--r-- 1 root root 3 Sep 3 14:45 serial.old-rw-r--r-- 1 root root 3972 Sep 3 14:54 server.crt-rw-r--r-- 1 root root 761 Sep 3 14:54 server.csr-rw------- 1 root root 916 Sep 3 14:54 server.key5、为客户端生成客户端证书文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263[root@mylinux1 2.0]# ./build-key tomGenerating a 1024 bit RSA private key.............++++++...................++++++writing new private key to 'tom.key'-----You are about to be asked to enter information that will be incorporatedinto your certificate request.What you are about to enter is what is called a Distinguished Name or a DN.There are quite a few fields but you can leave some blankFor some fields there will be a default value,If you enter '.', the field will be left blank.-----Country Name (2 letter code) [CN]:State or Province Name (full name) [GD]:Locality Name (eg, city) [Shenzhen]:Organization Name (eg, company) [contoso.com]:Organizational Unit Name (eg, section) []:TechCommon Name (eg, your name or your server's hostname) [tom]:Email Address [admin@contoso.com]:tom@contoso.comPlease enter the following 'extra' attributesto be sent with your certificate requestA challenge password []:123456An optional company name []:contoso.comUsing configuration from /opt/tools/open***-2.0.9/easy-rsa/2.0/openssl.cnfCheck that the request matches the signatureSignature okThe Subject's Distinguished Name is as followscountryName :PRINTABLE:'CN'stateOrProvinceName :PRINTABLE:'GD'localityName :PRINTABLE:'Shenzhen'organizationName :PRINTABLE:'contoso.com'organizationalUnitName:PRINTABLE:'Tech'commonName :PRINTABLE:'tom'emailAddress :IA5STRING:'tom@contoso.com'Certificate is to be certified until Sep 1 07:03:30 2026 GMT (3650 days)Sign the certificate? [y/n]:y1 out of 1 certificate requests certified, commit? [y/n]yWrite out database with 1 new entriesData Base Updated[root@mylinux1 2.0]# ll keys/total 68-rw-r--r-- 1 root root 3972 Sep 3 14:54 01.pem-rw-r--r-- 1 root root 3838 Sep 3 15:03 02.pem-rw-r--r-- 1 root root 1298 Sep 3 14:50 ca.crt-rw------- 1 root root 916 Sep 3 14:50 ca.key-rw-r--r-- 1 root root 245 Sep 3 14:51 dh1024.pem-rw-r--r-- 1 root root 223 Sep 3 15:03 index.txt-rw-r--r-- 1 root root 21 Sep 3 15:03 index.txt.attr-rw-r--r-- 1 root root 21 Sep 3 14:54 index.txt.attr.old-rw-r--r-- 1 root root 114 Sep 3 14:54 index.txt.old-rw-r--r-- 1 root root 3 Sep 3 15:03 serial-rw-r--r-- 1 root root 3 Sep 3 14:54 serial.old-rw-r--r-- 1 root root 3972 Sep 3 14:54 server.crt-rw-r--r-- 1 root root 761 Sep 3 14:54 server.csr-rw------- 1 root root 916 Sep 3 14:54 server.key-rw-r--r-- 1 root root 3838 Sep 3 15:03 tom.crt-rw-r--r-- 1 root root 753 Sep 3 15:03 tom.csr-rw------- 1 root root 916 Sep 3 15:03 tom.key另外，上面帮Tom生产的客户端证书文件时不含密码的，也即用户通过该证书登录openVPN时不需要输入密码即可拨入，当然我们在/easy-rsa/2.0目录下看到除了build-key命令外，还有一个build-key-pass，这个就是生成带密码的客户端证书文件，例如我们需要为jerry创建一个带密码的客户端证书，方法如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869[root@mylinux1 2.0]# ./build-key-pass jerryGenerating a 1024 bit RSA private key...........................................................++++++........++++++writing new private key to 'jerry.key'Enter PEM pass phrase:Verifying - Enter PEM pass phrase:-----You are about to be asked to enter information that will be incorporatedinto your certificate request.What you are about to enter is what is called a Distinguished Name or a DN.There are quite a few fields but you can leave some blankFor some fields there will be a default value,If you enter '.', the field will be left blank.-----Country Name (2 letter code) [CN]:State or Province Name (full name) [GD]:Locality Name (eg, city) [Shenzhen]:Organization Name (eg, company) [contoso.com]:Organizational Unit Name (eg, section) []:TechCommon Name (eg, your name or your server's hostname) [jerry]:Email Address [admin@contoso.com]:jerry@contoso.comPlease enter the following 'extra' attributesto be sent with your certificate requestA challenge password []:123456An optional company name []:contoso.comUsing configuration from /opt/tools/open***-2.0.9/easy-rsa/2.0/openssl.cnfCheck that the request matches the signatureSignature okThe Subject's Distinguished Name is as followscountryName :PRINTABLE:'CN'stateOrProvinceName :PRINTABLE:'GD'localityName :PRINTABLE:'Shenzhen'organizationName :PRINTABLE:'contoso.com'organizationalUnitName:PRINTABLE:'Tech'commonName :PRINTABLE:'jerry'emailAddress :IA5STRING:'jerry@contoso.com'Certificate is to be certified until Sep 1 07:07:40 2026 GMT (3650 days)Sign the certificate? [y/n]:y1 out of 1 certificate requests certified, commit? [y/n]yWrite out database with 1 new entriesData Base Updated[root@mylinux1 2.0]# ll keys/total 84-rw-r--r-- 1 root root 3972 Sep 3 14:54 01.pem-rw-r--r-- 1 root root 3838 Sep 3 15:03 02.pem-rw-r--r-- 1 root root 3850 Sep 3 15:07 03.pem-rw-r--r-- 1 root root 1298 Sep 3 14:50 ca.crt-rw------- 1 root root 916 Sep 3 14:50 ca.key-rw-r--r-- 1 root root 245 Sep 3 14:51 dh1024.pem-rw-r--r-- 1 root root 336 Sep 3 15:07 index.txt-rw-r--r-- 1 root root 21 Sep 3 15:07 index.txt.attr-rw-r--r-- 1 root root 21 Sep 3 15:03 index.txt.attr.old-rw-r--r-- 1 root root 223 Sep 3 15:03 index.txt.old-rw-r--r-- 1 root root 3850 Sep 3 15:07 jerry.crt-rw-r--r-- 1 root root 761 Sep 3 15:07 jerry.csr-rw------- 1 root root 1041 Sep 3 15:07 jerry.key-rw-r--r-- 1 root root 3 Sep 3 15:07 serial-rw-r--r-- 1 root root 3 Sep 3 15:03 serial.old-rw-r--r-- 1 root root 3972 Sep 3 14:54 server.crt-rw-r--r-- 1 root root 761 Sep 3 14:54 server.csr-rw------- 1 root root 916 Sep 3 14:54 server.key-rw-r--r-- 1 root root 3838 Sep 3 15:03 tom.crt-rw-r--r-- 1 root root 753 Sep 3 15:03 tom.csr-rw------- 1 root root 916 Sep 3 15:03 tom.key6、编辑Open×××服务器配置文件/etc/server.conf1234567891011121314151617181920cat /opt/tools/open***-2.0.9/sample-config-files/server.conf|egrep -v ";|#|^$" &gt;/tmp/tempcp /tmp/temp /etc/server.confvi /etc/server.conf[root@mylinux1 2.0]# cat /etc/server.confport 1194 #端口proto udp #协议dev tunca /opt/tools/open***-2.0.9/easy-rsa/2.0/keys/ca.crtcert /opt/tools/open***-2.0.9/easy-rsa/2.0/keys/server.crtkey /opt/tools/open***-2.0.9/easy-rsa/2.0/keys/server.keydh /opt/tools/open***-2.0.9/easy-rsa/2.0/keys/dh1024.pemserver 10.8.0.0 255.255.255.0push "route 172.16.0.0 255.255.255.0" #你办公室内网的网段ifconfig-pool-persist ipp.txtkeepalive 10 120comp-lzopersist-keypersist-tunstatus open***-status.logverb 57、启动OpenVPN服务1）首先检查iptables是否关闭，SELinux是否禁用，以免对OpenVPN造成干扰2）开启系统自身的IP转发功能临时：1echo "1" &gt;/proc/sys/net/ipv4/ip_forward永久：1/etc/sysctl.conf 中添加/修改:net.ipv4.ip_forward = 13）启动Open***服务123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196[root@mylinux1 2.0]# /usr/local/sbin/openvpn --config /etc/server.conf &amp;Sat Sep 3 17:31:22 2016 us=753145 Current Parameter Settings:Sat Sep 3 17:31:22 2016 us=753266 config = '/etc/server.conf'Sat Sep 3 17:31:22 2016 us=753279 mode = 1Sat Sep 3 17:31:22 2016 us=753287 persist_config = DISABLEDSat Sep 3 17:31:22 2016 us=753296 persist_mode = 1Sat Sep 3 17:31:22 2016 us=753303 show_ciphers = DISABLEDSat Sep 3 17:31:22 2016 us=753311 show_digests = DISABLEDSat Sep 3 17:31:22 2016 us=753319 show_engines = DISABLEDSat Sep 3 17:31:22 2016 us=753326 genkey = DISABLEDSat Sep 3 17:31:22 2016 us=753334 key_pass_file = '[UNDEF]'Sat Sep 3 17:31:22 2016 us=753341 show_tls_ciphers = DISABLEDSat Sep 3 17:31:22 2016 us=753349 proto = 1Sat Sep 3 17:31:22 2016 us=753357 local = '192.168.100.120'Sat Sep 3 17:31:22 2016 us=753365 remote_list = NULLSat Sep 3 17:31:22 2016 us=753381 remote_random = DISABLEDSat Sep 3 17:31:22 2016 us=753391 local_port = 1194Sat Sep 3 17:31:22 2016 us=753399 remote_port = 1194Sat Sep 3 17:31:22 2016 us=753406 remote_float = DISABLEDSat Sep 3 17:31:22 2016 us=753414 ipchange = '[UNDEF]'Sat Sep 3 17:31:22 2016 us=753422 bind_local = ENABLEDSat Sep 3 17:31:22 2016 us=753429 dev = 'tun'Sat Sep 3 17:31:22 2016 us=753437 dev_type = '[UNDEF]'Sat Sep 3 17:31:22 2016 us=753445 dev_node = '[UNDEF]'Sat Sep 3 17:31:22 2016 us=753452 tun_ipv6 = DISABLEDSat Sep 3 17:31:22 2016 us=753460 ifconfig_local = '10.8.0.1'Sat Sep 3 17:31:22 2016 us=753470 ifconfig_remote_netmask = '10.8.0.2'Sat Sep 3 17:31:22 2016 us=753478 ifconfig_noexec = DISABLEDSat Sep 3 17:31:22 2016 us=753485 ifconfig_nowarn = DISABLEDSat Sep 3 17:31:22 2016 us=753493 shaper = 0Sat Sep 3 17:31:22 2016 us=753501 tun_mtu = 1500Sat Sep 3 17:31:22 2016 us=753509 tun_mtu_defined = ENABLEDSat Sep 3 17:31:22 2016 us=753517 link_mtu = 1500Sat Sep 3 17:31:22 2016 us=753524 link_mtu_defined = DISABLEDSat Sep 3 17:31:22 2016 us=753532 tun_mtu_extra = 0Sat Sep 3 17:31:22 2016 us=753539 tun_mtu_extra_defined = DISABLEDSat Sep 3 17:31:22 2016 us=753547 fragment = 0Sat Sep 3 17:31:22 2016 us=753555 mtu_discover_type = -1Sat Sep 3 17:31:22 2016 us=753562 mtu_test = 0Sat Sep 3 17:31:22 2016 us=753570 mlock = DISABLEDSat Sep 3 17:31:22 2016 us=753578 keepalive_ping = 10Sat Sep 3 17:31:22 2016 us=753585 keepalive_timeout = 120Sat Sep 3 17:31:22 2016 us=753593 inactivity_timeout = 0Sat Sep 3 17:31:22 2016 us=753600 ping_send_timeout = 10Sat Sep 3 17:31:22 2016 us=753608 ping_rec_timeout = 240Sat Sep 3 17:31:22 2016 us=753616 ping_rec_timeout_action = 2Sat Sep 3 17:31:22 2016 us=753623 ping_timer_remote = DISABLEDSat Sep 3 17:31:22 2016 us=753631 remap_sigusr1 = 0Sat Sep 3 17:31:22 2016 us=753639 explicit_exit_notification = 0Sat Sep 3 17:31:22 2016 us=753646 persist_tun = ENABLEDSat Sep 3 17:31:22 2016 us=753654 persist_local_ip = DISABLEDSat Sep 3 17:31:22 2016 us=753661 persist_remote_ip = DISABLEDSat Sep 3 17:31:22 2016 us=753669 persist_key = ENABLEDSat Sep 3 17:31:22 2016 us=753676 mssfix = 1450Sat Sep 3 17:31:22 2016 us=753684 passtos = DISABLEDSat Sep 3 17:31:22 2016 us=753692 resolve_retry_seconds = 1000000000Sat Sep 3 17:31:22 2016 us=753700 connect_retry_seconds = 5Sat Sep 3 17:31:22 2016 us=753707 username = '[UNDEF]'Sat Sep 3 17:31:22 2016 us=753715 groupname = '[UNDEF]'Sat Sep 3 17:31:22 2016 us=753722 chroot_dir = '[UNDEF]'Sat Sep 3 17:31:22 2016 us=753730 cd_dir = '[UNDEF]'Sat Sep 3 17:31:22 2016 us=753737 writepid = '[UNDEF]'Sat Sep 3 17:31:22 2016 us=753745 up_script = '[UNDEF]'Sat Sep 3 17:31:22 2016 us=753752 down_script = '[UNDEF]'Sat Sep 3 17:31:22 2016 us=753760 down_pre = DISABLEDSat Sep 3 17:31:22 2016 us=753768 up_restart = DISABLEDSat Sep 3 17:31:22 2016 us=753775 up_delay = DISABLEDSat Sep 3 17:31:22 2016 us=753782 daemon = DISABLEDSat Sep 3 17:31:22 2016 us=753790 inetd = 0Sat Sep 3 17:31:22 2016 us=753798 log = DISABLEDSat Sep 3 17:31:22 2016 us=753805 suppress_timestamps = DISABLEDSat Sep 3 17:31:22 2016 us=753813 nice = 0Sat Sep 3 17:31:22 2016 us=753820 verbosity = 5Sat Sep 3 17:31:22 2016 us=753828 mute = 0Sat Sep 3 17:31:22 2016 us=753835 gremlin = 0Sat Sep 3 17:31:22 2016 us=753844 status_file = 'open***-status.log'Sat Sep 3 17:31:22 2016 us=753851 status_file_version = 1Sat Sep 3 17:31:22 2016 us=753859 status_file_update_freq = 60Sat Sep 3 17:31:22 2016 us=753867 occ = ENABLEDSat Sep 3 17:31:22 2016 us=753874 rcvbuf = 65536Sat Sep 3 17:31:22 2016 us=753882 sndbuf = 65536Sat Sep 3 17:31:22 2016 us=753890 socks_proxy_server = '[UNDEF]'Sat Sep 3 17:31:22 2016 us=753897 socks_proxy_port = 0Sat Sep 3 17:31:22 2016 us=753909 socks_proxy_retry = DISABLEDSat Sep 3 17:31:22 2016 us=753918 fast_io = DISABLEDSat Sep 3 17:31:22 2016 us=753926 comp_lzo = ENABLEDSat Sep 3 17:31:22 2016 us=753933 comp_lzo_adaptive = ENABLEDSat Sep 3 17:31:22 2016 us=753947 route_script = '[UNDEF]'Sat Sep 3 17:31:22 2016 us=753956 route_default_gateway = '[UNDEF]'Sat Sep 3 17:31:22 2016 us=753964 route_noexec = DISABLEDSat Sep 3 17:31:22 2016 us=753972 route_delay = 0Sat Sep 3 17:31:22 2016 us=753980 route_delay_window = 30Sat Sep 3 17:31:22 2016 us=753988 route_delay_defined = DISABLEDSat Sep 3 17:31:22 2016 us=753997 route 10.8.0.0/255.255.255.0/nil/nilSat Sep 3 17:31:22 2016 us=754005 management_addr = '[UNDEF]'Sat Sep 3 17:31:22 2016 us=754013 management_port = 0Sat Sep 3 17:31:22 2016 us=754021 management_user_pass = '[UNDEF]'Sat Sep 3 17:31:22 2016 us=754029 management_log_history_cache = 250Sat Sep 3 17:31:22 2016 us=754037 management_echo_buffer_size = 100Sat Sep 3 17:31:22 2016 us=754045 management_query_passwords = DISABLEDSat Sep 3 17:31:22 2016 us=754053 management_hold = DISABLEDSat Sep 3 17:31:22 2016 us=754061 shared_secret_file = '[UNDEF]'Sat Sep 3 17:31:22 2016 us=754069 key_direction = 0Sat Sep 3 17:31:22 2016 us=754078 ciphername_defined = ENABLEDSat Sep 3 17:31:22 2016 us=754086 ciphername = 'BF-CBC'Sat Sep 3 17:31:22 2016 us=754094 authname_defined = ENABLEDSat Sep 3 17:31:22 2016 us=754102 authname = 'SHA1'Sat Sep 3 17:31:22 2016 us=754110 keysize = 0Sat Sep 3 17:31:22 2016 us=754118 engine = DISABLEDSat Sep 3 17:31:22 2016 us=754126 replay = ENABLEDSat Sep 3 17:31:22 2016 us=754134 mute_replay_warnings = DISABLEDSat Sep 3 17:31:22 2016 us=754142 replay_window = 0Sat Sep 3 17:31:22 2016 us=754150 replay_time = 0Sat Sep 3 17:31:22 2016 us=754158 packet_id_file = '[UNDEF]'Sat Sep 3 17:31:22 2016 us=754166 use_iv = ENABLEDSat Sep 3 17:31:22 2016 us=754174 test_crypto = DISABLEDSat Sep 3 17:31:22 2016 us=754182 tls_server = ENABLEDSat Sep 3 17:31:22 2016 us=754753 tls_client = DISABLEDSat Sep 3 17:31:22 2016 us=754766 key_method = 2Sat Sep 3 17:31:22 2016 us=754774 ca_file = '/opt/tools/open***-2.0.9/easy-rsa/2.0/keys/ca.crt'Sat Sep 3 17:31:22 2016 us=754783 dh_file = '/opt/tools/open***-2.0.9/easy-rsa/2.0/keys/dh1024.pem'Sat Sep 3 17:31:22 2016 us=754791 cert_file = '/opt/tools/open***-2.0.9/easy-rsa/2.0/keys/server.crt'Sat Sep 3 17:31:22 2016 us=754800 priv_key_file = '/opt/tools/open***-2.0.9/easy-rsa/2.0/keys/server.key'Sat Sep 3 17:31:22 2016 us=754808 pkcs12_file = '[UNDEF]'Sat Sep 3 17:31:22 2016 us=754817 cipher_list = '[UNDEF]'Sat Sep 3 17:31:22 2016 us=754824 tls_verify = '[UNDEF]'Sat Sep 3 17:31:22 2016 us=754832 tls_remote = '[UNDEF]'Sat Sep 3 17:31:22 2016 us=754840 crl_file = '[UNDEF]'Sat Sep 3 17:31:22 2016 us=754848 ns_cert_type = 0Sat Sep 3 17:31:22 2016 us=754856 tls_timeout = 2Sat Sep 3 17:31:22 2016 us=754864 renegotiate_bytes = 0Sat Sep 3 17:31:22 2016 us=754872 renegotiate_packets = 0Sat Sep 3 17:31:22 2016 us=754880 renegotiate_seconds = 3600Sat Sep 3 17:31:22 2016 us=754888 handshake_window = 60Sat Sep 3 17:31:22 2016 us=754896 transition_window = 3600Sat Sep 3 17:31:22 2016 us=754904 single_session = DISABLEDSat Sep 3 17:31:22 2016 us=754913 tls_exit = DISABLEDSat Sep 3 17:31:22 2016 us=754920 tls_auth_file = '[UNDEF]'Sat Sep 3 17:31:22 2016 us=754930 server_network = 10.8.0.0Sat Sep 3 17:31:22 2016 us=754939 server_netmask = 255.255.255.0Sat Sep 3 17:31:22 2016 us=754948 server_bridge_ip = 0.0.0.0Sat Sep 3 17:31:22 2016 us=754957 server_bridge_netmask = 0.0.0.0Sat Sep 3 17:31:22 2016 us=754965 server_bridge_pool_start = 0.0.0.0Sat Sep 3 17:31:22 2016 us=754974 server_bridge_pool_end = 0.0.0.0Sat Sep 3 17:31:22 2016 us=754982 push_list = 'route 172.16.100.0 255.255.255.0,route 10.8.0.1,ping 10,ping-restart 120'Sat Sep 3 17:31:22 2016 us=754991 ifconfig_pool_defined = ENABLEDSat Sep 3 17:31:22 2016 us=754999 ifconfig_pool_start = 10.8.0.4Sat Sep 3 17:31:22 2016 us=755008 ifconfig_pool_end = 10.8.0.251Sat Sep 3 17:31:22 2016 us=755017 ifconfig_pool_netmask = 0.0.0.0Sat Sep 3 17:31:22 2016 us=755025 ifconfig_pool_persist_filename = 'ipp.txt'Sat Sep 3 17:31:22 2016 us=755033 ifconfig_pool_persist_refresh_freq = 600Sat Sep 3 17:31:22 2016 us=755041 ifconfig_pool_linear = DISABLEDSat Sep 3 17:31:22 2016 us=755049 n_bcast_buf = 256Sat Sep 3 17:31:22 2016 us=755057 tcp_queue_limit = 64Sat Sep 3 17:31:22 2016 us=755065 real_hash_size = 256Sat Sep 3 17:31:22 2016 us=755073 virtual_hash_size = 256Sat Sep 3 17:31:22 2016 us=755085 client_connect_script = '[UNDEF]'Sat Sep 3 17:31:22 2016 us=755094 learn_address_script = '[UNDEF]'Sat Sep 3 17:31:22 2016 us=755102 client_disconnect_script = '[UNDEF]'Sat Sep 3 17:31:22 2016 us=755110 client_config_dir = '[UNDEF]'Sat Sep 3 17:31:22 2016 us=755118 ccd_exclusive = DISABLEDSat Sep 3 17:31:22 2016 us=755126 tmp_dir = '[UNDEF]'Sat Sep 3 17:31:22 2016 us=755134 push_ifconfig_defined = DISABLEDSat Sep 3 17:31:22 2016 us=755143 push_ifconfig_local = 0.0.0.0Sat Sep 3 17:31:22 2016 us=755151 push_ifconfig_remote_netmask = 0.0.0.0Sat Sep 3 17:31:22 2016 us=755160 enable_c2c = DISABLEDSat Sep 3 17:31:22 2016 us=755168 duplicate_cn = DISABLEDSat Sep 3 17:31:22 2016 us=755175 cf_max = 0Sat Sep 3 17:31:22 2016 us=755184 cf_per = 0Sat Sep 3 17:31:22 2016 us=756792 max_clients = 1024Sat Sep 3 17:31:22 2016 us=756814 max_routes_per_client = 256Sat Sep 3 17:31:22 2016 us=756833 client_cert_not_required = DISABLEDSat Sep 3 17:31:22 2016 us=756851 username_as_common_name = DISABLEDSat Sep 3 17:31:22 2016 us=756869 auth_user_pass_verify_script = '[UNDEF]'Sat Sep 3 17:31:22 2016 us=756887 auth_user_pass_verify_script_via_file = DISABLEDSat Sep 3 17:31:22 2016 us=756904 client = DISABLEDSat Sep 3 17:31:22 2016 us=756921 pull = DISABLEDSat Sep 3 17:31:22 2016 us=756939 auth_user_pass_file = '[UNDEF]'Sat Sep 3 17:31:22 2016 us=756959 Open××× 2.0.9 x86_64-unknown-linux [SSL] [LZO] [EPOLL] built on Sep 3 2016Sat Sep 3 17:31:22 2016 us=760411 Diffie-Hellman initialized with 1024 bit keySat Sep 3 17:31:22 2016 us=762382 TLS-Auth MTU parms [ L:1544 D:140 EF:40 EB:0 ET:0 EL:0 ]Sat Sep 3 17:31:22 2016 us=766494 TUN/TAP device tun0 openedSat Sep 3 17:31:22 2016 us=770382 TUN/TAP TX queue length set to 100Sat Sep 3 17:31:22 2016 us=770482 /sbin/ifconfig tun0 10.8.0.1 pointopoint 10.8.0.2 mtu 1500Sat Sep 3 17:31:22 2016 us=779535 /sbin/route add -net 10.8.0.0 netmask 255.255.255.0 gw 10.8.0.2Sat Sep 3 17:31:22 2016 us=796985 Data Channel MTU parms [ L:1544 D:1450 EF:44 EB:135 ET:0 EL:0 AF:3/1 ]Sat Sep 3 17:31:22 2016 us=797059 Listening for incoming TCP connection on 192.168.100.120:1194Sat Sep 3 17:31:22 2016 us=797140 Socket Buffers: R=[87380-&gt;131072] S=[16384-&gt;131072]Sat Sep 3 17:31:22 2016 us=797201 TCPv4_SERVER link local (bound): 192.168.100.120:1194Sat Sep 3 17:31:22 2016 us=797250 TCPv4_SERVER link remote: [undef]Sat Sep 3 17:31:22 2016 us=797269 MULTI: multi_init called, r=256 v=256Sat Sep 3 17:31:22 2016 us=797313 IFCONFIG POOL: base=10.8.0.4 size=62Sat Sep 3 17:31:22 2016 us=797355 IFCONFIG POOL LISTSat Sep 3 17:31:22 2016 us=797366 tom,10.8.0.4Sat Sep 3 17:31:22 2016 us=797397 MULTI: TCP INIT maxclients=1024 maxevents=1028Sat Sep 3 17:31:22 2016 us=797419 Initialization Sequence Completed看到“Initialization Sequence Completed”字样，说明OpenVPN启动成功。12345678[root@mylinux1 2.0]# ps -ef|grep openvpnroot 25318 922 0 17:31 pts/0 00:00:00 /usr/local/sbin/open*** --config /etc/server.confroot 25360 3091 0 19:47 pts/1 00:00:00 grep open***[root@mylinux1 2.0]# netstat -lnt|grep 1194tcp 0 0 192.168.100.120:1194 0.0.0.0:* LISTEN [root@mylinux1 2.0]# lsof -i :1194COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEopen*** 25318 root 5u IPv4 28576 0t0 TCP mylinux.contoso.com:open*** (LISTEN)通过上面的命令，也可以说明OpenVPN服务已经在运行中了，至此，安装部署结束。]]></content>
      <categories>
        <category>技术</category>
        <category>openVPN</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>openVPN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS7 重新分配分区大小]]></title>
    <url>%2F2022%2F0981f2b47f.html</url>
    <content type="text"><![CDATA[服务器安装CentOS7系统时，没注意点了自动分区，空间大部分分到了/home目录。 需重新分配分区，把空间加到/目录。操作步骤：1.首先你需要备份home文件夹里面的内容（新系统若是没有创建其他帐户可以不备份）1[root@localhost ~]# cp -pr /home/ /homebak/2.卸载 home ：（如果出现 home 存在进程，使用 fuser -m -v -i -k /home 终止 home 下的进程，最后使用 umount /home 卸载 /home）1[root@localhost ~]# umount /home3.删除home扇区：1[root@localhost ~]# lvremove /dev/mapper/centos-home4.给/目录所在的扇区增加800G:1[root@localhost ~]# lvextend -L +800G /dev/mapper/centos-root5.扩展/dev/mapper/centos-root文件系统:1[root@localhost ~]# xfs_growfs /dev/mapper/centos-root6.根据 vgdisplay 中的free PE 的大小确定还有多少空间可分配：1[root@localhost ~]# vgdisplay7.重新创建home lv ：1[root@localhost ~]# lvcreate -L 76G -n home centos8.创建文件系统：1[root@localhost ~]# mkfs.xfs /dev/centos/home9.挂载 home：1[root@localhost ~]# mount /dev/centos/home /home重新分区后的分区大小：10.把备份的东西cp回home，删掉备份。]]></content>
      <categories>
        <category>技术</category>
        <category>分区</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>分区</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ssh免密登陆]]></title>
    <url>%2F2022%2F093032f6ee.html</url>
    <content type="text"><![CDATA[为了保证一台Linux主机的安全，所以我们每个主机登录的时候一般我们都设置账号密码登录。但是很多时候为了操作方便，我们都通过设置SSH免密码登录。环境：主机1（充当跳板机）：版本：CentOS6.5IP地址：192.168.10.10主机2（充当线上服务器）：版本：CentOS7IP地址：192.168.10.20实践主机1上操作：生成密钥：1[root@a ~]# ssh-keygen把公钥推到主机2上：[root@a .ssh]# ssh-copy-id -i id_rsa.pub root@192.168.10.20 （或者直接把 id_rsa.pub里的内容直接拷贝到主机2的用户家目录里的.ssh/authorized_keys，给600的权限。）最后直接在主机1上登录到主机2上：]]></content>
      <categories>
        <category>技术</category>
        <category>SSH</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>SSH免密</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx URL重写规则配置详解]]></title>
    <url>%2F2022%2F093ccacdb4.html</url>
    <content type="text"><![CDATA[访问重写rewrite是Nginx HTTP请求处理过程中的一个重要功能，它是以模块的形式存在于代码中的，其功能是对用户请求的URI进行PCRE正则重写，然后返回30×重定向跳转或按条件执行相关配置。rewrite模块内置了类似脚本语言的set、if、break、return配置指令，通过这些指令，用户可以在HTTP请求处理过程中对URI进行更灵活的操作控制。rewrite模块提供的指令可以分两类，一类是标准配置指令，这部分指令只是对指定的操作进行相应的操作控制；另一类是脚本指令，这部分指令可以在HTTP指令域内以类似脚本编程的形式进行编写。rewrite模块介绍：nginx的重写模块是一个简单的正则表达式匹配与一个虚拟堆叠机结合。依赖于PCRE库，因此需要安装pcre。根据相关变量重定向和选择不同的配置，从一个location跳转到另一个location，不过这样的循环最多可以执行10次，超过后nginx将返回500错误。同时，重写模块包含set指令，来创建新的变量并设其值，这在有些情景下非常有用的，如记录条件标识、传递参数到其他location、记录做了什么等等。rewrite模块指令：1234break语法：break默认值：none使用字段：server, location, if作用:完成当前设置的重写规则，停止执行其他的重写规则。1234if语法：if (condition) &#123; … &#125;默认值：none使用字段：server, location注意：尽量考虑使用trp_files代替。判断的条件可以有以下值：12345678910111213一个变量的名称：空字符串”“或者一些“0”开始的字符串为false。字符串比较：使用=或!=运算符正则表达式匹配：使用~(区分大小写)和~*(不区分大小写)，取反运算!~ 和!~*。文件是否存在：使用-f和!-f操作符目录是否存在：使用-d和!-d操作符文件、目录、符号链接是否存在：使用-e和!-e操作符文件是否可执行：使用-x和!-x操作符return语法：return code默认值：none使用字段：server, location, if按照相关的正则表达式与字符串修改URI，指令按照在配置文件中出现的顺序执行。可以在重写指令后面添加标记。注意：如果替换的字符串以http://开头，请求将被重定向，并且不再执行多余的rewrite指令。尾部的标记(flag)可以是以下的值：1234567891011last – 停止处理重写模块指令，之后搜索location与更改后的URI匹配。break – 完成重写指令。redirect – 返回302临时重定向，如果替换字段用http://开头则被使用。permanent – 返回301永久重定向。rewrite_log语法：rewrite_log on | off默认值：rewrite_log off使用字段：server, location, if变量：无启用时将在error log中记录notice级别的重写日志。1234set语法：set variable value默认值：none使用字段：server, location, if为给定的变量设置一个特定值。1234uninitialized_variable_warn语法：uninitialized_variable_warn on|off默认值：uninitialized_variable_warn on使用字段：http, server, location, if控制是否记录未初始化变量的警告信息。重写规则组成部分：第一部分–正则表达式可以使用括号来捕获，后续可以根据位置来将其引用，位置变量值取决于捕获正则表达式中的顺序，$1引用第一个括号中的值，$2引用第二个括号中的值，以此类推。如：12345^/images/([a-z]&#123;2&#125;)/([a-z0-9]&#123;5&#125;)/(.*)\.(png|jpg|gif)$# $1是两个小写字母组成的字符串# $2是由小写字母和0到9的数字组成的5个字符的字符串# $3将是个文件名# $4是png、jpg、gif中的其中一个。第二部分–URL请求被改写。该URI可能包含正则表达式中的捕获的位置参数或这个级别下的nginx任何配置变量。如：1/data?file=$3.$4如果这个URI不匹配nginx配置的任何location，那么将给客户端返回301(永久重定向)或302(临时重定向)的状态码来表示重定向类型。该状态码可以通过第三个参数来明确指定。第三部分–标记flag标记说明：1234last #本条规则匹配完成后，继续向下匹配新的location URI规则break #本条规则匹配完成即终止，不再匹配后面的任何规则redirect #返回302临时重定向，浏览器地址会显示跳转后的URL地址permanent #返回301永久重定向，浏览器地址栏会显示跳转后的URL地址第三部分也就是尾部的标记(flag)。 last标记将导致重写后的URI搜索匹配nginx的其他location，最多可循环10次。如：1rewrite '^/images/([a-z]&#123;2&#125;)/([a-z0-9]&#123;5&#125;)/(.*)\.(png|jpg|gif)$' /data?file=$3.$4 last;实例1nginx配置文件：12345678910111213141516171819202122232425262728http &#123;#定义image日志格式log_format imagelog '[$time_local] ' $image_file ' ' $image_type ' ' $body_bytes_sent ' ' $status;#开启重写日志rewrite_log on;server &#123; root html; location / &#123; # 重写规则信息 error_log logs/rewrite.log notice; #注意这里要用‘’单引号引起来，避免&#123;&#125; rewrite '^/images/([a-z]&#123;2&#125;)/([a-z0-9]&#123;5&#125;)/(.*)\.(png|jpg|gif)$' /data?file=$3.$4; #注意不能在上面这条规则后面加上“last”参数，否则下面的set指令不会执行 set $image_file $3; set $image_type $4; &#125; location /data &#123; # 指定针对图片的日志格式，来分析图片类型和大小 access_log logs/images.log imageslog; root /data/images; #应用前面定义的变量。判断首先文件在不在，不在再判断目录在不在，如果还不在就跳转到最后一个url里 try_files /$arg_file /image404.html; &#125; location = /image404.html &#123; # 图片不存在返回特定的信息 return 404 "image not found\n"; &#125;&#125;图片位置：浏览器访问(测试机IP为192.168.10.10）：访问一个不存在的图片时：]]></content>
      <categories>
        <category>技术</category>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
        <tag>rewrite</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx模块ssi配置详解]]></title>
    <url>%2F2022%2F09a8404436.html</url>
    <content type="text"><![CDATA[什么是SSIServer Side Include，通常称为服务器端嵌入，是一种类似于ASP的基于服务器的网页制作技术。大多数（尤其是基于Unix平台）的WEB服务器如Netscape Enterprise Server等均支持SSI命令。为什么要用SSI用个例子来说明，一个静态化的页面中，需要嵌入一小块实时变化的内容，例如首页，大部分的页面内容需要缓存但是用户登录后的个人信息是动态信息，不能缓存。那么如何解决这个”页面部分缓存”问题，利用SSI就可以解决，在首页的静态页面中嵌入个人信息的动态页，由于是服务器端的嵌入，所以用户浏览的时候都是一个嵌入后的页面。nginx配置SSInginx 默认就自带了 SSI,不需要安装任何组件模块。主要是三个参数，ssi，ssi_silent_errors和ssi_types，均可以放在http,server和location的作用域下。配置修改nginx.conf配置文件增加以下内容：（ssi配置，可以放在http,server和location的作用域下，当前放到了location下了）123456789location / &#123; root html; #ssi配置开始 ssi on; ssi_silent_errors on; ssi_types text/shtml; #ssi配置结束 index index.html index.htm; &#125;参数详解：123456ssi on##开启ssi支持，默认是offssi_silent_errors on##默认值是off，开启后在处理SSI文件出错时不输出错误提示:”[an error occurred while processing the directive] ”ssi_types##默认是ssi_types text/html，所以如果需要htm和html支持，则不需要设置这句，如果需要shtml支持，则需要设置：ssi_types text/shtml编写a.html文件：123456789101112&lt;html&gt;&lt;head&gt;&lt;/head&gt; &lt;body&gt; introduce!! &lt;p&gt;hello,yixian.&lt;/p&gt; &lt;!--# include file="b.html" --&gt; &lt;/body&gt;&lt;/html&gt;编写b.html文件：1234567891011&lt;html&gt;&lt;head&gt;&lt;/head&gt; &lt;body&gt; hahaha nice to meet you. &lt;/body&gt;&lt;/html&gt;把编写的两个html都放到nginx的html目录中，并通过浏览器访问( http://192.168.10.10/a.html)，测试机IP为192.168.10.10，得到以下页面：可以看到A页面中引入了B页面的内容。]]></content>
      <categories>
        <category>技术</category>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
        <tag>ssi</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx清除缓存模块配置教程]]></title>
    <url>%2F2022%2F099f61dd71.html</url>
    <content type="text"><![CDATA[Nginx作为Web缓存服务器，它介于客户端和应用服务器之间，当用户通过浏览器访问一个URL时，web缓存服务器会去应用服务器获取要展示给用户的内容，将内容缓存到自己的服务器上，当下一次请求到来时，如果访问的是同一个URL，web缓存服务器就会直接将之前缓存的内容返回给客户端，而不是向应用服务器再次发送请求。web缓存降低了应用服务器、数据库的负载，减少了网络延迟，提高了用户访问的响应速度，增强了用户的体验。测试环境：NGINX版本及编译的模块：清除缓存模块版本：ngx_cache_purge-2.31.下载解压ngx_cache_purge模块123[root@a ~]#cd /root[root@a ~]#wget https://github.com/FRiCKLE/ngx_cache_purge/archive/2.3.tar.gz[root@a ~]#tar zxf 2.3.tar.gz2.创建NGINX所需依赖：1[root@a ~]yum install gcc gcc-c++ pcre pcre-devel openssl openssl-devel zlib zlib-devel编译安装NGINX：1234[root@a ~]#cd nginx-1.14.2[root@a nginx-1.14.2]# ./configure --add-module=/root/ngx_cache_purge-2.3 --prefix=/usr/local/nginx --with-http_ssl_module --with-http_flv_module --with-http_stub_status_module --with-http_gzip_static_module --with-pcre[root@a nginx-1.14.2]#make[root@a nginx-1.14.2]#make install3.创建一个用于缓存的目录：1[root@a nginx-1.14.2]#mkdir /usr/local/nginx/proxy_cache_path4.创建一个主页文件充当后端服务器默认主页：12[root@a nginx]#mkdir /usr/local/nginx/htmltest[root@a nginx]#vim /usr/local/nginx/htmltest/index.html5.配置nginx缓存（为了简单明了，配置文件里我只留了和缓存相关的配置）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556worker_processes 1;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; proxy_temp_path /usr/local/nginx/proxy_temp_path; proxy_cache_path /usr/local/nginx/proxy_cache_path levels=1:2 keys_zone=cache_one:200m inactive=1d max_size=1g;proxy_headers_hash_max_size 51200;proxy_headers_hash_bucket_size 6400;proxy_buffering on;proxy_buffer_size 128k;proxy_buffers 32 128k;proxy_busy_buffers_size 128k;proxy_temp_file_write_size 128k;server &#123; listen 8080; server_name 127.0.0.1; ##充当后端服务器 root /usr/local/nginx/htmltest; index index.html;&#125;server &#123;listen 80;server_name 192.168.10.10;root html;index index.html; location ~ /purge(/.*)&#123; allow all; proxy_cache_purge cache_one $host$1$is_args$args; &#125; location ~ .*\.(png|jpg)$ &#123; proxy_cache cache_one; proxy_cache_valid 200 302 304 1h; proxy_cache_key $host$uri$is_args$args; proxy_set_header Host $host; proxy_set_header X-Forward-For $remote_addr; add_header Nginx-Cache $upstream_cache_status; ##增加http头，可在浏览器访问时候观察缓存命中情况。MISS未命中，HIT命中，有5种状态 proxy_pass http://127.0.0.1:8080; ##将请求转到后端服务器 &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125;&#125;&#125;参数详解：123456789101112131415161718192021222324252627282930313233343536373839404142 proxy_connect_timeout 500;#跟后端服务器连接的超时时间_发起握手等候响应超时时间proxy_read_timeout 600;#连接成功后_等候后端服务器响应的时间_其实已经进入后端的排队之中等候处理proxy_send_timeout 500;#后端服务器数据回传时间_就是在规定时间内后端服务器必须传完所有数据proxy_buffer_size 128k;#代理请求缓存区_这个缓存区间会保存用户的头信息以供Nginx进行规则处理_一般只要能保存下头信息即可 proxy_buffers 4 128k;#同上 告诉Nginx保存单个用的几个Buffer最大用多大空间proxy_busy_buffers_size 256k;#如果系统很忙的时候可以申请更大的proxy_buffers 官方推荐*2proxy_temp_file_write_size 128k;#proxy缓存临时文件的大小proxy_temp_path /usr/local/nginx/temp;#用于指定本地目录来缓冲较大的代理请求proxy_cache_path /usr/local/nginx/cache levels=1:2 keys_zone=cache_one:200m inactive=1d max_size=1g;#设置web缓存区名为cache_one,内存缓存空间大小为200M，自动清除超过1天没有被访问过的缓存数据，硬盘缓存空间大小1gproxy_cache_path#定义缓存存储位置levels=1:2#设置缓存目录深度，最多能创建3层。keys_zone=cache_one:200m#定义缓存区域名称和内存缓存空间大小。max_size=1g#磁盘缓存空间最大使用值，达到配额后删除最少使用的缓存文件。inactive=60m#设置缓存时间，60分钟内没有被访问过就删除。use_temp_path=off#不使用temp_path指定的临时存储路径，直接将缓存文件写入指定的cache文件中，建议off。proxy_cache cache_one#缓存区域名称，要和keys_zone定义的名称一致proxy_cache_valid 200 302 304 60m#设置状态码为200 302 304过期时间为60分钟proxy_cache_key $host$uri$is_args$args#设置缓存的key，这里是以域名、URI、参数组成web缓存的key值，根据key值哈希存储缓存内容到二级缓存目录内expires 3d#缓存时间3天location ~ /purge(/.*) #用于手动清除缓存，allow表示只允许指定的IP才可以清除URL缓存location ~ .*\.(jsp|php)?$#扩展名以jsp或php结尾的不做缓存6.实践这是我们NGINX服务器默认主页：我们访问一下指定文件：可以访问到，但是我们的nginx服务器上并没有这个图片啊这是因为我们在配置的时候，把请求转到了后端服务器，我们可以到后端服务器上看一下是否存在这个文件:我们在后端服务器上找到了图片，那现在我们再把图片删掉，看一下是否还可以访问。1[root@a nginx]# rm -rf ./htmltest/jingyu.png现在我们再到浏览器访问一下指定文件：图片不是删除了吗？为什么还可以访问到？这是因为我们配置了缓存，我们可以到配置的路径下查看是否有文件：我们也可以在浏览器上按F12键查看缓存信息（在配置文件里我们配置了参数）：可以看到，已经生成了缓存文件。我们想要清除缓存的话，只需要在清除缓存网页的URI地址前加上purge即可，例如：192.168.10.10/purge/jingyu.png此时再访问的话就找不到文件了至此，清除缓存模块就完成了。]]></content>
      <categories>
        <category>技术</category>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell脚本记录]]></title>
    <url>%2F2022%2F094f3e6852.html</url>
    <content type="text"><![CDATA[在工作中经常会有重复性的工作，这种工作借助脚本帮助我们完成可以省心又省力，记录一下工作中用到的脚本phpphp5.6.10安装123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285#!/bin/bash## Description: This script is used to install php.#PackageDir=/opt/tools# 安装依赖包yum -y install gcc gcc-c++ autoconf libjpeg libjpeg-devel libpng libpng-devel freetype freetype-devel libxml2 libxml2-devel zlib zlib-devel glibc glibc-devel glib2 glib2-devel bzip2 bzip2-devel ncurses ncurses-devel curl curl-devel e2fsprogs e2fsprogs-devel krb5 krb5-devel libidn libidn-devel openssl openssl-devel openldap openldap-devel nss_ldap openldap-clients openldap-servers &gt;/dev/nullyum -y install gd-devel libjpeg-devel wget lrzsz libpng libpng-devel freetype freetype-devel libxml2 libxml2-devel curl-devel &gt;/dev/null# 创建安装包存放路径[ ! -d $&#123;PackageDir&#125; ] &amp;&amp; mkdir -p $&#123;PackageDir&#125; &amp;&amp; cd $&#123;PackageDir&#125;# 安装libiconvcd $&#123;PackageDir&#125;[ -f libiconv-1.14.tar.gz ] &amp;&amp; rm -rf libiconv-1.14.tar.gz[ -d libiconv-1.14 ] &amp;&amp; rm -rf libiconv-1.14wget -q http://ftp.gnu.org/pub/gnu/libiconv/libiconv-1.14.tar.gztar -zxf libiconv-1.14.tar.gzcd libiconv-1.14./configure --prefix=/usr/local &gt;/dev/null 2&gt;/dev/null 1&gt;/dev/nullif [ $? -eq 0 ]; then make &amp;&amp; make install &gt;/dev/null 2&gt;/dev/null [ $? -eq 0 ] &amp;&amp; echo -e "\033[1;32mInstall libiconv successful.\033[0m" || echo -e "\033[1;31mSome errors occured, install libiconv failed.\033[0m"else echo -e "\033[1;31mSome errors occured during configure libiconv.\033[0m" exit 9fi # 安装libmcryptcd $&#123;PackageDir&#125;[ -f libmcrypt-2.5.8.tar.gz ] &amp;&amp; rm -rf libmcrypt-2.5.8.tar.gz[ -d libmcrypt-2.5.8 ] &amp;&amp; rm -rf libmcrypt-2.5.8wget -q http://nchc.dl.sourceforge.net/project/mcrypt/Libmcrypt/2.5.8/libmcrypt-2.5.8.tar.gztar -zxf libmcrypt-2.5.8.tar.gzcd libmcrypt-2.5.8./configure &gt;/dev/null 2&gt;/dev/nullif [ $? -eq 0 ]; then make &amp;&amp; make install &gt;/dev/null 2&gt;/dev/null 1&gt;/dev/null [ $? -eq 0 ] &amp;&amp; echo -e "\033[1;32mInstall libmcrypt successful.\033[0m" || echo -e "\033[1;31mSome errors occured, install libmcrypt failed.\033[0m"else echo -e "\033[1;31mSome errors occured during configure libmcrypt.\033[0m" exit 8fi# 安装libltdlldconfig cd libltdl ./configure --enable-ltdl-install &gt;/dev/null 2&gt;/dev/nullif [ $? -eq 0 ]; then make &amp;&amp; make install &gt;/dev/null 2&gt;/dev/null 1&gt;/dev/null [ $? -eq 0 ] &amp;&amp; echo -e "\033[1;32mInstall libltdl successful.\033[0m" || echo -e "\033[1;31mSome errors occured, install libltdl failed.\033[0m"else echo -e "\033[1;31mSome errors occured during configure libltdl.\033[0m" exit 7fi# 安装mhashcd $&#123;PackageDir&#125;[ -f mhash-0.9.9.9.tar.gz ] &amp;&amp; rm -rf mhash-0.9.9.9.tar.gz[ -d mhash-0.9.9.9 ] &amp;&amp; rm -rf mhash-0.9.9.9wget -q http://nchc.dl.sourceforge.net/project/mhash/mhash/0.9.9.9/mhash-0.9.9.9.tar.gztar -zxf mhash-0.9.9.9.tar.gzcd mhash-0.9.9.9./configure &gt;/dev/null 2&gt;/dev/nullif [ $? -eq 0 ]; then make &amp;&amp; make install &gt;/dev/null 2&gt;/dev/null 1&gt;/dev/null [ $? -eq 0 ] &amp;&amp; echo -e "\033[1;32mInstall mhash successful.\033[0m" || echo -e "\033[1;31mSome errors occured, install mhash failed.\033[0m"else echo -e "\033[1;31mSome errors occured during configure mhash.\033[0m" exit 6fi# 创建lib库软连接ln -s /usr/local/lib/libmcrypt.la /usr/lib/libmcrypt.laln -s /usr/local/lib/libmcrypt.so /usr/lib/libmcrypt.soln -s /usr/local/lib/libmcrypt.so.4 /usr/lib/libmcrypt.so.4ln -s /usr/local/lib/libmcrypt.so.4.4.8 /usr/lib/libmcrypt.so.4.4.8ln -s /usr/local/lib/libmhash.a /usr/lib/libmhash.aln -s /usr/local/lib/libmhash.la /usr/lib/libmhash.laln -s /usr/local/lib/libmhash.so /usr/lib/libmhash.soln -s /usr/local/lib/libmhash.so.2 /usr/lib/libmhash.so.2ln -s /usr/local/lib/libmhash.so.2.0.1 /usr/lib/libmhash.so.2.0.1ln -s /usr/local/bin/libmcrypt-config /usr/bin/libmcrypt-config# 安装mcryptcd $&#123;PackageDir&#125;[ -f mcrypt-2.6.8.tar.gz ] &amp;&amp; rm -rf mcrypt-2.6.8.tar.gz[ -d mcrypt-2.6.8 ] &amp;&amp; rm -rf mcrypt-2.6.8wget -q http://ncu.dl.sourceforge.net/project/mcrypt/MCrypt/2.6.8/mcrypt-2.6.8.tar.gztar -zxf mcrypt-2.6.8.tar.gzcd mcrypt-2.6.8ldconfig./configure &gt;/dev/null 2&gt;/dev/nullif [ $? -eq 0 ]; then make &amp;&amp; make install &gt;/dev/null 2&gt;/dev/null 1&gt;/dev/null [ $? -eq 0 ] &amp;&amp; echo -e "\033[1;32mInstall mcrypt successful.\033[0m" || echo -e "\033[1;31mSome errors occured, install mcrypt failed.\033[0m"else echo -e "\033[1;31mSome errors occured during configure mcrypt.\033[0m" exit 5fi# 创建php用户useradd -u 607 -s /sbin/nologin www[ $? -eq 0 ] &amp;&amp; echo -e "\033[1;32mPHP user www added.\033[0m" || echo -e "\033[1;31mAdd php user failed.\033[0m"# 安装PHPPVERSION=5.6.10cd $&#123;PackageDir&#125;[ -f php-$&#123;PVERSION&#125;.tar.gz ] &amp;&amp; rm -rf php-$&#123;PVERSION&#125;.tar.gz[ -d php-$&#123;PVERSION&#125; ] &amp;&amp; rm -rf php-$&#123;PVERSION&#125;wget -q http://cn2.php.net/distributions/php-$&#123;PVERSION&#125;.tar.gztar -zxf php-$&#123;PVERSION&#125;.tar.gz cd php-$&#123;PVERSION&#125;./configure --prefix=/usr/local/php \--with-config-file-path=/usr/local/php/etc \--with-libxml-dir \--enable-xml \--enable-fpm \--with-fpm-user=www \--with-fpm-group=www \--enable-bcmath \--enable-mbstring \--enable-gd-native-ttf \--enable-sockets \--enable-mysqlnd \--with-mysql=mysqlnd \--with-mysqli=mysqlnd \--with-pdo-mysql=mysqlnd \--enable-zip \--enable-inline-optimization \--with-gd \--with-bz2 \--with-zlib \--with-mcrypt \--with-mhash \--with-openssl \--with-xmlrpc \--with-iconv-dir \--with-freetype-dir \--with-jpeg-dir \--with-png-dir \--without-pear \--disable-ipv6 \--disable-pdo \--with-gettext \--disable-debug \--without-pdo-sqlite \--disable-rpath \--enable-shmop \--enable-sysvsem \--with-curl \--enable-mbregex \--enable-pcntl \--enable-soap \--enable-sigchild \--enable-pdo &gt;/dev/null 2&gt;/dev/nullif [ $? -eq 0 ]; then make ZEND_EXTRA_LIBS='-liconv' &gt;/dev/null 2&gt;/dev/null 1&gt;/dev/null if [ $? -eq 0 ]; then make install &gt;/dev/null 2&gt;/dev/null 1&gt;/dev/null [ $? -eq 0 ] &amp;&amp; echo -e "\033[1;32mInstall php$&#123;PVERSION&#125; successful.\033[0m" || echo -e "\033[1;31mSome errors occured, install php$&#123;PVERSION&#125; failed.\033[0m" else echo -e "\033[1;31mSome errors occured during make php$&#123;PVERSION&#125;.\033[0m" exit 4 fielse echo -e "\033[1;31mSome errors occured during configure php$&#123;PVERSION&#125;.\033[0m" exit 3fi # 添加php到环境变量echo -e 'PATH=$PATH:/usr/local/php/bin:/usr/local/php/sbin' &gt;&gt; /etc/profilesource /etc/profile# 创建php-fpm配置文件cd /usr/local/php/etc/[ -e php-fpm.conf ] &amp;&amp; mv php-fpm.conf php-fpm.conf.bak$(date +%F)cp php-fpm.conf.default php-fpm.conf# 生成php.ini文件[ -e /etc/php.ini ] &amp;&amp; mv /etc/php.ini /etc/php.ini.bak$(date +%F)cp /opt/tools/php-5.6.10/php.ini-production /etc/php.ini# 配置php-fpm启动脚本并添加开机启动[ -e /etc/init.d/php-fpm ] &amp;&amp; mv /etc/init.d/php-fpm /etc/init.d/php-fpm.bak$(date +%F)cp /opt/tools/php-5.6.10/sapi/fpm/init.d.php-fpm /etc/init.d/php-fpmchmod 755 /etc/init.d/php-fpmchkconfig --add php-fpmchkconfig --list|grep php-fpm &gt;/dev/nullif [ $? -eq 0 ]; then echo -e "\033[1;32mAdd php-fpm into chkconfig list successful.\033[0m" || echo -e "\033[1;31mAdd php-fpm into chkconfig list failed.\033[0m" chkconfig php-fpm on chkconfig --list|grep 3:on|grep php &gt;/dev/null if [ $? -eq 0 ];then echo -e "\033[1;32mChange php-fpm status to on in chkconfig list successful.\033[0m" else echo -e "\033[1;31mChange php-fpm status to on in chkconfig list failed.\033[0m" fielse echo -e "\033[1;31mAdd php-fpm into chkconfig list failed.\033[0m"fi# 启动php-fpm/etc/init.d/php-fpm start &gt;/dev/null[ $? -eq 0 ] &amp;&amp; echo -e "\033[1;32mStarting php-fpm successful.\033[0m" || echo -e "\033[1;31mStarting php-fpm failed.\033[0m". /etc/profilesource /etc/profile# 安装libmemcached依赖cd $&#123;PackageDir&#125;wget -q https://launchpad.net/libmemcached/1.0/1.0.18/+download/libmemcached-1.0.18.tar.gztar -zxf libmemcached-1.0.18.tar.gz cd libmemcached-1.0.18./configure --prefix=/usr/local/libmemcached --with-memcached &gt;/dev/null 2&gt;/dev/nullif [ $? -eq 0 ]; then make &amp;&amp; make install &gt;/dev/null 2&gt;/dev/null [ $? -eq 0 ] &amp;&amp; echo -e "\033[1;32mInstall libmemcached package successful.\033[0m" || echo -e "\033[1;31mInstall libmemcached package failed.\033[0m"else echo -e "\033[1;31mSome errors occured during configure libmemcached.\033[0m" exit 11fi# 安装memcached包cd $&#123;PackageDir&#125;wget -q https://pecl.php.net/get/memcached-2.2.0.tgztar -zxf memcached-2.2.0.tgz cd memcached-2.2.0/usr/local/php/bin/phpize ./configure --enable-memcached --with-php-config=/usr/local/php/bin/php-config --with-libmemcached-dir=/usr/local/libmemcached &gt;/dev/null 2&gt;/dev/nullif [ $? -eq 0 ]; then make &amp;&amp; make install &gt;/dev/null 2&gt;/dev/null [ $? -eq 0 ] &amp;&amp; echo -e "\033[1;32mInstall memcached package successful.\033[0m" || echo -e "\033[1;31mInstall memcached package failed.\033[0m"else echo -e "\033[1;31mSome errors occured during configure memcached.\033[0m" exit 12fi# 添加memcached扩展[ -e /usr/local/php/etc/php.ini ] &amp;&amp; mv /usr/local/php/etc/php.ini /usr/local/php/etc/php.ini.bak cp /etc/php.ini /usr/local/php/etc/echo 'extension=memcached.so' &gt;&gt; /usr/local/php/etc/php.ini /usr/local/php/bin/php -m |grep memcached &gt;/dev/null[ $? -eq 0 ] &amp;&amp; echo -e "\033[1;32mAdd memcached extension to php successful.\033[0m" || echo -e "\033[1;31mAdd memcached extension to php failed.\033[0m"# 安装phpredis包cd $&#123;PackageDir&#125;wget -q https://github.com/phpredis/phpredis/archive/2.2.4.tar.gztar -zxf 2.2.4.tar.gzcd phpredis-2.2.4//usr/local/php/bin/phpize ./configure --with-php-config=/usr/local/php/bin/php-config &gt;/dev/null 2&gt;/dev/nullif [ $? -eq 0 ]; then make &amp;&amp; make install &gt;/dev/null 2&gt;/dev/null [ $? -eq 0 ] &amp;&amp; echo -e "\033[1;32mInstall phpredis package successful.\033[0m" || echo -e "\033[1;31mInstall phpredis package failed.\033[0m"else echo -e "\033[1;31mSome errors occured during configure phpredis.\033[0m" exit 13fi# 添加redis扩展echo 'extension=redis.so' &gt;&gt; /usr/local/php/etc/php.ini/usr/local/php/bin/php -m |grep redis &gt;/dev/null[ $? -eq 0 ] &amp;&amp; echo -e "\033[1;32mAdd redis extension to php successful.\033[0m" || echo -e "\033[1;31mAdd redis extension to php failed.\033[0m"chown -R www. /usr/local/php/. /etc/profilephp7.3.4安装123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248#!/bin/bash## Description: This script is used to install php.#source /etc/profilePackageDir=/opt/tools# 安装依赖包yum -y install gcc gcc-c++ autoconf libjpeg libjpeg-devel libpng libpng-devel freetype freetype-devel libxml2 libxml2-devel zlib zlib-devel glibc glibc-devel glib2 glib2-devel bzip2 bzip2-devel ncurses ncurses-devel curl curl-devel e2fsprogs e2fsprogs-devel krb5 krb5-devel libidn libidn-devel openssl openssl-devel openldap openldap-devel nss_ldap openldap-clients openldap-servers &gt;/dev/nullyum -y install gd-devel libjpeg-devel libpng libpng-devel freetype freetype-devel libxml2 libxml2-devel curl-devel &gt;/dev/null# 创建安装包存放路径[ ! -d $&#123;PackageDir&#125; ] &amp;&amp; mkdir -p $&#123;PackageDir&#125; &amp;&amp; cd $&#123;PackageDir&#125;# 安装libiconvcd $&#123;PackageDir&#125;[ -f libiconv-1.14.tar.gz ] &amp;&amp; rm -rf libiconv-1.14.tar.gz[ -d libiconv-1.14 ] &amp;&amp; rm -rf libiconv-1.14wget -q http://ftp.gnu.org/pub/gnu/libiconv/libiconv-1.14.tar.gztar -zxf libiconv-1.14.tar.gzcd libiconv-1.14./configure --prefix=/usr/local &gt;/dev/null 2&gt;/dev/null 1&gt;/dev/nullif [ $? -eq 0 ]; then make &amp;&amp; make install &gt;/dev/null 2&gt;/dev/null [ $? -eq 0 ] &amp;&amp; echo -e "\033[1;32mInstall libiconv successful.\033[0m" || echo -e "\033[1;31mSome errors occured, install libiconv failed.\033[0m"else echo -e "\033[1;31mSome errors occured during configure libiconv.\033[0m" exit 9fi # 安装libmcryptcd $&#123;PackageDir&#125;[ -f libmcrypt-2.5.8.tar.gz ] &amp;&amp; rm -rf libmcrypt-2.5.8.tar.gz[ -d libmcrypt-2.5.8 ] &amp;&amp; rm -rf libmcrypt-2.5.8wget -q http://nchc.dl.sourceforge.net/project/mcrypt/Libmcrypt/2.5.8/libmcrypt-2.5.8.tar.gztar -zxf libmcrypt-2.5.8.tar.gzcd libmcrypt-2.5.8./configure &gt;/dev/null 2&gt;/dev/nullif [ $? -eq 0 ]; then make &amp;&amp; make install &gt;/dev/null 2&gt;/dev/null 1&gt;/dev/null [ $? -eq 0 ] &amp;&amp; echo -e "\033[1;32mInstall libmcrypt successful.\033[0m" || echo -e "\033[1;31mSome errors occured, install libmcrypt failed.\033[0m"else echo -e "\033[1;31mSome errors occured during configure libmcrypt.\033[0m" exit 8fi# 安装libltdl/sbin/ldconfig cd libltdl ./configure --enable-ltdl-install &gt;/dev/null 2&gt;/dev/nullif [ $? -eq 0 ]; then make &amp;&amp; make install &gt;/dev/null 2&gt;/dev/null 1&gt;/dev/null [ $? -eq 0 ] &amp;&amp; echo -e "\033[1;32mInstall libltdl successful.\033[0m" || echo -e "\033[1;31mSome errors occured, install libltdl failed.\033[0m"else echo -e "\033[1;31mSome errors occured during configure libltdl.\033[0m" exit 7fi# 安装mhashcd $&#123;PackageDir&#125;[ -f mhash-0.9.9.9.tar.gz ] &amp;&amp; rm -rf mhash-0.9.9.9.tar.gz[ -d mhash-0.9.9.9 ] &amp;&amp; rm -rf mhash-0.9.9.9wget -q http://nchc.dl.sourceforge.net/project/mhash/mhash/0.9.9.9/mhash-0.9.9.9.tar.gztar -zxf mhash-0.9.9.9.tar.gzcd mhash-0.9.9.9./configure &gt;/dev/null 2&gt;/dev/nullif [ $? -eq 0 ]; then make &amp;&amp; make install &gt;/dev/null 2&gt;/dev/null 1&gt;/dev/null [ $? -eq 0 ] &amp;&amp; echo -e "\033[1;32mInstall mhash successful.\033[0m" || echo -e "\033[1;31mSome errors occured, install mhash failed.\033[0m"else echo -e "\033[1;31mSome errors occured during configure mhash.\033[0m" exit 6fi# 创建lib库软连接ln -s /usr/local/lib/libmcrypt.la /usr/lib/libmcrypt.laln -s /usr/local/lib/libmcrypt.so /usr/lib/libmcrypt.soln -s /usr/local/lib/libmcrypt.so.4 /usr/lib/libmcrypt.so.4ln -s /usr/local/lib/libmcrypt.so.4.4.8 /usr/lib/libmcrypt.so.4.4.8ln -s /usr/local/lib/libmhash.a /usr/lib/libmhash.aln -s /usr/local/lib/libmhash.la /usr/lib/libmhash.laln -s /usr/local/lib/libmhash.so /usr/lib/libmhash.soln -s /usr/local/lib/libmhash.so.2 /usr/lib/libmhash.so.2ln -s /usr/local/lib/libmhash.so.2.0.1 /usr/lib/libmhash.so.2.0.1ln -s /usr/local/bin/libmcrypt-config /usr/bin/libmcrypt-config/sbin/ldconfig# 安装mcryptcd $&#123;PackageDir&#125;[ -f mcrypt-2.6.8.tar.gz ] &amp;&amp; rm -rf mcrypt-2.6.8.tar.gz[ -d mcrypt-2.6.8 ] &amp;&amp; rm -rf mcrypt-2.6.8wget -q https://nchc.dl.sourceforge.net/project/mcrypt/MCrypt/2.6.8/mcrypt-2.6.8.tar.gztar -zxf mcrypt-2.6.8.tar.gzcd mcrypt-2.6.8/sbin/ldconfig./configure &gt;/dev/null 2&gt;/dev/nullif [ $? -eq 0 ]; then make &amp;&amp; make install &gt;/dev/null 2&gt;/dev/null 1&gt;/dev/null [ $? -eq 0 ] &amp;&amp; echo -e "\033[1;32mInstall mcrypt successful.\033[0m" || echo -e "\033[1;31mSome errors occured, install mcrypt failed.\033[0m"else echo -e "\033[1;31mSome errors occured during configure mcrypt.\033[0m" exit 5fi# 安装libzip1.2cd $&#123;PackageDir&#125;[ -f libzip-1.2.0.tar.gz ] &amp;&amp; rm -rf libzip-1.2.0.tar.gz[ -d libzip-1.2.0 ] &amp;&amp; rm -rf libzip-1.2.0wget -q https://nih.at/libzip/libzip-1.2.0.tar.gztar -zxf libzip-1.2.0.tar.gzcd libzip-1.2.0./configure &gt;/dev/null 2&gt;/dev/nullif [ $? -eq 0 ]; then make &amp;&amp; make install &gt;/dev/null 2&gt;/dev/null 1&gt;/dev/null [ $? -eq 0 ] &amp;&amp; echo -e "\033[1;32mInstall libzip successful.\033[0m" || echo -e "\033[1;31mSome errors occured, install libzip failed.\033[0m"else echo -e "\033[1;31mSome errors occured during configure libzip.\033[0m" exit 15fi# 指定zipconf声明文件路径cp /usr/local/lib/libzip/include/zipconf.h /usr/local/include/zipconf.h# 添加搜索路径到配置文件echo '/usr/local/lib64/usr/local/lib/usr/lib/usr/lib64' &gt;&gt; /etc/ld.so.conf# 更新配置/sbin/ldconfig -v# 创建php用户useradd -u 607 -s /sbin/nologin www[ $? -eq 0 ] &amp;&amp; echo -e "\033[1;32mPHP user www added.\033[0m" || echo -e "\033[1;31mAdd php user failed.\033[0m"# 安装PHPPVERSION=7.3.4cd $&#123;PackageDir&#125;[ -f php-$&#123;PVERSION&#125;.tar.gz ] &amp;&amp; rm -rf php-$&#123;PVERSION&#125;.tar.gz[ -d php-$&#123;PVERSION&#125; ] &amp;&amp; rm -rf php-$&#123;PVERSION&#125;#wget -q http://cn2.php.net/distributions/php-$&#123;PVERSION&#125;.tar.gzwget -q http://ftp.ntu.edu.tw/php/distributions/php-7.3.4.tar.gztar -zxf php-$&#123;PVERSION&#125;.tar.gz cd php-$&#123;PVERSION&#125;./configure --prefix=/usr/local/php \--with-config-file-path=/usr/local/php/etc \--with-libxml-dir \--enable-xml \--enable-fpm \--with-fpm-user=www \--with-fpm-group=www \--enable-bcmath \--enable-mbstring \--enable-sockets \--enable-mysqlnd \--enable-opcache \--enable-session \--with-mysqli=mysqlnd \--with-pdo-mysql=mysqlnd \--enable-mysqlnd-compression-support \--enable-inline-optimization \--with-gd \--with-bz2 \--with-zlib \--enable-zip \--with-mhash \--with-openssl \--with-xmlrpc \--with-iconv-dir \--with-freetype-dir \--with-jpeg-dir \--with-png-dir \--without-pear \--disable-phar \--with-pcre-regex \--disable-ipv6 \--with-gettext \--disable-debug \--without-pdo-sqlite \--disable-rpath \--enable-shmop \--enable-sysvsem \--with-curl \--enable-mbregex \--enable-pcntl \--enable-soap \--enable-sigchild \--enable-pdo if [ $? -eq 0 ]; then make ZEND_EXTRA_LIBS='-liconv' if [ $? -eq 0 ]; then make install [ $? -eq 0 ] &amp;&amp; echo -e "\033[1;32mInstall php$&#123;PVERSION&#125; successful.\033[0m" || echo -e "\033[1;31mSome errors occured, install php$&#123;PVERSION&#125; failed.\033[0m" else echo -e "\033[1;31mSome errors occured during make php$&#123;PVERSION&#125;.\033[0m" exit 4 fielse echo -e "\033[1;31mSome errors occured during configure php$&#123;PVERSION&#125;.\033[0m" exit 3fi # 添加php到环境变量echo -e 'PATH=$PATH:/usr/local/php/bin:/usr/local/php/sbin' &gt;&gt; /etc/profilesource /etc/profile# 创建php-fpm配置文件cd /usr/local/php/etc/[ -e php-fpm.conf ] &amp;&amp; mv php-fpm.conf php-fpm.conf.bak$(date +%F)cp /usr/local/php/etc/php-fpm.d/www.conf.default /usr/local/php/etc/php-fpm.d/www.conf# 生成php.ini文件[ -e /etc/php.ini ] &amp;&amp; mv /etc/php.ini /etc/php.ini.bak$(date +%F)cp $&#123;PackageDir&#125;/php-$&#123;PVERSION&#125;/php.ini-production /etc/php.ini# 配置php-fpm启动脚本并添加开机启动[ -e /etc/init.d/php-fpm ] &amp;&amp; mv /etc/init.d/php-fpm /etc/init.d/php-fpm.bak$(date +%F)cp $&#123;PackageDir&#125;/php-$&#123;PVERSION&#125;/sapi/fpm/init.d.php-fpm /etc/init.d/php-fpmchmod 755 /etc/init.d/php-fpmchkconfig --add php-fpmchkconfig --list|grep php-fpm &gt;/dev/nullif [ $? -eq 0 ]; then echo -e "\033[1;32mAdd php-fpm into chkconfig list successful.\033[0m" || echo -e "\033[1;31mAdd php-fpm into chkconfig list failed.\033[0m" chkconfig php-fpm on chkconfig --list|grep 3:on|grep php &gt;/dev/null if [ $? -eq 0 ];then echo -e "\033[1;32mChange php-fpm status to on in chkconfig list successful.\033[0m" else echo -e "\033[1;31mChange php-fpm status to on in chkconfig list failed.\033[0m" fielse echo -e "\033[1;31mAdd php-fpm into chkconfig list failed.\033[0m"fiNginxNginx1.6.3安装123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248#!/bin/bash## Description: This script is used to install php.#source /etc/profilePackageDir=/opt/tools# 安装依赖包yum -y install gcc gcc-c++ autoconf libjpeg libjpeg-devel libpng libpng-devel freetype freetype-devel libxml2 libxml2-devel zlib zlib-devel glibc glibc-devel glib2 glib2-devel bzip2 bzip2-devel ncurses ncurses-devel curl curl-devel e2fsprogs e2fsprogs-devel krb5 krb5-devel libidn libidn-devel openssl openssl-devel openldap openldap-devel nss_ldap openldap-clients openldap-servers &gt;/dev/nullyum -y install gd-devel libjpeg-devel libpng libpng-devel freetype freetype-devel libxml2 libxml2-devel curl-devel &gt;/dev/null# 创建安装包存放路径[ ! -d $&#123;PackageDir&#125; ] &amp;&amp; mkdir -p $&#123;PackageDir&#125; &amp;&amp; cd $&#123;PackageDir&#125;# 安装libiconvcd $&#123;PackageDir&#125;[ -f libiconv-1.14.tar.gz ] &amp;&amp; rm -rf libiconv-1.14.tar.gz[ -d libiconv-1.14 ] &amp;&amp; rm -rf libiconv-1.14wget -q http://ftp.gnu.org/pub/gnu/libiconv/libiconv-1.14.tar.gztar -zxf libiconv-1.14.tar.gzcd libiconv-1.14./configure --prefix=/usr/local &gt;/dev/null 2&gt;/dev/null 1&gt;/dev/nullif [ $? -eq 0 ]; then make &amp;&amp; make install &gt;/dev/null 2&gt;/dev/null [ $? -eq 0 ] &amp;&amp; echo -e "\033[1;32mInstall libiconv successful.\033[0m" || echo -e "\033[1;31mSome errors occured, install libiconv failed.\033[0m"else echo -e "\033[1;31mSome errors occured during configure libiconv.\033[0m" exit 9fi # 安装libmcryptcd $&#123;PackageDir&#125;[ -f libmcrypt-2.5.8.tar.gz ] &amp;&amp; rm -rf libmcrypt-2.5.8.tar.gz[ -d libmcrypt-2.5.8 ] &amp;&amp; rm -rf libmcrypt-2.5.8wget -q http://nchc.dl.sourceforge.net/project/mcrypt/Libmcrypt/2.5.8/libmcrypt-2.5.8.tar.gztar -zxf libmcrypt-2.5.8.tar.gzcd libmcrypt-2.5.8./configure &gt;/dev/null 2&gt;/dev/nullif [ $? -eq 0 ]; then make &amp;&amp; make install &gt;/dev/null 2&gt;/dev/null 1&gt;/dev/null [ $? -eq 0 ] &amp;&amp; echo -e "\033[1;32mInstall libmcrypt successful.\033[0m" || echo -e "\033[1;31mSome errors occured, install libmcrypt failed.\033[0m"else echo -e "\033[1;31mSome errors occured during configure libmcrypt.\033[0m" exit 8fi# 安装libltdl/sbin/ldconfig cd libltdl ./configure --enable-ltdl-install &gt;/dev/null 2&gt;/dev/nullif [ $? -eq 0 ]; then make &amp;&amp; make install &gt;/dev/null 2&gt;/dev/null 1&gt;/dev/null [ $? -eq 0 ] &amp;&amp; echo -e "\033[1;32mInstall libltdl successful.\033[0m" || echo -e "\033[1;31mSome errors occured, install libltdl failed.\033[0m"else echo -e "\033[1;31mSome errors occured during configure libltdl.\033[0m" exit 7fi# 安装mhashcd $&#123;PackageDir&#125;[ -f mhash-0.9.9.9.tar.gz ] &amp;&amp; rm -rf mhash-0.9.9.9.tar.gz[ -d mhash-0.9.9.9 ] &amp;&amp; rm -rf mhash-0.9.9.9wget -q http://nchc.dl.sourceforge.net/project/mhash/mhash/0.9.9.9/mhash-0.9.9.9.tar.gztar -zxf mhash-0.9.9.9.tar.gzcd mhash-0.9.9.9./configure &gt;/dev/null 2&gt;/dev/nullif [ $? -eq 0 ]; then make &amp;&amp; make install &gt;/dev/null 2&gt;/dev/null 1&gt;/dev/null [ $? -eq 0 ] &amp;&amp; echo -e "\033[1;32mInstall mhash successful.\033[0m" || echo -e "\033[1;31mSome errors occured, install mhash failed.\033[0m"else echo -e "\033[1;31mSome errors occured during configure mhash.\033[0m" exit 6fi# 创建lib库软连接ln -s /usr/local/lib/libmcrypt.la /usr/lib/libmcrypt.laln -s /usr/local/lib/libmcrypt.so /usr/lib/libmcrypt.soln -s /usr/local/lib/libmcrypt.so.4 /usr/lib/libmcrypt.so.4ln -s /usr/local/lib/libmcrypt.so.4.4.8 /usr/lib/libmcrypt.so.4.4.8ln -s /usr/local/lib/libmhash.a /usr/lib/libmhash.aln -s /usr/local/lib/libmhash.la /usr/lib/libmhash.laln -s /usr/local/lib/libmhash.so /usr/lib/libmhash.soln -s /usr/local/lib/libmhash.so.2 /usr/lib/libmhash.so.2ln -s /usr/local/lib/libmhash.so.2.0.1 /usr/lib/libmhash.so.2.0.1ln -s /usr/local/bin/libmcrypt-config /usr/bin/libmcrypt-config/sbin/ldconfig# 安装mcryptcd $&#123;PackageDir&#125;[ -f mcrypt-2.6.8.tar.gz ] &amp;&amp; rm -rf mcrypt-2.6.8.tar.gz[ -d mcrypt-2.6.8 ] &amp;&amp; rm -rf mcrypt-2.6.8wget -q https://nchc.dl.sourceforge.net/project/mcrypt/MCrypt/2.6.8/mcrypt-2.6.8.tar.gztar -zxf mcrypt-2.6.8.tar.gzcd mcrypt-2.6.8/sbin/ldconfig./configure &gt;/dev/null 2&gt;/dev/nullif [ $? -eq 0 ]; then make &amp;&amp; make install &gt;/dev/null 2&gt;/dev/null 1&gt;/dev/null [ $? -eq 0 ] &amp;&amp; echo -e "\033[1;32mInstall mcrypt successful.\033[0m" || echo -e "\033[1;31mSome errors occured, install mcrypt failed.\033[0m"else echo -e "\033[1;31mSome errors occured during configure mcrypt.\033[0m" exit 5fi# 安装libzip1.2cd $&#123;PackageDir&#125;[ -f libzip-1.2.0.tar.gz ] &amp;&amp; rm -rf libzip-1.2.0.tar.gz[ -d libzip-1.2.0 ] &amp;&amp; rm -rf libzip-1.2.0wget -q https://nih.at/libzip/libzip-1.2.0.tar.gztar -zxf libzip-1.2.0.tar.gzcd libzip-1.2.0./configure &gt;/dev/null 2&gt;/dev/nullif [ $? -eq 0 ]; then make &amp;&amp; make install &gt;/dev/null 2&gt;/dev/null 1&gt;/dev/null [ $? -eq 0 ] &amp;&amp; echo -e "\033[1;32mInstall libzip successful.\033[0m" || echo -e "\033[1;31mSome errors occured, install libzip failed.\033[0m"else echo -e "\033[1;31mSome errors occured during configure libzip.\033[0m" exit 15fi# 指定zipconf声明文件路径cp /usr/local/lib/libzip/include/zipconf.h /usr/local/include/zipconf.h# 添加搜索路径到配置文件echo '/usr/local/lib64/usr/local/lib/usr/lib/usr/lib64' &gt;&gt; /etc/ld.so.conf# 更新配置/sbin/ldconfig -v# 创建php用户useradd -u 607 -s /sbin/nologin www[ $? -eq 0 ] &amp;&amp; echo -e "\033[1;32mPHP user www added.\033[0m" || echo -e "\033[1;31mAdd php user failed.\033[0m"# 安装PHPPVERSION=7.3.4cd $&#123;PackageDir&#125;[ -f php-$&#123;PVERSION&#125;.tar.gz ] &amp;&amp; rm -rf php-$&#123;PVERSION&#125;.tar.gz[ -d php-$&#123;PVERSION&#125; ] &amp;&amp; rm -rf php-$&#123;PVERSION&#125;#wget -q http://cn2.php.net/distributions/php-$&#123;PVERSION&#125;.tar.gzwget -q http://ftp.ntu.edu.tw/php/distributions/php-7.3.4.tar.gztar -zxf php-$&#123;PVERSION&#125;.tar.gz cd php-$&#123;PVERSION&#125;./configure --prefix=/usr/local/php \--with-config-file-path=/usr/local/php/etc \--with-libxml-dir \--enable-xml \--enable-fpm \--with-fpm-user=www \--with-fpm-group=www \--enable-bcmath \--enable-mbstring \--enable-sockets \--enable-mysqlnd \--enable-opcache \--enable-session \--with-mysqli=mysqlnd \--with-pdo-mysql=mysqlnd \--enable-mysqlnd-compression-support \--enable-inline-optimization \--with-gd \--with-bz2 \--with-zlib \--enable-zip \--with-mhash \--with-openssl \--with-xmlrpc \--with-iconv-dir \--with-freetype-dir \--with-jpeg-dir \--with-png-dir \--without-pear \--disable-phar \--with-pcre-regex \--disable-ipv6 \--with-gettext \--disable-debug \--without-pdo-sqlite \--disable-rpath \--enable-shmop \--enable-sysvsem \--with-curl \--enable-mbregex \--enable-pcntl \--enable-soap \--enable-sigchild \--enable-pdo if [ $? -eq 0 ]; then make ZEND_EXTRA_LIBS='-liconv' if [ $? -eq 0 ]; then make install [ $? -eq 0 ] &amp;&amp; echo -e "\033[1;32mInstall php$&#123;PVERSION&#125; successful.\033[0m" || echo -e "\033[1;31mSome errors occured, install php$&#123;PVERSION&#125; failed.\033[0m" else echo -e "\033[1;31mSome errors occured during make php$&#123;PVERSION&#125;.\033[0m" exit 4 fielse echo -e "\033[1;31mSome errors occured during configure php$&#123;PVERSION&#125;.\033[0m" exit 3fi # 添加php到环境变量echo -e 'PATH=$PATH:/usr/local/php/bin:/usr/local/php/sbin' &gt;&gt; /etc/profilesource /etc/profile# 创建php-fpm配置文件cd /usr/local/php/etc/[ -e php-fpm.conf ] &amp;&amp; mv php-fpm.conf php-fpm.conf.bak$(date +%F)cp /usr/local/php/etc/php-fpm.d/www.conf.default /usr/local/php/etc/php-fpm.d/www.conf# 生成php.ini文件[ -e /etc/php.ini ] &amp;&amp; mv /etc/php.ini /etc/php.ini.bak$(date +%F)cp $&#123;PackageDir&#125;/php-$&#123;PVERSION&#125;/php.ini-production /etc/php.ini# 配置php-fpm启动脚本并添加开机启动[ -e /etc/init.d/php-fpm ] &amp;&amp; mv /etc/init.d/php-fpm /etc/init.d/php-fpm.bak$(date +%F)cp $&#123;PackageDir&#125;/php-$&#123;PVERSION&#125;/sapi/fpm/init.d.php-fpm /etc/init.d/php-fpmchmod 755 /etc/init.d/php-fpmchkconfig --add php-fpmchkconfig --list|grep php-fpm &gt;/dev/nullif [ $? -eq 0 ]; then echo -e "\033[1;32mAdd php-fpm into chkconfig list successful.\033[0m" || echo -e "\033[1;31mAdd php-fpm into chkconfig list failed.\033[0m" chkconfig php-fpm on chkconfig --list|grep 3:on|grep php &gt;/dev/null if [ $? -eq 0 ];then echo -e "\033[1;32mChange php-fpm status to on in chkconfig list successful.\033[0m" else echo -e "\033[1;31mChange php-fpm status to on in chkconfig list failed.\033[0m" fielse echo -e "\033[1;31mAdd php-fpm into chkconfig list failed.\033[0m"finginx日志切割12345678910111213141516171819202122232425262728293031323334#!/bin/bash############################################# Description: This script is used to cut ## nginx logs. #############################################LogsPath=/usr/local/nginx/logsBakDir=/data/backup/nginx_logsPidFile=/usr/local/nginx/logs/nginx.pidYesterday=`date -d 'yesterday' +%F`OLDDATE=`date -d '15 days ago' +%F`#[ ! -d $&#123;BakDir&#125; ] &amp;&amp; mkdir -p $&#123;BakDir&#125;[ ! -d $&#123;BakDir&#125;/$&#123;Yesterday&#125; ] &amp;&amp; mkdir -p $&#123;BakDir&#125;/$&#123;Yesterday&#125; || rm -rf $&#123;BakDir&#125;/$&#123;Yesterday&#125;/* if [ ! -d $&#123;LogsPath&#125; ];then echo "Cannot find nginx log path." exit 1else cd $&#123;LogsPath&#125; for log in `ls *.log`;do mv $&#123;LogsPath&#125;/$&#123;log&#125; $&#123;BakDir&#125;/$&#123;Yesterday&#125;/ kill -USR1 `cat $PidFile` sleep 1 cd $&#123;BakDir&#125;/$&#123;Yesterday&#125;/ tar -zcf $&#123;log&#125;.tar.gz $log rm -rf $log done chown -R nginx.nginx $&#123;LogsPath&#125; # 检查并删除过期的日志文件 cd $&#123;BakDir&#125; &amp;&amp; rm -rf $&#123;OLDDATE&#125;fi检查域名到期时间脚本1（python）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104# -*- coding:utf-8 -*-"""Notice: This script is used to check SSL certificate expire date. Need Python3 env and requests module."""import reimport timeimport requestsimport jsonfrom datetime import datetimeimport traceback"""Notice: This script is used to check domain expire date. Env: Python3 + requests module"""# 定义用到的whois查询站点URLwhois_site = 'http://whoissoft.com/'# 定义要检查的域名列表domains = ['1.com', '2.cn', ]# 定义钉钉告警组robot_token = &#123; 'ops': 'f0f58a2d65412e2c34da2c579627c571212b6************'&#125;left_days = 0# 定义一个钉钉告警函数def alert_by_dingding(grpname, webhook_title, alert_message): if grpname in robot_token: try: token = robot_token[grpname] webhook_url = 'https://oapi.dingtalk.com/robot/send?access_token=%s' % token webhook_header = &#123; "Content-Type": "application/json", "charset": "utf-8" &#125; webhook_message = &#123; "msgtype": "markdown", "markdown": &#123; "title": webhook_title, "text": alert_message &#125; &#125; sendData = json.dumps(webhook_message, indent=1) requests.post(url=webhook_url, headers=webhook_header, data=sendData) except Exception as e: traceback.print_exc(file=open('/tmp/alert_by_dingding.log', 'w+'))# 定义一个用来获取域名信息的函数def get_domain_info(domain): req_url = whois_site + domain headers = &#123;'user-agent': 'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.1) Gecko/2008071615 Fedora/3.0.1-1.fc9 ' 'Firefox/3.0.1'&#125; r = requests.get(req_url, headers=headers, timeout=30) if r.status_code == requests.codes.ok: content = r.text try: # 从返回内容中获取域名到期时间 info = re.search('Registry Expiry Date: (.*?)\n', content) exp_date = re.split('&lt;br /&gt;', info.group(1))[0] # 将时间字符串转换为时间数组，进而计算出剩余天数 expire_date = datetime.strptime(exp_date, '%Y-%m-%dT%H:%M:%SZ') except AttributeError: info = re.search('Expiration Time: (.*?)\n', content) exp_date = re.split('&lt;br /&gt;', info.group(1))[0] # 将时间字符串转换为时间数组，进而计算出剩余天数 expire_date = datetime.strptime(exp_date, '%Y-%m-%d %H:%M:%S') except Exception as e: print("Error message: %s" % (str(e))) finally: left_days = (expire_date - datetime.now()).days expire_day = datetime.strftime(expire_date, '%Y-%m-%d') if left_days &lt;= 90: try: alert_title = "Domain Expiration!!!" alert_content = '&lt;font size="12" color="#ff3300"&gt;【警告】&lt;/font&gt;域名即将过期\n' \ '****\n' \ '域名：%s \n' \ '到期日期：%s \n' \ '剩余天数：%s' % (domain, expire_day, left_days) alert_by_dingding('ops', alert_title, alert_content) except Exception as e: traceback.print_exc(file=open('/tmp/alert_by_dingding.log', 'w+')) # 睡眠1s time.sleep(1)if __name__ == '__main__': if len(domains) &gt; 0: for domain in domains: get_domain_info(domain) else: print("Please check domain list first.")脚本2（python）12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879# _*_ coding:utf-8 _*_import requestsimport reimport datetimeimport jsonimport tracebackwhois_site = 'http://whois.chinaz.com/'#domains = ['2.com', '2.cn']domains = ['jingyu.top', 'dreame.tw', 'stary.ltd']robot_token = &#123; 'ops': 'f0f58a2d65412e2c34da2c579627c571212b6*********************'&#125;# 获取当前日期today = datetime.date.today()# 将当前日期转换为datetime类型tdate = datetime.datetime.strptime(str(today), '%Y-%m-%d')def alert_by_dingding(grpname, webhook_title, alert_message): if grpname in robot_token: try: token = robot_token[grpname] webhook_url = 'https://oapi.dingtalk.com/robot/send?access_token=%s' % token webhook_header = &#123; "Content-Type": "application/json", "charset": "utf-8" &#125; webhook_message = &#123; "msgtype": "markdown", "markdown": &#123; "title": webhook_title, "text": alert_message &#125; &#125; sendData = json.dumps(webhook_message, indent=1) requests.post(url=webhook_url, headers=webhook_header, data=sendData) except Exception as e: traceback.print_exc(file=open('/tmp/alert_by_dingding.log', 'w+'))def get_domain_expiration(domain): req_url = whois_site + domain headers = &#123;'user-agent': 'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.1) Gecko/2008071615 Fedora/3.0.1-1.fc9 ' 'Firefox/3.0.1'&#125; r = requests.get(req_url, headers=headers, timeout=30) if r.status_code == requests.codes.ok: content = r.text info = re.search('&lt;div class="fl WhLeList-left"&gt;过期时间&lt;/div&gt;&lt;div class="fr WhLeList-right"&gt;&lt;span&gt;(\w)+&lt;/span&gt;' , content).group() res = re.split('&lt;span&gt;|&lt;/span&gt;', info)[1] res2 = re.sub('(\D)', '-', res).strip('-') # 将到期日期转换为datetime类型 ddate = datetime.datetime.strptime(res2, '%Y-%m-%d') # 获取剩余天数 leftdays = (ddate - tdate).days if leftdays &lt;= 90: try: alert_title = "Domain Expiration!!!" alert_content = '&lt;font size="12" color="#ff3300"&gt;【警告】&lt;/font&gt;域名即将过期\n' \ '****\n' \ '域名：%s \n' \ '到期日期：%s \n' \ '剩余天数：%s' % (domain, ddate, leftdays) alert_by_dingding('ops', alert_title, alert_content) except Exception as e: print(alert_content)if __name__ == '__main__': for domain in domains: get_domain_expiration(domain)脚本3（shell）domain_list：12baidu.comqq.comdomain.sh：1234567891011121314151617181920212223242526272829303132#!/bin/bash#****************************************************************************************#Author: Jazz#***************************************************************************************#!/bin/bash# 检测域名有效期while read domain; do #取出域名过期时间 end_time=$(whois $domain | grep "Expiration Date" | awk '&#123;print $5&#125;' | cut -d 'T' -f 1) #转换成时间戳 end_times=$(date -d "$end_time" +%s) #以时间戳的形式显示当前时间 current_times=$(date +%s) #域名到期剩余天数 echo $end_times $current_times &amp;&gt; /dev/null let left_time="$end_times - $current_times" days=$(expr $left_time / 86400) if [[ $&#123;days&#125; -lt 0 ]];then echo "$domain 域名已过期" elif [[ $&#123;days&#125; -lt 60 ]];then echo "$domain 域名到期剩余天数: $&#123;days&#125;, the alarm is being sent" curl -X POST -H "Content-Type: application/json" \ -d '&#123;"msgtype": "markdown", "markdown": &#123;"content": "## &lt;font color=red size=22&gt;【警告】&lt;/font&gt;域名即将过期\n ****\n 域名：'$domain' \n 域名到期日: '$end_time' \n 剩余天数: '$days' \n &lt;font color=red size=18&gt;请及时续费！！&lt;/font&gt;"&#125;&#125;' \ https://qyapi.weixin.qq.com/cgi-bin/webhook/send?key=d60217ce-f8e7-4232-b8eb-9c991549127b else echo "$domain 域名到期剩余天数: $&#123;days&#125;" fi done &lt; /root/monitor/domain_list检查ssl证书到期时间检查ssl证书到期时间（python）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192# -*- coding:utf-8 -*-"""Notice: This script is used to check SSL certificate expire date. Need Python3 env and requests module."""import reimport timeimport requestsimport jsonfrom datetime import datetimeimport subprocessimport traceback# 定义要检查的域名列表domains = ['1.com', '2.cn']# 定义钉钉告警组robot_token = &#123; 'ops': 'f0f58a2d65412e2c34da2c579627c571212b6bced***********************'&#125;# 定义一个钉钉告警函数def alert_by_dingding(grpname, webhook_title, alert_message): if grpname in robot_token: try: token = robot_token[grpname] webhook_url = 'https://oapi.dingtalk.com/robot/send?access_token=%s' % token webhook_header = &#123; "Content-Type": "application/json", "charset": "utf-8" &#125; webhook_message = &#123; "msgtype": "markdown", "markdown": &#123; "title": webhook_title, "text": alert_message &#125; &#125; sendData = json.dumps(webhook_message, indent=1) requests.post(url=webhook_url, headers=webhook_header, data=sendData) except Exception as e: traceback.print_exc(file=open('/tmp/alert_by_dingding.log', 'w+'))# 定义一个用来获取ssl证书信息的函数def get_ssl_info(domain): cmd = 'curl -lvs https://&#123;&#125;/'.format(domain) sslinfo = subprocess.getstatusoutput(cmd)[1] try: # 使用正则表达获取证书过期时间 m = re.search('subject:(.*?)\n.*?start date:(.*?)\n.*?expire date:(.*?)\n.*?common name:(.*?)\n.*?issuer:(.*?)\n', sslinfo) start_date = m.group(2) expire_date = m.group(3) cert_name = m.group(4) except Exception as e: print("Error message: %s", str(e)) else: # time字符串转换为时间数组，跟当前的时间相减得出证书剩余有效天数 expire_date_format = datetime.strptime(expire_date, " %b %d %H:%M:%S %Y GMT") # expire_day仅用来保存过期日期（精确到天）,为了钉钉告警格式美观而设置 expire_day = datetime.strftime(expire_date_format, '%Y-%m-%d') left_days = (expire_date_format - datetime.now()).days if left_days &lt;= 90: try: alert_title = "Warning" alert_content = '&lt;font size="12" color="#ff3300"&gt;【警告】&lt;/font&gt;SSL证书即将过期\n' \ '****\n' \ '证书：%s \n' \ '证书到期日：%s \n' \ '剩余天数：%s' % (cert_name, expire_day, left_days) alert_by_dingding('ops', alert_title, alert_content) except Exception as e: #print(alert_content) print("Error message: %s" % str(e)) # 睡眠1s time.sleep(1)if __name__ == '__main__': if len(domains) &gt;0: for domain in domains: get_ssl_info(domain) else: print("Please check domain list first.")检查ssl证书到期时间（shell）https_list：12www.baidu.comwww.qq.comhttps.sh：1234567891011121314151617181920212223242526272829303132333435#!/bin/bash#****************************************************************************************#Author: Jazz#***************************************************************************************#!/bin/bash# 检测https证书有效期source /etc/profilewhile read line; doend_time=$(echo | timeout 1 openssl s_client -servername $line -connect $line:443 2&gt;/dev/null | openssl x509 -noout -enddate 2&gt; /dev/null | awk -F '=' '&#123;print $2&#125;')end_time_cst=$(date -d "$end_time" +"%Y-%m-%d")end_times=$(date -d "$end_time" +%s)current_times=$(date -d "$(date -u '+%b %d %T %Y GMT')" +%s)echo $end_times $current_times &amp;&gt; /dev/nulllet left_time="$end_times - $current_times"days=$(expr $left_time / 86400)if [[ $&#123;days&#125; -eq 0 ]];then echo "$line DNS解析异常"elif [[ "$&#123;days&#125;" -lt 0 ]];then echo "$line 证书已过期"elif [[ "$&#123;days&#125;" -lt 25 ]];then echo "$line 剩余天数: $&#123;days&#125;, the alarm is being sent" curl -X POST -H "Content-Type: application/json" \ -d '&#123;"msgtype": "markdown", "markdown": &#123;"content": "## &lt;font color=red size=22&gt;【警告】&lt;/font&gt;SSL证书即将过期\n ****\n 证书：'$line' \n 证书到期日: '$end_time_cst' \n 剩余天数: '$days' \n &lt;font color=red size=18&gt;请及时续期！！&lt;/font&gt;"&#125;&#125;' \ https://qyapi.weixin.qq.com/cgi-bin/webhook/send?key=d60217ce-f8e7-4232-b8eb-9c991549127belse echo "$line 剩余天数: $&#123;days&#125;"fidone &lt; /root/monitor/https_list监控网络设备故障1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465#!/bin/bash# 企业微信机器人Webhook地址WEBHOOK_URL="https://qyapi.weixin.qq.com/cgi-bin/webhook/send?key=********"# 路由器IP列表ROUTER_IPS=( 10.168.1.1 # 路由器A 10.168.2.1 # 路由器B 10.168.3.1 # 路由器C 10.168.4.1 # 路由器D 192.168.1.1 # 路由器E)# 日志文件路径LOG_FILE="/root/monitor_router/monitor.log"# 检测路由器状态check_router() &#123; local ip=$1 # 发送4个ping包，等待1秒超时 if ! ping -c 4 -W 1 "$ip" &gt; /dev/null 2&gt;&amp;1; then echo "[$(date '+%Y-%m-%d %H:%M:%S')] 路由器故障: $ip" &gt;&gt; "$LOG_FILE" send_alert "$ip" else echo "[$(date '+%Y-%m-%d %H:%M:%S')] 路由器正常: $ip" &gt;&gt; "$LOG_FILE" fi&#125;# 发送企业微信告警send_alert() &#123; local ip=$1 local msg="## &lt;font color=red size=22&gt;【路由器故障告警】&lt;/font&gt; \n ****\n 故障设备: $ip \n 发生时间: $(date '+%Y-%m-%d %H:%M:%S') \n &lt;@chenmingchang&gt;" # 构造JSON数据 local json_data=$(cat &lt;&lt;EOF&#123; "msgtype": "markdown", "markdown": &#123; "content": "$msg", &#125;&#125;EOF) # 发送POST请求 local response=$(curl -s -H "Content-Type: application/json" -d "$json_data" "$WEBHOOK_URL") if [[ $? -ne 0 ]]; then echo "[$(date '+%Y-%m-%d %H:%M:%S')] 告警发送失败: $ip" &gt;&gt; "$LOG_FILE" else echo "[$(date '+%Y-%m-%d %H:%M:%S')] 告警已发送: $ip" &gt;&gt; "$LOG_FILE" fi&#125;# 主循环main() &#123; echo "====== 开始路由器状态检查 ======" &gt; "$LOG_FILE" for ip in "$&#123;ROUTER_IPS[@]&#125;"; do check_router "$ip" done echo "====== 检查完成 ======\n" &gt;&gt; "$LOG_FILE"&#125;# 执行主函数mainMySQL备份1234567891011121314151617181920212223242526272829303132333435363738#!/bin/bash#MYSQL_ROOT_DIR=/usr/local/mysqlMYSQL_EXEC=$MYSQL_ROOT_DIR/bin/mysqlMYSQLDUMP_EXEC=$MYSQL_ROOT_DIR/bin/mysqldump MYSQL_USER=dbbackupMYSQL_PASSWD=backup@159DBNAMES=`$&#123;MYSQL_EXEC&#125; -h 127.0.0.1 -u$&#123;MYSQL_USER&#125; -p$&#123;MYSQL_PASSWD&#125; -e "show databases;"|grep -Ev "^Database|information_schema|performance_schema|test"`YESTERDAY=`date -d yesterday +%F`OLDDATE=`date -d '7 days ago' +%F`BAK_DIR=/storage/backup/mysql[ ! -d $BAK_DIR ] &amp;&amp; mkdir -p $BAK_DIR[ -d $BAK_DIR/$YESTERDAY ] &amp;&amp; rm -rf $BAK_DIR/$YESTERDAYmkdir -p $BAK_DIR/$YESTERDAYecho "MySQL database backup start on $(date +%F' '%T)"echo "========================================="for i in $&#123;DBNAMES&#125; do $&#123;MYSQLDUMP_EXEC&#125; --opt -h 127.0.0.1 -u$&#123;MYSQL_USER&#125; -p$&#123;MYSQL_PASSWD&#125; $i | gzip &gt; $&#123;BAK_DIR&#125;/$&#123;YESTERDAY&#125;/$i.sql.gz [ $? -eq 0 ] &amp;&amp; echo "Backup database $i successful." || echo "Backup database $i failed." sleep 5done $&#123;MYSQLDUMP_EXEC&#125; --opt -h 127.0.0.1 -u$&#123;MYSQL_USER&#125; -p$&#123;MYSQL_PASSWD&#125; --all-databases | gzip &gt; $&#123;BAK_DIR&#125;/$&#123;YESTERDAY&#125;/alldatabase_$&#123;YESTERDAY&#125;.sql.gz [ $? -eq 0 ] &amp;&amp; echo "Backup all databases successful." || echo "Backup all databases failed."cd $BAK_DIR &amp;&amp; rm -rf $&#123;OLDDATE&#125;echo "========================================="echo "MySQL database backup ended on $(date +%F' '%T)." echo "#########################################"发送文件到企业微信1234567891011121314151617181920#!/bin/bashmysql -u root -p你的密码 -h 172.28.81.148 -e "SQL查询语句" &gt; /root/finance/$(date +%F).xlsapi_get="https://qyapi.weixin.qq.com/cgi-bin/webhook/upload_media?key=机器人的KEY&amp;type=file"api_post="https://qyapi.weixin.qq.com/cgi-bin/webhook/send?key=机器人的KEY"scr_file=/root/finance/$(date +%F).xlssleep 3if [[ -f $scr_file ]];then file_name="/root/finance/$(date +%F).xls" media_id=$(curl -s -H 'Content-Type: multipart/form-data' $&#123;api_get&#125; -F "media=@$&#123;file_name&#125;" | jq -r .media_id)# 使用jq --arg进行传参，需注意'.file.media_id'对应其json格式的层级 jq --arg media_id "$media_id" '.file.media_id = $media_id' /root/finance/file.json &gt; /root/finance/new_file.json curl -H 'Content-Type: application/json' -d @/root/finance/new_file.json $&#123;api_post&#125;firm -f $scr_filefile.json内容：123456&#123; "msgtype": "file", "file": &#123; "media_id": "3a8asd892asd8asd" &#125;&#125;检查应用状态检查应用在Google Play商店状态（python）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106# 检查谷歌play应用的上线情况# 需要检查的进程列表import requestsimport sysimport datetime,timeimport hmacimport hashlibimport base64import urllibimport tracebackimport jsonapp_dict = &#123; "应用名1":'包名1', "应用名2":'包名2'&#125;def get_status(appName): url = 'https://play.google.com/store/apps/details' try: response = requests.get(url=url,params=&#123;'id': appName&#125;) except Exception as e: print("连接失败:&#123;0&#125;".format(e)) sys.exit(2) stdout = int(response.status_code) if stdout == 200: return True else: return False# 定义钉钉告警群def get_sign_timestamp(): timestamp = str(round(time.time() * 1000)) secret = 'SEC6eb32d8c662b538872b6dbe83c09674bb5********************' secret_enc = secret.encode('utf-8') string_to_sign = '&#123;&#125;\n&#123;&#125;'.format(timestamp, secret) string_to_sign_enc = string_to_sign.encode('utf-8') hmac_code = hmac.new(secret_enc, string_to_sign_enc, digestmod=hashlib.sha256).digest() sign = urllib.parse.quote_plus(base64.b64encode(hmac_code)) # print(timestamp,sign) return timestamp,sign# 钉钉机器人的token值robot_token = &#123; "ops": 'c3765d92d7b11d4c919f304a0a500ca653cf82c02****************'&#125;# 定义一个钉钉告警函数def alert_by_dingding(grpname, webhook_title, alert_message): if grpname in robot_token: try: token = robot_token[grpname] timestamp,sign = get_sign_timestamp() url = 'https://oapi.dingtalk.com/robot/send?access_token=&#123;0&#125;&amp;timestamp=&#123;1&#125;&amp;sign=&#123;2&#125;'.format(token,timestamp,sign) webhook_header = &#123; "Content-Type": "application/json", "charset": "utf-8" &#125; webhook_message = &#123; "msgtype": "markdown", "markdown": &#123; "title": webhook_title, "text": alert_message &#125; &#125; sendData = json.dumps(webhook_message, indent=1) requests.post(url=url, headers=webhook_header, data=sendData) except Exception as e: traceback.print_exc(e,file=open('/tmp/alert_by_dingding.log', 'w+'))def utc2local(): # “”“UTC时间转本地时间（+8:00) offset = datetime.timedelta(hours=8) utc_time = datetime.datetime.utcnow() print(offset) local_st = utc_time + offset return local_stdef main(): for app in app_dict: result = get_status(app_dict[app]) if result: print("&#123;0&#125;应用已上架".format(app)) continue else: print("&#123;0&#125;应用未上架,将发送报警信息".format(app)) date_now = utc2local().strftime("%Y-%m-%d %H:%M:%S") description="未在谷歌PLAY商店发现该应用，请检查" title = "检查应用上架情况" message = """## 故障**** **应用名称：** &#123;appName&#125; **应用包名：** &#123;appKey&#125; **告警描述：** &#123;TRIGGER_DESCRIPTION&#125; **告警时间：** &#123;EVENT_DATE&#125; **当前状态：** &lt;font size="6" color="#ff3300"&gt;PROBLEM&lt;/font&gt;""".format(appName=app,appKey=app_dict[app],TRIGGER_DESCRIPTION=description,EVENT_DATE=date_now) alert_by_dingding('ops',title,message)main()检查应用在Apple Store商店状态（python）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112# 检查ios应用的上线情况# 需要检查的进程列表import requestsimport sysimport datetime,timeimport hmacimport hashlibimport base64import urllibimport tracebackimport jsonimport timeapp_dict = &#123; "应用名1":'应用名1小写/idAPPID', "应用名2":'应用名2小写/idAPPID',&#125;url = 'https://apps.apple.com/us/app/'def build_uri(endpoint): return '/'.join([url, endpoint])def get_status(appName): try: response = requests.get(build_uri(appName)) except Exception as e: print("连接失败:&#123;0&#125;".format(e)) sys.exit(2) stdout = int(response.status_code) if stdout == 200: return True else: return False time.sleep( 90 )# 定义钉钉告警群def get_sign_timestamp(): timestamp = str(round(time.time() * 1000)) secret = 'SEC6eb32d8c662b538872b6dbe83c0967************' secret_enc = secret.encode('utf-8') string_to_sign = '&#123;&#125;\n&#123;&#125;'.format(timestamp, secret) string_to_sign_enc = string_to_sign.encode('utf-8') hmac_code = hmac.new(secret_enc, string_to_sign_enc, digestmod=hashlib.sha256).digest() sign = urllib.parse.quote_plus(base64.b64encode(hmac_code)) # print(timestamp,sign) return timestamp,sign# 钉钉机器人的token值robot_token = &#123; "ops": 'c3765d92d7b11d4c919f304a0a500ca653cf*********************'&#125;# 定义一个钉钉告警函数def alert_by_dingding(grpname, webhook_title, alert_message): if grpname in robot_token: try: token = robot_token[grpname] timestamp,sign = get_sign_timestamp() url = 'https://oapi.dingtalk.com/robot/send?access_token=&#123;0&#125;&amp;timestamp=&#123;1&#125;&amp;sign=&#123;2&#125;'.format(token,timestamp,sign) webhook_header = &#123; "Content-Type": "application/json", "charset": "utf-8" &#125; webhook_message = &#123; "msgtype": "markdown", "markdown": &#123; "title": webhook_title, "text": alert_message &#125; &#125; sendData = json.dumps(webhook_message, indent=1) requests.post(url=url, headers=webhook_header, data=sendData) except Exception as e: traceback.print_exc(e,file=open('/tmp/alert_by_dingding.log', 'w+'))def utc2local(): # “”“UTC时间转本地时间（+8:00) offset = datetime.timedelta(hours=8) utc_time = datetime.datetime.utcnow() print(offset) local_st = utc_time + offset return local_stdef main(): for app in app_dict: result = get_status(app_dict[app]) if result: print("&#123;0&#125;应用已上架".format(app)) continue else: print("&#123;0&#125;应用未上架,将发送报警信息".format(app)) date_now = utc2local().strftime("%Y-%m-%d %H:%M:%S") description="未在Apple Store发现该应用，请检查" title = "检查应用上架情况" message = """## 故障**** **应用名称：** &#123;appName&#125; **应用包名：** &#123;appKey&#125; **告警描述：** &#123;TRIGGER_DESCRIPTION&#125; **告警时间：** &#123;EVENT_DATE&#125; **当前状态：** &lt;font size="6" color="#ff3300"&gt;PROBLEM&lt;/font&gt;""".format(appName=app,appKey=app_dict[app],TRIGGER_DESCRIPTION=description,EVENT_DATE=date_now) alert_by_dingding('ops',title,message)main()]]></content>
      <categories>
        <category>技术</category>
        <category>脚本</category>
      </categories>
      <tags>
        <tag>脚本</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS7安装zabbix-agent]]></title>
    <url>%2F2022%2F09b1aa2744.html</url>
    <content type="text"><![CDATA[zabbix agent的安装方式主要有源码安装、rpm包安装、二进制包安装等多种方式，本文介绍YUM方式rpm包安装，此方式安装步骤简单、快捷。环境：首先关掉相关安全设置：1234[root@localhost ~]#setenforce 0[root@localhost ~]#sed -i 's#SELINUX=enforcing#SELINUX=disabled#g' /etc/selinux/config[root@localhost ~]#systemctl stop firewalld[root@localhost ~]#systemctl disable firewalld安装3.2版本的zabbix的yum源1[root@localhost ~]#rpm -ivh https://mirrors.aliyun.com/zabbix/zabbix/3.2/rhel/7/x86_64/zabbix-release-3.2-1.el7.noarch.rpm修改zabbix的repo文件中的源地址1[root@localhost ~]#vim /etc/yum.repos.d/zabbix.repo12345678910111213[zabbix]name=Zabbix Official Repository - $basearchbaseurl=http://mirrors.aliyun.com/zabbix/zabbix/3.2/rhel/7/$basearch/enabled=1gpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-ZABBIX-A14FE591[zabbix-non-supported]name=Zabbix Official Repository non-supported - $basearch baseurl=http://mirrors.aliyun.com/zabbix/non-supported/rhel/7/$basearch/enabled=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-ZABBIXgpgcheck=1安装zabbix-agent1[root@localhost ~]#yum -y install zabbix-agent修改agent的配置文件修改3个地方:Server是服务器地址ServerActive是开启主动模式，也写服务地址Hostname写上客户端服务器的主机名，这里写ip地址1[root@localhost ~]#vim /etc/zabbix/zabbix_agentd.conf123Server=192.168.10.10ServerActive=192.168.10.10Hostname=192.168.10.20启动zabbix-agent12[root@localhost ~]#systemctl enable zabbix-agent.service[root@localhost ~]#systemctl start zabbix-agent.service在zabbix后台创建主机：创建完成：]]></content>
      <categories>
        <category>技术</category>
        <category>zabbix</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS6.5安装zabbix3.0.3]]></title>
    <url>%2F2022%2F09f01cefab.html</url>
    <content type="text"><![CDATA[zabbix是一个基于WEB界面的提供分布式系统监视以及网络监视功能的企业级的开源解决方案。zabbix能监视各种网络参数，保证服务器系统的安全运营；并提供灵活的通知机制以让系统管理员快速定位/解决存在的各种问题。zabbix由2部分构成，zabbix server与可选组件zabbix agent。zabbix server可以通过SNMP，zabbix agent，ping，端口监视等方法提供对远程服务器/网络状态的监视，数据收集等功能，它可以运行在Linux，Solaris，HP-UX，AIX，Free BSD，Open BSD，OS X等平台上。环境：首先关掉相关安全设置1234[root@localhost ~]#service iptables stop[root@localhost ~]#chkconfig iptables off[root@localhost ~]#setenforce 0[root@localhost ~]#sed -i 's#SELINUX=enforcing#SELINUX=disabled#g' /etc/selinux/config卸载相关软件包（如果有的话），保持干净环境[root@localhost ~]#yum remove -y mysql* httpd* php* nginx* zabbix* 安装PHPZabbix 3.0对PHP的要求最低为5.4，而CentOS6默认为5.3.3，完全不满足要求，故需要利用第三方源，将PHP升级到5.4以上，注意，不支持PHP712345678[root@localhost ~]#rpm -ivh http://repo.webtatic.com/yum/el6/latest.rpm //安装epel源[root@localhost ~]#yum install php56w php56w-gd php56w-mysql php56w-bcmath php56w-mbstring php56w-xml php56w-ldap //安装软件包依赖[root@localhost ~]#vim /etc/php.ini //修改配置 date.timezone = Asia/Shanghai post_max_size = 32M max_execution_time = 300 max_input_time = 300 always_populate_raw_post_data = -1安装MySQLMySQL建议使用5.6版本，CentOS6默认为5.1，不建议使用，性能偏低12345[root@localhost ~]#wget http://repo.mysql.com/mysql-community-release-el6-5.noarch.rpm [root@localhost ~]#rpm -ivh mysql-community-release-el6-5.noarch.rpm //安装epel源[root@localhost ~]#yum install -y mysql-server mysql-devel //安装软件[root@localhost ~]#service mysqld start[root@localhost ~]#mysql_secure_installation #MySQL安全配置向导Enter current password for root (enter for none): &lt;–初次运行直接回车 Set root password? [Y/n] &lt;– 是否设置root用户密码，输入y并回车或直接回车 Remove anonymous users? [Y/n] &lt;– 是否删除匿名用户,生产环境建议删除，所以直接回车 Disallow root login remotely? [Y/n] &lt;–是否禁止root远程登录,根据自己的需求选择Y/n并回车,建议禁止 Remove test database and access to it? [Y/n] &lt;– 是否删除test数据库,直接回车 Reload privilege tables now? [Y/n] &lt;– 是否重新加载权限表，直接回车 123456789101112131415161718192021[root@localhost ~]#mysql -uroot -pmysql&gt; CREATE DATABASE zabbix CHARACTER SET utf8 COLLATE utf8_bin; #创建mysql库mysql&gt; GRANT ALL PRIVILEGES ON zabbix.* TO zabbix@% IDENTIFIED BY 'zabbix'; #给zabbix用户授权，密码是zabbixmysql&gt; GRANT ALL PRIVILEGES ON *.* TO 'zabbix'@'%'; #给zabbix用户授权所有库，表权限mysql&gt; flush privileges; #刷新MySQL的系统权限相关表mysql&gt; show databases; ±-------------------+| Database |±-------------------+| information_schema || mysql || performance_schema || zabbix |±-------------------+安装Zabbix123456789101112[root@localhost ~]#yum install httpd libxml2-devel net-snmp-devel libcurl-devel #安装httpd及依赖包[root@localhost ~]#groupadd -g 201 zabbix #创建zabbix用户[root@localhost ~]#useradd -g zabbix -u 201 -m zabbix[root@localhost ~]#wget http://sourceforge.net/projects/zabbix/files/ZABBIX%20Latest%20Stable/3.0.3/zabbix-3.0.3.tar.gz[root@localhost ~]#tar zxvf zabbix-3.0.3.tar.gz # 解压[root@localhost ~]#cd zabbix-3.0.3[root@localhost ~]#/usr/bin/mysql -uzabbix -pzabbix zabbix &lt; database/mysql/schema.sql #导入zabbix数据表结构[root@localhost ~]#/usr/bin/mysql -uzabbix -pzabbix zabbix &lt; database/mysql/images.sql #导入zabbix数据表结构[root@localhost ~]#/usr/bin/mysql -uzabbix -pzabbix zabbix &lt; database/mysql/data.sql #导入zabbix数据表结构[root@localhost ~]#/./configure --prefix=/usr/local/zabbix --sysconfdir=/etc/zabbix/ --enable-server --enable-agent --with-net-snmp --with-libcurl --with-mysql --with-libxml2 #编译[root@localhost ~]#/make &amp;&amp;make install #安装编译 zabbix 如果报下面错误，就做以下操作报错信息：configure: error: Not found mysqlclient library123[root@localhost ~]#find / -name libmysqlclient*[root@localhost ~]#ln -s /usr/lib64/mysql/libmysqlclient.so.18.1.0 /usr/lib64/mysql/libmysqlclient.so[root@localhost ~]#ln -s /usr/lib64/mysql/libmysqlclient_r.so.18.1.0 /usr/lib64/mysql/libmysqlclient_r.so配置Zabbix1[root@localhost ~]#vim /etc/zabbix/zabbix_server.confDBHost=192.168.10.10 #数据库ip地址 DBName=zabbix DBUser=zabbix DBPassword=zabbix ListenIP=192.168.10.10 #zabbix server ip地址 1234[root@localhost ~]#ln -s /usr/local/zabbix/sbin/* /usr/sbin/[root@localhost ~]#cp /root/zabbix-3.0.3/misc/init.d/fedora/core/zabbix_* /etc/init.d/[root@localhost ~]#chmod +x /etc/init.d/zabbix_*[root@localhost ~]#sed -i "s@BASEDIR=/usr/local@BASEDIR=/usr/local/zabbix@g" /etc/init.d/zabbix_server配置http WEB1[root@localhost ~]#vim /etc/httpd/conf/httpd.conf123ServerName 127.0.0.1DocumentRoot "/var/www/html"ServerName 192.168.10.10123[root@localhost ~]#mkdir -p /var/www/html/zabbix[root@localhost ~]#cp -r /root/zabbix-3.0.3/frontends/php/* /var/www/html/zabbix/[root@localhost ~]#chown -R apache.apache /var/www/html/zabbix/启动服务123[root@localhost ~]#chkconfig zabbix_server on[root@localhost ~]#/etc/init.d/zabbix_server start[root@localhost ~]#service httpd restart用浏览器访问http://192.168.10.10/zabbix/，并按照提示安装：全部都OK后，点击下一步：输入MySQL密码，点击下一步：继续下一步：点击完成：默认用户名/密码：Admin/zabbix，注意用户名也区分大小写至此zabbix的安装完成！可以查看zabbix服务端日志：1tail -100f /tmp/zabbix_server.log另外：zabbix server is not running 解决方法：1234561.关闭selinux2.关闭防火墙3.登录mysql数据库，查看是否可以登录.4.打开 vim /var/www/html/zabbix/conf/zabbix.conf.php 将server改成ip地址： $DB['SERVER'] = '192.168.10.10'; #ip为MySQL服务器地址. $ZBX_SERVER = '192.168.10.10'; #ip为zabbix sever服务器地址.]]></content>
      <categories>
        <category>技术</category>
        <category>zabbix</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux_shell条件判断if中的-a到-z的含义]]></title>
    <url>%2F2022%2F09c131c9f7.html</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031[ -a FILE ] 如果 FILE 存在则为真。[ -b FILE ] 如果 FILE 存在且是一个块特殊文件则为真。[ -c FILE ] 如果 FILE 存在且是一个字特殊文件则为真。[ -d FILE ] 如果 FILE 存在且是一个目录则为真。[ -e FILE ] 如果 FILE 存在则为真。[ -f FILE ] 如果 FILE 存在且是一个普通文件则为真。[ -g FILE ] 如果 FILE 存在且已经设置了SGID则为真。 [ -h FILE ] 如果 FILE 存在且是一个符号连接则为真。[ -k FILE ] 如果 FILE 存在且已经设置了粘制位则为真。[ -p FILE ] 如果 FILE 存在且是一个名字管道(F如果O)则为真。[ -r FILE ] 如果 FILE 存在且是可读的则为真。[ -s FILE ] 如果 FILE 存在且大小不为0则为真。[ -t FD ] 如果文件描述符 FD 打开且指向一个终端则为真。[ -u FILE ] 如果 FILE 存在且设置了SUID (set user ID)则为真。[ -w FILE ] 如果 FILE 如果 FILE 存在且是可写的则为真。[ -x FILE ] 如果 FILE 存在且是可执行的则为真。[ -O FILE ] 如果 FILE 存在且属有效用户ID则为真。[ -G FILE ] 如果 FILE 存在且属有效用户组则为真。[ -L FILE ] 如果 FILE 存在且是一个符号连接则为真。[ -N FILE ] 如果 FILE 存在 and has been mod如果ied since it was last read则为真。[ -S FILE ] 如果 FILE 存在且是一个套接字则为真。[ FILE1 -nt FILE2 ] 如果 FILE1 has been changed more recently than FILE2, or 如果 FILE1 exists and FILE2 does not则为真。[ FILE1 -ot FILE2 ] 如果 FILE1 比 FILE2 要老, 或者 FILE2 存在且 FILE1 不存在则为真。[ FILE1 -ef FILE2 ] 如果 FILE1 和 FILE2 指向相同的设备和节点号则为真。[ -o OPTIONNAME ] 如果 shell选项 “OPTIONNAME” 开启则为真。[ -z STRING ] “STRING” 的长度为零则为真。[ -n STRING ] or [ STRING ] “STRING” 的长度为非零 non-zero则为真。[ STRING1 == STRING2 ] 如果2个字符串相同。 “=” may be used instead of “==” for strict POSIX compliance则为真。[ STRING1 != STRING2 ] 如果字符串不相等则为真。[ STRING1 &lt; STRING2 ] 如果 “STRING1” sorts before “STRING2” lexicographically in the current locale则为真。[ STRING1 &gt; STRING2 ] 如果 “STRING1” sorts after “STRING2” lexicographically in the current locale则为真。[ ARG1 OP ARG2 ] “OP” is one of -eq, -ne, -lt, -le, -gt or -ge. These arithmetic binary operators return true if “ARG1” is equal to, not equal to, less than, less than or equal to, greater than, or greater than or equal to “ARG2”, respectively. “ARG1” and “ARG2” are integers.=====================================================================基本上和其他脚本语言一样。没有太大区别。不过值得注意的是。[]里面的条件判断。1、字符串判断12345str1 = str2 当两个串有相同内容、长度时为真str1 != str2 当串str1和str2不等时为真-n str1 当串的长度大于0时为真(串非空)-z str1 当串的长度为0时为真(空串)str1 当串str1为非空时为真2、数字的判断123456int1 -eq int2 两数相等为真int1 -ne int2 两数不等为真int1 -gt int2 int1大于int2为真int1 -ge int2 int1大于等于int2为真int1 -lt int2 int1小于int2为真int1 -le int2 int1小于等于int2为真3、文件的判断123456789-r file 用户可读为真-w file 用户可写为真-x file 用户可执行为真-f file 文件为正规文件为真-d file 文件为目录为真-c file 文件为字符特殊文件为真-b file 文件为块特殊文件为真-s file 文件大小非0时为真-t file 当文件描述符(默认为1)指定的设备为终端时为真4、复杂逻辑判断123-a 与-o 或! 非]]></content>
      <categories>
        <category>技术</category>
        <category>脚本</category>
      </categories>
      <tags>
        <tag>脚本</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS 6新增网卡]]></title>
    <url>%2F2022%2F0925ad3e6c.html</url>
    <content type="text"><![CDATA[有时候我们在装完系统后有可能有新增网卡的需求在这种情况下，linux不会主动去添加配置文件ifcfg-eth1的，如果需要使用这块网卡，我们需要复制ifcfg-eth0为ifcfg-eth1，并修改ifcfg-eth1配置：12#[root@localhost ~]# cp /etc/sysconfig/network-scripts/ifcfg-eth0 /etc/sysconfig/network-scripts/ifcfg-eth1#[root@localhost ~]# vi /etc/sysconfig/network-scripts/ifcfg-eth1123456789DEVICE=eth1 #修改网卡名为eth1HWADDR=00:1A:A0:FC:79:D6 #修改为新网卡的MAC地址TYPE=EthernetUUID=3aeb57ea-c5c0-42fa-8dbd-b8eeb3fcadf8 #新设备的UUIDONBOOT=yesNM_CONTROLLED=yesBOOTPROTO=staticIPADDR=192.168.20.10 #修改为为新网卡指派的IP地址NETMASK=255.255.255.0如果为了方便 就只保留 DEVICE， ONBOOT， BOOTPROTO, IPADDR, NETMASK五行就行重启网络服务：1#[root@localhost ~]# service network restart查看IP地址：ps：HWADDR和UUID可以不进行添加，但是如果不进行设置的的话，就需要把这二行删除，不能保留在配置文件里。如果不设置HWADDR的话，系统会在启动网卡时读取硬件的MAC地址；使用ifconfig -a 可以查看到mac地址（HWaddr）而UUID存在的目标则是如果多块网卡在硬件安装的位置发生变化时，系统会根据UUID判断应该读取哪 个配置文件。因为在linux系统里硬件的名称不是固定的，硬件的名称如:eth0 eth1 sda1 sda2 是按硬件安装时的顺序进行命名的。这样的话，如果硬件安装的位置发生变化，就可能出现读取错误配置文件的情况。为了防止这种情况发生，就产生了UUID。查看UUID：若提示无此命令的话，需要安装NetworkManager:启动NetworkManager服务：]]></content>
      <categories>
        <category>桌面</category>
        <category>VMware</category>
      </categories>
      <tags>
        <tag>VMware</tag>
        <tag>网卡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[虚拟机CentOS 6连接外网]]></title>
    <url>%2F2022%2F09612fe0a2.html</url>
    <content type="text"><![CDATA[1.点击编辑，打开虚拟网络编辑器，可以设置子网网段 2.修改网络配置方法一（临时）：使用命令临时指派一个IP给网卡1#ifconfig eth0 192.168.10.10 netmask 255.255.255.0 up方法二（永久）：打开网络配置文件1#[root@localhost ~]# vi /etc/sysconfig/network-scripts/ifcfg-eth0IP地址要在第一步的子网网段范围内，HWADDR要和网卡物理地址一致。然后，重启一下网络服务：1#[root@localhost ~]# service network restart完成后就可以连接外网了ps：网络连接三种模式详解12345桥接模式：相当于在物理主机与虚拟机网卡之间架设了一座桥梁，从而可以通过物理主机的网卡上网。NAT模式：让VM虚拟机的网络服务发挥路由器的作用，使得通过虚拟机软件模拟的主机可以通过物理主机上网，在物理机中NAT虚拟机网卡对应的物理网卡是VMnet8。仅主机模式：仅让虚拟机内的主机与物理主机通信，不能上网，在物理机机中仅主机模式模拟网卡对应的物理网卡是VMnet1。]]></content>
      <categories>
        <category>桌面</category>
        <category>VMware</category>
      </categories>
      <tags>
        <tag>VMware</tag>
        <tag>虚拟网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker入门及实战演练]]></title>
    <url>%2F2022%2F0915f4552.html</url>
    <content type="text"><![CDATA[简介Docker 是一个开源的应用容器引擎，让开发者可以打包他们的应用以及依赖包到一个可移植的容器中，然后发布到任何流行的 Linux 机器上，也可以实现虚拟化。容器是完全使用沙箱机制，相互之间不会有任何接口。几乎没有性能开销，可以很容易地在机器和数据中心中运行。Docker基本组成镜像(Image)镜像，就是面向对象中的类，相当于一个模板。从本质上来说，镜像相当于一个文件系统。Docker 镜像是一个特殊的文件系统，除了提供容器运行时所需的程序、库、资源、配置等文件外，还包含了一些为运行时准备的一些配置参数（如匿名卷、环境变量、用户等）。镜像不包含任何动态数据，其内容在构建之后也不会被改变。容器(Container)容器，就是类创建的实例，就是依据镜像这个模板创建出来的实体。容器的实质是进程，但与直接在宿主执行的进程不同，容器进程运行于属于自己的独立的命名空间。因此容器可以拥有自己的root 文件系统、自己的网络配置、自己的进程空间，甚至自己的用户ID 空间。容器内的进程是运行在一个隔离的环境里，使用起来，就好像是在一个独立于宿主的系统下操作一样。这种特性使得容器封装的应用比直接在宿主运行更加安全。仓库(Repository)仓库，从认识上来说，就好像软件包上传下载站，有各种软件的不同版本被上传供用户下载。镜像构建完成后，可以很容易的在当前宿主机上运行，但是，如果需要在其它服务器上使用这个镜像，我们就需要一个集中的存储、分发镜像的服务，Docker Registry 就是这样的服务。Docker的优势1.更高效的利用系统资源：由于容器不需要进行硬件虚拟以及运行完整操作系统等额外开销，Docker 对系统资源的利用率更高。无论是应用执行速度、内存损耗或者文件存储速度，都要比传统虚拟机技术更高效。因此，相比虚拟机技术，一个相同配置的主机，往往可以运行更多数量的应用。2.更快速的启动时间：传统的虚拟机技术启动应用服务往往需要数分钟，而Docker 容器应用，由于直接运行于宿主内核，无需启动完整的操作系统，因此可以做到秒级、甚至毫秒级的启动时间。3.一致的运行环境：开发过程中一个常见的问题是环境一致性问题。由于开发环境、测试环境、生产环境不一 致，导致有些bug 并未在开发过程中被发现。而Docker 的镜像提供了除内核外完整的运行时环境，确保了应用运行环境一致性，从而不会再出现这类问题。4.持续交付和部署：Docker是build once，run everywhere. 使用Docker 可以通过定制应用镜像来实现持续集成、持续交付、部署。开发人员可以通过Dockerfile 来进行镜像构建，并结合持续集成(Continuous Integration) 系统进行集成测试，而运维人员则可以直接在生产环境中快速部署该镜像。5.更轻松的迁移：Docker 使用的分层存储以及镜像的技术，使得应用重复部分的复用更为容易，也使得应用的维护更新更加简单，基于基础镜像进一步扩展镜像也变得非常简单。传统开发流程:Docker环境开发流程:与传统虚拟机对比Docker命令创建镜像1.1基于已有的镜像容器创建1docker commit [options] container [repository[:tag]]option: -a,--author=“” #作者信息 -m,--message=“” #提交信息 -p,--pause=true #提交时暂停容器运行 1.2基于本地模板导入创建docker load &lt; ***.tar --本地模板文件tar 1.3基于Dockerfile文件构建镜像docker build -t image-name basedir 删除镜像docker rmi image #image可以是标签或者ID docker rmi –f image #强制删除镜像 注意：用docker rmi 命令删除镜像时，首先要删除容器，再删除镜像。否则会提示镜像在容器中运行。镜像管理指令:创建/启动/停止/删除容器docker create image #创建的容器是停止状态 docker start/stop container_id #启动/停止容器 docker run image #创建并启动容器 docker rm container_id #删除容器 创建容器常用选项管理容器常用命令镜像与容器联系镜像不是一个单一的文件，而是有多层构成。我们可以通过docker history &lt;ID/NAME&gt; 查看镜像中各层内容及大小，每层对应着Dockerfile中的一条指令。Docker镜像默认存储在/var/lib/docker/中。容器其实是在镜像的最上面加了一层读写层，在运行容器里做的任何文件改动，都会写到这个读写层。如果容器删除了，最上面的读写层也就删除了，改动也就丢失了。Docker使用存储驱动管理镜像每层内容及可读写层的容器层。将主机数据挂载到容器Docker提供三种不同的方式将数据从宿主机挂载到容器中：volumes，bind mounts和tmpfs。123volumes：Docker管理宿主机文件系统的一部分（/var/lib/docker/volumes）。bind mounts：可以存储在宿主机系统的任意位置。tmpfs：挂载存储在宿主机系统的内存中，而不会写入宿主机的文件系统。volume注意：12# 如果没有指定卷，自动创建。# 建议使用--mount，更通用。Bind Mounts注意：12# 如果源文件/目录没有存在，不会自动创建，会抛出一个错误。# 如果挂载目录在容器中非空目录，则该目录现有内容将被隐藏。tmpfs容器中使用 tmpfs：12docker run -d -it --name nginx-test --mount type=tmpfs,destination=/usr/share/nginx/html nginxdocker run -d -it --name nginx-test --tmpfs /usr/share/nginx/html nginx注意：123# tmpfs方式仅存储在主机系统的内存中，不会写入主机的文件系统。# tmpfs挂载不能在容器间共享。# tmpfs只能在Linux容器上工作，不能在Windows容器上工作。Docker实战–构建lnmp环境，搭建WordPress博客实验环境：Docker安装首先安装依赖包安装Docker（之前已安装过，所以提示已经安装）查看Docker是否安装成功启动Docker并加入开机自启动12[root@localhost ~]# systemctl start docker[root@localhost ~]# systemctl enable docker用Dockerfile方式构建镜像Dockerfile指令环境说明：在本文中我都是基于centos 7.5系统，nginx和php用的源码包来构建，如果你不想用源码包，也可用yum方式构建。12nginx，用的是源码包来构建，版本为nginx-1.12.2.tar.gz，下载地址http://nginx.org/en/download.html/php，也用的源码包来构建，版本为php-5.6.31.tar.gz，下载地址http://php.net/downloads.php创建镜像时所需文件在Dockerfiles目录下创建了两个目录（nginx，php），里面分别存放Dockerfile文件、源码包。nginx目录下放了nginx.conf配置文件，php目录下也放置了php.ini配置文件（在实际环境中，这两个文件是经常需要修改的，单独拿出来后在启动容器时你可以把这两个文件mount到容器中，便于管理。）。nginx 构建Dockerfile内容：分析一下Dockerfile的内容，当你构建镜像时，它会根据你编排好的内容一步一步的执行下去，如果当中的某一步执行不下去，会立刻停止构建。上面的大部分指令都很好理解，大家可以对照上文的Dockerfile指令图进行理解，最后一个指令我要详细说明一下：CMD [“./sbin/nginx”,“-g”,“daemon off;”]./sbin/nginx ，就是正常启动nginx服务；-g： 设置配置文件外的全局指令，也就是启动nginx时设置了daemon off参数，默认参数是打开的on，是否以守护进程的方式运行nginx，守护进程是指脱离终端并且在后台运行的进程。这里设置为off，也就是不让它在后台运行。为什么我们启动nginx容器时不让它在后台运行呢，docker 容器默认会把容器内部第一个进程，也就是pid=1的程序作为docker容器是否正在运行的依据，如果docker 容器pid挂了，那么docker容器便会直接退出。nginx.conf 内容：配置中主要添加了 location ~ .php这一段的内容，其中fastcgi_pass的 lnmp_php，这个是后面启动php容器时的名称。当匹配到php的请求时，它会转发给lnmp_php这个容器php-fpm服务来处理。正常情况下，如果php服务不是跑在容器中，lnmp_php这个内容一般写php服务器的Ip地址。build 构建nginx镜像切换到nginx目录下：构建：1[root@localhost nginx]# docker build -t nginx:1.12.2查看镜像是否构建成功：PHP构建Dockerfile内容：php.ini 内容（PHP默认的配置内容就好，实在找不到的可以复制粘贴我的）[PHP] engine = On short_open_tag = Off asp_tags = Off precision = 14 output_buffering = 4096 zlib.output_compression = Off implicit_flush = Off unserialize_callback_func = serialize_precision = 17 disable_functions = disable_classes = zend.enable_gc = On expose_php = On max_execution_time = 300 max_input_time = 300 memory_limit = 128M error_reporting = E_ALL &amp; ~E_DEPRECATED &amp; ~E_STRICT display_errors = Off display_startup_errors = Off log_errors = On log_errors_max_len = 1024 ignore_repeated_errors = Off ignore_repeated_source = Off report_memleaks = On track_errors = Off html_errors = On variables_order = &quot;GPCS&quot; request_order = &quot;GP&quot; register_argc_argv = Off auto_globals_jit = On post_max_size = 32M auto_prepend_file = auto_append_file = default_mimetype = &quot;text/html&quot; always_populate_raw_post_data = -1 doc_root = user_dir = enable_dl = Off file_uploads = On upload_max_filesize = 2M max_file_uploads = 20 allow_url_fopen = On allow_url_include = Off default_socket_timeout = 60 [CLI Server] cli_server.color = On [Date] date.timezone = Asia/Shanghai [filter] [iconv] [intl] [sqlite] [sqlite3] [Pcre] [Pdo] [Pdo_mysql] pdo_mysql.cache_size = 2000 pdo_mysql.default_socket= [Phar] [mail function] SMTP = localhost smtp_port = 25 mail.add_x_header = On [SQL] sql.safe_mode = Off [ODBC] odbc.allow_persistent = On odbc.check_persistent = On odbc.max_persistent = -1 odbc.max_links = -1 odbc.defaultlrl = 4096 odbc.defaultbinmode = 1 [Interbase] ibase.allow_persistent = 1 ibase.max_persistent = -1 ibase.max_links = -1 ibase.timestampformat = &quot;%Y-%m-%d %H:%M:%S&quot; ibase.dateformat = &quot;%Y-%m-%d&quot; ibase.timeformat = &quot;%H:%M:%S&quot; [MySQL] mysql.allow_local_infile = On mysql.allow_persistent = On mysql.cache_size = 2000 mysql.max_persistent = -1 mysql.max_links = -1 mysql.default_port = mysql.default_socket = mysql.default_host = mysql.default_user = mysql.default_password = mysql.connect_timeout = 60 mysql.trace_mode = Off [MySQLi] mysqli.max_persistent = -1 mysqli.allow_persistent = On mysqli.max_links = -1 mysqli.cache_size = 2000 mysqli.default_port = 3306 mysqli.default_socket = mysqli.default_host = mysqli.default_user = mysqli.default_pw = mysqli.reconnect = Off [mysqlnd] mysqlnd.collect_statistics = On mysqlnd.collect_memory_statistics = Off [OCI8] [PostgreSQL] pgsql.allow_persistent = On pgsql.auto_reset_persistent = Off pgsql.max_persistent = -1 pgsql.max_links = -1 pgsql.ignore_notice = 0 pgsql.log_notice = 0 [Sybase-CT] sybct.allow_persistent = On sybct.max_persistent = -1 sybct.max_links = -1 sybct.min_server_severity = 10 sybct.min_client_severity = 10 [bcmath] bcmath.scale = 0 [browscap] [Session] session.save_handler = files session.use_strict_mode = 0 session.use_cookies = 1 session.use_only_cookies = 1 session.name = PHPSESSID session.auto_start = 0 session.cookie_lifetime = 0 session.cookie_path = / session.cookie_domain = session.cookie_httponly = session.serialize_handler = php session.gc_probability = 1 session.gc_divisor = 1000 session.gc_maxlifetime = 1440 session.referer_check = session.cache_limiter = nocache session.cache_expire = 180 session.use_trans_sid = 0 session.hash_function = 0 session.hash_bits_per_character = 5 url_rewriter.tags = &quot;a=href,area=href,frame=src,input=src,form=fakeentry&quot; [MSSQL] mssql.allow_persistent = On mssql.max_persistent = -1 mssql.max_links = -1 mssql.min_error_severity = 10 mssql.min_message_severity = 10 mssql.compatibility_mode = Off mssql.secure_connection = Off [Assertion] [COM] [mbstring] [gd] [exif] [Tidy] tidy.clean_output = Off [soap] soap.wsdl_cache_enabled=1 soap.wsdl_cache_dir=&quot;/tmp&quot; soap.wsdl_cache_ttl=86400 soap.wsdl_cache_limit = 5 [sysvshm] [ldap] ldap.max_links = -1 [mcrypt] [dba] [opcache] [curl] build构建php镜像php源码包、php.ini、Dockerfile都准备好了之后，现在我们可以来用docker build来构建这个镜像了：切换到php目录下：构建php镜像：1[root@localhost php]# docker build -t php:5.6.31 .查看镜像是否构建成功：运行容器创建自定义网络lnmp先创建一个自定义网络，运行ningx、php这些容器的时候加入到lnmp网络中来：1234# 查看默认网络：[root@localhost php]# docker network ls创建：[root@localhost php]# docker network create lnmp创建php容器创建容器：1[root@localhost php]# docker run -itd --name lnmp_php --network lnmp -v /app/wwwroot:/usr/local/nginx/html php:5.6.31参数说明：12345-itd： # 在容器中打开一个伪终端进行交互操作，并在后台运行；--name： # 为容器分配一个名字lnmp_php；--network： # 为容器指定一个网络环境为lnmp网络；--mout： # 把宿主机的/app/wwwroot目录挂载到容器的/usr/local/nginx/html目录，挂载也相当于数据持久化；php:5.6.31： # 指定刚才构建的php镜像来启动容器；查看php容器是否运行：创建nginx容器：创建容器：1[root@localhost php]# docker run -itd --name lnmp_nginx --network lnmp -p 80:80 -v /app/wwwroot:/usr/local/nginx/html nginx:1.12.2查看容器是否运行：测试访问创建一个index.html静态页面来访问：1[root@localhost wordpress]# echo "Dockerfile lnmp test" &gt; /app/wwwroot/index.html用浏览器访问宿主机的IP：再创建一个index.php文件开测试：1[root@localhost wordpress]# echo "&lt;? phpinfo();" &gt; /app/wwwroot/index.php访问PHP页面：创建mysql容器1[root@localhost ~]# docker run -itd --network lnmp -p 3306:3306 --name mysql -e MYSQL_ROOT_PASSWORD=123456 mysql:5.6 --character-set-server=utf8 #本地没有mysql5.6的镜像的话，会自动从仓库拉取；查看mysql容器是否启动：进到mysql容器里创建wordpress数据库：查看wordpress是否创建成功：Ctrl+p再Ctrl+q退出mysql容器，回到宿主机下载wordpress博客系统测试lnmp下载wordpress文件到/app/wwwroot目录下wordpress文件下载地址： https://cn.wordpress.org/wordpress-4.7.4-zh_CN.tar.gz解压wordpress压缩包并访问测试：1root@localhost ~]# tar -zxvf wordpress-4.7.4-zh_CN.tar.gz打开浏览器访问：http://容器宿主机IP/wordpress/wp-admin/setup-config.php配置wordpress博客：提交：输入信息安装Wordpress： 文件迁移可用docker save命令将镜像保存为一个tar文件：1[root@localhost ~]# docker save nginx:1.12.2 | gzip &gt; nginx1.12.2.tar.gz再用docker load命令导入：1[root@localhost ~]# docker load &lt; nginx.1.12.2.tar.gz]]></content>
      <categories>
        <category>技术</category>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux脚本小知识之含义]]></title>
    <url>%2F2022%2F0949af059c.html</url>
    <content type="text"><![CDATA[Linux系统中的Shell脚本语言内设了用于接收参数的变量，变量之间可以使用空格间隔。0对应的是当前Shell脚本程序的名称，#对应的是总共有几个参数，*对应的是所有位置的参数值，∗对应的是所有位置的参数值，?对应的是显示上一次命令的执行返回值，而$1、$2、$3……则分别对应着第N个位置的参数值。下面进行一个简单的小脚本编写：1[root@localhost tmp]# vim shell.sh编写完成后执行一下脚本：1[root@localhost tmp]# sh shell.sh wo shi chen ming chang上面的“wo shi chen ming chang”是我们执行脚本时输入的5个参数，最后一个命令执行返回值为0表示命令执行成功。]]></content>
      <categories>
        <category>技术</category>
        <category>脚本</category>
      </categories>
      <tags>
        <tag>脚本</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[电脑开机按F1键进系统]]></title>
    <url>%2F2022%2F091a9b166c.html</url>
    <content type="text"><![CDATA[问题名称：电脑开机要按F1键才能进系统问题表现：启动电脑后，需要按F1键才能进系统，下面有提示：511-cpu fan not detected解决方法：进入BIOS，选择power菜单，进入其下面的Thermal ，普通进到这个下面，只有两项： CPU Fan Speed 和 System Fan Speed 。在Thermal界面下，按CTRL 和 A 按键 ，然后f10进入，会多出两项： CPU Fan Check 和 System Fan Check 。这就是解决问题的关键。 更改CPU Fan Check的值为 disabled,然后保存重启即可 。]]></content>
      <categories>
        <category>桌面</category>
        <category>故障</category>
      </categories>
      <tags>
        <tag>系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[电脑开机黑屏]]></title>
    <url>%2F2022%2F09fc1c96cc.html</url>
    <content type="text"><![CDATA[问题名称：电脑开不了机问题表现：电脑显示屏正常，但进不了系统，开机显示屏黑屏，无反应；拆开主机时散热器时转时不转。解决方法：更换电源注意事项：出现这种情况，大多数情况是电源，内存，纽扣电池出现问题；还解决不了可逐一排除电源，内存，硬盘，处理器，主板是否出现问题。电源接线：连接主板供电：24针连接CPU：8针连接独立显卡：6针连接硬盘：一般有4个接口可用，不够时可用转换器接光驱的接口。]]></content>
      <categories>
        <category>桌面</category>
        <category>故障</category>
      </categories>
      <tags>
        <tag>黑屏</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[打印机工作异常]]></title>
    <url>%2F2022%2F09f28a3245.html</url>
    <content type="text"><![CDATA[1． 问题名称：打印机工作异常2． 问题表现：打印文件时，打印机没反应，打印机后台显示文档被挂起。3． 解决方法：3.1、关闭打印服务：开始-运行-services.msc或打开控制面板-管理工具-服务，打开服务列表，找到Print Spooler(打印服务)，关闭(右击，点“关闭”）。3.2、删除打印缓存：进入c:\windows\system32\spool\printers(如果是windows 2000，则是c:\winnt\system32\spool\printers)，删除printers文件夹中的文件(打印缓存)。]]></content>
      <categories>
        <category>桌面</category>
        <category>打印机</category>
      </categories>
      <tags>
        <tag>打印机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVN客户端使用]]></title>
    <url>%2F2022%2F096cbb58b6.html</url>
    <content type="text"><![CDATA[Linux命令行SVN使用教程客户端使用：(如果是第一次提交文件，很可能会出现“svn：’.’不是工作副本”，即当前目录不是工作副本，这个时候需要用到import)eg：1svn import . url1、将文件checkout到本地目录1svn checkout path #path是服务器上的目录例如：1svn checkout svn://192.168.10.60/jingyu简写：1svn co2、往版本库中添加新的文件1svn add file例如：1svn add test.php #添加test.php1svn add *.php #添加当前目录下所有的php文件3、将改动的文件提交到版本库1svn commit -m “LogMessage“ [-N] [–no-unlock] PATH #如果选择了保持锁，就使用–no-unlock开关例如：1svn commit -m “add test file for my test“ test.php简写：1svn ci4、加锁/解锁1svn lock -m “LockMessage“ [–force] PATH例如：12svn lock -m “lock test file“ test.phpsvn unlock PATH5、更新到某个版本1svn update -r m path例如：123svn update #如果后面没有接目录，默认将当前目录以及子目录下的所有文件都更新到最新版本。svn update -r 200 test.php #将版本库中的文件test.php还原到版本200svn update test.php #更新，于版本库同步。如果在提交的时候提示过期的话，是因为冲突，需要先update，修改文件，然后清除 svn resolved，最后再提交commit简写：1svn up6、查看文件或者目录状态1svn status path（目录下的文件和子目录的状态，正常状态不显示）【?：不在svn的控制中；M：内容被修改；C：发生冲突；A：预定加入到版本库；K：被锁定】M状态一般比较多1svn status -v path(显示文件和子目录状态)第一列保持相同，第二列显示工作版本号，第三和第四列显示最后一次修改的版本号和修改人。注：svn status、svn diff和 svn revert这三条命令在没有网络的情况下也可以执行的，原因是svn在本地的.svn中保留了本地版本的原始拷贝。简写：1svn st7、删除文件1svn delete path -m “delete test fle“例如：1svn delete svn://192.168.10.60/opt/svn/jingyu/test.php -m “delete test file”或者直接svn delete test.php 然后再svn ci -m ‘delete test file‘，推荐使用这种简写：1svn (del, remove, rm)8、查看日志1svn log path例如：1svn log test.php #显示这个文件的所有修改记录，及其版本号的变化9、查看文件详细信息1svn info path例如：1svn info test.php10、比较差异1svn diff path #将修改的文件与基础版本比较例如：1svn diff test.php1svn diff -r m:n path #对版本m和版本n比较差异例如：1svn diff -r 200:201 test.php简写：1svn di11、将两个版本之间的差异合并到当前文件1svn merge -r m:n path例如：1svn merge -r 200:205 test.php #将版本200与205之间的差异合并到当前文件，但是一般都会产生冲突，需要处理一下12、SVN 帮助12svn helpsvn help ci——————————————————————————以上是常用命令，下面写几个不经常用的——————————————————————————13、版本库下的文件和目录列表1svn list path显示path目录下的所有属于版本库的文件和目录简写：1svn ls14、创建纳入版本控制下的新目录1svn mkdir #创建纳入版本控制下的新目录用法:1、mkdir PATH…2、mkdir URL…创建版本控制的目录。1、每一个以工作副本 PATH 指定的目录，都会创建在本地端，并且加入新增调度，以待下一次的提交。2、每个以URL指定的目录，都会透过立即提交于仓库中创建。在这两个情况下，所有的中间目录都必须事先存在。15、恢复本地修改1svn revert #恢复原始未改变的工作副本文件 (恢复大部份的本地修改)用法: revert PATH…注意: 本子命令不会存取网络，并且会解除冲突的状况。但是它不会恢复被删除的目录16、代码库URL变更1svn switch (sw) #更新工作副本至不同的URL用法:1、switch URL [PATH]2、switch –relocate FROM TO [PATH…]1、更新你的工作副本，映射到一个新的URL，其行为跟“svn update”很像，也会将服务器上文件与本地文件合并。这是将工作副本对应到同一仓库中某个分支或者标记的方法。2、改写工作副本的URL元数据，以反映单纯的URL上的改变。当仓库的根URL变动(比如方案名或是主机名称变动)，但是工作副本仍旧对映到同一仓库的同一目录时使用这个命令更新工作副本与仓库的对应关系。17、解决冲突1svn resolved #移除工作副本的目录或文件的“冲突”状态用法: resolved PATH…注意: 本子命令不会依语法来解决冲突或是移除冲突标记；它只是移除冲突的相关文件，然后让 PATH 可以再次提交。18、输出指定文件或URL的内容。12svn cat 目标[@版本]…如果指定了版本，将从指定的版本开始查找。svn cat -r PREV filename &gt; filename (PREV 是上一版本,也可以写具体版本号,这样输出结果是可以提交的)]]></content>
      <categories>
        <category>技术</category>
        <category>SVN</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>SVN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux部署SVN服务器]]></title>
    <url>%2F2022%2F09dc8e267.html</url>
    <content type="text"><![CDATA[SVN是Subversion的简称，是一个开放源代码的版本控制系统，用于多个人共同开发同一个项目，共用资源的目的。环境：CentOs6.1查看是否安装了svn工具：rpm -qa | grep subversion (查看版本：svnserve –version)（未安装的可以用yum install subversion 安装）1、建立SVN的根目录1mkdir -p /opt/svn/tshop/2、建立一个产品仓库1svnadmin create /opt/svn/tshop/如果你们的研发中心有多个产品组，每个产品组可以建立一个SVN仓库目录用途说明：1234hooks目录：放置hook脚本文件的目录locks目录：用来放置subversion的db锁文件和db_logs锁文件的目录，用来追踪存取文件库的客户端format文件：是一个文本文件，里面只放了一个整数，表示当前文件库配置的版本号conf目录：是这个仓库的配置文件（仓库的用户访问账号、权限等）3、修改版本配置库文件123456vi /opt/svn/tshop/conf/svnserve.confanon-access = none # 注意这里必须设置，否则所有用户不用密码就可以访问auth-access = writepassword-db = passwdauthz-db = authzrealm = tshop对用户配置文件的修改是立即生效的，不必重启svn。4、开始设置passwd用户账号信息1vi /opt/svn/tshop/conf/passwd123456[users]#harry = harryssecret#sally = sallyssecret###===下面是我添加的用户信息#######chenmingchang = 123456svntest = 1234565、开始设置authz. 用户访问权限1vi /opt/svn/tshop/conf/authz12345678910111213141516[groups]devteam = chenmingchang, svntest #devteam组包括两个用户[/]chenmingchang = rwsvntest = r[tshop:/tb2c]@devteam = rwsvntest =[tshop:/tb2b2c]@devteam = rwchenmingchang = r6、注意：权限配置文件中出现的用户名必须已在用户配置文件中定义。对权限配置文件的修改立即生效，不必重启svn。用户组格式：[groups]= ,其中，1个用户组可以包含1个或多个用户，用户间以逗号分隔。版本库目录格式：[&lt;版本库&gt;:/项目/目录]@&lt;用户组名&gt; = &lt;权限&gt;&lt;用户名&gt; = &lt;权限&gt;其中，方框号内部分可以有多种写法:[/],表示根目录及以下，根目录是svnserve启动时指定的，我们指定为/opt/svn/tshop，[/]就是表示对全部版本库设置权限。[tshop:/] 表示对版本库tshop设置权限；[tshop:/abc] 表示对版本库tshop中的abc项目设置权限；[tshop:/abc/aaa] 表示对版本库tshop中的abc项目的aaa目录设置权限；权限主体可以是用户组、用户或，用户组在前面加@，表示全部用户。权限可以是w、r、wr和空，空表示没有任何权限。7、启动svn：1svnserve -d -r /opt/svn/tshop #默认的启动端口号为36908、检查是否启动：1netstat -tunlp | grep svn或1ps –ef | grep svn9、将svn加入到开机启动编辑rc.local文件：1vi /etc/rc.d/rc.local加入并保存如下启动命令：1/usr/local/svn/bin/svnserve –d --listen-port 3690 -r /opt/svn/tshop10、如果想停止svn，则使用如下命令：1killall svnserve或1pkill svn11、如果想将svn作为服务：在/etc/rc.d/init.d/目录下新建名为svn的文件:1touch /etc/rc.d/init.d/svn并设置权限为755：1chmod 755 /etc/rc.d/init.d/svn编辑svn文件：1vi /etc/rc.d/init.d/svn在里面添加如下代码：1234567891011121314151617181920212223242526272829#!/bin/bash#build this file in /etc/rc.d/init.d/svn#chmod 755 /etc/rc.d/init.d/svn#centos下可以用如下命令管理svn: service svn start(restart/stop)SVN_HOME=/opt/svn/tshopif [ ! -f “/usr/local/svn/bin/svnserve” ]thenecho “svnserver startup: cannot start”exitficase “$1” instart)echo “Starting svnserve…”/usr/local/svn/bin/svnserve -d --listen-port 9999 -r $SVN_HOMEecho “Finished!”;;stop)echo “Stoping svnserve…”killall svnserveecho “Finished!”;;restart)$0 stop$0 start;;*)echo "Usage: svn &#123; start | stop | restart &#125; "exit 1esac保存，之后便可以以service svn start(restart/stop)方式启动SVN。至此，svn服务器配置完毕。下一篇将讲解如何在客户端上使用svn。]]></content>
      <categories>
        <category>技术</category>
        <category>SVN</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>SVN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[虚拟机安装Linux系统]]></title>
    <url>%2F2022%2F09dc84a67d.html</url>
    <content type="text"><![CDATA[VMware 可以使你在一台计算机上同时运行多个操作系统，例如同时运行 Windows、Linux 和 Mac OS。在计算机上直接安装多个操作系统，同一个时刻只能运行一个操作系统，重启才可以切换；而 Vmware 可以同时运行多个操作系统，可以像 Windows 应用程序一样来回切换。1.VMware1.1 打开VMware Workstation，单击“创建新的虚拟机”选项，并在弹出的“新建虚拟机向导”界面中选择“典型”单选按钮，然后单击“下一步”按钮。1.2 选中“稍后安装操作系统”单选按钮，然后单击“下一步”按钮1.3 将客户机操作系统的类型选择为“Linux”，版本为“Red Hat Enterprise Linux 7 64位”，然后单击“下一步”按钮。1.4 填写“虚拟机名称”字段，并在选择安装位置之后单击“下一步”按钮1.5 将虚拟机系统的“最大磁盘大小”设置为20.0GB（默认即可），然后单击“下一步”按钮1.6 单击“自定义硬件”按钮1.7 建议将虚拟机系统内存的可用量设置为2GB，最低不应低于1GB1.8 根据物理机的性能设置CPU处理器的数量以及每个处理器的核心数量，并开启虚拟化功能1.9 光驱设备此时应在“使用ISO镜像文件”中选中了下载好的RHEL系统镜像文件（镜像文件链接： https://pan.baidu.com/s/1PbEu4MKBovHdi3E-mgqQIw 提取码: tj6j ）1.10 VM虚拟机软件为用户提供了3种可选的网络模式，分别为桥接模式、NAT模式与仅主机模式。这里选择“仅主机模式”。桥接模式：相当于在物理主机与虚拟机网卡之间架设了一座桥梁，从而可以通过物理主机的网卡上网。NAT模式：让VM虚拟机的网络服务发挥路由器的作用，使得通过虚拟机软件模拟的主机可以通过物理主机上网，在物理机中NAT虚拟机网卡对应的物理网卡是VMnet8。仅主机模式：仅让虚拟机内的主机与物理主机通信，不能上网，在物理机机中仅主机模式模拟网卡对应的物理网卡是VMnet1。1.11 返回到虚拟机配置向导界面后单击“完成”按钮安装linux系统安装RHEL 7或CentOS 7系统时，您的电脑的CPU需要支持VT（Virtualization Technology，虚拟化技术）。所谓VT，指的是让单台计算机能够分割出多个独立资源区，并让每个资源区按照需要模拟出系统的一项技术，其本质就是通过中间层实现计算机资源的管理和再分配，让系统资源的利用率最大化。如果开启虚拟机后提示“CPU不支持VT技术”等报错信息，请重启电脑并进入到BIOS中把VT虚拟化功能开启即可。2.1 在虚拟机管理界面中单击“开启此虚拟机”按钮后数秒就看到RHEL 7系统安装界面。在界面中，Test this media &amp; install Red Hat Enterprise Linux 7.0和Troubleshooting的作用分别是校验光盘完整性后再安装以及启动救援模式。此时通过键盘的方向键选择Install Red Hat Enterprise Linux 7.0选项来直接安装Linux系统。2.2 接下来按回车键后开始加载安装镜像，所需时间大约在30～60秒2.3 选择系统的安装语言后单击Continue按钮2.4 在安装界面中单击SOFTWARE SELECTION选项2.5 在界面中单击选中Server with GUI单选按钮，然后单击左上角的Done按钮2.6 返回到RHEL 7系统安装主界面，单击NETWORK &amp; HOSTNAME选项后，将Hostname字段设置为你想设置的名称，然后单击左上角的Done按钮2.7 返回到安装主界面，单击INSTALLATION DESTINATION选项来选择安装媒介并设置分区。此时不需要进行任何修改，单击左上角的Done按钮即可2.8 返回到安装主界面，单击Begin Installation按钮后即可看到安装进度，在此处选择ROOT PASSWORD2.9 设置root管理员的密码。若坚持用弱口令的密码则需要单击2次左上角的Done按钮才可以确认2.10 Linux系统安装过程一般在30～60分钟，在安装过程期间耐心等待即可。安装完成后单击Reboot按钮2.11 重启系统后将看到系统的初始化界面，单击LICENSE INFORMATION选项2.12 选中I accept the license agreement复选框，然后单击左上角的Done按钮2.13 返回到初始化界面后单击FINISH CONFIGURATION选项，即可看到Kdump服务的设置界面。如果暂时不打算调试系统内核，也可以取消选中Enable kdump复选框，然后单击Forward按钮2.14 选中No, I prefer to register at a later time单选按钮，然后单击Finish按钮。此处设置为不注册系统2.15 虚拟机软件中的RHEL 7系统经过又一次的重启后，看到系统的欢迎界面，在界面中选择默认的语言English (United States)，然后单击Next按钮。2.16 将系统的输入来源类型选择为English (US)，然后单击Next按钮2.17 为RHEL 7系统创建一个本地的普通用户，该账户的用户名为xxx，密码为xxx，然后单击Next按钮2.18 设置系统的时区，然后单击Next按钮2.19 单击Start using Red Hat Enterprise Linux Server按钮，至此，RHEL 7系统完成了全部的安装和部署工作。最后，希望入门的小伙伴不要依赖图形界面，要熟悉字符界面，才能快速提升，将来在生产环境中使用的也都是字符界面。图形界面转字符界面：使用Ctrl+Alt+F1~Ctrl+Alt+F6，可切换到不同的字符控制台。字符界面转图形界面：startx或使用Ctrl+Alt+F7修改Linux 默认启动级别（模式）：1.CentOs6.1: 以root 身份 vi /etc/inittab，找到 id:5:initdefault: 所在的行（这里的id 可能是其他数字）。修改id 值：3就是文字模式，5就是界面模式。保存该文件，reboot 后即生效。2.Red Hat7.0: 在命令行中输入systemctl get-default，意为获取当前启动模式,graphical.target 为图形界面启动,multi-user.target 为字符界面启动，然后在命令行中输入systemctl set-default xxx.target 即可修改为对应的模式，最后重启生效。]]></content>
      <categories>
        <category>桌面</category>
        <category>系统</category>
      </categories>
      <tags>
        <tag>VMware</tag>
        <tag>Linux系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AWS-CDN配置]]></title>
    <url>%2F2022%2F09b959f0ea.html</url>
    <content type="text"><![CDATA[配置网站一个网站使用EC2作为服务器，域名直接解析ip来访问域名是托管到cloudflare，在其它平台也一样目前没有配置https，使用网站工具来测试延迟https://tool.chinaz.com/sitespeed这是国际测速，中国香港最快要445ms最慢要2000ms接下来使用AWS的CloudFront服务为我们网站配置CDN申请ssl证书因为我还要给我的网站配置https所以还需要到AWS的Certificate Manager服务里申请我们这个子域的ssl证书 输入我们的子域验证方法我们选择dns验证，密钥算法默认点击请求注意目前其状态是等待验证，证书还没有颁发成功点进去，我们需要给域名配置一个cname的解析才能验证成功把cname的名称和值配置到域名即可接着等待其状态变为验证成功在等待的这段时间我们先把其A记录解析到网站ip的记录删除等待证书状态已验证成功这个证书是给AWS的ALB负载均衡器和CloudFront使用的，我们并不能拿到证书文件创建负载均衡器CloudFront并不能直接指定EC2作为源，其需要负载均衡器，所以我们要给EC2创建一个负载均衡器我们先要创建一个目标组目标类型选择实例，目标组名称随意这里不变（除非你的网站不是运行在80端口）选择EC2实例所在的VPC勾选我们的实例记得点击 包含如下待处理事项按钮然后点击创建目标组即可接下来创建负载均衡器选择第一个Application Load Balancer名称随意，模式选择面向互联网网络映射的话我这里是默认VPC所以就全选了，如果是自己创建的VPC则选择公有子网安全组选择开放了80和443端口的侦听器和路由我们选择https协议，目标组就是我们前面创建的目标组接下来选择证书，就是我们前面AWS给我们域名颁发的的证书其它的都默认即可，点击下面的创建负载均衡器。创建CloudFront点击创建分配源就选择我们前面创建的负载均衡器协议选择https下面的都默认直到缓存这里，CDN缓存策略我选择的都是全部缓存，要根据自己网站实际需要缓存的内容来选择WAF防火墙选择不开启价格级别选择所有边缘节点然后我们点击添加备用域名这里就要加入我们的域名，下面选择我们之前创建的ssl证书 接下来点击最下面的创建即可然后会给我们一个CloudFront的域名我们复制下来到我们域名的DNS解析哪里，添加CNAME解析到CloudFront接下来我们等待CloudFront部署完成测试现在中国香港最快246ms最慢772ms，对比前面域名直接解析快了很多我们可以ping一下域名看看]]></content>
      <categories>
        <category>技术</category>
        <category>CDN</category>
      </categories>
      <tags>
        <tag>AWS</tag>
        <tag>CDN</tag>
      </tags>
  </entry>
</search>
