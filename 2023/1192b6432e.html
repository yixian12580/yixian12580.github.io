<!DOCTYPE html><html class="theme-next mist use-motion" lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><script></script><link rel="stylesheet" href="/lib/needsharebutton/needsharebutton.css"><script src="/lib/pace/pace.min.js?v=1.0.2"></script><link rel="stylesheet" href="/lib/pace/pace-theme-bounce.min.css?v=1.0.2"><link rel="stylesheet" href="/lib/fancybox/source/jquery.fancybox.css"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Roboto Mono:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2"><link rel="stylesheet" href="/css/main.css?v=7.1.1"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.1.1"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.1.1"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.1.1"><link rel="mask-icon" href="/images/logo.svg?v=7.1.1" color="#222"><script id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Mist",version:"7.1.1",sidebar:{position:"left",display:"post",offset:12,onmobile:!0,dimmer:!1},back2top:!0,back2top_sidebar:!0,fancybox:!0,fastclick:!0,lazyload:!0,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"shrinkIn",post_header:"slideLeftIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideDownIn"}},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><script>!function(e,t,o,c,a,i,n){e.DaoVoiceObject=a,e[a]=e[a]||function(){(e[a].q=e[a].q||[]).push(arguments)},e[a].l=+new Date,i=t.createElement(o),n=t.getElementsByTagName(o)[0],i.async=1,i.src=c,i.charset="utf-8",n.parentNode.insertBefore(i,n)}(window,document,"script",("https:"==document.location.protocol?"https:":"http:")+"//widget.daovoice.io/widget/0f81ff2f.js","daovoice"),daovoice("init",{app_id:"aac7f9b5"}),daovoice("update")</script><meta name="description" content="Kubeadm 是一个K8s 部署工具，提供kubeadm init 和kubeadm join，用于快速部署Kubernetes 集群。"><meta name="keywords" content="K8S,Kubernetes"><meta property="og:type" content="article"><meta property="og:title" content="K8S集群部署"><meta property="og:url" content="https://yixian12580.github.io/2023/1192b6432e.html"><meta property="og:site_name" content="逸贤 | Blog"><meta property="og:description" content="Kubeadm 是一个K8s 部署工具，提供kubeadm init 和kubeadm join，用于快速部署Kubernetes 集群。"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https://yixian12580.github.io/2023/1192b6432e/image-20231106182459560.png"><meta property="og:image" content="https://yixian12580.github.io/2023/1192b6432e/image-20231106182537167.png"><meta property="og:updated_time" content="2025-02-13T03:15:21.000Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="K8S集群部署"><meta name="twitter:description" content="Kubeadm 是一个K8s 部署工具，提供kubeadm init 和kubeadm join，用于快速部署Kubernetes 集群。"><meta name="twitter:image" content="https://yixian12580.github.io/2023/1192b6432e/image-20231106182459560.png"><link rel="alternate" href="/atom.xml" title="逸贤 | Blog" type="application/atom+xml"><link rel="canonical" href="https://yixian12580.github.io/2023/1192b6432e"><script id="page.configurations">CONFIG.page={sidebar:""}</script><title>K8S集群部署 | 逸贤 | Blog</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-title,.use-motion .comments,.use-motion .menu-item,.use-motion .motion-element,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .logo,.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">逸贤 | Blog</span> <span class="logo-line-after"><i></i></span></a></div></div><div class="site-nav-toggle"><button aria-label="切换导航栏"><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-bookmark"></i><br>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>关于</a></li><li class="menu-item menu-item-互动"><a href="/guestbook/" rel="section"><i class="menu-item-icon fa fa-fw fa-comments"></i><br>互动</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>搜索</a></li></ul><div class="site-search"><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class="search-icon"><i class="fa fa-search"></i> </span><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span><div class="local-search-input-wrapper"><input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input"></div></div><div id="local-search-result"></div></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="https://yixian12580.github.io/2023/1192b6432e.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="丨逸贤丨"><meta itemprop="description" content="三十年众生牛马，搏十年丰功伟绩。"><meta itemprop="image" content="/images/avatar.jpg"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="逸贤 | Blog"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">K8S集群部署</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2023-11-06 18:05:48" itemprop="dateCreated datePublished" datetime="2023-11-06T18:05:48+08:00">2023-11-06</time> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2025-02-13 11:15:21" itemprop="dateModified" datetime="2025-02-13T11:15:21+08:00">2025-02-13</time> </span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/技术/" itemprop="url" rel="index"><span itemprop="name">技术</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/技术/K8S/" itemprop="url" rel="index"><span itemprop="name">K8S</span></a></span> </span><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> 阅读次数： <span class="busuanzi-value" id="busuanzi_value_page_pv"></span></span><div class="post-symbolscount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">本文字数：</span> <span title="本文字数">35k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span title="阅读时长">32 分钟</span></div></div></header><div class="post-body" itemprop="articleBody"><p>Kubeadm 是一个K8s 部署工具，提供kubeadm init 和kubeadm join，用于快速部署Kubernetes 集群。</p><a id="more"></a><h2 id="完整的卸载k8s（有需要的话）"><a href="#完整的卸载k8s（有需要的话）" class="headerlink" title="完整的卸载k8s（有需要的话）"></a>完整的卸载k8s（有需要的话）</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先清理运行到k8s群集中的pod，使用</span></span><br><span class="line">kubectl delete node --all</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用脚本停止所有k8s服务</span></span><br><span class="line"><span class="keyword">for</span> service <span class="keyword">in</span> kube-apiserver kube-controller-manager kubectl kubelet etcd kube-proxy kube-scheduler; </span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">    systemctl stop <span class="variable">$service</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用命令卸载k8s</span></span><br><span class="line">kubeadm reset -f</span><br><span class="line"></span><br><span class="line"><span class="comment"># 卸载k8s相关程序</span></span><br><span class="line">yum -y remove kube*</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除相关的配置文件</span></span><br><span class="line">modprobe -r ipip</span><br><span class="line">lsmod</span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后手动删除配置文件和flannel网络配置和flannel网口：</span></span><br><span class="line">rm -rf /etc/cni</span><br><span class="line">rm -rf /root/.kube</span><br><span class="line"><span class="comment"># 删除cni网络</span></span><br><span class="line">ifconfig cni0 down</span><br><span class="line">ip link delete cni0</span><br><span class="line">ifconfig flannel.1 down</span><br><span class="line">ip link delete flannel.1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除残留的配置文件</span></span><br><span class="line">rm -rf ~/.kube/</span><br><span class="line">rm -rf /etc/kubernetes/</span><br><span class="line">rm -rf /etc/systemd/system/kubelet.service.d</span><br><span class="line">rm -rf /etc/systemd/system/kubelet.service</span><br><span class="line">rm -rf /etc/systemd/system/multi-user.target.wants/kubelet.service</span><br><span class="line">rm -rf /var/lib/kubelet</span><br><span class="line">rm -rf /usr/libexec/kubernetes/kubelet-plugins</span><br><span class="line">rm -rf /usr/bin/kube*</span><br><span class="line">rm -rf /opt/cni</span><br><span class="line">rm -rf /var/lib/etcd</span><br><span class="line">rm -rf /var/etcd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 更新镜像</span></span><br><span class="line">yum clean all</span><br><span class="line">yum makecache</span><br></pre></td></tr></table></figure><hr><h2 id="安装kube集群（4节点）"><a href="#安装kube集群（4节点）" class="headerlink" title="安装kube集群（4节点）"></a>安装kube集群（4节点）</h2><p>k8s重置命令（如果初始化的过程出现了错误就使用重置命令）：<code>kubeadm reset</code></p><h3 id="准备工作（所有的节点都执行）"><a href="#准备工作（所有的节点都执行）" class="headerlink" title="准备工作（所有的节点都执行）"></a>准备工作（所有的节点都执行）</h3><p>编辑4台服务器的 <code>/etc/hosts</code> 文件 ,添加下面内容（每个节点都执行一遍）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">192.168.2.1 node1</span><br><span class="line">192.168.2.2 node2</span><br><span class="line">192.168.2.3 node3</span><br><span class="line">192.168.2.4 node4</span><br></pre></td></tr></table></figure><p>设置hostname（以node1为例）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hostnamectl <span class="built_in">set</span>-hostname  node1  <span class="comment"># node1 是自定义名字</span></span><br></pre></td></tr></table></figure><p>或者修改 <code>/etc/hostname</code> 文件，写入<code>node1</code>（其他的子节点都一样）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/hostname</span><br></pre></td></tr></table></figure><p>修改之后<code>/etc/hostname</code>的内容为：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">node1</span><br></pre></td></tr></table></figure><p>所有节点执行时间同步：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动chronyd服务</span></span><br><span class="line">systemctl start chronyd</span><br><span class="line">systemctl <span class="built_in">enable</span> chronyd</span><br><span class="line">date</span><br></pre></td></tr></table></figure><p>所有节点禁用SELinux和Firewalld服务：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl <span class="built_in">disable</span> firewalld</span><br><span class="line"></span><br><span class="line">sed -i <span class="string">'s/enforcing/disabled/'</span> /etc/selinux/config <span class="comment"># 重启后生效</span></span><br></pre></td></tr></table></figure><p>所有节点禁用swap分区：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 临时禁用swap分区</span></span><br><span class="line">swapoff -a</span><br><span class="line"></span><br><span class="line"><span class="comment"># 永久禁用swap分区</span></span><br><span class="line">vi /etc/fstab </span><br><span class="line"><span class="comment"># 注释掉下面的设置</span></span><br><span class="line"><span class="comment"># /dev/mapper/centos-swap swap</span></span><br><span class="line"><span class="comment"># 之后需要重启服务器生效</span></span><br></pre></td></tr></table></figure><p>所有节点添加网桥过滤和地址转发功能：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cat &gt; /etc/sysctl.d/kubernetes.conf &lt;&lt; EOF</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line">net.bridge.bridge-nf-call-iptables = 1</span><br><span class="line">net.ipv4.ip_forward = 1</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后执行,生效</span></span><br><span class="line">sysctl --system</span><br></pre></td></tr></table></figure><p>然后所有节点安装docker-ce（略）</p><p>需要注意的是要配置docker的cgroupdriver：</p><p>打开docker配置文件进行修改，该文件默认情况下不存在，可以新建一个，不同操作系统的文件位置不一样。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/docker/daemon.json</span><br></pre></td></tr></table></figure><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  // 添加这行</span><br><span class="line">  "exec-opts": ["native.cgroupdriver=systemd"],</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>重新加载配置并重启服务：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl restart docker</span><br></pre></td></tr></table></figure><p>最终配置文件内容如下：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">        <span class="attr">"exec-opts"</span>: [</span><br><span class="line">                        <span class="string">"native.cgroupdriver=systemd"</span></span><br><span class="line">        ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>所有节点的kubernetes镜像切换成国内源：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cat &gt; /etc/yum.repos.d/kubernetes.repo &lt;&lt; EOF</span><br><span class="line">[kubernetes]</span><br><span class="line">name=Kubernetes</span><br><span class="line">baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=0</span><br><span class="line">repo_gpgcheck=0</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>所有节点安装指定版本 kubeadm，kubelet 和 kubectl（我这里选择<code>1.23.0</code>版本的）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">yum install -y kubelet-1.23.0 kubeadm-1.23.0 kubectl-1.23.0</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置kubelet开机启动（看你自己）</span></span><br><span class="line">systemctl <span class="built_in">enable</span> kubelet</span><br></pre></td></tr></table></figure><h3 id="更改kubelet的容器路径（如果需要的话，不需要可以跳过）"><a href="#更改kubelet的容器路径（如果需要的话，不需要可以跳过）" class="headerlink" title="更改kubelet的容器路径（如果需要的话，不需要可以跳过）"></a>更改kubelet的容器路径（如果需要的话，不需要可以跳过）</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf</span><br></pre></td></tr></table></figure><p>修改完之后配置文件如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[Service]</span><br><span class="line">Environment=<span class="string">"KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --root-dir=/mnt/sdb_new/kubelet/ --kubeconfig=/etc/kubernetes/kubelet.conf"</span></span><br></pre></td></tr></table></figure><p>使配置生效：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl restart docker</span><br><span class="line">systemctl restart kubelet</span><br></pre></td></tr></table></figure><h2 id="部署Kubernetes集群"><a href="#部署Kubernetes集群" class="headerlink" title="部署Kubernetes集群"></a>部署Kubernetes集群</h2><h3 id="覆盖kubernetes的镜像地址（只需要在master节点上操作初始化命令）"><a href="#覆盖kubernetes的镜像地址（只需要在master节点上操作初始化命令）" class="headerlink" title="覆盖kubernetes的镜像地址（只需要在master节点上操作初始化命令）"></a>覆盖kubernetes的镜像地址（只需要在master节点上操作初始化命令）</h3><ol><li>首先要覆盖kubeadm的镜像地址，因为这个是外网的无法访问，需要替换成国内的镜像地址，使用此命令列出集群在配置过程中需要哪些镜像：</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 home]<span class="comment"># kubeadm config images list</span></span><br><span class="line">I0418 18:26:04.047449   19242 version.go:255] remote version is much newer: v1.27.1; falling back to: stable-1.23</span><br><span class="line">k8s.gcr.io/kube-apiserver:v1.23.17</span><br><span class="line">k8s.gcr.io/kube-controller-manager:v1.23.17</span><br><span class="line">k8s.gcr.io/kube-scheduler:v1.23.17</span><br><span class="line">k8s.gcr.io/kube-proxy:v1.23.17</span><br><span class="line">k8s.gcr.io/pause:3.6</span><br><span class="line">k8s.gcr.io/etcd:3.5.1-0</span><br><span class="line">k8s.gcr.io/coredns/coredns:v1.8.6</span><br></pre></td></tr></table></figure><ol start="2"><li>更改为阿里云的镜像地址：</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 home]<span class="comment"># kubeadm config images list  --image-repository registry.aliyuncs.com/google_containers</span></span><br><span class="line">I0418 18:28:18.740057   20021 version.go:255] remote version is much newer: v1.27.1; falling back to: stable-1.23</span><br><span class="line">registry.aliyuncs.com/google_containers/kube-apiserver:v1.23.17</span><br><span class="line">registry.aliyuncs.com/google_containers/kube-controller-manager:v1.23.17</span><br><span class="line">registry.aliyuncs.com/google_containers/kube-scheduler:v1.23.17</span><br><span class="line">registry.aliyuncs.com/google_containers/kube-proxy:v1.23.17</span><br><span class="line">registry.aliyuncs.com/google_containers/pause:3.6</span><br><span class="line">registry.aliyuncs.com/google_containers/etcd:3.5.1-0</span><br><span class="line">registry.aliyuncs.com/google_containers/coredns:v1.8.6</span><br></pre></td></tr></table></figure><ol start="3"><li>然后将镜像手动拉取下来，这样在初始化的时候回更快一些（还有一个办法就是直接在docker上把镜像pull下来，docker只要配置一下国内源即可快速的将镜像pull下来）：</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 home]<span class="comment"># kubeadm config images pull  --image-repository registry.aliyuncs.com/google_containers</span></span><br><span class="line">I0418 18:28:31.795554   20088 version.go:255] remote version is much newer: v1.27.1; falling back to: stable-1.23</span><br><span class="line">[config/images] Pulled registry.aliyuncs.com/google_containers/kube-apiserver:v1.23.17</span><br><span class="line">[config/images] Pulled registry.aliyuncs.com/google_containers/kube-controller-manager:v1.23.17</span><br><span class="line">[config/images] Pulled registry.aliyuncs.com/google_containers/kube-scheduler:v1.23.17</span><br><span class="line">[config/images] Pulled registry.aliyuncs.com/google_containers/kube-proxy:v1.23.17</span><br><span class="line">[config/images] Pulled registry.aliyuncs.com/google_containers/pause:3.6</span><br><span class="line">[config/images] Pulled registry.aliyuncs.com/google_containers/etcd:3.5.1-0</span><br><span class="line">[config/images] Pulled registry.aliyuncs.com/google_containers/coredns:v1.8.6</span><br></pre></td></tr></table></figure><h3 id="初始化kubernetes（只需要在master节点上操作初始化命令）"><a href="#初始化kubernetes（只需要在master节点上操作初始化命令）" class="headerlink" title="初始化kubernetes（只需要在master节点上操作初始化命令）"></a>初始化kubernetes（只需要在master节点上操作初始化命令）</h3><p>初始化 Kubernetes，指定网络地址段 和 镜像地址（后续的子节点可以使用join命令进行动态的追加）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 home]<span class="comment"># kubeadm init \</span></span><br><span class="line">  --apiserver-advertise-address=192.168.2.1 \</span><br><span class="line">  --image-repository registry.aliyuncs.com/google_containers \</span><br><span class="line">  --kubernetes-version v1.23.0 \</span><br><span class="line">  --service-cidr=10.96.0.0/12 \</span><br><span class="line">  --pod-network-cidr=10.244.0.0/16 \</span><br><span class="line">  --ignore-preflight-errors=all</span><br><span class="line"></span><br><span class="line"><span class="comment"># –apiserver-advertise-address # 集群通告地址(master 机器IP，这里用的万兆网)</span></span><br><span class="line"><span class="comment"># –image-repository # 由于默认拉取镜像地址k8s.gcr.io国内无法访问，这里指定阿里云镜像仓库地址</span></span><br><span class="line"><span class="comment"># –kubernetes-version #K8s版本，与上面安装的一致</span></span><br><span class="line"><span class="comment"># –service-cidr #集群内部虚拟网络，Pod统一访问入口，可以不用更改，直接用上面的参数</span></span><br><span class="line"><span class="comment"># –pod-network-cidr #Pod网络，与下面部署的CNI网络组件yaml中保持一致，可以不用更改，直接用上面的参数</span></span><br></pre></td></tr></table></figure><p>执行完之后要手动执行一些参数（尤其是 <strong>加入集群的join命令</strong> 需要复制记录下载）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">[addons] Applied essential addon: kube-proxy</span><br><span class="line"></span><br><span class="line">Your Kubernetes control-plane has initialized successfully!</span><br><span class="line"></span><br><span class="line">To start using your cluster, you need to run the following as a regular user:</span><br><span class="line"></span><br><span class="line">  mkdir -p <span class="variable">$HOME</span>/.kube</span><br><span class="line">  sudo cp -i /etc/kubernetes/admin.conf <span class="variable">$HOME</span>/.kube/config</span><br><span class="line">  sudo chown $(id -u):$(id -g) <span class="variable">$HOME</span>/.kube/config</span><br><span class="line"></span><br><span class="line">Alternatively, <span class="keyword">if</span> you are the root user, you can run:</span><br><span class="line"></span><br><span class="line">  <span class="built_in">export</span> KUBECONFIG=/etc/kubernetes/admin.conf</span><br><span class="line"></span><br><span class="line">You should now deploy a pod network to the cluster.</span><br><span class="line">Run <span class="string">"kubectl apply -f [podnetwork].yaml"</span> with one of the options listed at:</span><br><span class="line">  https://kubernetes.io/docs/concepts/cluster-administration/addons/</span><br><span class="line"></span><br><span class="line">Then you can join any number of worker nodes by running the following on each as root:</span><br><span class="line"></span><br><span class="line">kubeadm join 192.168.2.1:6443 --token ochspx.15in9qkiu5z8tx2y \</span><br><span class="line">        --discovery-token-ca-cert-hash sha256:1f31202107af96a07df9fd78c3aa9bb44fd40076ac123e8ff28d6ab691a02a31</span><br></pre></td></tr></table></figure><p>执行参数：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 home]<span class="comment"># mkdir -p $HOME/.kube</span></span><br><span class="line">[root@node1 home]<span class="comment"># sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span></span><br><span class="line">[root@node1 home]<span class="comment"># sudo chown $(id -u):$(id -g) $HOME/.kube/config</span></span><br><span class="line">[root@node1 home]<span class="comment"># </span></span><br><span class="line">[root@node1 home]<span class="comment"># vim /root/.bash_profile</span></span><br></pre></td></tr></table></figure><p>加入以下这段：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 超级用户变量</span></span><br><span class="line"><span class="built_in">export</span> KUBECONFIG=/etc/kubernetes/admin.conf</span><br><span class="line"><span class="comment"># 设置别名</span></span><br><span class="line"><span class="built_in">alias</span> k=kubectl</span><br><span class="line"><span class="comment"># 设置kubectl命令补齐功能</span></span><br><span class="line"><span class="built_in">source</span> &lt;(kubectl completion bash)</span><br></pre></td></tr></table></figure><p>激活 <code>.bash_profile</code>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 home]<span class="comment"># source /root/.bash_profile</span></span><br></pre></td></tr></table></figure><p>这段要<strong>复制记录</strong>下来（来自k8s初始化成功之后出现的<code>join</code>命令，需要先配置完Flannel才能加入子节点），后续子节点加入master节点需要执行这段命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubeadm join 192.168.2.1:6443 --token ochspx.15in9qkiu5z8tx2y \</span><br><span class="line">        --discovery-token-ca-cert-hash sha256:1f31202107af96a07df9fd78c3aa9bb44fd40076ac123e8ff28d6ab691a02a31</span><br></pre></td></tr></table></figure><h3 id="设定kubeletl网络（主节点部署）"><a href="#设定kubeletl网络（主节点部署）" class="headerlink" title="设定kubeletl网络（主节点部署）"></a>设定kubeletl网络（主节点部署）</h3><p>部署容器网络，CNI网络插件(在Master上执行，著名的有flannel、calico、canal和kube-router等，简单易用的实现是为CoreOS提供的flannel项目)，这里使用Flannel实现。</p><p>下载<code>kube-flannel.yml</code>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 home]<span class="comment"># wget https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml</span></span><br></pre></td></tr></table></figure><p>然后修改配置文件，找到如下位置，修改 <code>Newwork</code> 与执行 <code>kubeadm init</code> 输入的网段一致：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">net-conf.json: |</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="string">"Network"</span>: <span class="string">"10.244.0.0/16"</span>,</span><br><span class="line">    <span class="string">"Backend"</span><span class="string">": &#123;</span></span><br><span class="line"><span class="string">      "</span>Type<span class="string">": "</span>vxlan<span class="string">"</span></span><br><span class="line"><span class="string">  &#125;</span></span><br><span class="line"><span class="string">&#125;</span></span><br></pre></td></tr></table></figure><p>修改配置之后安装组件（如果安装的时候卡在pull镜像的时候，试一试手动用docker将镜像拉取下来）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 home]<span class="comment"># kubectl apply -f kube-flannel.yml</span></span><br></pre></td></tr></table></figure><p>查看<code>flannel pod</code>状态（必须要为<code>Running</code>状态，如果<code>kube-flannel</code>起不来，那么就用<code>kubectl describe pod kube-flannel-ds-f5jn6 -n kube-flannel</code>命令查看<code>pod</code>起不来的原因，然后去搜度娘获取解决方案）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 home]<span class="comment"># # 必须所有的容器都是Running</span></span><br><span class="line">[root@node1 home]<span class="comment"># kubectl get pod --all-namespaces</span></span><br><span class="line">NAMESPACE      NAME                                 READY   STATUS    RESTARTS   AGE</span><br><span class="line">kube-flannel   kube-flannel-ds-f5jn6                1/1     Running   0          8m21s</span><br><span class="line">kube-system    coredns-6d8c4cb4d-ctqw5              1/1     Running   0          42m</span><br><span class="line">kube-system    coredns-6d8c4cb4d-n52fq              1/1     Running   0          42m</span><br><span class="line">kube-system    etcd-k8s-master                      1/1     Running   0          42m</span><br><span class="line">kube-system    kube-apiserver-k8s-master            1/1     Running   0          42m</span><br><span class="line">kube-system    kube-controller-manager-k8s-master   1/1     Running   0          42m</span><br><span class="line">kube-system    kube-proxy-swpkz                     1/1     Running   0          42m</span><br><span class="line">kube-system    kube-scheduler-k8s-master            1/1     Running   0          42m</span><br></pre></td></tr></table></figure><p>查看通信状态：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 home]<span class="comment"># kubectl get pod -n kube-system</span></span><br><span class="line">NAME                                 READY   STATUS    RESTARTS   AGE</span><br><span class="line">coredns-6d8c4cb4d-ctqw5              1/1     Running   0          52m</span><br><span class="line">coredns-6d8c4cb4d-n52fq              1/1     Running   0          52m</span><br><span class="line">etcd-k8s-master                      1/1     Running   0          53m</span><br><span class="line">kube-apiserver-k8s-master            1/1     Running   0          53m</span><br><span class="line">kube-controller-manager-k8s-master   1/1     Running   0          53m</span><br><span class="line">kube-proxy-swpkz                     1/1     Running   0          52m</span><br><span class="line">kube-scheduler-k8s-master            1/1     Running   0          53m</span><br><span class="line">[root@node1 home]<span class="comment"># </span></span><br><span class="line">[root@node1 home]<span class="comment"># 获取主节点的状态</span></span><br><span class="line">[root@node1 home]<span class="comment"># kubectl get cs</span></span><br><span class="line">Warning: v1 ComponentStatus is deprecated <span class="keyword">in</span> v1.19+</span><br><span class="line">NAME                 STATUS    MESSAGE                         ERROR</span><br><span class="line">controller-manager   Healthy   ok</span><br><span class="line">scheduler            Healthy   ok</span><br><span class="line">etcd-0               Healthy   &#123;<span class="string">"health"</span>:<span class="string">"true"</span>,<span class="string">"reason"</span>:<span class="string">""</span>&#125;</span><br><span class="line">[root@node1 home]<span class="comment"># kubectl get node</span></span><br><span class="line">NAME         STATUS   ROLES                  AGE   VERSION</span><br><span class="line">node1        Ready    control-plane,master   52m   v1.23.0</span><br></pre></td></tr></table></figure><p>查看节点状态（此时还只有主节点，还没添加子节点）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 home]<span class="comment"># kubectl get node</span></span><br><span class="line">NAME         STATUS   ROLES                  AGE   VERSION</span><br><span class="line">node1        Ready    control-plane,master   53m   v1.23.0</span><br></pre></td></tr></table></figure><p><strong>至此 K8s master主服务器 已经部署完成！</strong></p><h3 id="子节点加入集群（在子节点上操作）"><a href="#子节点加入集群（在子节点上操作）" class="headerlink" title="子节点加入集群（在子节点上操作）"></a>子节点加入集群（在子节点上操作）</h3><p>初始化会生成<code>join</code>命令，需要在<strong>子节点</strong>执行即可，以下<code>token</code>作为举例，以实际为主，例如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@node2 home]<span class="comment"># kubeadm join 192.168.2.1:6443 --token ochspx.15in9qkiu5z8tx2y         --discovery-token-ca-cert-hash sha256:1f31202107af96a07df9fd78c3aa9bb44fd40076ac123e8ff28d6ab691a02a31</span></span><br><span class="line">[preflight] Running pre-flight checks</span><br><span class="line">[preflight] Reading configuration from the cluster...</span><br><span class="line">[preflight] FYI: You can look at this config file with <span class="string">'kubectl -n kube-system get cm kubeadm-config -o yaml'</span></span><br><span class="line">[kubelet-start] Writing kubelet configuration to file <span class="string">"/var/lib/kubelet/config.yaml"</span></span><br><span class="line">[kubelet-start] Writing kubelet environment file with flags to file <span class="string">"/var/lib/kubelet/kubeadm-flags.env"</span></span><br><span class="line">[kubelet-start] Starting the kubelet</span><br><span class="line">[kubelet-start] Waiting <span class="keyword">for</span> the kubelet to perform the TLS Bootstrap...</span><br><span class="line"></span><br><span class="line">This node has joined the cluster:</span><br><span class="line">* Certificate signing request was sent to apiserver and a response was received.</span><br><span class="line">* The Kubelet was informed of the new secure connection details.</span><br><span class="line"></span><br><span class="line">Run <span class="string">'kubectl get nodes'</span> on the control-plane to see this node join the cluster.</span><br></pre></td></tr></table></figure><p>默认的 <code>join token</code> 有效期限为24小时，当过期后该 <code>token</code> 就不能用了，这时需要重新创建 <code>token</code>，创建新的<code>join token</code>需要在主节点上创建，创建命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 home]<span class="comment"># kubeadm token create --print-join-command</span></span><br></pre></td></tr></table></figure><p>加入之后再在主节点查看集群中节点的状态（<strong>必须要都为<code>Ready</code>状态</strong>）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 home]<span class="comment"># kubectl get nodes</span></span><br><span class="line">NAME         STATUS     ROLES                  AGE     VERSION</span><br><span class="line">node1        Ready      control-plane,master   63m     v1.23.0</span><br><span class="line">node2        Ready      &lt;none&gt;                 3m57s   v1.23.0</span><br><span class="line">node3        Ready      &lt;none&gt;                 29s     v1.23.0</span><br></pre></td></tr></table></figure><p><strong>如果所有的节点<code>STATUS</code>都为<code>Ready</code>的话，那么到此，所有的子节点加入完成！</strong></p><h3 id="删除子节点（在master主节点上操作）"><a href="#删除子节点（在master主节点上操作）" class="headerlink" title="删除子节点（在master主节点上操作）"></a>删除子节点（在master主节点上操作）</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl drain &lt;node name&gt; --delete-local-data --force --ignore-daemonsets</span></span><br><span class="line"><span class="comment"># 其中 &lt;node name&gt; 是在k8s集群中使用 &lt;kubectl get nodes&gt; 查询到的节点名称</span></span><br><span class="line"><span class="comment"># 假设这里删除 node3 子节点</span></span><br><span class="line">[root@node1 home]<span class="comment"># kubectl drain node3 --delete-local-data --force --ignore-daemonsets</span></span><br><span class="line">[root@node1 home]<span class="comment"># kubectl delete node node3</span></span><br></pre></td></tr></table></figure><p>然后在删除的子节点上操作重置k8s（重置k8s会删除一些配置文件），这里在<code>node3</code>子节点上操作：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">[root@node3 home]<span class="comment"># # 子节点重置k8s</span></span><br><span class="line">[root@node3 home]<span class="comment"># kubeadm reset</span></span><br><span class="line">[reset] WARNING: Changes made to this host by <span class="string">'kubeadm init'</span> or <span class="string">'kubeadm join'</span> will be reverted.</span><br><span class="line">[reset] Are you sure you want to proceed? [y/N]: y</span><br><span class="line">[preflight] Running pre-flight checks</span><br><span class="line">W0425 01:59:40.412616   15604 removeetcdmember.go:80] [reset] No kubeadm config, using etcd pod spec to get data directory</span><br><span class="line">[reset] No etcd config found. Assuming external etcd</span><br><span class="line">[reset] Please, manually reset etcd to prevent further issues</span><br><span class="line">[reset] Stopping the kubelet service</span><br><span class="line">[reset] Unmounting mounted directories <span class="keyword">in</span> <span class="string">"/var/lib/kubelet"</span></span><br><span class="line">[reset] Deleting contents of config directories: [/etc/kubernetes/manifests /etc/kubernetes/pki]</span><br><span class="line">[reset] Deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf]</span><br><span class="line">[reset] Deleting contents of stateful directories: [/var/lib/kubelet /var/lib/dockershim /var/run/kubernetes /var/lib/cni]</span><br><span class="line"></span><br><span class="line">The reset process does not clean CNI configuration. To <span class="keyword">do</span> so, you must remove /etc/cni/net.d</span><br><span class="line"></span><br><span class="line">The reset process does not reset or clean up iptables rules or IPVS tables.</span><br><span class="line">If you wish to reset iptables, you must <span class="keyword">do</span> so manually by using the <span class="string">"iptables"</span> <span class="built_in">command</span>.</span><br><span class="line"></span><br><span class="line">If your cluster was setup to utilize IPVS, run ipvsadm --clear (or similar)</span><br><span class="line">to reset your system<span class="string">'s IPVS tables.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">The reset process does not clean your kubeconfig files and you must remove them manually.</span></span><br><span class="line"><span class="string">Please, check the contents of the $HOME/.kube/config file.</span></span><br></pre></td></tr></table></figure><p>然后在被删除的子节点上手动删除k8s配置文件、flannel网络配置文件 和 flannel网口：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@node3 home]<span class="comment"># rm -rf /etc/cni/net.d/</span></span><br><span class="line">[root@node3 home]<span class="comment"># rm -rf /root/.kube/config</span></span><br><span class="line">[root@node3 home]<span class="comment"># # 删除cni网络</span></span><br><span class="line">[root@node3 home]<span class="comment"># ifconfig cni0 down</span></span><br><span class="line">[root@node3 home]<span class="comment"># ip link delete cni0</span></span><br><span class="line">[root@node3 home]<span class="comment"># ifconfig flannel.1 down</span></span><br><span class="line">[root@node3 home]<span class="comment"># ip link delete flannel.1</span></span><br></pre></td></tr></table></figure><hr><h2 id="部署k8s-dashboard（这里使用Kubepi）"><a href="#部署k8s-dashboard（这里使用Kubepi）" class="headerlink" title="部署k8s dashboard（这里使用Kubepi）"></a>部署k8s dashboard（这里使用Kubepi）</h2><p><strong>Kubepi</strong>是一个简单高效的k8s集群图形化管理工具，方便日常管理K8S集群，高效快速的查询日志定位问题的工具</p><p>部署KubePI（随便在哪个节点部署，我这里在主节点部署）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 home]<span class="comment"># docker pull kubeoperator/kubepi-server</span></span><br><span class="line">[root@node1 home]<span class="comment"># # 运行容器</span></span><br><span class="line">[root@node1 home]<span class="comment"># docker run --privileged -itd --restart=unless-stopped --name kube_dashboard -v /home/docker-mount/kubepi/:/var/lib/kubepi/ -p 8000:80 kubeoperator/kubepi-server</span></span><br></pre></td></tr></table></figure><p>登录：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 地址: http://192.168.2.1:8000</span></span><br><span class="line"><span class="comment"># 默认用户名：admin</span></span><br><span class="line"><span class="comment"># 默认密码：kubepi</span></span><br></pre></td></tr></table></figure><p>填写集群名称，默认认证模式，填写<code>apisever</code>地址及<code>token</code>：</p><img src="/2023/1192b6432e/image-20231106182459560.png"><p>kubepi导入集群:</p><p>获取登录需要用到的ip地址和登录token：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 home]<span class="comment"># # 在 k8s 主节点上创建用户，并获取token</span></span><br><span class="line">[root@node1 home]<span class="comment"># kubectl create sa kubepi-user --namespace kube-system</span></span><br><span class="line">serviceaccount/kubepi-user created</span><br><span class="line">[root@node1 home]<span class="comment"># kubectl create clusterrolebinding kubepi-user --clusterrole=cluster-admin --serviceaccount=kube-system:kubepi-user</span></span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/kubepi-user created</span><br><span class="line">[root@node1 home]<span class="comment"># </span></span><br><span class="line">[root@node1 home]<span class="comment"># # 在主节点上获取新建的用户 kubeapi-user 的 token</span></span><br><span class="line">[root@node1 home]<span class="comment"># kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep kubepi-user | awk '&#123;print $1&#125;') | grep token: | awk '&#123;print $2&#125;'</span></span><br><span class="line">eyJhbGciOiJSUzI1NiIsImtpZCI6IkhVeUtyc1BpU1JvRnVacXVqVk1PTFRkaUlIZm1KQTV6Wk9WSExSRllmd0kifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJrdWJlcGktdXNlci10b2tlbi10cjVsMiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJrdWJlcGktdXNlciIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjJiYzlhZDRjLWVjZTItNDE2Mi04MDc1LTA2NTI0NDg0MzExZiIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTprdWJlcGktdXNlciJ9.QxkR1jBboqTYiVUUVO4yGhfWmlLDA5wHLo_ZnjAuSLZQDyVevCgBluL6l7y7UryRdId6FmBZ-L0QitvOuTsurcjGL2QHxPE_yZsNW7s9K7eikxJ8q-Q_yOvnADtAueH_tcMGRGW9Zyec2TlmcGTZCNaNUme84TfMlWqX7oP3GGJGMbMGN7H4fPXh-Qqrdp-0MJ3tP-dk3koZUEu3amrq8ExSmjIAjso_otrgFWbdSOMkCXKsqb9yuZzaw7u5Cy18bH_HW6RbNCRT5jGs5aOwzuMAd0HQ5iNm-5OISI4Da6jGdjipLXejcC1H-xWgLlJBx0RQWu41yoPNF57cG1NubQ</span><br><span class="line">[root@node1 home]<span class="comment"># </span></span><br><span class="line">[root@node1 home]<span class="comment"># # 在主节点上获取 apiserver 地址</span></span><br><span class="line">[root@node1 home]<span class="comment"># cat ~/.kube/config | grep server: | awk '&#123;print $2&#125;'</span></span><br><span class="line">https://192.168.2.1:6443</span><br></pre></td></tr></table></figure><p>将上面获取的<code>api地址</code>和<code>token</code>填入页面即可，name可以自己随意取。</p><img src="/2023/1192b6432e/image-20231106182537167.png"><p><strong>到此，KubePI安装完成！</strong></p><h2 id="安装metrics-k8s集群监控插件"><a href="#安装metrics-k8s集群监控插件" class="headerlink" title="安装metrics k8s集群监控插件"></a>安装metrics k8s集群监控插件</h2><p>k8s metrics插件提供了 <code>top</code> 命令可用于统计 k8s集群资源 的使用情况，它包含有 <code>node</code> 和 <code>pod</code> 两个⼦命令，分别显⽰ <code>node</code> 节点和 <code>Pod</code> 对象的资源使⽤信息。</p><p>kubectl top 命令依赖于 metrics 接口。k8s 系统默认未安装该接口，需要单独部署：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master k8s-install]# kubectl top pod</span><br><span class="line">error: Metrics API not available</span><br></pre></td></tr></table></figure><h3 id="下载部署文件"><a href="#下载部署文件" class="headerlink" title="下载部署文件"></a>下载部署文件</h3><p>下载 metrics 接口的部署文件 metrics-server-components.yaml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master k8s-install]# wget https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml -O metrics-server-components.yaml</span><br><span class="line">--2022-10-11 00:13:01--  https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml</span><br><span class="line">正在解析主机 github.com (github.com)... 20.205.243.166</span><br><span class="line">正在连接 github.com (github.com)|20.205.243.166|:443... 已连接。</span><br><span class="line">已发出 HTTP 请求，正在等待回应... 302 Found</span><br><span class="line">位置：https://github.com/kubernetes-sigs/metrics-server/releases/download/metrics-server-helm-chart-3.8.2/components.yaml [跟随至新的 URL]</span><br><span class="line">--2022-10-11 00:13:01--  https://github.com/kubernetes-sigs/metrics-server/releases/download/metrics-server-helm-chart-3.8.2/components.yaml</span><br><span class="line">再次使用存在的到 github.com:443 的连接。</span><br><span class="line">已发出 HTTP 请求，正在等待回应... 302 Found</span><br><span class="line">位置：https://objects.githubusercontent.com/github-production-release-asset-2e65be/92132038/d85e100a-2404-4c5e-b6a9-f3814ad4e6e5?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20221010%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20221010T161303Z&amp;X-Amz-Expires=300&amp;X-Amz-Signature=efa1ff5dd16b6cd86b6186adb3b4c72afed8197bdf08e2bffcd71b9118137831&amp;X-Amz-SignedHeaders=host&amp;actor_id=0&amp;key_id=0&amp;repo_id=92132038&amp;response-content-disposition=attachment%3B%20filename%3Dcomponents.yaml&amp;response-content-type=application%2Foctet-stream [跟随至新的 URL]</span><br><span class="line">--2022-10-11 00:13:02--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/92132038/d85e100a-2404-4c5e-b6a9-f3814ad4e6e5?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20221010%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20221010T161303Z&amp;X-Amz-Expires=300&amp;X-Amz-Signature=efa1ff5dd16b6cd86b6186adb3b4c72afed8197bdf08e2bffcd71b9118137831&amp;X-Amz-SignedHeaders=host&amp;actor_id=0&amp;key_id=0&amp;repo_id=92132038&amp;response-content-disposition=attachment%3B%20filename%3Dcomponents.yaml&amp;response-content-type=application%2Foctet-stream</span><br><span class="line">正在解析主机 objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...</span><br><span class="line">正在连接 objects.githubusercontent.com (objects.githubusercontent.com)|185.199.109.133|:443... 已连接。</span><br><span class="line">已发出 HTTP 请求，正在等待回应... 200 OK</span><br><span class="line">长度：4181 (4.1K) [application/octet-stream]</span><br><span class="line">正在保存至: “metrics-server-components.yaml”</span><br><span class="line"></span><br><span class="line">100%[============================================================================================================================&gt;] 4,181       --.-K/s 用时 0.01s   </span><br><span class="line"></span><br><span class="line">2022-10-11 00:13:10 (385 KB/s) - 已保存 “metrics-server-components.yaml” [4181/4181])</span><br></pre></td></tr></table></figure><h3 id="修改镜像地址"><a href="#修改镜像地址" class="headerlink" title="修改镜像地址"></a>修改镜像地址</h3><p>将部署文件中镜像地址修改为国内的地址。大概在部署文件的第 140 行。</p><p>原配置是：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">image: k8s.gcr.io/metrics-server/metrics-server:v0.6.1</span><br></pre></td></tr></table></figure><p>修改后的配置是：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">image: registry.cn-hangzhou.aliyuncs.com/google_containers/metrics-server:v0.6.1</span><br></pre></td></tr></table></figure><p>可使用如下命令实现修改：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -i &apos;s/k8s.gcr.io\/metrics-server/registry.cn-hangzhou.aliyuncs.com\/google_containers/g&apos; metrics-server-components.yaml</span><br></pre></td></tr></table></figure><h3 id="部署-metrics-接口"><a href="#部署-metrics-接口" class="headerlink" title="部署 metrics 接口"></a>部署 metrics 接口</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master k8s-install]# kubectl create -f metrics-server-components.yaml </span><br><span class="line">serviceaccount/metrics-server created</span><br><span class="line">clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created</span><br><span class="line">clusterrole.rbac.authorization.k8s.io/system:metrics-server created</span><br><span class="line">rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created</span><br><span class="line">service/metrics-server created</span><br><span class="line">deployment.apps/metrics-server created</span><br><span class="line">apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created</span><br></pre></td></tr></table></figure><p>查看该 metric pod 的运行情况：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master k8s-install]# kubectl get pods --all-namespaces | grep metrics</span><br><span class="line">kube-system   metrics-server-6ffc8966f5-84hbb      0/1     Running   0              2m23s</span><br></pre></td></tr></table></figure><p>查看该 pod 的情况，发现是探针问题：Readiness probe failed: HTTP probe failed with statuscode: 500</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master k8s-install]# kubectl describe pod metrics-server-6ffc8966f5-84hbb -n kube-system</span><br><span class="line">Name:                 metrics-server-6ffc8966f5-84hbb</span><br><span class="line">Namespace:            kube-system</span><br><span class="line">Priority:             2000000000</span><br><span class="line">Priority Class Name:  system-cluster-critical</span><br><span class="line">Node:                 k8s-slave2/192.168.100.22</span><br><span class="line">Start Time:           Tue, 11 Oct 2022 00:27:33 +0800</span><br><span class="line">Labels:               k8s-app=metrics-server</span><br><span class="line">                      pod-template-hash=6ffc8966f5</span><br><span class="line">Annotations:          &lt;none&gt;</span><br><span class="line">Status:               Running</span><br><span class="line">IP:                   10.244.2.9</span><br><span class="line">IPs:</span><br><span class="line">  IP:           10.244.2.9</span><br><span class="line">Controlled By:  ReplicaSet/metrics-server-6ffc8966f5</span><br><span class="line">Containers:</span><br><span class="line">  metrics-server:</span><br><span class="line">    Container ID:  docker://e913a075e0381b98eabfb6e298f308ef69dfbd7c672bdcfb75bb2ff3e4b5a0a4</span><br><span class="line">    Image:         registry.cn-hangzhou.aliyuncs.com/google_containers/metrics-server:v0.6.1</span><br><span class="line">    Image ID:      docker-pullable://registry.cn-hangzhou.aliyuncs.com/google_containers/metrics-server@sha256:5ddc6458eb95f5c70bd13fdab90cbd7d6ad1066e5b528ad1dcb28b76c5fb2f00</span><br><span class="line">    Port:          4443/TCP</span><br><span class="line">    Host Port:     0/TCP</span><br><span class="line">    Args:</span><br><span class="line">      --cert-dir=/tmp</span><br><span class="line">      --secure-port=4443</span><br><span class="line">      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname</span><br><span class="line">      --kubelet-use-node-status-port</span><br><span class="line">      --metric-resolution=15s</span><br><span class="line">    State:          Running</span><br><span class="line">      Started:      Tue, 11 Oct 2022 00:27:45 +0800</span><br><span class="line">    Ready:          False</span><br><span class="line">    Restart Count:  0</span><br><span class="line">    Requests:</span><br><span class="line">      cpu:        100m</span><br><span class="line">      memory:     200Mi</span><br><span class="line">    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3</span><br><span class="line">    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3</span><br><span class="line">    Environment:  &lt;none&gt;</span><br><span class="line">    Mounts:</span><br><span class="line">      /tmp from tmp-dir (rw)</span><br><span class="line">      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-x2spb (ro)</span><br><span class="line">Conditions:</span><br><span class="line">  Type              Status</span><br><span class="line">  Initialized       True </span><br><span class="line">  Ready             False </span><br><span class="line">  ContainersReady   False </span><br><span class="line">  PodScheduled      True </span><br><span class="line">Volumes:</span><br><span class="line">  tmp-dir:</span><br><span class="line">    Type:       EmptyDir (a temporary directory that shares a pod&apos;s lifetime)</span><br><span class="line">    Medium:     </span><br><span class="line">    SizeLimit:  &lt;unset&gt;</span><br><span class="line">  kube-api-access-x2spb:</span><br><span class="line">    Type:                    Projected (a volume that contains injected data from multiple sources)</span><br><span class="line">    TokenExpirationSeconds:  3607</span><br><span class="line">    ConfigMapName:           kube-root-ca.crt</span><br><span class="line">    ConfigMapOptional:       &lt;nil&gt;</span><br><span class="line">    DownwardAPI:             true</span><br><span class="line">QoS Class:                   Burstable</span><br><span class="line">Node-Selectors:              kubernetes.io/os=linux</span><br><span class="line">Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s</span><br><span class="line">                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s</span><br><span class="line">Events:</span><br><span class="line">  Type     Reason     Age                     From               Message</span><br><span class="line">  ----     ------     ----                    ----               -------</span><br><span class="line">  Normal   Scheduled  7m27s                   default-scheduler  Successfully assigned kube-system/metrics-server-6ffc8966f5-84hbb to k8s-slave2</span><br><span class="line">  Normal   Pulling    7m26s                   kubelet            Pulling image &quot;registry.cn-hangzhou.aliyuncs.com/google_containers/metrics-server:v0.6.1&quot;</span><br><span class="line">  Normal   Pulled     7m15s                   kubelet            Successfully pulled image &quot;registry.cn-hangzhou.aliyuncs.com/google_containers/metrics-server:v0.6.1&quot; in 10.976606194s</span><br><span class="line">  Normal   Created    7m15s                   kubelet            Created container metrics-server</span><br><span class="line">  Normal   Started    7m15s                   kubelet            Started container metrics-server</span><br><span class="line">  Warning  Unhealthy  2m17s (x31 over 6m47s)  kubelet            Readiness probe failed: HTTP probe failed with statuscode: 500</span><br><span class="line"></span><br><span class="line">进而查看 pod 的日志：</span><br><span class="line">[root@k8s-master k8s-install]# kubectl logs metrics-server-6ffc8966f5-84hbb -n kube-system </span><br><span class="line">I1010 16:27:46.228594       1 serving.go:342] Generated self-signed cert (/tmp/apiserver.crt, /tmp/apiserver.key)</span><br><span class="line">I1010 16:27:46.633494       1 secure_serving.go:266] Serving securely on [::]:4443</span><br><span class="line">I1010 16:27:46.633585       1 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController</span><br><span class="line">I1010 16:27:46.633616       1 shared_informer.go:240] Waiting for caches to sync for RequestHeaderAuthRequestController</span><br><span class="line">I1010 16:27:46.633653       1 dynamic_serving_content.go:131] &quot;Starting controller&quot; name=&quot;serving-cert::/tmp/apiserver.crt::/tmp/apiserver.key&quot;</span><br><span class="line">I1010 16:27:46.634221       1 tlsconfig.go:240] &quot;Starting DynamicServingCertificateController&quot;</span><br><span class="line">W1010 16:27:46.634296       1 shared_informer.go:372] The sharedIndexInformer has started, run more than once is not allowed</span><br><span class="line">I1010 16:27:46.634365       1 configmap_cafile_content.go:201] &quot;Starting controller&quot; name=&quot;client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file&quot;</span><br><span class="line">I1010 16:27:46.634370       1 shared_informer.go:240] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file</span><br><span class="line">I1010 16:27:46.634409       1 configmap_cafile_content.go:201] &quot;Starting controller&quot; name=&quot;client-ca::kube-system::extension-apiserver-authentication::client-ca-file&quot;</span><br><span class="line">I1010 16:27:46.634415       1 shared_informer.go:240] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file</span><br><span class="line">E1010 16:27:46.641663       1 scraper.go:140] &quot;Failed to scrape node&quot; err=&quot;Get \&quot;https://192.168.100.22:10250/metrics/resource\&quot;: x509: cannot validate certificate for 192.168.100.22 because it doesn&apos;t contain any IP SANs&quot; node=&quot;k8s-slave2&quot;</span><br><span class="line">E1010 16:27:46.645389       1 scraper.go:140] &quot;Failed to scrape node&quot; err=&quot;Get \&quot;https://192.168.100.20:10250/metrics/resource\&quot;: x509: cannot validate certificate for 192.168.100.20 because it doesn&apos;t contain any IP SANs&quot; node=&quot;k8s-master&quot;</span><br><span class="line">E1010 16:27:46.652261       1 scraper.go:140] &quot;Failed to scrape node&quot; err=&quot;Get \&quot;https://192.168.100.21:10250/metrics/resource\&quot;: x509: cannot validate certificate for 192.168.100.21 because it doesn&apos;t contain any IP SANs&quot; node=&quot;k8s-slave1&quot;</span><br><span class="line">I1010 16:27:46.733747       1 shared_informer.go:247] Caches are synced for RequestHeaderAuthRequestController </span><br><span class="line">I1010 16:27:46.735167       1 shared_informer.go:247] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file </span><br><span class="line">I1010 16:27:46.735194       1 shared_informer.go:247] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file </span><br><span class="line">E1010 16:28:01.643646       1 scraper.go:140] &quot;Failed to scrape node&quot; err=&quot;Get \&quot;https://192.168.100.22:10250/metrics/resource\&quot;: x509: cannot validate certificate for 192.168.100.22 because it doesn&apos;t contain any IP SANs&quot; node=&quot;k8s-slave2&quot;</span><br><span class="line">E1010 16:28:01.643805       1 scraper.go:140] &quot;Failed to scrape node&quot; err=&quot;Get \&quot;https://192.168.100.21:10250/metrics/resource\&quot;: x509: cannot validate certificate for 192.168.100.21 because it doesn&apos;t contain any IP SANs&quot; node=&quot;k8s-slave1&quot;</span><br><span class="line">E1010 16:28:01.646721       1 scraper.go:140] &quot;Failed to scrape node&quot; err=&quot;Get \&quot;https://192.168.100.20:10250/metrics/resource\&quot;: x509: cannot validate certificate for 192.168.100.20 because it doesn&apos;t contain any IP SANs&quot; node=&quot;k8s-master&quot;</span><br><span class="line">I1010 16:28:13.397373       1 server.go:187] &quot;Failed probe&quot; probe=&quot;metric-storage-ready&quot; err=&quot;no metrics to serve&quot;</span><br></pre></td></tr></table></figure><p>基本可以确定 pod 异常是因为：Readiness Probe 探针检测到 Metris 容器启动后对 http Get 探针存活没反应，具体原因是：cannot validate certificate for 192.168.100.22 because it doesn’t contain any IP SANs” node=”k8s-slave2”</p><p>查看 metrics-server 的文档（<a href="https://link.zhihu.com/?target=https%3A//link.segmentfault.com/%3Fenc%3DLHp8vLvsXQ8%2Byt1%2FzV9Stg%3D%3D.brVN9B7OHIL7n9%2Fn8A4cIZe2cEyp%2BZMhJKHHxr%2FREeY5Tqsd8Ap7rZUwRhb47K%2Fw1RJybc8%2BCUL06q%2FyPx4BJg%3D%3D" target="_blank" rel="noopener">https://github.com/kubernetes…</a>），有如下一段说明：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Kubelet certificate needs to be signed by cluster Certificate Authority (or disable certificate validation by passing </span><br><span class="line">--kubelet-insecure-tls to Metrics Server)</span><br></pre></td></tr></table></figure><p>意思是：kubelet 证书需要由集群证书颁发机构签名(或者通过向 Metrics Server 传递参数 –kubelet-insecure-tls 来禁用证书验证)。<br>由于是测试环境，我们选择使用参数<code>禁用证书验证，生产环境不推荐这样做！！！</code></p><p>在大概 139 行的位置追加参数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - args:</span><br><span class="line">    - --cert-dir=/tmp</span><br><span class="line">    - --secure-port=4443</span><br><span class="line">    - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname</span><br><span class="line">    - --kubelet-use-node-status-port</span><br><span class="line">    - --metric-resolution=15s</span><br><span class="line">    - --kubelet-insecure-tls</span><br></pre></td></tr></table></figure><p>apply 部署文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master k8s-install]# kubectl apply -f metrics-server-components.yaml</span><br><span class="line">Warning: resource serviceaccounts/metrics-server is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.</span><br><span class="line">serviceaccount/metrics-server configured</span><br><span class="line">Warning: resource clusterroles/system:aggregated-metrics-reader is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.</span><br><span class="line">clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader configured</span><br><span class="line">Warning: resource clusterroles/system:metrics-server is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.</span><br><span class="line">clusterrole.rbac.authorization.k8s.io/system:metrics-server configured</span><br><span class="line">Warning: resource rolebindings/metrics-server-auth-reader is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.</span><br><span class="line">rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader configured</span><br><span class="line">Warning: resource clusterrolebindings/metrics-server:system:auth-delegator is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator configured</span><br><span class="line">Warning: resource clusterrolebindings/system:metrics-server is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server configured</span><br><span class="line">Warning: resource services/metrics-server is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.</span><br><span class="line">service/metrics-server configured</span><br><span class="line">Warning: resource deployments/metrics-server is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.</span><br><span class="line">deployment.apps/metrics-server configured</span><br><span class="line">Warning: resource apiservices/v1beta1.metrics.k8s.io is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.</span><br><span class="line">apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io configured</span><br></pre></td></tr></table></figure><p>metrics pod 已经正常运行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master k8s-install]# kubectl get pod -A | grep metrics</span><br><span class="line">kube-system   metrics-server-fd9598766-8zphn       1/1     Running   0              89s</span><br></pre></td></tr></table></figure><p>再次执行 kubectl top 命令成功：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master k8s-install]# kubectl top pod</span><br><span class="line">NAME                            CPU(cores)   MEMORY(bytes)   </span><br><span class="line">front-end-59bc6df748-699vb      0m           3Mi             </span><br><span class="line">front-end-59bc6df748-r7pkr      0m           3Mi             </span><br><span class="line">kucc4                           1m           2Mi             </span><br><span class="line">legacy-app                      1m           1Mi             </span><br><span class="line">my-demo-nginx-998bbf8f5-9t9pw   0m           0Mi             </span><br><span class="line">my-demo-nginx-998bbf8f5-lfgvw   0m           0Mi             </span><br><span class="line">my-demo-nginx-998bbf8f5-nfn7r   1m           0Mi             </span><br><span class="line">nginx-kusc00401                 0m           3Mi</span><br><span class="line">[root@k8s-master k8s-install]# </span><br><span class="line">[root@k8s-master k8s-install]# kubectl top node</span><br><span class="line">NAME         CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   </span><br><span class="line">k8s-master   232m         5%     1708Mi          46%       </span><br><span class="line">k8s-slave1   29m          1%     594Mi           34%       </span><br><span class="line">k8s-slave2   25m          1%     556Mi           32%</span><br></pre></td></tr></table></figure><h2 id="k8s常用命令集合"><a href="#k8s常用命令集合" class="headerlink" title="k8s常用命令集合"></a>k8s常用命令集合</h2><p>k8s常用命令集合：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看当前集群的所有的节点</span></span><br><span class="line">kubectl get node</span><br><span class="line"><span class="comment"># 显示 Node 的详细信息（一般用不着）</span></span><br><span class="line">kubectl describe node node1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看所有的pod</span></span><br><span class="line">kubectl get pod --all-namespaces</span><br><span class="line"><span class="comment"># 查看pod的详细信息</span></span><br><span class="line">kubectl get pods -o wide --all-namespaces</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看所有创建的服务</span></span><br><span class="line">kubectl get service</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看所有的deploy</span></span><br><span class="line">kubectl get deploy</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重启 pod（这个方式会删除原来的pod，然后再重新生成一个pod达到重启的目的）</span></span><br><span class="line"><span class="comment"># 有yaml文件的重启</span></span><br><span class="line">kubectl replace --force -f xxx.yaml</span><br><span class="line"><span class="comment"># 无yaml文件的重启</span></span><br><span class="line">kubectl get pod &lt;POD_NAME&gt; -n &lt;NAMESPACE&gt; -o yaml | kubectl replace --force -f -</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看pod的详细信息</span></span><br><span class="line">kubectl describe pod nfs-client-provisioner-65c77c7bf9-54rdp -n default</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据 yaml 文件创建Pod资源</span></span><br><span class="line">kubectl apply -f pod.yaml</span><br><span class="line"><span class="comment"># 删除基于 pod.yaml 文件定义的Pod </span></span><br><span class="line">kubectl delete -f pod.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment">#获取 Kubernetes 集群中的 Pod 列表，并将详细结果显示为宽表格</span></span><br><span class="line">kubectl get pod -o wide</span><br><span class="line"><span class="comment">#基于现有的pod导出yaml文件</span></span><br><span class="line">kubectl get pod &lt;pod_name&gt; -o yaml &gt; pod.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment">#根据CPU使用率自动扩展 Pod 的数量</span></span><br><span class="line">kubectl autoscale deployment &lt;deployment_name&gt; --min=&lt;min_replicas&gt; --max=&lt;max_replicas&gt; --cpu-percent=&lt;cpu_percent&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看容器的日志</span></span><br><span class="line">kubectl logs &lt;pod-name&gt;</span><br><span class="line"><span class="comment"># 实时查看日志</span></span><br><span class="line">kubectl logs -f &lt;pod-name&gt;</span><br><span class="line"><span class="comment"># 若 pod 只有一个容器，可以不加 -c</span></span><br><span class="line">kubectl <span class="built_in">log</span>  &lt;pod-name&gt; -c &lt;container_name&gt;</span><br><span class="line"><span class="comment"># 返回所有标记为 app=frontend 的 pod 的合并日志</span></span><br><span class="line">kubectl logs -l app=frontend</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过bash获得 pod 中某个容器的TTY，相当于登录容器</span></span><br><span class="line"><span class="comment"># kubectl exec -it &lt;pod-name&gt; -c &lt;container-name&gt; -- bash</span></span><br><span class="line">eg:</span><br><span class="line">kubectl <span class="built_in">exec</span> -it redis-master-cln81 -- bash</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看 endpoint 列表</span></span><br><span class="line">kubectl get endpoints</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看已有的token</span></span><br><span class="line">kubeadm token list</span><br><span class="line"></span><br><span class="line"><span class="comment">#滚动升级（在yaml文件里修改新的images）</span></span><br><span class="line">kubectl apply -f svc-zipkin.yaml --record</span><br><span class="line">或者直接用命令，更新名为my-deployment的部署中my-container容器的镜像</span><br><span class="line">kubectl <span class="built_in">set</span> image deployment/my-deployment my-container=my-image:my-tag</span><br><span class="line"><span class="comment">#暂停升级</span></span><br><span class="line">kubectl rollout pause deployment zipkin-server</span><br><span class="line"><span class="comment">#继续升级</span></span><br><span class="line">kubectl rollout resume deployment zipkin-server</span><br><span class="line"><span class="comment">#查看升级历史</span></span><br><span class="line">kubectl rollout <span class="built_in">history</span> deployment zipkin-server</span><br><span class="line"><span class="comment">#回滚到上一级</span></span><br><span class="line">kubectl rollout undo deployment zipkin-server</span><br><span class="line"><span class="comment">#回滚制定版本（根据rollout history的查看结果）</span></span><br><span class="line">kubectl rollout undo deployment zipkin-server --to-revision=13</span><br></pre></td></tr></table></figure></div><div><div id="reward-container"><div>Thank you for your accept. mua！</div><button id="reward-button" disable="enable" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">打赏</button><div id="qr" style="display:none"><div style="display:inline-block"><img src="/images/reward/wechatpay.png" alt="丨逸贤丨 微信支付"><p>微信支付</p></div><div style="display:inline-block"><img src="/images/reward/alipay.png" alt="丨逸贤丨 支付宝"><p>支付宝</p></div></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者： </strong>丨逸贤丨</li><li class="post-copyright-link"><strong>本文链接：</strong> <a href="https://yixian12580.github.io/2023/1192b6432e.html" title="K8S集群部署">https://yixian12580.github.io/2023/1192b6432e.html</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><div><div><div style="text-align:center;color:#ccc;font-size:16px">-------------本文结束<i class="fa fa-heart"></i>感谢您的阅读-------------</div></div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/K8S/" rel="tag"><i class="fa fa-tag"></i> K8S</a> <a href="/tags/Kubernetes/" rel="tag"><i class="fa fa-tag"></i> Kubernetes</a></div><div class="post-widgets"><div class="social_share"><div id="needsharebutton-postbottom"><span class="btn"><i class="fa fa-share-alt" aria-hidden="true"></i></span></div></div></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2023/11fbdfb25a.html" rel="next" title="K8S入门"><i class="fa fa-chevron-left"></i> K8S入门</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/2024/02bbbec077.html" rel="prev" title="使用VMware安装wim或esd格式的Win10镜像">使用VMware安装wim或esd格式的Win10镜像 <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article></div><div style="text-align:center"><a href="https://github.com/yixian12580/blog-source/edit/main/source/_posts/K8S集群部署.md" target="_blank">编辑文章✏</a></div></div><div class="comments" id="comments"><div id="lv-container" data-id="city" data-uid="MTAyMC81NzIwNy8zMzY3MQ=="></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">站点概览</li></ul><div class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="丨逸贤丨"><p class="site-author-name" itemprop="name">丨逸贤丨</p><div class="site-description motion-element" itemprop="description">三十年众生牛马，搏十年丰功伟绩。</div></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">149</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">45</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">137</span> <span class="site-state-item-name">标签</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/yixian12580" title="GitHub &rarr; https://github.com/yixian12580" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a href="mailto:1083662409@qq.com" title="E-Mail &rarr; mailto:1083662409@qq.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a> </span><span class="links-of-author-item"><a href="https://weibo.com/u/5107819265" title="Weibo &rarr; https://weibo.com/u/5107819265" rel="noopener" target="_blank"><i class="fa fa-fw fa-weibo"></i>Weibo</a> </span><span class="links-of-author-item"><a href="tencent://message/?uin=1083662409&Site=&menu=yes" title="QQ &rarr; tencent://message/?uin=1083662409&Site=&menu=yes" rel="noopener" target="_blank"><i class="fa fa-fw fa-qq"></i>QQ</a></span></div><div class="links-of-blogroll motion-element links-of-blogroll-inline"><div class="links-of-blogroll-title"><i class="fa fa-fw fa-book"></i> 推荐阅读</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"><a href="http://www.ttlsa.com/" title="http://www.ttlsa.com/" rel="noopener" target="_blank">运维生存时间</a></li><li class="links-of-blogroll-item"><a href="https://www.iyunw.cn" title="https://www.iyunw.cn" rel="noopener" target="_blank">爱运维</a></li><li class="links-of-blogroll-item"><a href="https://nginxconfig.io/" title="https://nginxconfig.io/" rel="noopener" target="_blank">Nginxconfig</a></li><li class="links-of-blogroll-item"><a href="http://linux.51yip.com/" title="http://linux.51yip.com/" rel="noopener" target="_blank">Linux命令手册</a></li><li class="links-of-blogroll-item"><a href="https://echarts.apache.org/index.html" title="https://echarts.apache.org/index.html" rel="noopener" target="_blank">echarts可视化库</a></li><li class="links-of-blogroll-item"><a href="https://yixian12580.netlify.app/admin/" title="https://yixian12580.netlify.app/admin/" rel="noopener" target="_blank">博客管理</a></li></ul></div></div></div><div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#完整的卸载k8s（有需要的话）"><span class="nav-number">1.</span> <span class="nav-text">完整的卸载k8s（有需要的话）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#安装kube集群（4节点）"><span class="nav-number">2.</span> <span class="nav-text">安装kube集群（4节点）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#准备工作（所有的节点都执行）"><span class="nav-number">2.1.</span> <span class="nav-text">准备工作（所有的节点都执行）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#更改kubelet的容器路径（如果需要的话，不需要可以跳过）"><span class="nav-number">2.2.</span> <span class="nav-text">更改kubelet的容器路径（如果需要的话，不需要可以跳过）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#部署Kubernetes集群"><span class="nav-number">3.</span> <span class="nav-text">部署Kubernetes集群</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#覆盖kubernetes的镜像地址（只需要在master节点上操作初始化命令）"><span class="nav-number">3.1.</span> <span class="nav-text">覆盖kubernetes的镜像地址（只需要在master节点上操作初始化命令）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#初始化kubernetes（只需要在master节点上操作初始化命令）"><span class="nav-number">3.2.</span> <span class="nav-text">初始化kubernetes（只需要在master节点上操作初始化命令）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#设定kubeletl网络（主节点部署）"><span class="nav-number">3.3.</span> <span class="nav-text">设定kubeletl网络（主节点部署）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#子节点加入集群（在子节点上操作）"><span class="nav-number">3.4.</span> <span class="nav-text">子节点加入集群（在子节点上操作）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#删除子节点（在master主节点上操作）"><span class="nav-number">3.5.</span> <span class="nav-text">删除子节点（在master主节点上操作）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#部署k8s-dashboard（这里使用Kubepi）"><span class="nav-number">4.</span> <span class="nav-text">部署k8s dashboard（这里使用Kubepi）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#安装metrics-k8s集群监控插件"><span class="nav-number">5.</span> <span class="nav-text">安装metrics k8s集群监控插件</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#下载部署文件"><span class="nav-number">5.1.</span> <span class="nav-text">下载部署文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#修改镜像地址"><span class="nav-number">5.2.</span> <span class="nav-text">修改镜像地址</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#部署-metrics-接口"><span class="nav-number">5.3.</span> <span class="nav-text">部署 metrics 接口</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#k8s常用命令集合"><span class="nav-number">6.</span> <span class="nav-text">k8s常用命令集合</span></a></li></ol></div></div></div><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span id="scrollpercent"><span>0</span>%</span></div></div></aside><link rel="stylesheet" href="/dist/APlayer.min.css"><div id="aplayer"></div><script type="text/javascript" src="/dist/APlayer.min.js"></script><style>.aplayer .aplayer-lrc{height:35px}.aplayer .aplayer-lrc p{font-size:16px;font-weight:700;line-height:19.2px!important}.aplayer .aplayer-lrc p.aplayer-lrc-current{color:#ff1493}.aplayer.aplayer-narrow .aplayer-body{left:-66px!important}</style><div class="aplayer" data-id="7622186670" data-server="netease" data-type="playlist" data-fixed="true" data-order="random" data-autoplay="false"></div><script src="/dist/Meting.min.js"></script></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright" style="text-align:center">&copy; 2022 – <span itemprop="copyrightYear">2026</span> <span class="with-love" id="animate"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">逸贤</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart">站点字数合计:</i> </span><span title="站点总字数">1.1m</span></div><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div class="run_time" style="text-align:center"><span id="timeDate">载入天数...</span><span id="times">载入时分秒...</span><script>var now=new Date;function createtime(){var n=new Date("09/08/2022 10:00:00");now.setTime(now.getTime()+250),days=(now-n)/1e3/60/60/24,dnum=Math.floor(days),hours=(now-n)/1e3/60/60-24*dnum,hnum=Math.floor(hours),1==String(hnum).length&&(hnum="0"+hnum),minutes=(now-n)/1e3/60-1440*dnum-60*hnum,mnum=Math.floor(minutes),1==String(mnum).length&&(mnum="0"+mnum),seconds=(now-n)/1e3-86400*dnum-3600*hnum-60*mnum,snum=Math.round(seconds),1==String(snum).length&&(snum="0"+snum),document.getElementById("timeDate").innerHTML="本站已安全运行 "+dnum+" 天 ",document.getElementById("times").innerHTML=hnum+" 小时 "+mnum+" 分 "+snum+" 秒"}setInterval("createtime()",250)</script></div><div class="busuanzi-count" style="text-align:center"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item-icon"><i class="fa fa-user"></i> </span><span class="site-uv" title="总访客量"><span class="busuanzi-value" id="busuanzi_value_site_uv"></span> 人 </span><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="site-pv" title="总访问量"><span class="busuanzi-value" id="busuanzi_value_site_pv"></span> 次</span></div></div></footer></div><script>"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script src="/lib/jquery/index.js?v=2.1.3"></script><script src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script src="/lib/fancybox/source/jquery.fancybox.pack.js"></script><script src="/js/utils.js?v=7.1.1"></script><script src="/js/motion.js?v=7.1.1"></script><script src="/js/schemes/muse.js?v=7.1.1"></script><script src="/js/scrollspy.js?v=7.1.1"></script><script src="/js/post-details.js?v=7.1.1"></script><script src="/js/next-boot.js?v=7.1.1"></script><script>window.livereOptions={refer:"2023/1192b6432e.html"},function(e,t){var n,r=e.getElementsByTagName(t)[0];"function"!=typeof LivereTower&&((n=e.createElement(t)).src="https://cdn-city.livere.com/js/embed.dist.js",n.async=!0,r.parentNode.insertBefore(n,r))}(document,"script")</script><script>// Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url).replace(/\/{2,}/g, '/');
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x"></i></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x"></i></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });</script><script src="/lib/needsharebutton/needsharebutton.js"></script><script>pbOptions={iconStyle:"box",boxForm:"horizontal",position:"bottomCenter",networks:"Weibo,Wechat,Douban,QQZone,Twitter,Facebook"},new needShareButton("#needsharebutton-postbottom",pbOptions)</script><script src="/lib/bookmark/bookmark.min.js?v=1.0"></script><script>bookmark.scrollToMark("auto","#更多")</script><script>$(".highlight").not(".gist .highlight").each(function(t,e){var n=$("<div>").addClass("highlight-wrap");$(e).after(n),n.append($("<button>").addClass("copy-btn").append("复制").on("click",function(t){var e=$(this).parent().find(".code").find(".line").map(function(t,e){return $(e).text()}).toArray().join("\n"),n=document.createElement("textarea"),o=window.pageYOffset||document.documentElement.scrollTop;n.style.top=o+"px",n.style.position="absolute",n.style.opacity="0",n.readOnly=!0,n.value=e,document.body.appendChild(n),n.select(),n.setSelectionRange(0,e.length),n.readOnly=!1,document.execCommand("copy")?$(this).text("复制成功"):$(this).text("复制失败"),n.blur(),$(this).blur()})).on("mouseleave",function(t){var e=$(this).find(".copy-btn");setTimeout(function(){e.text("复制")},300)}).append(e)})</script></body></html><script type="text/javascript" src="/js/src/clicklove.js"></script><script type="text/javascript" src="/js/click_show_text.js"></script><script type="text/javascript" src="/js/FunnyTitle.js"></script><script src="/live2d-widget/autoload.js"></script>